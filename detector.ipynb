{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 32 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "## import necessary package\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "# from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "## Config\n",
    "tqdm.pandas(desc='pandas bar')\n",
    "from pandarallel import pandarallel\n",
    "import argparse\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=False)\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import cross_val_predict, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import ExtraTreesClassifier,AdaBoostClassifier\n",
    "import ast\n",
    "import re\n",
    "# from cleanlab.filter import find_label_issues\n",
    "# from cleanlab.classification import CleanLearning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the sampled Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([532, 157, 213, 228, 238, 256, 332, 386, 411, 493, 572, 698, 956,\n",
       "        22,  24,  42,  56,  57,  93,  94])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# beer_index = np.load('/home/yanmy/raha/raha-master/datasets/beers/detector/index.npy')\n",
    "# flights_index = np.load('/home/yanmy/raha/raha-master/datasets/flights/detector/index.npy')\n",
    "# tax_index = np.load('/home/yanmy/raha/raha-master/datasets/tax/detector/index.npy')\n",
    "# rayyan_index = np.load('/home/yanmy/raha/raha-master/datasets/rayyan/detector/index.npy')\n",
    "# imdb_index = np.load('/home/yanmy/raha/raha-master/datasets/imdb/label.npy')\n",
    "hospital_index = np.load('/home/yanmy/raha/raha-master/datasets/hospital/detector/index.npy')\n",
    "hospital_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the sampled Index for Error Detector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hospital_detector_train = pd.read_csv('/home/yanmy/raha/raha-master/datasets/hospital/detector/train.csv',index_col=0)\n",
    "# hospital_detector_test = pd.read_csv('/home/yanmy/raha/raha-master/datasets/hospital/detector/test_cell.csv',index_col=0)\n",
    "# hospital_detector_test\n",
    "# hospital_detector_index = np.load('/home/yanmy/raha/raha-master/datasets/hospital/detector/detection.npy')\n",
    "# hospital_detector_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beers_Detector_File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pre-Process data for Beers\n",
    "beer_clean = pd.read_csv('datasets/beers/clean.csv').fillna('')\n",
    "beer_dirty = pd.read_csv('datasets/beers/dirty.csv').fillna('')\n",
    "beer_dirty.columns = beer_clean.columns\n",
    "def try_convert_to_int(row):\n",
    "    for x,y in row.items():\n",
    "        if(x in ['ounces','ibu']):\n",
    "            try:\n",
    "                row[x] = int(y)\n",
    "            except:\n",
    "                row[x] = y\n",
    "    return row\n",
    "beer_clean = beer_clean.apply(try_convert_to_int,axis=1).astype(str)\n",
    "beer_dirty = beer_dirty.apply(try_convert_to_int,axis=1).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# beer_detector_train = pd.read_csv('/home/yanmy/raha/raha-master/datasets/beers/detector/multi-view/train_aug.csv',index_col=0)\n",
    "# beer_detector_test = pd.read_csv('/home/yanmy/raha/raha-master/datasets/beers/detector/multi-view/test.csv',index_col=0)\n",
    "# beer_detector_test\n",
    "beer_detector_index = np.load('/home/yanmy/raha/raha-master/datasets/beers/detector/multi-view/detection_cell_5_refine.npy')\n",
    "beer_detector_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flights Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_detector_train = pd.read_csv('/home/yanmy/raha/raha-master/datasets/flights/detector/multi-view/train.csv',index_col=0)\n",
    "flights_detector_test = pd.read_csv('/home/yanmy/raha/raha-master/datasets/flights/detector/multi-view/test.csv',index_col=0)\n",
    "flights_detector_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tax Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16511/4042012506.py:3: DtypeWarning: Columns (12,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tax_dirty = pd.read_csv('datasets/tax/dirty.csv').fillna('').astype(str)\n"
     ]
    }
   ],
   "source": [
    "### Tax Data\n",
    "tax_clean = pd.read_csv('datasets/tax/clean.csv').fillna('').astype(str)\n",
    "tax_dirty = pd.read_csv('datasets/tax/dirty.csv').fillna('').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tax_detector_train = pd.read_csv('/home/yanmy/raha/raha-master/datasets/tax/detector/multi-view/train_aug.csv',index_col=0)\n",
    "tax_detector_test = pd.read_csv('/home/yanmy/raha/raha-master/datasets/tax/detector/test.csv',index_col=0)\n",
    "tax_detector_test\n",
    "tax_detector_index = np.load('/home/yanmy/raha/raha-master/datasets/tax/detector/detection_cell.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2929, 15)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_detector_index.reshape((-1,15)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_detector_all = np.where(np.sum(tax_clean!=tax_dirty,axis=1)!=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_detector_index_process = np.zeros((200000,15))\n",
    "tax_detector_index = tax_detector_index.reshape((-1,15))\n",
    "for i in range(len(tax_detector_all)):\n",
    "    index = tax_detector_all[i] ## True Index in full tax_table\n",
    "    tax_detector_index_process[index] = tax_detector_index[i]\n",
    "tax_detector_index_process = tax_detector_index_process.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB_File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_clean = pd.read_csv('datasets/imdb/clean.csv').fillna('')\n",
    "imdb_dirty = pd.read_csv('datasets/imdb/dirty.csv').fillna('')\n",
    "def Str2Int(row):\n",
    "    for index in range(6):\n",
    "        temp = row[index]\n",
    "        try:\n",
    "            row[index] = str(int(temp))\n",
    "        except:\n",
    "            continue\n",
    "    return row\n",
    "imdb_clean = imdb_clean.parallel_apply(Str2Int,axis=1)\n",
    "imdb_dirty = imdb_dirty.parallel_apply(Str2Int,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_detector_index_all = np.where(np.sum(imdb_clean!=imdb_dirty,axis=1)!=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    48,     52,     67, ..., 999956, 999987, 999994])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_detector_index_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imdb_detector_train = pd.read_csv('/home/yanmy/raha/raha-master/datasets/imdb/detector/train.csv',index_col=0)\n",
    "# imdb_detector_test = pd.read_csv('/home/yanmy/raha/raha-master/datasets/imdb/detector/test_all.csv',index_col=0)\n",
    "imdb_detector_index = np.load('/home/yanmy/raha/raha-master/datasets/imdb/detector/detector_final.npy')\n",
    "imdb_detector_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_detector_test = pd.read_csv('/home/yanmy/raha/raha-master/datasets/imdb/detector/test.csv',index_col=0)\n",
    "imdb_detector_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_detector_index_process = np.zeros((1000000,6))\n",
    "# imdb_detector_index = imdb_detector_index.reshape((-1,6))\n",
    "for i in range(len(imdb_detector_index_all)):\n",
    "    index = imdb_detector_index_all[i] ## True Index in full tax_table\n",
    "    imdb_detector_index_process[index] = imdb_detector_index[i]\n",
    "imdb_detector_index_process = imdb_detector_index_process.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rayyan_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "rayyan_clean = pd.read_csv('datasets/rayyan/clean.csv').fillna('')\n",
    "rayyan_dirty = pd.read_csv('datasets/rayyan/dirty.csv').fillna('')\n",
    "def Str2Int(row):\n",
    "    for index in range(11):\n",
    "        temp = row[index]\n",
    "        try:\n",
    "            row[index] = str(int(temp))\n",
    "        except:\n",
    "            continue\n",
    "    return row\n",
    "rayyan_clean = rayyan_clean.apply(Str2Int,axis=1)\n",
    "rayyan_dirty = rayyan_dirty.apply(Str2Int,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>article_title</th>\n",
       "      <th>article_language</th>\n",
       "      <th>journal_title</th>\n",
       "      <th>jounral_abbreviation</th>\n",
       "      <th>journal_issn</th>\n",
       "      <th>article_jvolumn</th>\n",
       "      <th>article_jissue</th>\n",
       "      <th>article_jcreated_at</th>\n",
       "      <th>article_pagination</th>\n",
       "      <th>author_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>235295</td>\n",
       "      <td>Late repair of injuries of the anal sphincter</td>\n",
       "      <td>eng</td>\n",
       "      <td>Proc R Soc Med</td>\n",
       "      <td>Proceedings of the Royal Society of Medicine</td>\n",
       "      <td>0035-9157 (Print) 0035-9157</td>\n",
       "      <td>64</td>\n",
       "      <td>12</td>\n",
       "      <td>1/1/71</td>\n",
       "      <td>1187-9</td>\n",
       "      <td>{\"A. G. Parks\",\"J. F. McPartlin\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>498345</td>\n",
       "      <td>Ebola Virus GP Gene Polyadenylation Versus RNA...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>The Journal of infectious diseases</td>\n",
       "      <td>J. Infect. Dis.</td>\n",
       "      <td>1537-6613</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4/2/15</td>\n",
       "      <td></td>\n",
       "      <td>{\"Valentina A Volchkova\",\"Jaroslav Vorac\",\"Phi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>789958</td>\n",
       "      <td>Duane retraction syndrome associated with ocul...</td>\n",
       "      <td>eng</td>\n",
       "      <td>Indian journal of ophthalmology</td>\n",
       "      <td>Indian J Ophthalmol</td>\n",
       "      <td>0301-4738</td>\n",
       "      <td>54</td>\n",
       "      <td>4</td>\n",
       "      <td>12/1/06</td>\n",
       "      <td>283-4</td>\n",
       "      <td>{\"Jitendra Jethani\",\"Shashikant Shetty\",\"Suche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>169865</td>\n",
       "      <td>[Noninvasive prenatal diagnosis of trisomy 21,...</td>\n",
       "      <td>pol</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>1/1/13</td>\n",
       "      <td>714-9 ST  - [Noninvasive prenatal diagnosis of...</td>\n",
       "      <td>{\"G. Jakiel\",\"K. Gorzelnik\",\"J. G. Zimowski\",\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>803653</td>\n",
       "      <td>Diagnosis and Management of Cutaneous B-cell L...</td>\n",
       "      <td>eng</td>\n",
       "      <td>Dermatologic clinics</td>\n",
       "      <td>Dermatol Clin</td>\n",
       "      <td>1558-0520</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>10/1/15</td>\n",
       "      <td>835-40</td>\n",
       "      <td>{\"Lauren C Pinter-Brown\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>979472</td>\n",
       "      <td>Distribution of silicotic collagenization in r...</td>\n",
       "      <td>eng</td>\n",
       "      <td>Am Rev Respir Dis</td>\n",
       "      <td>The American review of respiratory disease</td>\n",
       "      <td>0003-0805 (Print) 0003-0805</td>\n",
       "      <td>144</td>\n",
       "      <td>2</td>\n",
       "      <td>1/1/91</td>\n",
       "      <td>297-301</td>\n",
       "      <td>{\"S. L. Lee\",\"G. K. Sluis-Cremer\",\"P. A. Hessel\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>814758</td>\n",
       "      <td>Mega-ampere submicrosecond generator GIT-32</td>\n",
       "      <td></td>\n",
       "      <td>Review of Scientific Instruments</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>78</td>\n",
       "      <td>3</td>\n",
       "      <td>1/1/07</td>\n",
       "      <td>33501</td>\n",
       "      <td>{\"E. V. Kumpyak\",\"V. N. Kiselev\",\"A. V. Kharlo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>226417</td>\n",
       "      <td>Correct Performance of Pelvic Muscle Exercises...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>Female pelvic medicine &amp; reconstructive surgery</td>\n",
       "      <td>Female Pelvic Med Reconstr Surg</td>\n",
       "      <td>2154-4212</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>10/27/14</td>\n",
       "      <td></td>\n",
       "      <td>{\"Katharine O'Dell\",\"Padma Kandadai\",\"Jyot Sai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>968815</td>\n",
       "      <td>Long-term survival after \"liver first\" approac...</td>\n",
       "      <td>English</td>\n",
       "      <td>International Journal of Colorectal Disease</td>\n",
       "      <td></td>\n",
       "      <td>-4473</td>\n",
       "      <td>26</td>\n",
       "      <td>9</td>\n",
       "      <td>1/1/11</td>\n",
       "      <td>1219-1220</td>\n",
       "      <td>{\"M. Heuer\",\"S. Radunz\",\"A. Paul\",\"G. C. Sotir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>734259</td>\n",
       "      <td>Pharmacoeconomic Evaluation of Tiotropium Brom...</td>\n",
       "      <td></td>\n",
       "      <td>European Journal of Health Economics</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1/1/12</td>\n",
       "      <td>71-80</td>\n",
       "      <td>{\"S. Iannazzo\",\"M. Miravitlles\",\"L. Pradelli\",...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                      article_title  \\\n",
       "0    235295      Late repair of injuries of the anal sphincter   \n",
       "1    498345  Ebola Virus GP Gene Polyadenylation Versus RNA...   \n",
       "2    789958  Duane retraction syndrome associated with ocul...   \n",
       "3    169865  [Noninvasive prenatal diagnosis of trisomy 21,...   \n",
       "4    803653  Diagnosis and Management of Cutaneous B-cell L...   \n",
       "..      ...                                                ...   \n",
       "995  979472  Distribution of silicotic collagenization in r...   \n",
       "996  814758        Mega-ampere submicrosecond generator GIT-32   \n",
       "997  226417  Correct Performance of Pelvic Muscle Exercises...   \n",
       "998  968815  Long-term survival after \"liver first\" approac...   \n",
       "999  734259  Pharmacoeconomic Evaluation of Tiotropium Brom...   \n",
       "\n",
       "    article_language                                    journal_title  \\\n",
       "0                eng                                   Proc R Soc Med   \n",
       "1                ENG               The Journal of infectious diseases   \n",
       "2                eng                  Indian journal of ophthalmology   \n",
       "3                pol                                                    \n",
       "4                eng                             Dermatologic clinics   \n",
       "..               ...                                              ...   \n",
       "995              eng                                Am Rev Respir Dis   \n",
       "996                                  Review of Scientific Instruments   \n",
       "997              ENG  Female pelvic medicine & reconstructive surgery   \n",
       "998          English      International Journal of Colorectal Disease   \n",
       "999                              European Journal of Health Economics   \n",
       "\n",
       "                             jounral_abbreviation  \\\n",
       "0    Proceedings of the Royal Society of Medicine   \n",
       "1                                 J. Infect. Dis.   \n",
       "2                             Indian J Ophthalmol   \n",
       "3                                                   \n",
       "4                                   Dermatol Clin   \n",
       "..                                            ...   \n",
       "995    The American review of respiratory disease   \n",
       "996                                                 \n",
       "997               Female Pelvic Med Reconstr Surg   \n",
       "998                                                 \n",
       "999                                                 \n",
       "\n",
       "                    journal_issn article_jvolumn article_jissue  \\\n",
       "0    0035-9157 (Print) 0035-9157              64             12   \n",
       "1                      1537-6613                                  \n",
       "2                      0301-4738              54              4   \n",
       "3                                             84              0   \n",
       "4                      1558-0520              33              4   \n",
       "..                           ...             ...            ...   \n",
       "995  0003-0805 (Print) 0003-0805             144              2   \n",
       "996                                           78              3   \n",
       "997                    2154-4212                                  \n",
       "998                        -4473              26              9   \n",
       "999                                           13              1   \n",
       "\n",
       "    article_jcreated_at                                 article_pagination  \\\n",
       "0                1/1/71                                             1187-9   \n",
       "1                4/2/15                                                      \n",
       "2               12/1/06                                              283-4   \n",
       "3                1/1/13  714-9 ST  - [Noninvasive prenatal diagnosis of...   \n",
       "4               10/1/15                                             835-40   \n",
       "..                  ...                                                ...   \n",
       "995              1/1/91                                            297-301   \n",
       "996              1/1/07                                              33501   \n",
       "997            10/27/14                                                      \n",
       "998              1/1/11                                          1219-1220   \n",
       "999              1/1/12                                              71-80   \n",
       "\n",
       "                                           author_list  \n",
       "0                    {\"A. G. Parks\",\"J. F. McPartlin\"}  \n",
       "1    {\"Valentina A Volchkova\",\"Jaroslav Vorac\",\"Phi...  \n",
       "2    {\"Jitendra Jethani\",\"Shashikant Shetty\",\"Suche...  \n",
       "3    {\"G. Jakiel\",\"K. Gorzelnik\",\"J. G. Zimowski\",\"...  \n",
       "4                            {\"Lauren C Pinter-Brown\"}  \n",
       "..                                                 ...  \n",
       "995  {\"S. L. Lee\",\"G. K. Sluis-Cremer\",\"P. A. Hessel\"}  \n",
       "996  {\"E. V. Kumpyak\",\"V. N. Kiselev\",\"A. V. Kharlo...  \n",
       "997  {\"Katharine O'Dell\",\"Padma Kandadai\",\"Jyot Sai...  \n",
       "998  {\"M. Heuer\",\"S. Radunz\",\"A. Paul\",\"G. C. Sotir...  \n",
       "999  {\"S. Iannazzo\",\"M. Miravitlles\",\"L. Pradelli\",...  \n",
       "\n",
       "[1000 rows x 11 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_dirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COL article_title VAL Late repair of injuries ...</td>\n",
       "      <td>COL article_title VAL Late repair of injuries ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COL article_title VAL Late repair of injuries ...</td>\n",
       "      <td>COL article_language VAL eng</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COL article_title VAL Late repair of injuries ...</td>\n",
       "      <td>COL journal_title VAL Proc R Soc Med</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COL article_title VAL Late repair of injuries ...</td>\n",
       "      <td>COL jounral_abbreviation VAL Proceedings of th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COL article_title VAL Late repair of injuries ...</td>\n",
       "      <td>COL journal_issn VAL 0035-9157 (Print) 0035-9157</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>COL article_title VAL Pharmacoeconomic Evaluat...</td>\n",
       "      <td>COL article_jvolumn VAL 13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>COL article_title VAL Pharmacoeconomic Evaluat...</td>\n",
       "      <td>COL article_jissue VAL 1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>COL article_title VAL Pharmacoeconomic Evaluat...</td>\n",
       "      <td>COL article_jcreated_at VAL 1/1/12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>COL article_title VAL Pharmacoeconomic Evaluat...</td>\n",
       "      <td>COL article_pagination VAL 71-80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>COL article_title VAL Pharmacoeconomic Evaluat...</td>\n",
       "      <td>COL author_list VAL {\"S. Iannazzo\",\"M. Miravit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0  \\\n",
       "0     COL article_title VAL Late repair of injuries ...   \n",
       "1     COL article_title VAL Late repair of injuries ...   \n",
       "2     COL article_title VAL Late repair of injuries ...   \n",
       "3     COL article_title VAL Late repair of injuries ...   \n",
       "4     COL article_title VAL Late repair of injuries ...   \n",
       "...                                                 ...   \n",
       "9995  COL article_title VAL Pharmacoeconomic Evaluat...   \n",
       "9996  COL article_title VAL Pharmacoeconomic Evaluat...   \n",
       "9997  COL article_title VAL Pharmacoeconomic Evaluat...   \n",
       "9998  COL article_title VAL Pharmacoeconomic Evaluat...   \n",
       "9999  COL article_title VAL Pharmacoeconomic Evaluat...   \n",
       "\n",
       "                                                      1  2  \n",
       "0     COL article_title VAL Late repair of injuries ...  0  \n",
       "1                         COL article_language VAL eng   0  \n",
       "2                 COL journal_title VAL Proc R Soc Med   0  \n",
       "3     COL jounral_abbreviation VAL Proceedings of th...  0  \n",
       "4     COL journal_issn VAL 0035-9157 (Print) 0035-9157   0  \n",
       "...                                                 ... ..  \n",
       "9995                        COL article_jvolumn VAL 13   0  \n",
       "9996                          COL article_jissue VAL 1   0  \n",
       "9997                COL article_jcreated_at VAL 1/1/12   1  \n",
       "9998                  COL article_pagination VAL 71-80   0  \n",
       "9999  COL author_list VAL {\"S. Iannazzo\",\"M. Miravit...  0  \n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_detector_train = pd.read_csv('/home/yanmy/raha/raha-master/datasets/rayyan/detector/multi-view/test.csv',index_col=0)\n",
    "rayyan_detector_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_detector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1117"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_detector = np.load('datasets/rayyan/detector/multi-view/detector_20.npy').reshape((1000,10))\n",
    "rayyan_detector.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('GEIL_Data/rayyan/detector/detector.npy',rayyan_detector.reshape((1000,10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9989584883201905"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_true = beer_detector_index.flatten(),y_pred=np.array(beer_clean.iloc[:,2:]!=beer_dirty.iloc[:,2:]).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_clean = pd.read_csv('datasets/hospital/clean.csv').astype(str)\n",
    "hospital_dirty = pd.read_csv('datasets/hospital/dirty.csv').astype(str)\n",
    "hospital_query = pd.read_csv('datasets/hospital/dirty_query.csv')\n",
    "hospital_dirty.columns = hospital_clean.columns\n",
    "hospital_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "509"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix = np.array(hospital_clean!=hospital_dirty)\n",
    "input_matrix.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所选行的索引: [532, 157, 213, 228, 238, 256, 332, 386, 411, 493, 572, 698, 956, 22, 24, 42, 56, 57, 93, 94]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def find_max_coverage(matrix):\n",
    "    # 转置矩阵以便按列计算列和\n",
    "    transposed_matrix = np.transpose(matrix)\n",
    "    \n",
    "    # 初始化一个列表，用于记录每列的和以及列的索引\n",
    "    column_sums = [(sum(column), index) for index, column in enumerate(transposed_matrix)]\n",
    "    \n",
    "    # 按列和降序排序\n",
    "    column_sums.sort(reverse=True)\n",
    "    \n",
    "    selected_rows = []\n",
    "    selected_columns = set()\n",
    "    \n",
    "    for _, column_index in column_sums:\n",
    "        # 如果所选列已经包含了这一列，跳过\n",
    "        if column_index in selected_columns:\n",
    "            continue\n",
    "        \n",
    "        # 找到可以添加的行\n",
    "        best_row = None\n",
    "        best_row_sum = -1\n",
    "        \n",
    "        for row_index, row in enumerate(matrix):\n",
    "            if row_index in selected_rows:\n",
    "                continue\n",
    "            \n",
    "            # 计算将此行添加到已选行中后的行之和\n",
    "            new_row_sum = sum(row)\n",
    "            \n",
    "            if new_row_sum > best_row_sum:\n",
    "                best_row_sum = new_row_sum\n",
    "                best_row = row_index\n",
    "        \n",
    "        # 如果找到了合适的行，添加它\n",
    "        if best_row is not None:\n",
    "            selected_rows.append(best_row)\n",
    "            selected_columns.update([column_index])\n",
    "            \n",
    "            # 如果已经选择的行数超过20，停止\n",
    "            if len(selected_rows) >= 20:\n",
    "                break\n",
    "    \n",
    "    return selected_rows\n",
    "\n",
    "# 示例用法\n",
    "if __name__ == \"__main__\":\n",
    "    matrix = np.random.randint(2, size=(1000, 20))  # 随机生成一个1000x20的二进制矩阵\n",
    "    selected_rows = find_max_coverage(input_matrix)\n",
    "    print(\"所选行的索引:\", selected_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix[selected_rows].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 4, 0, 0, 3, 4, 3, 2, 5, 5, 7, 3, 4, 0, 4, 4, 1, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix_select.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 找到选中的20个tuple\n",
    "input_matrix_select = input_matrix[selected_rows]\n",
    "detector_list = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "for label_tuple in selected_rows:\n",
    "    for i in range(len(hospital_clean.columns)):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = hospital_clean.iloc[label_tuple]\n",
    "        dirty_context = hospital_dirty.iloc[label_tuple]\n",
    "        clean_cell = hospital_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = hospital_dirty.iloc[label_tuple,i]\n",
    "        single_context_clean = 'COL %s VAL %s ' % (hospital_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (hospital_clean.columns[i],dirty_cell)\n",
    "        for c in range(20):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (hospital_clean.columns[c],hospital_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (hospital_dirty.columns[c],hospital_dirty.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            detector_list.append([all_context_dirty,single_context_dirty,1])\n",
    "            detector_list.append([all_context_clean,single_context_clean,0])\n",
    "            detector_list.append([all_context_dirty,single_context_clean,0])\n",
    "            # detector_list.append([single_context_dirty,1])\n",
    "        else:\n",
    "            detector_list.append([all_context_dirty,single_context_dirty,0])\n",
    "            detector_list.append([all_context_clean,single_context_clean,0])\n",
    "            detector_list.append([all_context_dirty,single_context_clean,0])\n",
    "        # else:\n",
    "        #     detector_list.append([all_context_dirty,1])\n",
    "        #     detector_list.append([single_context_dirty,1])\n",
    "            \n",
    "                # break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:15<00:00, 65.06it/s]\n"
     ]
    }
   ],
   "source": [
    "input_matrix_select = input_matrix[selected_rows]\n",
    "detector_list_all = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "# for label_tuple in tqdm(selected_rows):\n",
    "for label_tuple in tqdm(range(len(hospital_clean))):\n",
    "    for i in range(len(hospital_clean.columns)):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = hospital_clean.iloc[label_tuple]\n",
    "        dirty_context = hospital_dirty.iloc[label_tuple]\n",
    "        clean_cell = hospital_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = hospital_dirty.iloc[label_tuple,i]\n",
    "        single_context_clean = 'COL %s VAL %s ' % (hospital_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (hospital_clean.columns[i],dirty_cell)\n",
    "        for c in range(20):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (hospital_clean.columns[c],hospital_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (hospital_dirty.columns[c],hospital_dirty.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            detector_list_all.append([all_context_dirty,single_context_dirty,1])\n",
    "        else:\n",
    "            detector_list_all.append([all_context_dirty,single_context_dirty,0])\n",
    "            # detector_list.append([single_context_dirty,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COL index VAL 533 COL ProviderNumber VAL 10023...</td>\n",
       "      <td>COL index VAL 533</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COL index VAL 533 COL ProviderNumber VAL 10023...</td>\n",
       "      <td>COL index VAL 533</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COL index VAL 533 COL ProviderNumber VAL 10023...</td>\n",
       "      <td>COL ProviderNumber VAL 10023</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COL index VAL 533 COL ProviderNumber VAL 10023...</td>\n",
       "      <td>COL ProviderNumber VAL 10023</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COL index VAL 533 COL ProviderNumber VAL 10023...</td>\n",
       "      <td>COL HospitalName VAL baptist medical cexter so...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1192</th>\n",
       "      <td>COL index VAL 95 COL ProviderNumber VAL 10007 ...</td>\n",
       "      <td>COL Score VAL 60%</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>COL index VAL 95 COL ProviderNumber VAL 10007 ...</td>\n",
       "      <td>COL Sample VAL 5 patients</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>COL index VAL 95 COL ProviderNumber VAL 10007 ...</td>\n",
       "      <td>COL Sample VAL 5 patients</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>COL index VAL 95 COL ProviderNumber VAL 10007 ...</td>\n",
       "      <td>COL Stateavg VAL al_ami-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>COL index VAL 95 COL ProviderNumber VAL 10007 ...</td>\n",
       "      <td>COL Stateavg VAL al_ami-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>854 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0  \\\n",
       "0     COL index VAL 533 COL ProviderNumber VAL 10023...   \n",
       "1     COL index VAL 533 COL ProviderNumber VAL 10023...   \n",
       "3     COL index VAL 533 COL ProviderNumber VAL 10023...   \n",
       "4     COL index VAL 533 COL ProviderNumber VAL 10023...   \n",
       "6     COL index VAL 533 COL ProviderNumber VAL 10023...   \n",
       "...                                                 ...   \n",
       "1192  COL index VAL 95 COL ProviderNumber VAL 10007 ...   \n",
       "1194  COL index VAL 95 COL ProviderNumber VAL 10007 ...   \n",
       "1195  COL index VAL 95 COL ProviderNumber VAL 10007 ...   \n",
       "1197  COL index VAL 95 COL ProviderNumber VAL 10007 ...   \n",
       "1198  COL index VAL 95 COL ProviderNumber VAL 10007 ...   \n",
       "\n",
       "                                                      1  2  \n",
       "0                                    COL index VAL 533   0  \n",
       "1                                    COL index VAL 533   0  \n",
       "3                         COL ProviderNumber VAL 10023   0  \n",
       "4                         COL ProviderNumber VAL 10023   0  \n",
       "6     COL HospitalName VAL baptist medical cexter so...  1  \n",
       "...                                                 ... ..  \n",
       "1192                                 COL Score VAL 60%   0  \n",
       "1194                         COL Sample VAL 5 patients   0  \n",
       "1195                         COL Sample VAL 5 patients   0  \n",
       "1197                         COL Stateavg VAL al_ami-1   0  \n",
       "1198                         COL Stateavg VAL al_ami-1   0  \n",
       "\n",
       "[854 rows x 3 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(detector_list).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list).drop_duplicates().to_csv('datasets/hospital/detector/test_cell.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(detector_list)[1].value_counts()\n",
    "# pd.DataFrame(detector_list).to_csv('datasets/hospital/detector/train.csv')\n",
    "dirty_cell_indice = np.argwhere(input_matrix==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dirty_cell_indice:\n",
    "    clean_cell = hospital_clean.iloc[d[0],d[1]]\n",
    "    dirty_cell = hospital_dirty.iloc[d[0],d[1]]\n",
    "    dirty_value = hospital_dirty.iloc[:,d[1]].unique()\n",
    "    if(clean_cell not in dirty_value):\n",
    "        print(d)\n",
    "        print(clean_cell)\n",
    "        print(dirty_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = np.load('datasets/hospital/detector/detection.npy')\n",
    "detector = detector.reshape((1000,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_cell_indice = np.argwhere(detector!=input_matrix)\n",
    "for d in false_cell_indice:\n",
    "    clean_cell = hospital_clean.iloc[d[0],d[1]]\n",
    "    dirty_cell = hospital_dirty.iloc[d[0],d[1]]\n",
    "    # dirty_value = hospital_dirty.iloc[:,d[1]].unique()\n",
    "    # if(clean_cell not in dirty_value):\n",
    "    print(d,input_matrix[d[0],d[1]],detector[d[0],d[1]])\n",
    "    print(clean_cell)\n",
    "    print(dirty_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util\n",
    "def generate_similarity_dict(similarity_matrix):\n",
    "    similarity_dict = {}\n",
    "    num_elements = similarity_matrix.shape[0]\n",
    "\n",
    "    for i in range(num_elements):\n",
    "        # Get similarity scores for element i with all other elements\n",
    "        similarity_scores = similarity_matrix[i, :]\n",
    "        \n",
    "        # Get indices of top 50 similar elements (excluding self)\n",
    "        top_indices = np.argsort(similarity_scores)[::-1]\n",
    "        \n",
    "        # Add the list of top 50 similar elements to the dictionary\n",
    "        similarity_dict[i] = top_indices.tolist()\n",
    "\n",
    "    return similarity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/yanmy/anaconda3/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda112_nocublaslt.so\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.2/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.0\n",
      "CUDA SETUP: Detected CUDA version 112\n",
      "CUDA SETUP: Loading binary /home/yanmy/anaconda3/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda112_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanmy/anaconda3/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /home/yanmy/anaconda3 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/yanmy/anaconda3/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-15 00:03:32,958] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer,util,LoggingHandler, losses, InputExample\n",
    "model = SentenceTransformer('/home/yanmy/sentence_transformer_model/bge-large-en-1.5/').to('cuda:3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util\n",
    "sim_dict = {}\n",
    "value_dict = {}\n",
    "for i in range(20):\n",
    "    value_list = list(hospital_dirty.iloc[:,i].unique())\n",
    "    value_dict[i] = value_list\n",
    "    embedding_A = model.encode(value_list,show_progress_bar=True)\n",
    "    sim_A_B = util.cos_sim(embedding_A, embedding_A).numpy()\n",
    "    sim_A_B_dict = generate_similarity_dict(sim_A_B) \n",
    "    sim_dict[i] = sim_A_B_dict   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['36067',\n",
       " '3x0x7',\n",
       " '35653',\n",
       " '35150',\n",
       " 'x5150',\n",
       " '99508',\n",
       " '99559',\n",
       " '36360',\n",
       " 'x6x60',\n",
       " '35960']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_and_sort_strings(input_list, target_element):\n",
    "    # 如果目标元素长度小于等于5，则无需相似度限制\n",
    "    if len(target_element) <= 5:\n",
    "        return sorted([s for s in input_list if len(s) == len(target_element)], reverse=True)\n",
    "\n",
    "    # 计算目标元素字母的频率\n",
    "    target_element_letter_frequency = {}\n",
    "    for letter in target_element:\n",
    "        if letter in target_element_letter_frequency:\n",
    "            target_element_letter_frequency[letter] += 1\n",
    "        else:\n",
    "            target_element_letter_frequency[letter] = 1\n",
    "\n",
    "    def is_similar(string):\n",
    "        # 计算当前字符串字母的频率\n",
    "        string_letter_frequency = {}\n",
    "        for letter in string:\n",
    "            if letter in string_letter_frequency:\n",
    "                string_letter_frequency[letter] += 1\n",
    "            else:\n",
    "                string_letter_frequency[letter] = 1\n",
    "\n",
    "        # 计算相同字母的比例\n",
    "        common_letters = set(target_element_letter_frequency.keys()) & set(string_letter_frequency.keys())\n",
    "        common_letter_count = sum(min(target_element_letter_frequency[letter], string_letter_frequency[letter]) for letter in common_letters)\n",
    "        total_letter_count = len(target_element)  # 假设目标元素的长度与字符串长度相同\n",
    "\n",
    "        similarity_ratio = common_letter_count / total_letter_count\n",
    "        return similarity_ratio > 0.8\n",
    "\n",
    "    # 找到与目标元素长度相同且相似度超过80%的字符串\n",
    "    matching_strings = [s for s in input_list if len(s) == len(target_element) and is_similar(s)]\n",
    "\n",
    "    # 根据字母顺序进行倒序排序\n",
    "    sorted_strings = sorted(matching_strings, reverse=True)\n",
    "\n",
    "    return sorted_strings\n",
    "def find_elements_around_element(input_list, target_element):\n",
    "    # 确保目标元素在列表中\n",
    "    if target_element not in input_list:\n",
    "        return []\n",
    "\n",
    "    index = input_list.index(target_element)\n",
    "    start_index = max(0, index - 5)\n",
    "    end_index = min(len(input_list), index + 6)\n",
    "\n",
    "    # 使用切片获取目标元素前五位到后五位的元素，不包括目标元素本身\n",
    "    elements_around = input_list[start_index:end_index]\n",
    "    elements_around.remove(target_element)  # 移除目标元素自身\n",
    "\n",
    "    return elements_around\n",
    "\n",
    "\n",
    "find_elements_around_element(value_dict[8],'3x1x0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_cell_indice = np.argwhere(detector!=input_matrix)\n",
    "for d in false_cell_indice:\n",
    "    clean_cell = hospital_clean.iloc[d[0],d[1]]\n",
    "    dirty_cell = hospital_dirty.iloc[d[0],d[1]]\n",
    "    # dirty_value = hospital_dirty.iloc[:,d[1]].unique()\n",
    "    # if(clean_cell not in dirty_value):\n",
    "    print(d,input_matrix[d[0],d[1]],detector[d[0],d[1]])\n",
    "    print(clean_cell)\n",
    "    print(dirty_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 496/496 [00:00<00:00, 2309.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# for d in detector\n",
    "detector_indice = np.argwhere(detector==1)\n",
    "detector_list_all = []\n",
    "candidate_length = {}\n",
    "right_loc = {}\n",
    "for d in tqdm(detector_indice):\n",
    "    \n",
    "    label_tuple = d[0] ## 行\n",
    "    i = d[1] ## 列\n",
    "    all_context_clean = ''\n",
    "    all_context_dirty = ''\n",
    "    # clean_context = hospital_clean.iloc[label_tuple]\n",
    "    dirty_context = hospital_dirty.iloc[label_tuple]\n",
    "    clean_cell = hospital_clean.iloc[label_tuple,i]\n",
    "    dirty_cell = hospital_dirty.iloc[label_tuple,i]\n",
    "    # single_context_clean = 'COL %s VAL %s ' % (hospital_clean.columns[i],clean_cell)\n",
    "    # single_context_dirty = 'COL %s VAL %s ' % (hospital_clean.columns[i],dirty_cell)\n",
    "    for c in range(20):\n",
    "        # all_context_clean += 'COL %s VAL %s ' % (hospital_clean.columns[c],hospital_clean.iloc[label_tuple,c])\n",
    "        all_context_dirty += 'COL %s VAL %s ' % (hospital_dirty.columns[c],hospital_dirty.iloc[label_tuple,c])\n",
    "    # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "    # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "    # detector_list.append([single_context_clean,0])        \n",
    "    # detector_list.append([all_context_clean,0])\n",
    "    # if(dirty_cell!=clean_cell):\n",
    "    #     detector_list_all.append([all_context_dirty,single_context_dirty,1])\n",
    "    # else:\n",
    "    #     detector_list_all.append([all_context_dirty,single_context_dirty,0])\n",
    "    candidate_length[str([d[0],d[1]])] = len(find_elements_around_element(value_dict[i],dirty_cell))\n",
    "    for candidate in find_elements_around_element(value_dict[i],dirty_cell):\n",
    "        if(candidate==clean_cell):\n",
    "            single_context_dirty = 'COL %s VAL %s ' % (hospital_clean.columns[i],candidate)\n",
    "            detector_list_all.append([all_context_dirty,single_context_dirty,0])\n",
    "        else:\n",
    "            single_context_dirty = 'COL %s VAL %s ' % (hospital_clean.columns[i],candidate)\n",
    "            detector_list_all.append([all_context_dirty,single_context_dirty,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['birmingham', 'birmingxam', 'sheffield', 'sheffxeld', 'dothan', 'boaz'],\n",
       " 'birminghxm')"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_elements_around_element(value_dict[6],hospital_dirty.iloc[3,6]),hospital_dirty.iloc[3,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_all).to_csv('datasets/hospital/detector/cleaning.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0],\n",
       "       [  0,   1],\n",
       "       [  0,   2],\n",
       "       ...,\n",
       "       [999,  17],\n",
       "       [999,  18],\n",
       "       [999,  19]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector_indice_norm = np.argwhere(detector==0)\n",
    "detector_indice_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(detector.sum(axis=0)>0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1xx19', 'x0005', '1000x', 'x00xx', 'x00x5', '1xx15', '1xx16',\n",
       "       '100x8', '100x6', 'x0x08', '1xx24', 'x0027', 'x0029', '1xx29',\n",
       "       '1xx32', '100x4', '100x5', '1xx35', '1xx36', '1003x', '1xx39',\n",
       "       '100x9', '1xx44', '1xx45', 'x0045', '1xx47', '1004x'], dtype=object)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hospital_dirty.iloc[np.where(detector[:,1]==1)[0],1].unique()\n",
    "# np.where(detector[:,1]==1)[0]\n",
    "detector_list_all = []\n",
    "noise_col = np.where(detector.sum(axis=0)>0)[0]\n",
    "for d in tqdm(detector_indice_norm):\n",
    "    i = d[1]\n",
    "    label_tuple = d[0] ## 行\n",
    "    if(i in noise_col):\n",
    "        detection_candidate = hospital_dirty.iloc[np.where(detector[:,i]==1)[0],i].unique() ## 可能有错误的dirty data的集合\n",
    "        \n",
    "        ## 列\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        \n",
    "        # clean_context = hospital_clean.iloc[label_tuple]\n",
    "        dirty_context = hospital_dirty.iloc[label_tuple]\n",
    "        # clean_cell = hospital_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = hospital_dirty.iloc[label_tuple,i]\n",
    "        if(find_elements_around_element(value_dict[i],dirty_cell)):\n",
    "            \n",
    "        # single_context_clean = 'COL %s VAL %s ' % (hospital_clean.columns[i],clean_cell)\n",
    "        # single_context_dirty = 'COL %s VAL %s ' % (hospital_clean.columns[i],dirty_cell)\n",
    "        for c in range(20):\n",
    "            # all_context_clean += 'COL %s VAL %s ' % (hospital_clean.columns[c],hospital_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (hospital_dirty.columns[c],hospital_dirty.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        # if(dirty_cell!=clean_cell):\n",
    "        #     detector_list_all.append([all_context_dirty,single_context_dirty,1])\n",
    "        # else:\n",
    "        #     detector_list_all.append([all_context_dirty,single_context_dirty,0])\n",
    "        candidate_length[str([d[0],d[1]])] = len(find_elements_around_element(value_dict[i],dirty_cell))\n",
    "        for candidate in find_elements_around_element(value_dict[i],dirty_cell):\n",
    "            if(candidate==clean_cell):\n",
    "                single_context_dirty = 'COL %s VAL %s ' % (hospital_clean.columns[i],candidate)\n",
    "                detector_list_all.append([all_context_dirty,single_context_dirty,0])\n",
    "            else:\n",
    "                single_context_dirty = 'COL %s VAL %s ' % (hospital_clean.columns[i],candidate)\n",
    "                detector_list_all.append([all_context_dirty,single_context_dirty,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = pd.read_csv('datasets/movies_1/clean.csv').astype(str)\n",
    "dirty = pd.read_csv('datasets/movies_1/dirty.csv').astype(str)\n",
    "dirty.columns = clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 190,   17,  190,  190,   43,  129,    0,    0,    0,    0,  187,\n",
       "        187, 7158,    0,  180,    0,  188])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean.iloc[58,-1],dirty.iloc[58,-1]\n",
    "np.where(np.array(clean!=dirty)[:,0]==1)\n",
    "np.array(clean!=dirty).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1797,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def replace_random_char_with_x(input_string):\n",
    "    if not input_string:\n",
    "        return input_string\n",
    "\n",
    "    # 随机选择要替换的字符\n",
    "    char_to_replace = random.choice(input_string)\n",
    "\n",
    "    # 使用字符串的 replace 方法替换所有相同字符为 'x'\n",
    "    result_string = input_string.replace(char_to_replace, 'x')\n",
    "\n",
    "    return result_string\n",
    "def create_provider_index_dict(dataframe):\n",
    "    # 确保输入的 dataframe 包含 ProviderNumber 列\n",
    "    if 'ProviderNumber' not in dataframe.columns:\n",
    "        return None\n",
    "\n",
    "    # 创建一个空字典来存储结果\n",
    "    provider_index_dict = {}\n",
    "\n",
    "    # 遍历 DataFrame 的行\n",
    "    for index, row in dataframe.iterrows():\n",
    "        provider_number = row['ProviderNumber']\n",
    "\n",
    "        # 如果 ProviderNumber 已经在字典中，将索引追加到对应的列表\n",
    "        if provider_number in provider_index_dict:\n",
    "            provider_index_dict[provider_number].append(index)\n",
    "        else:\n",
    "            # 否则，创建一个新的键值对\n",
    "            provider_index_dict[provider_number] = [index]\n",
    "\n",
    "    return provider_index_dict\n",
    "# 测试函数\n",
    "# input_str = \"hello, world!\"\n",
    "# result = replace_random_char_with_x(input_str)\n",
    "# print(f\"Input: {input_str}\")\n",
    "# print(f\"Output: {result}\")\n",
    "def select_two_different_elements(input_list, given_element):\n",
    "    # 将输入列表转换为 NumPy 数组\n",
    "    np_array = np.array(input_list)\n",
    "\n",
    "    # 创建一个掩码，标记与给定元素不同的元素\n",
    "    mask = np_array != given_element\n",
    "\n",
    "    # 获取所有与给定元素不同的元素的索引\n",
    "    valid_indices = np.where(mask)[0]\n",
    "\n",
    "    # 检查是否有足够的元素用于选择\n",
    "    if len(valid_indices) < 2:\n",
    "        return None\n",
    "\n",
    "    # 随机选择两个不同的索引\n",
    "    selected_indices = np.random.choice(valid_indices, 2, replace=False)\n",
    "\n",
    "    # 根据索引获取对应的元素\n",
    "    selected_elements = np_array[selected_indices]\n",
    "\n",
    "    return selected_elements\n",
    "hospital_cluster = create_provider_index_dict(hospital_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hospital_dirty.iloc[hospital_cluster['10018']].iloc[:,-4].\n",
    "coreset_detect = np.where(detector.sum(axis=1)==0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_dirty['index'] = hospital_dirty['index'].astype(int)\n",
    "hospital_clean['index'] = hospital_clean['index'].astype(int)\n",
    "hospital_dirty_dict = hospital_dirty.set_index('index').to_dict('index')\n",
    "hospital_clean_dict = hospital_clean.set_index('index').to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_dirty['index'] = hospital_dirty['index'].astype(int)\n",
    "hospital_clean['index'] = hospital_clean['index'].astype(int)\n",
    "hospital_dirty_dict = hospital_dirty.iloc[:,:-1].set_index('index').to_dict('index')\n",
    "hospital_clean_dict = hospital_clean.set_index('index').to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('datasets/hospital/detector/hospital_cluster.npy',hospital_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 构造cleaning数据集\n",
    "import json\n",
    "noise_col = np.where(detector.sum(axis=0)>0)[0]\n",
    "header = list(hospital_dirty.columns)\n",
    "safe_value = ['empty'] ## 不注入噪声的类型\n",
    "training_list = []\n",
    "for h in tqdm(hospital_cluster): ## 对比学习的采样策略在同cluster内进行\n",
    "    coreset_subset = [n for n in hospital_cluster[h] if n in coreset_detect]\n",
    "    for coreset_tuple in coreset_subset: ## 取遍coreset所有的tuple，剩下的n-1 tuple是参考对象,coreset_tuple是index\n",
    "        noise_col_subset = np.random.choice(noise_col,5,replace=False) ## 取5个col作为注入噪声的对象\n",
    "        for noise_col_single in noise_col_subset:\n",
    "            col_name = header[noise_col_single] ## 从index转成列名\n",
    "            if(hospital_dirty_dict[coreset_tuple][col_name] not in safe_value and len(coreset_subset)>2):\n",
    "                temp_dict = hospital_dirty.iloc[coreset_tuple,1:].to_dict() ## coreset tuple字典\n",
    "                clean_cell = temp_dict[col_name]\n",
    "                dirty_cell = replace_random_char_with_x(clean_cell) ## 注入噪声\n",
    "                temp_dict[col_name] = dirty_cell\n",
    "                coreset_reference = select_two_different_elements(coreset_subset,coreset_tuple)\n",
    "                template_dict = {}\n",
    "                clean_dict = {}\n",
    "                template_dict[col_name] = ''\n",
    "                clean_dict[col_name] = clean_cell\n",
    "                text_head = 'You are an expert in Cleaning Hospital Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\nTake these rows as reference:\\n\\n' % (col_name, json.dumps(template_dict), json.dumps(temp_dict))\n",
    "                \n",
    "                # ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]), json.dumps(hospital_dirty_dict[coreset_reference[1]]))\n",
    "                \n",
    "                ICL_text = '%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]))\n",
    "                \n",
    "                training_list.append([text_head,ICL_text,'',json.dumps(clean_dict)])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b618e1f284914fd8a2176ef1fa125d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 加入标注数据\n",
    "# header = list(hospital_dirty.columns)\n",
    "# safe_value = ['empty'] ## 不注入噪声的类型\n",
    "# training_list = []\n",
    "# for h in tqdm(hospital_cluster): ## 对比学习的采样策略在同cluster内进行\n",
    "#     coreset_subset = [n for n in hospital_cluster[h] if n in coreset_detect]\n",
    "#     for coreset_tuple in coreset_subset: ## 取遍coreset所有的tuple，剩下的n-1 tuple是参考对象,coreset_tuple是index\n",
    "#         noise_col_subset = np.random.choice(noise_col,5,replace=False) ## 取5个col作为注入噪声的对象\n",
    "#         for noise_col_single in noise_col_subset:\n",
    "#             col_name = header[noise_col_single] ## 从index转成列名\n",
    "#             if(hospital_dirty_dict[coreset_tuple][col_name] not in safe_value and len(coreset_subset)>2):\n",
    "#                 temp_dict = hospital_dirty.iloc[coreset_tuple,1:].to_dict() ## coreset tuple字典\n",
    "#                 clean_cell = temp_dict[col_name]\n",
    "#                 dirty_cell = replace_random_char_with_x(clean_cell) ## 注入噪声\n",
    "#                 temp_dict[col_name] = dirty_cell\n",
    "#                 coreset_reference = select_two_different_elements(coreset_subset,coreset_tuple)\n",
    "#                 template_dict = {}\n",
    "#                 clean_dict = {}\n",
    "#                 template_dict[col_name] = ''\n",
    "#                 clean_dict[col_name] = clean_cell\n",
    "#                 text_head = 'You are an expert in Cleaning Hospital Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\nTake these rows as reference:\\n\\n' % (col_name, json.dumps(template_dict), json.dumps(temp_dict))\n",
    "                \n",
    "#                 ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]), json.dumps(hospital_dirty_dict[coreset_reference[1]]))\n",
    "                \n",
    "#                 training_list.append([text_head,ICL_text,'',json.dumps(clean_dict)])\n",
    "training_list_label = []\n",
    "for label_tuple in tqdm(selected_rows):\n",
    "    for noise_col_single in range(len(header)):\n",
    "        col_name = header[noise_col_single]\n",
    "        template_dict = {}\n",
    "        template_dict[col_name] = ''\n",
    "        clean_dict = {}\n",
    "        temp_dict = hospital_dirty.iloc[label_tuple,1:].to_dict()\n",
    "        clean_cell = hospital_clean.iloc[label_tuple,noise_col_single]\n",
    "        dirty_cell = hospital_dirty.iloc[label_tuple,noise_col_single]\n",
    "        clean_dict[col_name] = clean_cell\n",
    "        if(clean_cell!=dirty_cell):\n",
    "            text_head = 'You are an expert in Cleaning Hospital Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\nTake these rows as reference:\\n\\n' % (col_name, json.dumps(template_dict), json.dumps(temp_dict))\n",
    "            cluster = [hospital_cluster[h] for h in hospital_cluster if hospital_cluster[h].__contains__(label_tuple)][0] ## 找到label tuple所在的簇\n",
    "            cluster_coreset = [c for c in cluster if c in coreset_detect] ## 找到簇内coreset元素\n",
    "            coreset_reference = np.random.choice(cluster_coreset,2,replace=False) ## 取两个\n",
    "            # ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]), json.dumps(hospital_dirty_dict[coreset_reference[1]]))\n",
    "            ICL_text = '%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]))\n",
    "            training_list_label.append([text_head,ICL_text,'',json.dumps(clean_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list_label_pd = pd.DataFrame(training_list_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given element: 2\n",
      "Resulting list: [1, 4]\n"
     ]
    }
   ],
   "source": [
    "def get_list_for_given_element(result_dict, given_element):\n",
    "    if given_element in result_dict:\n",
    "        return result_dict[given_element]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# 示例用法\n",
    "result_dict = {1: [0, 2, 6], 2: [1, 4], 3: [3], 4: [5]}\n",
    "given_element = 2\n",
    "result_list = get_list_for_given_element(result_dict, given_element)\n",
    "print(f\"Given element: {given_element}\")\n",
    "print(f\"Resulting list: {result_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list_pd = pd.DataFrame(training_list).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...</td>\n",
       "      <td></td>\n",
       "      <td>{\"MeasureName\": \"surgery patients who were giv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ZipCode\": \"35233\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...</td>\n",
       "      <td></td>\n",
       "      <td>{\"Stateavg\": \"al_scip-inf-1\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...</td>\n",
       "      <td></td>\n",
       "      <td>{\"MeasureCode\": \"scip-inf-1\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...</td>\n",
       "      <td></td>\n",
       "      <td>{\"City\": \"birmingham\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10005\", \"HospitalName\": \"m...</td>\n",
       "      <td></td>\n",
       "      <td>{\"PhoneNumber\": \"2565938310\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10006\", \"HospitalName\": \"e...</td>\n",
       "      <td></td>\n",
       "      <td>{\"City\": \"florence\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10006\", \"HospitalName\": \"e...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ZipCode\": \"35631\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10007\", \"HospitalName\": \"m...</td>\n",
       "      <td></td>\n",
       "      <td>{\"Address1\": \"702 n main st\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10007\", \"HospitalName\": \"m...</td>\n",
       "      <td></td>\n",
       "      <td>{\"MeasureName\": \"heart attack patients given a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3003 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0  \\\n",
       "0   You are an expert in Cleaning Hospital Dataset...   \n",
       "1   You are an expert in Cleaning Hospital Dataset...   \n",
       "2   You are an expert in Cleaning Hospital Dataset...   \n",
       "3   You are an expert in Cleaning Hospital Dataset...   \n",
       "4   You are an expert in Cleaning Hospital Dataset...   \n",
       "..                                                ...   \n",
       "49  You are an expert in Cleaning Hospital Dataset...   \n",
       "50  You are an expert in Cleaning Hospital Dataset...   \n",
       "51  You are an expert in Cleaning Hospital Dataset...   \n",
       "52  You are an expert in Cleaning Hospital Dataset...   \n",
       "53  You are an expert in Cleaning Hospital Dataset...   \n",
       "\n",
       "                                                    1 2   \\\n",
       "0   {\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...      \n",
       "1   {\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...      \n",
       "2   {\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...      \n",
       "3   {\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...      \n",
       "4   {\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...      \n",
       "..                                                ... ..   \n",
       "49  {\"ProviderNumber\": \"10005\", \"HospitalName\": \"m...      \n",
       "50  {\"ProviderNumber\": \"10006\", \"HospitalName\": \"e...      \n",
       "51  {\"ProviderNumber\": \"10006\", \"HospitalName\": \"e...      \n",
       "52  {\"ProviderNumber\": \"10007\", \"HospitalName\": \"m...      \n",
       "53  {\"ProviderNumber\": \"10007\", \"HospitalName\": \"m...      \n",
       "\n",
       "                                                    3  \n",
       "0   {\"MeasureName\": \"surgery patients who were giv...  \n",
       "1                                {\"ZipCode\": \"35233\"}  \n",
       "2                       {\"Stateavg\": \"al_scip-inf-1\"}  \n",
       "3                       {\"MeasureCode\": \"scip-inf-1\"}  \n",
       "4                              {\"City\": \"birmingham\"}  \n",
       "..                                                ...  \n",
       "49                      {\"PhoneNumber\": \"2565938310\"}  \n",
       "50                               {\"City\": \"florence\"}  \n",
       "51                               {\"ZipCode\": \"35631\"}  \n",
       "52                      {\"Address1\": \"702 n main st\"}  \n",
       "53  {\"MeasureName\": \"heart attack patients given a...  \n",
       "\n",
       "[3003 rows x 4 columns]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_list_pd = pd.concat([training_list_pd,training_list_label_pd]).drop_duplicates()\n",
    "training_list_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list_pd['instruction'] = training_list_pd[0] + training_list_pd[1]\n",
    "training_list_pd['input'] = training_list_pd[2]\n",
    "training_list_pd['output'] = training_list_pd[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(training_list_pd.iloc[:,-3:].to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/hospital/hospital-train.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COL index VAL 1 COL ProviderNumber VAL 10018 C...</td>\n",
       "      <td>COL MeasureName VAL surgery patients who were ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COL index VAL 1 COL ProviderNumber VAL 10018 C...</td>\n",
       "      <td>COL MeasureName VAL surgery patients who were ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COL index VAL 1 COL ProviderNumber VAL 10018 C...</td>\n",
       "      <td>COL MeasureName VAL surgery patients whose pre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COL index VAL 1 COL ProviderNumber VAL 10018 C...</td>\n",
       "      <td>COL MeasureName VAL all heart surgery patients...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COL index VAL 1 COL ProviderNumber VAL 10018 C...</td>\n",
       "      <td>COL MeasureName VAL surgery patients needing h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4413</th>\n",
       "      <td>COL index VAL 998 COL ProviderNumber VAL 10050...</td>\n",
       "      <td>COL MeasureName VAL heart attack patxents gxve...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4414</th>\n",
       "      <td>COL index VAL 998 COL ProviderNumber VAL 10050...</td>\n",
       "      <td>COL MeasureName VAL pneumonix pxtients given i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4415</th>\n",
       "      <td>COL index VAL 998 COL ProviderNumber VAL 10050...</td>\n",
       "      <td>COL MeasureName VAL pneumonia patients assesse...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4416</th>\n",
       "      <td>COL index VAL 998 COL ProviderNumber VAL 10050...</td>\n",
       "      <td>COL MeasureName VAL childrenxwhoxreceivedxreli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4417</th>\n",
       "      <td>COL index VAL 998 COL ProviderNumber VAL 10050...</td>\n",
       "      <td>COL MeasureName VAL hearx failure paxienxs giv...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4418 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0  \\\n",
       "0     COL index VAL 1 COL ProviderNumber VAL 10018 C...   \n",
       "1     COL index VAL 1 COL ProviderNumber VAL 10018 C...   \n",
       "2     COL index VAL 1 COL ProviderNumber VAL 10018 C...   \n",
       "3     COL index VAL 1 COL ProviderNumber VAL 10018 C...   \n",
       "4     COL index VAL 1 COL ProviderNumber VAL 10018 C...   \n",
       "...                                                 ...   \n",
       "4413  COL index VAL 998 COL ProviderNumber VAL 10050...   \n",
       "4414  COL index VAL 998 COL ProviderNumber VAL 10050...   \n",
       "4415  COL index VAL 998 COL ProviderNumber VAL 10050...   \n",
       "4416  COL index VAL 998 COL ProviderNumber VAL 10050...   \n",
       "4417  COL index VAL 998 COL ProviderNumber VAL 10050...   \n",
       "\n",
       "                                                      1  2  \n",
       "0     COL MeasureName VAL surgery patients who were ...  1  \n",
       "1     COL MeasureName VAL surgery patients who were ...  1  \n",
       "2     COL MeasureName VAL surgery patients whose pre...  1  \n",
       "3     COL MeasureName VAL all heart surgery patients...  1  \n",
       "4     COL MeasureName VAL surgery patients needing h...  1  \n",
       "...                                                 ... ..  \n",
       "4413  COL MeasureName VAL heart attack patxents gxve...  1  \n",
       "4414  COL MeasureName VAL pneumonix pxtients given i...  1  \n",
       "4415  COL MeasureName VAL pneumonia patients assesse...  1  \n",
       "4416  COL MeasureName VAL childrenxwhoxreceivedxreli...  1  \n",
       "4417  COL MeasureName VAL hearx failure paxienxs giv...  1  \n",
       "\n",
       "[4418 rows x 3 columns]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('/home/yanmy/raha/raha-master/datasets/hospital/detector/cleaning.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "md5: 1aa1ab5fc3e25a8e79276bd8c6501c86\n",
      "sha1: 97cab578fd6899177eb88c09f629e22d96be3620\n",
      "sha256: 93f538d157bf4e15ae7cd20020f4b72e8f912a3ae8551eb688a730e748e6f936\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "\n",
    "def encrypt(fpath: str, algorithm: str) -> str:\n",
    "    with open(fpath, 'rb') as f:\n",
    "        return hashlib.new(algorithm, f.read()).hexdigest()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for algorithm in ('md5', 'sha1', 'sha256'):\n",
    "        hexdigest = encrypt('/home/yanmy/LLaMA-Efficient-Tuning-main/data/RE/RE_train_10.json', algorithm)\n",
    "        print(f'{algorithm}: {hexdigest}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "95ee149094747afbe2ee7c72447cf0526a5c8406"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "587f7e3470ff45b09f77fc9edcbc59c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Lengths: 322\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "prompt_opt = pd.read_json('/home/yanmy/LLaMA-Efficient-Tuning-main/data/imdb/imdb-train.json')\n",
    "# 加载 RoBERTa tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('/home/yanmy/roberta-base/')\n",
    "\n",
    "# 给定的列表\n",
    "# text_list = [\n",
    "#     \"This is the first sentence.\",\n",
    "#     \"Here is another sentence.\",\n",
    "#     \"Yet another example sentence.\"\n",
    "# ]\n",
    "\n",
    "# 统计每个元素的 token 长度\n",
    "token_lengths = [len(tokenizer.encode(text, add_special_tokens=True)) for text in tqdm(prompt_opt['instruction'].to_list())]\n",
    "\n",
    "print(\"Token Lengths:\", max(token_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 18, 182, 464, 724, 730, 541, 250, 114,  27,   5]),\n",
       " array([584., 597., 610., 623., 636., 649., 662., 675., 688., 701., 714.]))"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(token_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tuple_id</th>\n",
       "      <th>src</th>\n",
       "      <th>flight</th>\n",
       "      <th>sched_dep_time</th>\n",
       "      <th>act_dep_time</th>\n",
       "      <th>sched_arr_time</th>\n",
       "      <th>act_arr_time</th>\n",
       "      <th>count</th>\n",
       "      <th>text</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:16 a.m.</td>\n",
       "      <td>9:40 a.m.</td>\n",
       "      <td>9:32 a.m.</td>\n",
       "      <td>0</td>\n",
       "      <td>COL src VAL aa COL flight VAL AA-3859-IAH-ORD ...</td>\n",
       "      <td>You are an expert in data cleaning. Based on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-1733-ORD-PHX</td>\n",
       "      <td>7:45 p.m.</td>\n",
       "      <td>7:58 p.m.</td>\n",
       "      <td>10:30 p.m.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>COL src VAL aa COL flight VAL AA-1733-ORD-PHX ...</td>\n",
       "      <td>You are an expert in data cleaning. Based on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-1640-MIA-MCO</td>\n",
       "      <td>6:30 p.m.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7:25 p.m.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>COL src VAL aa COL flight VAL AA-1640-MIA-MCO ...</td>\n",
       "      <td>You are an expert in data cleaning. Based on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-518-MIA-JFK</td>\n",
       "      <td>6:40 a.m.</td>\n",
       "      <td>6:54 a.m.</td>\n",
       "      <td>9:25 a.m.</td>\n",
       "      <td>9:28 a.m.</td>\n",
       "      <td>0</td>\n",
       "      <td>COL src VAL aa COL flight VAL AA-518-MIA-JFK C...</td>\n",
       "      <td>You are an expert in data cleaning. Based on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-3756-ORD-SLC</td>\n",
       "      <td>12:15 p.m.</td>\n",
       "      <td>12:41 p.m.</td>\n",
       "      <td>2:45 p.m.</td>\n",
       "      <td>2:50 p.m.</td>\n",
       "      <td>0</td>\n",
       "      <td>COL src VAL aa COL flight VAL AA-3756-ORD-SLC ...</td>\n",
       "      <td>You are an expert in data cleaning. Based on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2371</th>\n",
       "      <td>2372</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>UA-3099-PHX-PHL</td>\n",
       "      <td>11:55 a.m.</td>\n",
       "      <td>11:43 a.m.</td>\n",
       "      <td>6:17 p.m.</td>\n",
       "      <td>5:38 p.m.</td>\n",
       "      <td>1</td>\n",
       "      <td>COL src VAL world-flight-tracker COL flight VA...</td>\n",
       "      <td>You are an expert in data cleaning. Based on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2372</th>\n",
       "      <td>2373</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-4198-ORD-CLE</td>\n",
       "      <td>10:40 a.m.</td>\n",
       "      <td>10:54 a.m.</td>\n",
       "      <td>12:55 p.m.</td>\n",
       "      <td>12:50 p.m.</td>\n",
       "      <td>0</td>\n",
       "      <td>COL src VAL world-flight-tracker COL flight VA...</td>\n",
       "      <td>You are an expert in data cleaning. Based on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2373</th>\n",
       "      <td>2374</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>CO-45-EWR-MIA</td>\n",
       "      <td>4:00 p.m.</td>\n",
       "      <td>3:58 p.m.</td>\n",
       "      <td>7:05 p.m.</td>\n",
       "      <td>6:36 p.m.</td>\n",
       "      <td>0</td>\n",
       "      <td>COL src VAL world-flight-tracker COL flight VA...</td>\n",
       "      <td>You are an expert in data cleaning. Based on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2374</th>\n",
       "      <td>2375</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-3809-PHX-LAX</td>\n",
       "      <td>6:00 a.m.</td>\n",
       "      <td>6:10 a.m.</td>\n",
       "      <td>6:40 a.m.</td>\n",
       "      <td>6:19 a.m.</td>\n",
       "      <td>0</td>\n",
       "      <td>COL src VAL world-flight-tracker COL flight VA...</td>\n",
       "      <td>You are an expert in data cleaning. Based on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2375</th>\n",
       "      <td>2376</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-59-JFK-SFO</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:39 a.m.</td>\n",
       "      <td>10:45 a.m.</td>\n",
       "      <td>11:12 a.m.</td>\n",
       "      <td>0</td>\n",
       "      <td>COL src VAL world-flight-tracker COL flight VA...</td>\n",
       "      <td>You are an expert in data cleaning. Based on t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2376 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tuple_id                   src           flight sched_dep_time  \\\n",
       "0            1                    aa  AA-3859-IAH-ORD      7:10 a.m.   \n",
       "1            2                    aa  AA-1733-ORD-PHX      7:45 p.m.   \n",
       "2            3                    aa  AA-1640-MIA-MCO      6:30 p.m.   \n",
       "3            4                    aa   AA-518-MIA-JFK      6:40 a.m.   \n",
       "4            5                    aa  AA-3756-ORD-SLC     12:15 p.m.   \n",
       "...        ...                   ...              ...            ...   \n",
       "2371      2372  world-flight-tracker  UA-3099-PHX-PHL     11:55 a.m.   \n",
       "2372      2373  world-flight-tracker  AA-4198-ORD-CLE     10:40 a.m.   \n",
       "2373      2374  world-flight-tracker    CO-45-EWR-MIA      4:00 p.m.   \n",
       "2374      2375  world-flight-tracker  AA-3809-PHX-LAX      6:00 a.m.   \n",
       "2375      2376  world-flight-tracker    AA-59-JFK-SFO      7:10 a.m.   \n",
       "\n",
       "     act_dep_time sched_arr_time act_arr_time  count  \\\n",
       "0       7:16 a.m.      9:40 a.m.    9:32 a.m.      0   \n",
       "1       7:58 p.m.     10:30 p.m.          NaN      1   \n",
       "2             NaN      7:25 p.m.          NaN      2   \n",
       "3       6:54 a.m.      9:25 a.m.    9:28 a.m.      0   \n",
       "4      12:41 p.m.      2:45 p.m.    2:50 p.m.      0   \n",
       "...           ...            ...          ...    ...   \n",
       "2371   11:43 a.m.      6:17 p.m.    5:38 p.m.      1   \n",
       "2372   10:54 a.m.     12:55 p.m.   12:50 p.m.      0   \n",
       "2373    3:58 p.m.      7:05 p.m.    6:36 p.m.      0   \n",
       "2374    6:10 a.m.      6:40 a.m.    6:19 a.m.      0   \n",
       "2375    7:39 a.m.     10:45 a.m.   11:12 a.m.      0   \n",
       "\n",
       "                                                   text  \\\n",
       "0     COL src VAL aa COL flight VAL AA-3859-IAH-ORD ...   \n",
       "1     COL src VAL aa COL flight VAL AA-1733-ORD-PHX ...   \n",
       "2     COL src VAL aa COL flight VAL AA-1640-MIA-MCO ...   \n",
       "3     COL src VAL aa COL flight VAL AA-518-MIA-JFK C...   \n",
       "4     COL src VAL aa COL flight VAL AA-3756-ORD-SLC ...   \n",
       "...                                                 ...   \n",
       "2371  COL src VAL world-flight-tracker COL flight VA...   \n",
       "2372  COL src VAL world-flight-tracker COL flight VA...   \n",
       "2373  COL src VAL world-flight-tracker COL flight VA...   \n",
       "2374  COL src VAL world-flight-tracker COL flight VA...   \n",
       "2375  COL src VAL world-flight-tracker COL flight VA...   \n",
       "\n",
       "                                                  query  \n",
       "0     You are an expert in data cleaning. Based on t...  \n",
       "1     You are an expert in data cleaning. Based on t...  \n",
       "2     You are an expert in data cleaning. Based on t...  \n",
       "3     You are an expert in data cleaning. Based on t...  \n",
       "4     You are an expert in data cleaning. Based on t...  \n",
       "...                                                 ...  \n",
       "2371  You are an expert in data cleaning. Based on t...  \n",
       "2372  You are an expert in data cleaning. Based on t...  \n",
       "2373  You are an expert in data cleaning. Based on t...  \n",
       "2374  You are an expert in data cleaning. Based on t...  \n",
       "2375  You are an expert in data cleaning. Based on t...  \n",
       "\n",
       "[2376 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Flight Detector Test\n",
    "flight_clean = pd.read_csv('datasets/flights/clean.csv').fillna('').astype(str)\n",
    "flight_dirty = pd.read_csv('datasets/flights/dirty.csv').fillna('').astype(str)\n",
    "flight_query = pd.read_csv('datasets/flights/dirty_query.csv',index_col=0)\n",
    "flight_dirty.columns = flight_clean.columns\n",
    "flight_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_Error = np.array(flight_dirty!=flight_clean).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_Data_Error = All_Error.sum()\n",
    "Correct_Fixed_Error = 0\n",
    "All_Fixed_Error = 0\n",
    "for x in range(All_Error.shape[0]):\n",
    "    for y in range(All_Error.shape[1]):\n",
    "        dirty_cell = flight_dirty.iloc[x,y]\n",
    "        fixed_cell = flight_dirty_clean.iloc[x,y]\n",
    "        clean_cell = flight_clean.iloc[x,y]\n",
    "        if(fixed_cell!=dirty_cell):\n",
    "            All_Fixed_Error += 1\n",
    "            if(fixed_cell==clean_cell):\n",
    "                Correct_Fixed_Error += 1\n",
    "Precision = Correct_Fixed_Error / All_Fixed_Error\n",
    "Recall = Correct_Fixed_Error / All_Data_Error\n",
    "F1 = (2*Precision*Recall) / (Precision + Recall)\n",
    "print(Precision,Recall,F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4729, 4897, 4920)"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Correct_Fixed_Error,All_Fixed_Error,All_Data_Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9656932816009802 0.9611788617886179 0.9634307833350311\n"
     ]
    }
   ],
   "source": [
    "Precision = Correct_Fixed_Error / All_Fixed_Error\n",
    "Recall = Correct_Fixed_Error / All_Data_Error\n",
    "F1 = (2*Precision*Recall) / (Precision + Recall)\n",
    "print(Precision,Recall,F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.9656932816009802 0.9611788617886179 0.9634307833350311 22 labels(P/R/F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flight_clean['flight'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flight_dirty['sched_dep_time'].unique()\n",
    "import re\n",
    "\n",
    "def is_clean_time_format(time_str):\n",
    "    # Define a regular expression pattern for a clean time format (HH:MM a.m./p.m.)\n",
    "    time_pattern = r'^\\d{1,2}:\\d{2} (a\\.m\\.|p\\.m\\.)$'\n",
    "    \n",
    "    # Use regex to check if the time string matches the expected pattern\n",
    "    if re.match(time_pattern, time_str):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_clean_time_format('11:55 a.m.')\n",
    "def Check_Clean_Time_Format(row):\n",
    "    count = 0\n",
    "    for x,y in row[3:].items():\n",
    "        if not (is_clean_time_format(y)):\n",
    "            count += 1\n",
    "    return count\n",
    "flight_dirty['count'] = flight_dirty.apply(Check_Clean_Time_Format,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flight_dirty[flight_dirty['count']==0]\n",
    "flight_unique = list(flight_dirty['flight'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 6\n",
    "flight_cluster_dict = {}\n",
    "for i in range(100):\n",
    "    temp_dict = {}\n",
    "    i_0 = flight_clean[flight_clean['flight']==flight_unique[i]].iloc[0,3]\n",
    "    i_1 = flight_clean[flight_clean['flight']==flight_unique[i]].iloc[0,4]\n",
    "    i_2 = flight_clean[flight_clean['flight']==flight_unique[i]].iloc[0,5]\n",
    "    i_3 = flight_clean[flight_clean['flight']==flight_unique[i]].iloc[0,6]\n",
    "    temp_dict['all'] = len(flight_dirty[(flight_dirty['flight']==flight_unique[i]) & (flight_dirty['count']==0)])\n",
    "    # print(len(flight_dirty[(flight_dirty['flight']==flight_unique[i]) & (flight_dirty['count']==0)]))\n",
    "    # flight_dirty[(flight_dirty['flight']==flight_unique[i]) & (flight_dirty['sched_dep_time']==i_0)& (flight_dirty['act_dep_time']==i_1)& (flight_dirty['sched_arr_time']==i_2)& (flight_dirty['act_arr_time']==i_3)]\n",
    "    temp_dict['clean'] = len(flight_dirty[(flight_dirty['flight']==flight_unique[i]) & (flight_dirty['sched_dep_time']==i_0)& (flight_dirty['act_dep_time']==i_1)& (flight_dirty['sched_arr_time']==i_2)& (flight_dirty['act_arr_time']==i_3)])\n",
    "    flight_cluster_dict[i] = temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tuple_id</th>\n",
       "      <th>src</th>\n",
       "      <th>flight</th>\n",
       "      <th>sched_dep_time</th>\n",
       "      <th>act_dep_time</th>\n",
       "      <th>sched_arr_time</th>\n",
       "      <th>act_arr_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td>10:45 a.m.</td>\n",
       "      <td>10:55 a.m.</td>\n",
       "      <td>2:20 p.m.</td>\n",
       "      <td>1:40 p.m.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tuple_id src           flight sched_dep_time act_dep_time sched_arr_time  \\\n",
       "10       11  aa  AA-1886-BOS-MIA     10:45 a.m.   10:55 a.m.      2:20 p.m.   \n",
       "\n",
       "   act_arr_time  \n",
       "10    1:40 p.m.  "
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_clean[flight_clean['flight']==flight_unique[i]].iloc[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tuple_id</th>\n",
       "      <th>src</th>\n",
       "      <th>flight</th>\n",
       "      <th>sched_dep_time</th>\n",
       "      <th>act_dep_time</th>\n",
       "      <th>sched_arr_time</th>\n",
       "      <th>act_arr_time</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td>10:45 a.m.</td>\n",
       "      <td>10:55 a.m.</td>\n",
       "      <td>2:20 p.m.</td>\n",
       "      <td>1:40 p.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>71</td>\n",
       "      <td>helloflight</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td></td>\n",
       "      <td>10:54 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>1:36 p.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>171</td>\n",
       "      <td>boston</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td></td>\n",
       "      <td>10:55 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>1:34 p.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>275</td>\n",
       "      <td>weather</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td>10:45 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>2:20 p.m.</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>358</td>\n",
       "      <td>airtravelcenter</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td></td>\n",
       "      <td>10:54 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>1:36 p.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>458</td>\n",
       "      <td>flightview</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td></td>\n",
       "      <td>10:55 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>1:40 p.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>650</td>\n",
       "      <td>panynj</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td></td>\n",
       "      <td>10:55 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>1:40 p.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>769</td>\n",
       "      <td>flightexplorer</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td></td>\n",
       "      <td>10:55 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>1:36 p.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>832</td>\n",
       "      <td>flights</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td></td>\n",
       "      <td>10:55 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>1:40 p.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>1012</td>\n",
       "      <td>travelocity</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td>10:45 a.m.</td>\n",
       "      <td>10:42 a.m.</td>\n",
       "      <td>2:20 p.m.</td>\n",
       "      <td>1:40 p.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>1091</td>\n",
       "      <td>foxbusiness</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td></td>\n",
       "      <td>10:55 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>1:40 p.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>1183</td>\n",
       "      <td>usatoday</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td>10:45 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>2:20 p.m.</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>1244</td>\n",
       "      <td>myrateplan</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td></td>\n",
       "      <td>10:54 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>1:36 p.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>1344</td>\n",
       "      <td>orbitz</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td>10:45aDec 1</td>\n",
       "      <td>10:42aDec 1</td>\n",
       "      <td>2:20 p.m.</td>\n",
       "      <td>1:40 p.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507</th>\n",
       "      <td>1508</td>\n",
       "      <td>flytecomm</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td></td>\n",
       "      <td>10:54 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>1:36 p.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1778</th>\n",
       "      <td>1779</td>\n",
       "      <td>flylouisville</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td></td>\n",
       "      <td>10:55 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>1:40 p.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>1971</td>\n",
       "      <td>allegiantair</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td></td>\n",
       "      <td>10:55 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>1:40 p.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2070</th>\n",
       "      <td>2071</td>\n",
       "      <td>businesstravellogue</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td></td>\n",
       "      <td>10:55 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>1:40 p.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2175</th>\n",
       "      <td>2176</td>\n",
       "      <td>gofox</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td></td>\n",
       "      <td>10:55 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>1:40 p.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tuple_id                  src           flight sched_dep_time  \\\n",
       "10         11                   aa  AA-1886-BOS-MIA     10:45 a.m.   \n",
       "70         71          helloflight  AA-1886-BOS-MIA                  \n",
       "170       171               boston  AA-1886-BOS-MIA                  \n",
       "274       275              weather  AA-1886-BOS-MIA     10:45 a.m.   \n",
       "357       358      airtravelcenter  AA-1886-BOS-MIA                  \n",
       "457       458           flightview  AA-1886-BOS-MIA                  \n",
       "649       650               panynj  AA-1886-BOS-MIA                  \n",
       "768       769       flightexplorer  AA-1886-BOS-MIA                  \n",
       "831       832              flights  AA-1886-BOS-MIA                  \n",
       "1011     1012          travelocity  AA-1886-BOS-MIA     10:45 a.m.   \n",
       "1090     1091          foxbusiness  AA-1886-BOS-MIA                  \n",
       "1182     1183             usatoday  AA-1886-BOS-MIA     10:45 a.m.   \n",
       "1243     1244           myrateplan  AA-1886-BOS-MIA                  \n",
       "1343     1344               orbitz  AA-1886-BOS-MIA    10:45aDec 1   \n",
       "1507     1508            flytecomm  AA-1886-BOS-MIA                  \n",
       "1778     1779        flylouisville  AA-1886-BOS-MIA                  \n",
       "1970     1971         allegiantair  AA-1886-BOS-MIA                  \n",
       "2070     2071  businesstravellogue  AA-1886-BOS-MIA                  \n",
       "2175     2176                gofox  AA-1886-BOS-MIA                  \n",
       "\n",
       "     act_dep_time sched_arr_time act_arr_time  count  \n",
       "10     10:55 a.m.      2:20 p.m.    1:40 p.m.      0  \n",
       "70     10:54 a.m.                   1:36 p.m.      2  \n",
       "170    10:55 a.m.                   1:34 p.m.      2  \n",
       "274                    2:20 p.m.                   2  \n",
       "357    10:54 a.m.                   1:36 p.m.      2  \n",
       "457    10:55 a.m.                   1:40 p.m.      2  \n",
       "649    10:55 a.m.                   1:40 p.m.      2  \n",
       "768    10:55 a.m.                   1:36 p.m.      2  \n",
       "831    10:55 a.m.                   1:40 p.m.      2  \n",
       "1011   10:42 a.m.      2:20 p.m.    1:40 p.m.      0  \n",
       "1090   10:55 a.m.                   1:40 p.m.      2  \n",
       "1182                   2:20 p.m.                   2  \n",
       "1243   10:54 a.m.                   1:36 p.m.      2  \n",
       "1343  10:42aDec 1      2:20 p.m.    1:40 p.m.      2  \n",
       "1507   10:54 a.m.                   1:36 p.m.      2  \n",
       "1778   10:55 a.m.                   1:40 p.m.      2  \n",
       "1970   10:55 a.m.                   1:40 p.m.      2  \n",
       "2070   10:55 a.m.                   1:40 p.m.      2  \n",
       "2175   10:55 a.m.                   1:40 p.m.      2  "
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 10\n",
    "i_0 = flight_clean[flight_clean['flight']==flight_unique[i]].iloc[0,3]\n",
    "i_1 = flight_clean[flight_clean['flight']==flight_unique[i]].iloc[0,4]\n",
    "i_2 = flight_clean[flight_clean['flight']==flight_unique[i]].iloc[0,5]\n",
    "i_3 = flight_clean[flight_clean['flight']==flight_unique[i]].iloc[0,6]\n",
    "temp_dict['all'] = len(flight_dirty[(flight_dirty['flight']==flight_unique[i]) & (flight_dirty['count']==0)])\n",
    "# print(len(flight_dirty[(flight_dirty['flight']==flight_unique[i]) & (flight_dirty['count']==0)]))\n",
    "# flight_dirty[(flight_dirty['flight']==flight_unique[i]) & (flight_dirty['sched_dep_time']==i_0)& (flight_dirty['act_dep_time']==i_1)& (flight_dirty['sched_arr_time']==i_2)& (flight_dirty['act_arr_time']==i_3)]\n",
    "temp_dict['clean'] = len(flight_dirty[(flight_dirty['flight']==flight_unique[i]) & (flight_dirty['sched_dep_time']==i_0)& (flight_dirty['act_dep_time']==i_1)& (flight_dirty['sched_arr_time']==i_2)& (flight_dirty['act_arr_time']==i_3)])\n",
    "flight_cluster_dict[i] = temp_dict\n",
    "flight_dirty[(flight_dirty['flight']==flight_unique[i]) & (flight_dirty['count']==0)]\n",
    "flight_dirty[(flight_dirty['flight']==flight_unique[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_select_flight = [1,2,5,14,31,32,33,36,37,53,58,64,72,74,76,82,85,89,90,93,95,98]\n",
    "len(cluster_select_flight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([32, 1, 37, 36, 89, 5, 85, 90, 82, 93, 74, 76, 33, 64, 2, 14, 53,\n",
       "            72, 98, 58],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(flight_cluster_dict).T.iloc[cluster_select_flight].sort_values('all').index[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1584,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_select_flight_core = [1, 2, 5, 31, 32, 33, 36, 37, 85, 89, 95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1585,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_select_flight_5 = [14, 53, 72, 98, 58]\n",
    "cluster_select_flight_10 = [74, 76, 33, 64, 2, 14, 53, 72, 98, 58]\n",
    "cluster_select_flight_15 = [5, 85, 90, 82, 93, 74, 76, 33, 64, 2, 14, 53, 72, 98, 58]\n",
    "cluster_select_flight_20 = [32, 1, 37, 36, 89, 5, 85, 90, 82, 93, 74, 76, 33, 64, 2, 14, 53,\n",
    "            72, 98, 58]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1586,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/yanmy/raha/raha-master/detector.ipynb 单元格 67\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y123sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m flight_dirty[(flight_dirty[\u001b[39m'\u001b[39m\u001b[39mflight\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m==\u001b[39mflight_unique[i]) \u001b[39m&\u001b[39m (flight_dirty[\u001b[39m'\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m)]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "flight_dirty[(flight_dirty['flight']==flight_unique[i]) & (flight_dirty['count']==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 创建示例DataFrame\n",
    "# data = {\n",
    "#     'A': [1, 2, 3, 4, 5],\n",
    "#     'B': [2, 2, 3, 9, 2],\n",
    "#     'C': [3, 3, 13, 14, 3],\n",
    "#     'D': [4, 4, 18, 19, 4],\n",
    "#     'E': [5, 5, 8, 9, 5]\n",
    "# }\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # 选择后四列\n",
    "def CoresetIndex(df):\n",
    "    df_tail = df.iloc[:, -4:]\n",
    "\n",
    "    # 计算后四列的哈希值\n",
    "    hash_values = df_tail.apply(tuple, axis=1).apply(hash)\n",
    "\n",
    "    # 找到最常出现的哈希值\n",
    "    most_common_hash = hash_values.mode().values[0]\n",
    "\n",
    "    # 找到对应的行索引\n",
    "    indices = hash_values[hash_values == most_common_hash].index\n",
    "    return indices\n",
    "\n",
    "# 输出索引\n",
    "# print(\"行数最多的行索引:\", indices[0])\n",
    "# C = CoresetIndex(test_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_dirty_clean = flight_dirty\n",
    "# flight_dirty_clean.iloc[:3,-5:-1] = flight_clean[flight_clean['flight']==flight_unique[i]].iloc[0:1,-4:]\n",
    "# flight_dirty_clean.iloc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_select_flight_1 = [14]\n",
    "cluster_select_flight_2 = [14, 53]\n",
    "cluster_select_flight_3 = [14, 53, 72]\n",
    "cluster_select_flight_4 = [14, 53, 72, 98]\n",
    "cluster_select_flight_5 = [14, 53, 72, 98, 58]\n",
    "cluster_select_flight_6 = [14, 53, 72, 98, 58 , 74]\n",
    "cluster_select_flight_7 = [14, 53, 72, 98, 58,74, 76]\n",
    "cluster_select_flight_8 = [14, 53, 72, 98, 58,74, 76, 33]\n",
    "cluster_select_flight_9 = [14, 53, 72, 98, 58,74, 76, 33, 64]\n",
    "cluster_select_flight_10 = [74, 76, 33, 64, 2, 14, 53, 72, 98, 58]\n",
    "cluster_select_flight_15 = [5, 85, 90, 82, 93, 74, 76, 33, 64, 2, 14, 53, 72, 98, 58]\n",
    "cluster_select_flight_20 = [32, 1, 37, 36, 89, 5, 85, 90, 82, 93, 74, 76, 33, 64, 2, 14, 53,\n",
    "            72, 98, 58]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32, 1, 37, 36, 89, 5, 85, 90, 82, 93, 74, 76, 33, 64, 2, 14, 53, 72, 98, 58]"
      ]
     },
     "execution_count": 2143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_select_flight_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tuple_id</th>\n",
       "      <th>src</th>\n",
       "      <th>flight</th>\n",
       "      <th>sched_dep_time</th>\n",
       "      <th>act_dep_time</th>\n",
       "      <th>sched_arr_time</th>\n",
       "      <th>act_arr_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:16 a.m.</td>\n",
       "      <td>9:40 a.m.</td>\n",
       "      <td>9:32 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>51</td>\n",
       "      <td>helloflight</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td></td>\n",
       "      <td>7:16 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>9:22 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>151</td>\n",
       "      <td>boston</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:16 a.m.</td>\n",
       "      <td>9:40 a.m.</td>\n",
       "      <td>9:22 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>251</td>\n",
       "      <td>weather</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>9:40 a.m.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>338</td>\n",
       "      <td>airtravelcenter</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td></td>\n",
       "      <td>7:16 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>9:22 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>438</td>\n",
       "      <td>flightview</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:16 a.m.</td>\n",
       "      <td>9:40 a.m.</td>\n",
       "      <td>9:22 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>538</td>\n",
       "      <td>flightstats</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:16 a.m.</td>\n",
       "      <td>9:40 a.m.</td>\n",
       "      <td>9:32 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>630</td>\n",
       "      <td>panynj</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:16 a.m.</td>\n",
       "      <td>9:40 a.m.</td>\n",
       "      <td>9:22 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>730</td>\n",
       "      <td>flightexplorer</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td></td>\n",
       "      <td>7:16 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>9:22 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>812</td>\n",
       "      <td>flights</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:16 a.m.</td>\n",
       "      <td>9:40 a.m.</td>\n",
       "      <td>9:32 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>912</td>\n",
       "      <td>flightarrival</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:16 a.m.</td>\n",
       "      <td>9:40 a.m.</td>\n",
       "      <td>9:22 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>970</td>\n",
       "      <td>flightwise</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:16 a.m.</td>\n",
       "      <td>9:30 a.m.</td>\n",
       "      <td>9:22 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>990</td>\n",
       "      <td>travelocity</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:02 a.m.</td>\n",
       "      <td>9:40 a.m.</td>\n",
       "      <td>9:32 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070</th>\n",
       "      <td>1071</td>\n",
       "      <td>foxbusiness</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:16 a.m.</td>\n",
       "      <td>9:40 a.m.</td>\n",
       "      <td>9:22 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>1171</td>\n",
       "      <td>usatoday</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>9:40 a.m.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1223</th>\n",
       "      <td>1224</td>\n",
       "      <td>myrateplan</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td></td>\n",
       "      <td>7:16 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>9:22 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1323</th>\n",
       "      <td>1324</td>\n",
       "      <td>orbitz</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td>7:10aDec 1</td>\n",
       "      <td>7:02aDec 1</td>\n",
       "      <td>9:40aDec 1</td>\n",
       "      <td>9:32aDec 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>1488</td>\n",
       "      <td>flytecomm</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td></td>\n",
       "      <td>7:16 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>9:22 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1637</th>\n",
       "      <td>1638</td>\n",
       "      <td>ord</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>9:40 a.m.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1661</th>\n",
       "      <td>1662</td>\n",
       "      <td>wunderground</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:16 a.m.</td>\n",
       "      <td>9:30 a.m.</td>\n",
       "      <td>9:22 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1758</th>\n",
       "      <td>1759</td>\n",
       "      <td>flylouisville</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:16 a.m.</td>\n",
       "      <td>9:40 a.m.</td>\n",
       "      <td>9:32 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1858</th>\n",
       "      <td>1859</td>\n",
       "      <td>quicktrip</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:16 a.m.</td>\n",
       "      <td>9:40 a.m.</td>\n",
       "      <td>9:32 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950</th>\n",
       "      <td>1951</td>\n",
       "      <td>allegiantair</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:16 a.m.</td>\n",
       "      <td>9:40 a.m.</td>\n",
       "      <td>9:22 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2050</th>\n",
       "      <td>2051</td>\n",
       "      <td>businesstravellogue</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:16 a.m.</td>\n",
       "      <td>9:40 a.m.</td>\n",
       "      <td>9:32 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2155</th>\n",
       "      <td>2156</td>\n",
       "      <td>gofox</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:16 a.m.</td>\n",
       "      <td>9:40 a.m.</td>\n",
       "      <td>9:22 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2255</th>\n",
       "      <td>2256</td>\n",
       "      <td>flightaware</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:16 a.m.</td>\n",
       "      <td>9:30 a.m.</td>\n",
       "      <td>9:22 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2321</th>\n",
       "      <td>2322</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:16 a.m.</td>\n",
       "      <td>9:40 a.m.</td>\n",
       "      <td>9:32 a.m.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tuple_id                   src           flight sched_dep_time  \\\n",
       "0           1                    aa  AA-3859-IAH-ORD      7:10 a.m.   \n",
       "50         51           helloflight  AA-3859-IAH-ORD                  \n",
       "150       151                boston  AA-3859-IAH-ORD      7:10 a.m.   \n",
       "250       251               weather  AA-3859-IAH-ORD      7:10 a.m.   \n",
       "337       338       airtravelcenter  AA-3859-IAH-ORD                  \n",
       "437       438            flightview  AA-3859-IAH-ORD      7:10 a.m.   \n",
       "537       538           flightstats  AA-3859-IAH-ORD      7:10 a.m.   \n",
       "629       630                panynj  AA-3859-IAH-ORD      7:10 a.m.   \n",
       "729       730        flightexplorer  AA-3859-IAH-ORD                  \n",
       "811       812               flights  AA-3859-IAH-ORD      7:10 a.m.   \n",
       "911       912         flightarrival  AA-3859-IAH-ORD      7:10 a.m.   \n",
       "969       970            flightwise  AA-3859-IAH-ORD      7:10 a.m.   \n",
       "989       990           travelocity  AA-3859-IAH-ORD      7:10 a.m.   \n",
       "1070     1071           foxbusiness  AA-3859-IAH-ORD      7:10 a.m.   \n",
       "1170     1171              usatoday  AA-3859-IAH-ORD      7:10 a.m.   \n",
       "1223     1224            myrateplan  AA-3859-IAH-ORD                  \n",
       "1323     1324                orbitz  AA-3859-IAH-ORD     7:10aDec 1   \n",
       "1487     1488             flytecomm  AA-3859-IAH-ORD                  \n",
       "1637     1638                   ord  AA-3859-IAH-ORD      7:10 a.m.   \n",
       "1661     1662          wunderground  AA-3859-IAH-ORD      7:10 a.m.   \n",
       "1758     1759         flylouisville  AA-3859-IAH-ORD      7:10 a.m.   \n",
       "1858     1859             quicktrip  AA-3859-IAH-ORD      7:10 a.m.   \n",
       "1950     1951          allegiantair  AA-3859-IAH-ORD      7:10 a.m.   \n",
       "2050     2051   businesstravellogue  AA-3859-IAH-ORD      7:10 a.m.   \n",
       "2155     2156                 gofox  AA-3859-IAH-ORD      7:10 a.m.   \n",
       "2255     2256           flightaware  AA-3859-IAH-ORD      7:10 a.m.   \n",
       "2321     2322  world-flight-tracker  AA-3859-IAH-ORD      7:10 a.m.   \n",
       "\n",
       "     act_dep_time sched_arr_time act_arr_time  \n",
       "0       7:16 a.m.      9:40 a.m.    9:32 a.m.  \n",
       "50      7:16 a.m.                   9:22 a.m.  \n",
       "150     7:16 a.m.      9:40 a.m.    9:22 a.m.  \n",
       "250                    9:40 a.m.               \n",
       "337     7:16 a.m.                   9:22 a.m.  \n",
       "437     7:16 a.m.      9:40 a.m.    9:22 a.m.  \n",
       "537     7:16 a.m.      9:40 a.m.    9:32 a.m.  \n",
       "629     7:16 a.m.      9:40 a.m.    9:22 a.m.  \n",
       "729     7:16 a.m.                   9:22 a.m.  \n",
       "811     7:16 a.m.      9:40 a.m.    9:32 a.m.  \n",
       "911     7:16 a.m.      9:40 a.m.    9:22 a.m.  \n",
       "969     7:16 a.m.      9:30 a.m.    9:22 a.m.  \n",
       "989     7:02 a.m.      9:40 a.m.    9:32 a.m.  \n",
       "1070    7:16 a.m.      9:40 a.m.    9:22 a.m.  \n",
       "1170                   9:40 a.m.               \n",
       "1223    7:16 a.m.                   9:22 a.m.  \n",
       "1323   7:02aDec 1     9:40aDec 1   9:32aDec 1  \n",
       "1487    7:16 a.m.                   9:22 a.m.  \n",
       "1637                   9:40 a.m.               \n",
       "1661    7:16 a.m.      9:30 a.m.    9:22 a.m.  \n",
       "1758    7:16 a.m.      9:40 a.m.    9:32 a.m.  \n",
       "1858    7:16 a.m.      9:40 a.m.    9:32 a.m.  \n",
       "1950    7:16 a.m.      9:40 a.m.    9:22 a.m.  \n",
       "2050    7:16 a.m.      9:40 a.m.    9:32 a.m.  \n",
       "2155    7:16 a.m.      9:40 a.m.    9:22 a.m.  \n",
       "2255    7:16 a.m.      9:30 a.m.    9:22 a.m.  \n",
       "2321    7:16 a.m.      9:40 a.m.    9:32 a.m.  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n"
     ]
    }
   ],
   "source": [
    "# flight_clean.iloc[C]\n",
    "flight_clean = pd.read_csv('datasets/flights/clean.csv').fillna('').astype(str)\n",
    "flight_dirty = pd.read_csv('datasets/flights/dirty.csv').fillna('').astype(str)\n",
    "flight_dirty_clean = flight_dirty.copy()\n",
    "def Check_Clean_Time_Format(row):\n",
    "    count = 0\n",
    "    for x,y in row[3:].items():\n",
    "        if not (is_clean_time_format(y)):\n",
    "            count += 1\n",
    "    return count\n",
    "# flight_temp = flight_dirty_10_error.copy()\n",
    "flight_temp = flight_dirty.copy()\n",
    "flight_temp['count'] = flight_temp.apply(Check_Clean_Time_Format,axis=1)\n",
    "\n",
    "# cluster_select_flight = cluster_select_flight_15\n",
    "cluster_select_flight = cluster_select_flight_20\n",
    "for i in range(100): ## Data Cleaning Clusters\n",
    "    # test = flight_dirty[flight_dirty['flight']==flight_unique[i]]\n",
    "    test = flight_temp[flight_temp['flight']==flight_unique[i]]\n",
    "    test_index = test.index\n",
    "    if(i in cluster_select_flight): ## 同cluster内有ground truth，传播结果\n",
    "        clean_cell = flight_clean[flight_clean['flight']==flight_unique[i]].iloc[0:1,-4:] ## clean time, last 4 cells\n",
    "        flight_dirty_clean.iloc[test_index,-4:] = clean_cell\n",
    "    else: ## Graph Method to Vote the most common clean files\n",
    "        try:\n",
    "            test_clean = test[test['count']==0]\n",
    "            C = CoresetIndex(test_clean)\n",
    "            clean_cell = flight_dirty.iloc[C[0],-4:] ## clean time, last 4 cells\n",
    "            flight_dirty_clean.iloc[test_index,-4:] = clean_cell\n",
    "        except:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flight_clean.iloc[C]\n",
    "flight_clean = pd.read_csv('datasets/flights/clean.csv').fillna('').astype(str)\n",
    "flight_dirty = pd.read_csv('datasets/flights/dirty.csv').fillna('').astype(str)\n",
    "flight_dirty_clean = flight_dirty.copy()\n",
    "def Check_Clean_Time_Format(row):\n",
    "    count = 0\n",
    "    for x,y in row[3:].items():\n",
    "        if not (is_clean_time_format(y)):\n",
    "            count += 1\n",
    "    return count\n",
    "# flight_temp = flight_dirty_10_error.copy()\n",
    "flight_temp = flight_dirty.copy()\n",
    "flight_temp['count'] = flight_temp.apply(Check_Clean_Time_Format,axis=1)\n",
    "\n",
    "# cluster_select_flight = cluster_select_flight_15\n",
    "cluster_select_flight = cluster_select_flight_20\n",
    "for i in range(100): ## Data Cleaning Clusters\n",
    "    # test = flight_dirty[flight_dirty['flight']==flight_unique[i]]\n",
    "    test = flight_temp[flight_temp['flight']==flight_unique[i]]\n",
    "    test_index = test.index\n",
    "    if(i in cluster_select_flight): ## 同cluster内有ground truth，传播结果\n",
    "        clean_cell = flight_clean[flight_clean['flight']==flight_unique[i]].iloc[0:1,-4:] ## clean time, last 4 cells\n",
    "        flight_dirty_clean.iloc[test_index,-4:] = clean_cell\n",
    "    else: ## Graph Method to Vote the most common clean files\n",
    "        try:\n",
    "            # test_clean = test[test['count']==0]\n",
    "            # C = CoresetIndex(test_clean)\n",
    "            # clean_cell = flight_dirty.iloc[C[0],-4:] ## clean time, last 4 cells\n",
    "            clean_cell = find_top_weighted_elements(test,a=1/64)\n",
    "            flight_dirty_clean.iloc[test_index,-4:] = pd.DataFrame(clean_cell).T\n",
    "        except:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tuple_id</th>\n",
       "      <th>src</th>\n",
       "      <th>flight</th>\n",
       "      <th>sched_dep_time</th>\n",
       "      <th>act_dep_time</th>\n",
       "      <th>sched_arr_time</th>\n",
       "      <th>act_arr_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:16 a.m.</td>\n",
       "      <td>9:40 a.m.</td>\n",
       "      <td>9:22 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-1733-ORD-PHX</td>\n",
       "      <td>7:45 p.m.</td>\n",
       "      <td>7:58 p.m.</td>\n",
       "      <td>10:30 p.m.</td>\n",
       "      <td>10:30 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-1640-MIA-MCO</td>\n",
       "      <td>6:30 p.m.</td>\n",
       "      <td>6:30 p.m.</td>\n",
       "      <td>7:25 p.m.</td>\n",
       "      <td>7:25 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-518-MIA-JFK</td>\n",
       "      <td>6:40 a.m.</td>\n",
       "      <td>6:54 a.m.</td>\n",
       "      <td>9:25 a.m.</td>\n",
       "      <td>9:28 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-3756-ORD-SLC</td>\n",
       "      <td>12:15 p.m.</td>\n",
       "      <td>12:42 p.m.</td>\n",
       "      <td>2:45 p.m.</td>\n",
       "      <td>2:50 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2371</th>\n",
       "      <td>2372</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>UA-3099-PHX-PHL</td>\n",
       "      <td>11:55 a.m.</td>\n",
       "      <td>11:55 a.m.</td>\n",
       "      <td>6:17 p.m.</td>\n",
       "      <td>5:38 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2372</th>\n",
       "      <td>2373</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-4198-ORD-CLE</td>\n",
       "      <td>10:40 a.m.</td>\n",
       "      <td>10:55 a.m.</td>\n",
       "      <td>12:55 p.m.</td>\n",
       "      <td>12:37 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2373</th>\n",
       "      <td>2374</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>CO-45-EWR-MIA</td>\n",
       "      <td>4:00 p.m.</td>\n",
       "      <td></td>\n",
       "      <td>7:05 p.m.</td>\n",
       "      <td>6:36 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2374</th>\n",
       "      <td>2375</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-3809-PHX-LAX</td>\n",
       "      <td>6:00 a.m.</td>\n",
       "      <td>6:11 a.m.</td>\n",
       "      <td>6:40 a.m.</td>\n",
       "      <td>6:19 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2375</th>\n",
       "      <td>2376</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-59-JFK-SFO</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:40 a.m.</td>\n",
       "      <td>10:45 a.m.</td>\n",
       "      <td>11:12 a.m.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2376 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     tuple_id                   src           flight sched_dep_time  \\\n",
       "0           1                    aa  AA-3859-IAH-ORD      7:10 a.m.   \n",
       "1           2                    aa  AA-1733-ORD-PHX      7:45 p.m.   \n",
       "2           3                    aa  AA-1640-MIA-MCO      6:30 p.m.   \n",
       "3           4                    aa   AA-518-MIA-JFK      6:40 a.m.   \n",
       "4           5                    aa  AA-3756-ORD-SLC     12:15 p.m.   \n",
       "...       ...                   ...              ...            ...   \n",
       "2371     2372  world-flight-tracker  UA-3099-PHX-PHL     11:55 a.m.   \n",
       "2372     2373  world-flight-tracker  AA-4198-ORD-CLE     10:40 a.m.   \n",
       "2373     2374  world-flight-tracker    CO-45-EWR-MIA      4:00 p.m.   \n",
       "2374     2375  world-flight-tracker  AA-3809-PHX-LAX      6:00 a.m.   \n",
       "2375     2376  world-flight-tracker    AA-59-JFK-SFO      7:10 a.m.   \n",
       "\n",
       "     act_dep_time sched_arr_time act_arr_time  \n",
       "0       7:16 a.m.      9:40 a.m.    9:22 a.m.  \n",
       "1       7:58 p.m.     10:30 p.m.   10:30 p.m.  \n",
       "2       6:30 p.m.      7:25 p.m.    7:25 p.m.  \n",
       "3       6:54 a.m.      9:25 a.m.    9:28 a.m.  \n",
       "4      12:42 p.m.      2:45 p.m.    2:50 p.m.  \n",
       "...           ...            ...          ...  \n",
       "2371   11:55 a.m.      6:17 p.m.    5:38 p.m.  \n",
       "2372   10:55 a.m.     12:55 p.m.   12:37 p.m.  \n",
       "2373                   7:05 p.m.    6:36 p.m.  \n",
       "2374    6:11 a.m.      6:40 a.m.    6:19 a.m.  \n",
       "2375    7:40 a.m.     10:45 a.m.   11:12 a.m.  \n",
       "\n",
       "[2376 rows x 7 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_dirty_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 1, 4]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def find_top_weighted_elements(df, a):\n",
    "    \"\"\"\n",
    "    对于给定的DataFrame，计算最后5列中前4列每个元素的加权值，并返回每列权重最高的元素。\n",
    "\n",
    "    :param df: 输入的DataFrame\n",
    "    :param a: 非零count的权重增加值\n",
    "    :return: 每列权重最高的元素列表\n",
    "    \"\"\"\n",
    "    if df.shape[1] < 5:\n",
    "        raise ValueError(\"DataFrame至少需要有5列。\")\n",
    "\n",
    "    # 提取最后5列\n",
    "    df = df.iloc[:, -5:]\n",
    "\n",
    "    # 初始化权重字典\n",
    "    weights = {col: {} for col in df.columns[:-1]}\n",
    "\n",
    "    # 遍历DataFrame的每一行\n",
    "    for _, row in df.iterrows():\n",
    "        for col in df.columns[:-1]:\n",
    "            cell_value = row[col]\n",
    "            count_value = row[df.columns[-1]]\n",
    "\n",
    "            # 根据count列的值更新权重\n",
    "            if count_value == 0:\n",
    "                weights[col][cell_value] = weights[col].get(cell_value, 0) + 1\n",
    "            else:\n",
    "                weights[col][cell_value] = weights[col].get(cell_value, 0) + a\n",
    "\n",
    "    # 找到每列权重最高的元素\n",
    "    top_elements = [max(weights[col], key=weights[col].get) for col in weights]\n",
    "\n",
    "    return top_elements\n",
    "\n",
    "# 示例DataFrame\n",
    "data = {\n",
    "    'A': [1, 2, 1, 3, 2],\n",
    "    'B': [2, 3, 4, 4, 3],\n",
    "    'C': [3, 1, 2, 1, 1],\n",
    "    'D': [4, 4, 3, 2, 4],\n",
    "    'Count': [0, 1, 0, 1, 1]\n",
    "}\n",
    "\n",
    "df_example = pd.DataFrame(data)\n",
    "\n",
    "# 测试函数\n",
    "find_top_weighted_elements(df_example, a=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9357530246141009 0.9117886178861788 0.9236154004529544\n"
     ]
    }
   ],
   "source": [
    "flight_clean = pd.read_csv('datasets/flights/clean.csv').fillna('').astype(str)\n",
    "flight_dirty = pd.read_csv('datasets/flights/dirty.csv').fillna('').astype(str)\n",
    "All_Data_Error = All_Error.sum()\n",
    "Correct_Fixed_Error = 0\n",
    "All_Fixed_Error = 0\n",
    "for x in range(All_Error.shape[0]):\n",
    "    for y in range(All_Error.shape[1]):\n",
    "        dirty_cell = flight_dirty.iloc[x,y]\n",
    "        fixed_cell = flight_dirty_clean.iloc[x,y]\n",
    "        clean_cell = flight_clean.iloc[x,y]\n",
    "        if(fixed_cell!=dirty_cell):\n",
    "            All_Fixed_Error += 1\n",
    "            if(fixed_cell==clean_cell):\n",
    "                Correct_Fixed_Error += 1\n",
    "Precision = Correct_Fixed_Error / All_Fixed_Error\n",
    "Recall = Correct_Fixed_Error / All_Data_Error\n",
    "F1 = (2*Precision*Recall) / (Precision + Recall)\n",
    "print(Precision,Recall,F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.5945884492951341 0.5315040650406504 0.5612792444730629 lambda=1/2\n",
    "0.739764052741152 0.65 0.6919831223628693 lambda=1\n",
    "0.8307797850445918 0.7384146341463415 0.7818788335306145 lambda=2\n",
    "0.8940193491644679 0.8264227642276423 0.8588931136459653 lambda=4\n",
    "0.9275021385799829 0.8815040650406504 0.9039182992913714 lambda=8\n",
    "0.9357530246141009 0.9117886178861788 0.9236154004529544 lambda=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1601,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_dirty_clean.to_csv('datasets/flights/detector/flight_correct_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flight_Result\n",
    "\n",
    "0.9808009909165979 0.9656504065040651 0.9731667349446949 20\n",
    "0.9168210829730287 0.9050813008130081 0.9109133681088268 15\n",
    "0.8734307470672978 0.8626016260162601 0.8679824112894978 10\n",
    "0.8631687242798354 0.8526422764227642 0.8578732106339467 9 \n",
    "0.8601153687680264 0.8485772357723578 0.8543073460200533 8\n",
    "0.8469934102141681 0.8359756097560975 0.8414484451718494 7 \n",
    "0.8327841845140033 0.8219512195121951 0.8273322422258592 6\n",
    "0.8184432121585541 0.8099593495934959 0.8141791807130452 5\n",
    "0.8150966680378445 0.8054878048780488 0.8102637497444286 4\n",
    "0.812037797863599 0.8034552845528455 0.8077237433592153 3\n",
    "0.8029709098411388 0.7910569105691057 0.7969693867103511 2\n",
    "0.7998347448874199 0.7869918699186992 0.7933613359286957 1\n",
    "0.7869430051813472 0.7717479674796748 0.7792714212416625 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tuple_id</th>\n",
       "      <th>src</th>\n",
       "      <th>flight</th>\n",
       "      <th>sched_dep_time</th>\n",
       "      <th>act_dep_time</th>\n",
       "      <th>sched_arr_time</th>\n",
       "      <th>act_arr_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:16 a.m.</td>\n",
       "      <td>9:40 a.m.</td>\n",
       "      <td>9:22 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-1733-ORD-PHX</td>\n",
       "      <td>7:45 p.m.</td>\n",
       "      <td>7:58 p.m.</td>\n",
       "      <td>10:30 p.m.</td>\n",
       "      <td>10:30 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-1640-MIA-MCO</td>\n",
       "      <td>6:30 p.m.</td>\n",
       "      <td>6:30 p.m.</td>\n",
       "      <td>7:25 p.m.</td>\n",
       "      <td>7:25 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-518-MIA-JFK</td>\n",
       "      <td>6:40 a.m.</td>\n",
       "      <td>6:54 a.m.</td>\n",
       "      <td>9:25 a.m.</td>\n",
       "      <td>9:28 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-3756-ORD-SLC</td>\n",
       "      <td>12:15 p.m.</td>\n",
       "      <td>12:41 p.m.</td>\n",
       "      <td>2:45 p.m.</td>\n",
       "      <td>2:50 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2371</th>\n",
       "      <td>2372</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>UA-3099-PHX-PHL</td>\n",
       "      <td>11:55 a.m.</td>\n",
       "      <td>11:55 a.m.</td>\n",
       "      <td>6:17 p.m.</td>\n",
       "      <td>5:38 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2372</th>\n",
       "      <td>2373</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-4198-ORD-CLE</td>\n",
       "      <td>10:40 a.m.</td>\n",
       "      <td>10:55 a.m.</td>\n",
       "      <td>12:55 p.m.</td>\n",
       "      <td>12:48 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2373</th>\n",
       "      <td>2374</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>CO-45-EWR-MIA</td>\n",
       "      <td>4:00 p.m.</td>\n",
       "      <td>3:58 p.m.</td>\n",
       "      <td>7:05 p.m.</td>\n",
       "      <td>6:36 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2374</th>\n",
       "      <td>2375</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-3809-PHX-LAX</td>\n",
       "      <td>6:00 a.m.</td>\n",
       "      <td>6:10 a.m.</td>\n",
       "      <td>6:40 a.m.</td>\n",
       "      <td>6:19 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2375</th>\n",
       "      <td>2376</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-59-JFK-SFO</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:39 a.m.</td>\n",
       "      <td>10:45 a.m.</td>\n",
       "      <td>11:12 a.m.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2376 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     tuple_id                   src           flight sched_dep_time  \\\n",
       "0           1                    aa  AA-3859-IAH-ORD      7:10 a.m.   \n",
       "1           2                    aa  AA-1733-ORD-PHX      7:45 p.m.   \n",
       "2           3                    aa  AA-1640-MIA-MCO      6:30 p.m.   \n",
       "3           4                    aa   AA-518-MIA-JFK      6:40 a.m.   \n",
       "4           5                    aa  AA-3756-ORD-SLC     12:15 p.m.   \n",
       "...       ...                   ...              ...            ...   \n",
       "2371     2372  world-flight-tracker  UA-3099-PHX-PHL     11:55 a.m.   \n",
       "2372     2373  world-flight-tracker  AA-4198-ORD-CLE     10:40 a.m.   \n",
       "2373     2374  world-flight-tracker    CO-45-EWR-MIA      4:00 p.m.   \n",
       "2374     2375  world-flight-tracker  AA-3809-PHX-LAX      6:00 a.m.   \n",
       "2375     2376  world-flight-tracker    AA-59-JFK-SFO      7:10 a.m.   \n",
       "\n",
       "     act_dep_time sched_arr_time act_arr_time  \n",
       "0       7:16 a.m.      9:40 a.m.    9:22 a.m.  \n",
       "1       7:58 p.m.     10:30 p.m.   10:30 p.m.  \n",
       "2       6:30 p.m.      7:25 p.m.    7:25 p.m.  \n",
       "3       6:54 a.m.      9:25 a.m.    9:28 a.m.  \n",
       "4      12:41 p.m.      2:45 p.m.    2:50 p.m.  \n",
       "...           ...            ...          ...  \n",
       "2371   11:55 a.m.      6:17 p.m.    5:38 p.m.  \n",
       "2372   10:55 a.m.     12:55 p.m.   12:48 p.m.  \n",
       "2373    3:58 p.m.      7:05 p.m.    6:36 p.m.  \n",
       "2374    6:10 a.m.      6:40 a.m.    6:19 a.m.  \n",
       "2375    7:39 a.m.     10:45 a.m.   11:12 a.m.  \n",
       "\n",
       "[2376 rows x 7 columns]"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_dirty_clean = flight_dirty_clean.iloc[:,:-1]\n",
    "flight_dirty_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(flight_dirty.iloc[:,:-1]!=flight_clean).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1, 2, 5, 31, 32, 33, 36, 37, 85, 89, 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1376,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_label_index = []\n",
    "for i in cluster_select_flight:\n",
    "    ind = flight_clean[flight_clean['flight']==flight_unique[i]].index\n",
    "    flight_label_index.append(ind[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "cluster_select_flight = [1,2,5,14,31,32,33,36,37,53,72,74,76,82,85,89,90,93,95,98]\n",
    "# cluster_select_flight_core = [1, 2, 5, 31, 32, 33, 36, 37, 85, 89, 95]\n",
    "\n",
    "# cluster_select_flight_add = np.random.choice(100,9,replace=False)\n",
    "# cluster_select_flight = np.concatenate([cluster_select_flight_core,cluster_select_flight_add])\n",
    "print(len(cluster_select_flight))\n",
    "# cluster_select_flight = [1,2,5,14,31,32,33,36,37,53,58,64,72,74,82,85,89,90,93,95,98]\n",
    "flight_clean = pd.read_csv('datasets/flights/clean.csv').fillna('').astype(str)\n",
    "flight_dirty = pd.read_csv('datasets/flights/dirty.csv').fillna('').astype(str)\n",
    "flight_query = pd.read_csv('datasets/flights/dirty_query.csv',index_col=0)\n",
    "flight_dirty.columns = flight_clean.columns\n",
    "def Check_Clean_Time_Format(row):\n",
    "    count = 0\n",
    "    for x,y in row[3:].items():\n",
    "        if not (is_clean_time_format(y)):\n",
    "            count += 1\n",
    "    return count\n",
    "flight_dirty['count'] = flight_dirty.apply(Check_Clean_Time_Format,axis=1)\n",
    "flight_dirty_clean = flight_dirty.copy()\n",
    "for i in range(100): ## Data Cleaning Clusters\n",
    "    test = flight_dirty[flight_dirty['flight']==flight_unique[i]]\n",
    "    test_index = test.index\n",
    "    if(i in cluster_select_flight): ## 同cluster内有ground truth，传播结果\n",
    "        clean_cell = flight_clean[flight_clean['flight']==flight_unique[i]].iloc[0:1,-4:] ## clean time, last 4 cells\n",
    "        flight_dirty_clean.iloc[test_index,-5:-1] = clean_cell\n",
    "    else: ## Graph Method to Vote the most common clean files\n",
    "        test_clean = test[test['count']==0]\n",
    "        C = CoresetIndex(test_clean)\n",
    "        clean_cell = flight_dirty.iloc[C[0],-5:-1] ## clean time, last 4 cells\n",
    "        flight_dirty_clean.iloc[test_index,-5:-1] = clean_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1372,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_Error = np.array(flight_clean!=flight_dirty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tuple_id</th>\n",
       "      <th>src</th>\n",
       "      <th>flight</th>\n",
       "      <th>sched_dep_time</th>\n",
       "      <th>act_dep_time</th>\n",
       "      <th>sched_arr_time</th>\n",
       "      <th>act_arr_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>81</td>\n",
       "      <td>helloflight</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>181</td>\n",
       "      <td>boston</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>276</td>\n",
       "      <td>weather</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>368</td>\n",
       "      <td>airtravelcenter</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>468</td>\n",
       "      <td>flightview</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>567</td>\n",
       "      <td>flightstats</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>660</td>\n",
       "      <td>panynj</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>749</td>\n",
       "      <td>flightexplorer</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>842</td>\n",
       "      <td>flights</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>923</td>\n",
       "      <td>flightarrival</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>1014</td>\n",
       "      <td>travelocity</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>1101</td>\n",
       "      <td>foxbusiness</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>1184</td>\n",
       "      <td>usatoday</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>1254</td>\n",
       "      <td>myrateplan</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353</th>\n",
       "      <td>1354</td>\n",
       "      <td>orbitz</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>1518</td>\n",
       "      <td>flytecomm</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>1603</td>\n",
       "      <td>mytripandmore</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1677</th>\n",
       "      <td>1678</td>\n",
       "      <td>wunderground</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1788</th>\n",
       "      <td>1789</td>\n",
       "      <td>flylouisville</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1887</th>\n",
       "      <td>1888</td>\n",
       "      <td>quicktrip</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>1981</td>\n",
       "      <td>allegiantair</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2080</th>\n",
       "      <td>2081</td>\n",
       "      <td>businesstravellogue</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2185</th>\n",
       "      <td>2186</td>\n",
       "      <td>gofox</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2271</th>\n",
       "      <td>2272</td>\n",
       "      <td>flightaware</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2337</th>\n",
       "      <td>2338</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tuple_id                   src           flight sched_dep_time  \\\n",
       "12         13                    aa  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "80         81           helloflight  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "180       181                boston  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "275       276               weather  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "367       368       airtravelcenter  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "467       468            flightview  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "566       567           flightstats  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "659       660                panynj  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "748       749        flightexplorer  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "841       842               flights  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "922       923         flightarrival  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1013     1014           travelocity  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1100     1101           foxbusiness  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1183     1184              usatoday  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1253     1254            myrateplan  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1353     1354                orbitz  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1517     1518             flytecomm  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1602     1603         mytripandmore  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1677     1678          wunderground  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1788     1789         flylouisville  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1887     1888             quicktrip  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1980     1981          allegiantair  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "2080     2081   businesstravellogue  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "2185     2186                 gofox  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "2271     2272           flightaware  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "2337     2338  world-flight-tracker  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "\n",
       "     act_dep_time sched_arr_time act_arr_time  \n",
       "12     10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "80     10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "180    10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "275    10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "367    10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "467    10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "566    10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "659    10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "748    10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "841    10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "922    10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "1013   10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "1100   10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "1183   10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "1253   10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "1353   10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "1517   10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "1602   10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "1677   10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "1788   10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "1887   10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "1980   10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "2080   10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "2185   10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "2271   10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "2337   10:18 a.m.     12:10 p.m.   11:56 a.m.  "
      ]
     },
     "execution_count": 1375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_clean[flight_dirty_clean[relation]!=flight_clean[relation]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tuple_id</th>\n",
       "      <th>src</th>\n",
       "      <th>flight</th>\n",
       "      <th>sched_dep_time</th>\n",
       "      <th>act_dep_time</th>\n",
       "      <th>sched_arr_time</th>\n",
       "      <th>act_arr_time</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>81</td>\n",
       "      <td>helloflight</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>181</td>\n",
       "      <td>boston</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>276</td>\n",
       "      <td>weather</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>368</td>\n",
       "      <td>airtravelcenter</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>468</td>\n",
       "      <td>flightview</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>567</td>\n",
       "      <td>flightstats</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>660</td>\n",
       "      <td>panynj</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>749</td>\n",
       "      <td>flightexplorer</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>842</td>\n",
       "      <td>flights</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>923</td>\n",
       "      <td>flightarrival</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>1014</td>\n",
       "      <td>travelocity</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>1101</td>\n",
       "      <td>foxbusiness</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>1184</td>\n",
       "      <td>usatoday</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>1254</td>\n",
       "      <td>myrateplan</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353</th>\n",
       "      <td>1354</td>\n",
       "      <td>orbitz</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>1518</td>\n",
       "      <td>flytecomm</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>1603</td>\n",
       "      <td>mytripandmore</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1677</th>\n",
       "      <td>1678</td>\n",
       "      <td>wunderground</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1788</th>\n",
       "      <td>1789</td>\n",
       "      <td>flylouisville</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1887</th>\n",
       "      <td>1888</td>\n",
       "      <td>quicktrip</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>1981</td>\n",
       "      <td>allegiantair</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2080</th>\n",
       "      <td>2081</td>\n",
       "      <td>businesstravellogue</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2185</th>\n",
       "      <td>2186</td>\n",
       "      <td>gofox</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2271</th>\n",
       "      <td>2272</td>\n",
       "      <td>flightaware</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2337</th>\n",
       "      <td>2338</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tuple_id                   src           flight sched_dep_time  \\\n",
       "12         13                    aa  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "80         81           helloflight  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "180       181                boston  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "275       276               weather  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "367       368       airtravelcenter  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "467       468            flightview  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "566       567           flightstats  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "659       660                panynj  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "748       749        flightexplorer  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "841       842               flights  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "922       923         flightarrival  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1013     1014           travelocity  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1100     1101           foxbusiness  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1183     1184              usatoday  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1253     1254            myrateplan  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1353     1354                orbitz  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1517     1518             flytecomm  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1602     1603         mytripandmore  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1677     1678          wunderground  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1788     1789         flylouisville  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1887     1888             quicktrip  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1980     1981          allegiantair  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "2080     2081   businesstravellogue  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "2185     2186                 gofox  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "2271     2272           flightaware  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "2337     2338  world-flight-tracker  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "\n",
       "     act_dep_time sched_arr_time act_arr_time  count  \n",
       "12     10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "80     10:19 a.m.     12:10 p.m.   11:50 a.m.      2  \n",
       "180    10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "275    10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "367    10:19 a.m.     12:10 p.m.   11:50 a.m.      2  \n",
       "467    10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "566    10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "659    10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "748    10:19 a.m.     12:10 p.m.   11:50 a.m.      2  \n",
       "841    10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "922    10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "1013   10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "1100   10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "1183   10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "1253   10:19 a.m.     12:10 p.m.   11:50 a.m.      2  \n",
       "1353   10:19 a.m.     12:10 p.m.   11:50 a.m.      3  \n",
       "1517   10:19 a.m.     12:10 p.m.   11:50 a.m.      2  \n",
       "1602   10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "1677   10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "1788   10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "1887   10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "1980   10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "2080   10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "2185   10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "2271   10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "2337   10:19 a.m.     12:10 p.m.   11:50 a.m.      0  "
      ]
     },
     "execution_count": 1374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_dirty_clean[flight_dirty_clean[relation]!=flight_clean[relation]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9832345123696585 0.9774390243902439 0.980328203037407\n"
     ]
    }
   ],
   "source": [
    "flight_clean = pd.read_csv('datasets/flights/clean.csv').fillna('').astype(str)\n",
    "flight_dirty = pd.read_csv('datasets/flights/dirty.csv').fillna('').astype(str)\n",
    "flight_query = pd.read_csv('datasets/flights/dirty_query.csv',index_col=0)\n",
    "flight_dirty.columns = flight_clean.columns\n",
    "All_Data_Error = All_Error.sum()\n",
    "Correct_Fixed_Error = 0\n",
    "All_Fixed_Error = 0\n",
    "for x in range(All_Error.shape[0]):\n",
    "    for y in range(All_Error.shape[1]):\n",
    "        dirty_cell = flight_dirty.iloc[x,y]\n",
    "        fixed_cell = flight_dirty_clean.iloc[x,y]\n",
    "        clean_cell = flight_clean.iloc[x,y]\n",
    "        if(fixed_cell!=dirty_cell):\n",
    "            All_Fixed_Error += 1\n",
    "            if(fixed_cell==clean_cell):\n",
    "                Correct_Fixed_Error += 1\n",
    "Precision = Correct_Fixed_Error / All_Fixed_Error\n",
    "Recall = Correct_Fixed_Error / All_Data_Error\n",
    "F1 = (2*Precision*Recall) / (Precision + Recall)\n",
    "print(Precision,Recall,F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Beer Dataset\n",
    "beer_clean = pd.read_csv('datasets/beers/clean.csv').fillna('')\n",
    "beer_dirty = pd.read_csv('datasets/beers/dirty.csv').fillna('')\n",
    "beer_dirty.columns = beer_clean.columns\n",
    "def try_convert_to_int(row):\n",
    "    for x,y in row.items():\n",
    "        if(x in ['ounces','ibu']):\n",
    "            try:\n",
    "                row[x] = int(y)\n",
    "            except:\n",
    "                row[x] = y\n",
    "    return row\n",
    "beer_clean = beer_clean.apply(try_convert_to_int,axis=1).astype(str)\n",
    "beer_dirty = beer_dirty.apply(try_convert_to_int,axis=1).astype(str)\n",
    "\n",
    "# beer_query = pd.read_csv('datasets/beers/dirty_query.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3357"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(beer_clean!=beer_dirty).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All_Data_Error: 3357"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[4,5,6,9,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = -1\n",
    "beer_label = beer_dirty[(beer_clean.iloc[:,9]!=beer_dirty.iloc[:,9]) & (beer_clean.iloc[:,5]!=beer_dirty.iloc[:,5])].sample(n=20)\n",
    "beer_label_index = beer_label.index\n",
    "# len(beer_dirty['brewery-name'].unique())\n",
    "# beer_dirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_matrix_beer = np.array(beer_clean!=beer_dirty)\n",
    "selected_rows_beer = beer_label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2410, 11)"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Detector Training\n",
    "input_matrix_select_beer = input_matrix_beer[selected_rows_beer]\n",
    "detector_list_beer = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "for label_tuple in selected_rows_beer:\n",
    "    for i in range(1,len(beer_clean.columns),1):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = beer_clean.iloc[label_tuple]\n",
    "        dirty_context = beer_dirty.iloc[label_tuple]\n",
    "        clean_cell = beer_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = beer_dirty.iloc[label_tuple,i]\n",
    "        single_context_clean = 'COL %s VAL %s ' % (beer_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (beer_clean.columns[i],dirty_cell)\n",
    "        for c in range(1,11,1):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (beer_clean.columns[c],beer_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (beer_dirty.columns[c],beer_dirty.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            detector_list_beer.append([all_context_dirty,single_context_dirty,1])\n",
    "            detector_list_beer.append([all_context_clean,single_context_clean,0])\n",
    "            detector_list_beer.append([all_context_dirty,single_context_clean,0])\n",
    "            # detector_list.append([single_context_dirty,1])\n",
    "        else:\n",
    "            detector_list_beer.append([all_context_dirty,single_context_dirty,0])\n",
    "            detector_list_beer.append([all_context_clean,single_context_clean,0])\n",
    "            detector_list_beer.append([all_context_dirty,single_context_clean,0])\n",
    "        # else:\n",
    "        #     detector_list.append([all_context_dirty,1])\n",
    "        #     detector_list.append([single_context_dirty,1])\n",
    "            \n",
    "                # break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa2e2bba197046278bd6bd40d822fd5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Detector Inference\n",
    "input_matrix_select_beer = input_matrix_beer[selected_rows_beer]\n",
    "detector_list_beer = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "# for label_tuple in tqdm(selected_rows):\n",
    "for label_tuple in tqdm(range(len(beer_clean))):\n",
    "    for i in range(1,len(beer_clean.columns),1):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = beer_clean.iloc[label_tuple]\n",
    "        dirty_context = beer_dirty.iloc[label_tuple]\n",
    "        clean_cell = beer_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = beer_dirty.iloc[label_tuple,i]\n",
    "        single_context_clean = 'COL %s VAL %s ' % (beer_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (beer_clean.columns[i],dirty_cell)\n",
    "        for c in range(1,11,1):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (beer_clean.columns[c],beer_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (beer_dirty.columns[c],beer_dirty.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            detector_list_beer.append([all_context_dirty,single_context_dirty,1])\n",
    "        else:\n",
    "            detector_list_beer.append([all_context_dirty,single_context_dirty,0])\n",
    "            # detector_list.append([single_context_dirty,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COL id VAL 1436 COL beer-name VAL Pub Beer COL...</td>\n",
       "      <td>COL id VAL 1436</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COL id VAL 1436 COL beer-name VAL Pub Beer COL...</td>\n",
       "      <td>COL beer-name VAL Pub Beer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COL id VAL 1436 COL beer-name VAL Pub Beer COL...</td>\n",
       "      <td>COL style VAL American Pale Lager</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COL id VAL 1436 COL beer-name VAL Pub Beer COL...</td>\n",
       "      <td>COL ounces VAL 12.0 oz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COL id VAL 1436 COL beer-name VAL Pub Beer COL...</td>\n",
       "      <td>COL abv VAL 0.05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24095</th>\n",
       "      <td>COL id VAL 84 COL beer-name VAL Rail Yard Ale ...</td>\n",
       "      <td>COL ibu VAL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24096</th>\n",
       "      <td>COL id VAL 84 COL beer-name VAL Rail Yard Ale ...</td>\n",
       "      <td>COL brewery_id VAL 424</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24097</th>\n",
       "      <td>COL id VAL 84 COL beer-name VAL Rail Yard Ale ...</td>\n",
       "      <td>COL brewery-name VAL Wynkoop Brewing Company</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24098</th>\n",
       "      <td>COL id VAL 84 COL beer-name VAL Rail Yard Ale ...</td>\n",
       "      <td>COL city VAL Denver</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24099</th>\n",
       "      <td>COL id VAL 84 COL beer-name VAL Rail Yard Ale ...</td>\n",
       "      <td>COL state VAL CO</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0  \\\n",
       "0      COL id VAL 1436 COL beer-name VAL Pub Beer COL...   \n",
       "1      COL id VAL 1436 COL beer-name VAL Pub Beer COL...   \n",
       "2      COL id VAL 1436 COL beer-name VAL Pub Beer COL...   \n",
       "3      COL id VAL 1436 COL beer-name VAL Pub Beer COL...   \n",
       "4      COL id VAL 1436 COL beer-name VAL Pub Beer COL...   \n",
       "...                                                  ...   \n",
       "24095  COL id VAL 84 COL beer-name VAL Rail Yard Ale ...   \n",
       "24096  COL id VAL 84 COL beer-name VAL Rail Yard Ale ...   \n",
       "24097  COL id VAL 84 COL beer-name VAL Rail Yard Ale ...   \n",
       "24098  COL id VAL 84 COL beer-name VAL Rail Yard Ale ...   \n",
       "24099  COL id VAL 84 COL beer-name VAL Rail Yard Ale ...   \n",
       "\n",
       "                                                   1  2  \n",
       "0                                   COL id VAL 1436   0  \n",
       "1                        COL beer-name VAL Pub Beer   0  \n",
       "2                 COL style VAL American Pale Lager   0  \n",
       "3                            COL ounces VAL 12.0 oz   1  \n",
       "4                                  COL abv VAL 0.05   0  \n",
       "...                                              ... ..  \n",
       "24095                                  COL ibu VAL    0  \n",
       "24096                        COL brewery_id VAL 424   0  \n",
       "24097  COL brewery-name VAL Wynkoop Brewing Company   0  \n",
       "24098                           COL city VAL Denver   0  \n",
       "24099                              COL state VAL CO   0  \n",
       "\n",
       "[24100 rows x 3 columns]"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector_list_beer_pd = pd.DataFrame(detector_list_beer)\n",
    "detector_list_beer_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector_list_beer_pd.to_csv('datasets/beers/detector/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_detector = np.load('datasets/beers/detector/detection.npy')\n",
    "beer_detector = beer_detector.reshape((-1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,    1,    2, ..., 2405, 2406, 2409]),)"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer_detector_coreset = beer_detector.sum(axis=1)\n",
    "np.where(beer_detector_coreset==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ICL_Beer(row):\n",
    "training_list_label = []\n",
    "for index in beer_label_index:\n",
    "    for i in range(1,11,1):\n",
    "        dirty_cell = beer_dirty.iloc[index,i]\n",
    "        clean_cell = beer_clean.iloc[index,i]\n",
    "        col_name = beer_clean.columns[i]\n",
    "        template_dict = {}\n",
    "        template_dict[col_name] = ''\n",
    "        temp_dict = beer_dirty.iloc[index,1:].to_dict()\n",
    "        clean_dict = {}\n",
    "        clean_dict[col_name] = clean_cell\n",
    "        coreset_reference = np.random.choice([c for c in beer_label_index if c!=index],3,replace=False)\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            text_head = 'You are an expert in cleaning Beers Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\nTake these rows as reference:\\n\\n' % (col_name, json.dumps(template_dict), json.dumps(temp_dict))\n",
    "            dict_0 = beer_clean.iloc[coreset_reference[0],1:].to_dict()\n",
    "            dict_1 = beer_clean.iloc[coreset_reference[1],1:].to_dict()\n",
    "            dict_2 = beer_clean.iloc[coreset_reference[2],1:].to_dict()\n",
    "            ICL_text = '%s\\n\\n%s\\n\\n%s\\n\\n' % (json.dumps(dict_0),json.dumps(dict_1),json.dumps(dict_2))\n",
    "            training_list_label.append([text_head,ICL_text,'',json.dumps(clean_dict)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in cleaning Beers Dataset. Given the dirty row Entity 1, you are required to correct the values of ounces in Entity 1.\n",
      "\n",
      "Return in json format.\n",
      "\n",
      "Output Format Example:\n",
      "\n",
      "{\"ounces\": \"\"}\n",
      "\n",
      "Entity 1:\n",
      "\n",
      "{\"id\": \"409\", \"beer-name\": \"Oaky's Oatmeal Stout\", \"style\": \"Oatmeal Stout\", \"ounces\": \"16.0 oz.\", \"abv\": \"0.047%\", \"ibu\": \"\", \"brewery_id\": \"542\", \"brewery-name\": \"Angry Minnow Brewing Company\", \"city\": \"Hayward WI\", \"state\": \"\"}\n",
      "\n",
      "Take these rows as reference:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(pd.DataFrame(training_list_label).iloc[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list_pd = pd.DataFrame(training_list_label)\n",
    "training_list_pd['instruction'] = training_list_pd[0] + training_list_pd[1]\n",
    "training_list_pd['input'] = training_list_pd[2]\n",
    "training_list_pd['output'] = training_list_pd[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"id\": \"1851\", \"beer-name\": \"Cougar\", \"style\":...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"16\"}</td>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"16\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"id\": \"1082\", \"beer-name\": \"RecreationAle\", \"...</td>\n",
       "      <td></td>\n",
       "      <td>{\"abv\": \"0.047\"}</td>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"abv\": \"0.047\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"id\": \"1082\", \"beer-name\": \"RecreationAle\", \"...</td>\n",
       "      <td></td>\n",
       "      <td>{\"city\": \"Hayward\"}</td>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"city\": \"Hayward\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"id\": \"736\", \"beer-name\": \"Ornery Amber Lager...</td>\n",
       "      <td></td>\n",
       "      <td>{\"state\": \"WI\"}</td>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"state\": \"WI\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"id\": \"1463\", \"beer-name\": \"Hideout Helles\", ...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"id\": \"938\", \"beer-name\": \"Gangway IPA\", \"sty...</td>\n",
       "      <td></td>\n",
       "      <td>{\"state\": \"OR\"}</td>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"state\": \"OR\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"id\": \"1254\", \"beer-name\": \"JP's Ould Sod Iri...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"id\": \"2619\", \"beer-name\": \"Insert Hop Refere...</td>\n",
       "      <td></td>\n",
       "      <td>{\"abv\": \"0.069\"}</td>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"abv\": \"0.069\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"id\": \"2105\", \"beer-name\": \"Even Keel\", \"styl...</td>\n",
       "      <td></td>\n",
       "      <td>{\"city\": \"Dillon\"}</td>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"city\": \"Dillon\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"id\": \"1851\", \"beer-name\": \"Cougar\", \"style\":...</td>\n",
       "      <td></td>\n",
       "      <td>{\"state\": \"CO\"}</td>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"state\": \"CO\"}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0  \\\n",
       "0   You are an expert in cleaning Beers Dataset. G...   \n",
       "1   You are an expert in cleaning Beers Dataset. G...   \n",
       "2   You are an expert in cleaning Beers Dataset. G...   \n",
       "3   You are an expert in cleaning Beers Dataset. G...   \n",
       "4   You are an expert in cleaning Beers Dataset. G...   \n",
       "..                                                ...   \n",
       "75  You are an expert in cleaning Beers Dataset. G...   \n",
       "76  You are an expert in cleaning Beers Dataset. G...   \n",
       "77  You are an expert in cleaning Beers Dataset. G...   \n",
       "78  You are an expert in cleaning Beers Dataset. G...   \n",
       "79  You are an expert in cleaning Beers Dataset. G...   \n",
       "\n",
       "                                                    1 2                     3  \\\n",
       "0   {\"id\": \"1851\", \"beer-name\": \"Cougar\", \"style\":...        {\"ounces\": \"16\"}   \n",
       "1   {\"id\": \"1082\", \"beer-name\": \"RecreationAle\", \"...        {\"abv\": \"0.047\"}   \n",
       "2   {\"id\": \"1082\", \"beer-name\": \"RecreationAle\", \"...     {\"city\": \"Hayward\"}   \n",
       "3   {\"id\": \"736\", \"beer-name\": \"Ornery Amber Lager...         {\"state\": \"WI\"}   \n",
       "4   {\"id\": \"1463\", \"beer-name\": \"Hideout Helles\", ...        {\"ounces\": \"12\"}   \n",
       "..                                                ... ..                  ...   \n",
       "75  {\"id\": \"938\", \"beer-name\": \"Gangway IPA\", \"sty...         {\"state\": \"OR\"}   \n",
       "76  {\"id\": \"1254\", \"beer-name\": \"JP's Ould Sod Iri...        {\"ounces\": \"12\"}   \n",
       "77  {\"id\": \"2619\", \"beer-name\": \"Insert Hop Refere...        {\"abv\": \"0.069\"}   \n",
       "78  {\"id\": \"2105\", \"beer-name\": \"Even Keel\", \"styl...      {\"city\": \"Dillon\"}   \n",
       "79  {\"id\": \"1851\", \"beer-name\": \"Cougar\", \"style\":...         {\"state\": \"CO\"}   \n",
       "\n",
       "                                          instruction input  \\\n",
       "0   You are an expert in cleaning Beers Dataset. G...         \n",
       "1   You are an expert in cleaning Beers Dataset. G...         \n",
       "2   You are an expert in cleaning Beers Dataset. G...         \n",
       "3   You are an expert in cleaning Beers Dataset. G...         \n",
       "4   You are an expert in cleaning Beers Dataset. G...         \n",
       "..                                                ...   ...   \n",
       "75  You are an expert in cleaning Beers Dataset. G...         \n",
       "76  You are an expert in cleaning Beers Dataset. G...         \n",
       "77  You are an expert in cleaning Beers Dataset. G...         \n",
       "78  You are an expert in cleaning Beers Dataset. G...         \n",
       "79  You are an expert in cleaning Beers Dataset. G...         \n",
       "\n",
       "                 output  \n",
       "0      {\"ounces\": \"16\"}  \n",
       "1      {\"abv\": \"0.047\"}  \n",
       "2   {\"city\": \"Hayward\"}  \n",
       "3       {\"state\": \"WI\"}  \n",
       "4      {\"ounces\": \"12\"}  \n",
       "..                  ...  \n",
       "75      {\"state\": \"OR\"}  \n",
       "76     {\"ounces\": \"12\"}  \n",
       "77     {\"abv\": \"0.069\"}  \n",
       "78   {\"city\": \"Dillon\"}  \n",
       "79      {\"state\": \"CO\"}  \n",
       "\n",
       "[80 rows x 7 columns]"
      ]
     },
     "execution_count": 626,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_list_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_list_pd\n",
    "json.dump(training_list_pd.iloc[:,-3:].to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/beer/beer-train.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Data on Hospital\n",
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 16])"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector_indice[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc39f8d9e3c4e879b5678ce3557249e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/496 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 检查所有detector要求detect的元素，组成一个list，查找并附加coreset\n",
    "# for d in detector\n",
    "detector_indice = np.argwhere(detector==1)\n",
    "detector_list_all = []\n",
    "candidate_length = {}\n",
    "right_loc = {}\n",
    "training_list_label = []\n",
    "for d in tqdm(detector_indice):\n",
    "    \n",
    "    label_tuple = d[0] ## 行\n",
    "    i = d[1] ## 列\n",
    "    col_name = hospital_clean.columns[i] ## 列名\n",
    "    all_context_clean = ''\n",
    "    all_context_dirty = ''\n",
    "    # clean_context = hospital_clean.iloc[label_tuple]\n",
    "    dirty_context = hospital_dirty.iloc[label_tuple]\n",
    "    clean_cell = hospital_clean.iloc[label_tuple,i]\n",
    "    dirty_cell = hospital_dirty.iloc[label_tuple,i]\n",
    "    \n",
    "    template_dict = {}\n",
    "    template_dict[col_name] = ''\n",
    "    temp_dict = hospital_dirty_dict[label_tuple + 1]\n",
    "    \n",
    "    clean_dict = {}\n",
    "    clean_dict[col_name] = clean_cell\n",
    "    # single_context_clean = 'COL %s VAL %s ' % (hospital_clean.columns[i],clean_cell)\n",
    "    # single_context_dirty = 'COL %s VAL %s ' % (hospital_clean.columns[i],dirty_cell)\n",
    "    # if(clean_cell!=dirty_cell):\n",
    "    text_head = 'You are an expert in Cleaning Hospital Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\nTake these rows as reference:\\n\\n' % (col_name, json.dumps(template_dict), json.dumps(temp_dict))\n",
    "    cluster = [hospital_cluster[h] for h in hospital_cluster if hospital_cluster[h].__contains__(label_tuple)][0] ## 找到label tuple所在的簇\n",
    "    cluster_coreset = [c for c in cluster if c in coreset_detect] ## 找到簇内coreset元素\n",
    "    coreset_reference = np.random.choice(cluster_coreset,1,replace=False) ## 取两个\n",
    "    # ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]), json.dumps(hospital_dirty_dict[coreset_reference[1]]))\n",
    "    ICL_text = '%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]))\n",
    "    training_list_label.append([text_head,ICL_text,'',json.dumps(clean_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pd.DataFrame(training_list_label).iloc[0,3])\n",
    "training_list_pd = pd.DataFrame(training_list_label)\n",
    "training_list_pd['instruction'] = training_list_pd[0] + training_list_pd[1]\n",
    "training_list_pd['input'] = training_list_pd[2]\n",
    "training_list_pd['output'] = training_list_pd[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(training_list_pd.iloc[:,-3:].to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/hospital/hospital-test.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "496"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training_list_pd\n",
    "len(detector_indice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'ProviderNumber', 'HospitalName', 'Address1', 'Address2',\n",
       "       'Address3', 'City', 'State', 'ZipCode', 'CountyName', 'PhoneNumber',\n",
       "       'HospitalType', 'HospitalOwner', 'EmergencyService', 'Condition',\n",
       "       'MeasureCode', 'MeasureName', 'Score', 'Sample', 'Stateavg'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hospital_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      scip-card-2\n",
       "1       scip-inf-1\n",
       "2       scip-inf-2\n",
       "3       scip-inf-3\n",
       "4       scip-inf-4\n",
       "          ...     \n",
       "995           pn-6\n",
       "996           pn-7\n",
       "997    scip-card-2\n",
       "998     scip-inf-1\n",
       "999     scip-inf-2\n",
       "Name: MeasureCode, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hospital_clean['MeasureCode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"MeasureName\": \"surgery patients who were tak...</td>\n",
       "      <td>{\"MeasureName\": \"surgery patients who were tak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"City\": \"birmingham\"}</td>\n",
       "      <td>{\"City\": \"birmingham\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"City\": \"birmingham\"}</td>\n",
       "      <td>{\"City\": \"birmingham\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"City\": \"sheffield\"}</td>\n",
       "      <td>{\"City\": \"sheffield\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ProviderNumber\": \"10019\"}</td>\n",
       "      <td>{\"ProviderNumber\": \"10019\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"HospitalType\": \"acute care hospitals\"}</td>\n",
       "      <td>{\"HospitalType\": \"acute care hospitals\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"City\": \"oneonta\"}</td>\n",
       "      <td>{\"City\": \"oneonta\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"Address1\": \"150 gilbreath drive\"}</td>\n",
       "      <td>{\"Address1\": \"150 gilbreath drive\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"HospitalName\": \"st vincents blount\"}</td>\n",
       "      <td>{\"HospitalName\": \"st vincents bount\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"MeasureName\": \"surgery patients who were tak...</td>\n",
       "      <td>{\"MeasureName\": \"surgery patients who were tak...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>508 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           instruction  input  \\\n",
       "0    You are an expert in Cleaning Hospital Dataset...    NaN   \n",
       "1    You are an expert in Cleaning Hospital Dataset...    NaN   \n",
       "2    You are an expert in Cleaning Hospital Dataset...    NaN   \n",
       "3    You are an expert in Cleaning Hospital Dataset...    NaN   \n",
       "4    You are an expert in Cleaning Hospital Dataset...    NaN   \n",
       "..                                                 ...    ...   \n",
       "503  You are an expert in Cleaning Hospital Dataset...    NaN   \n",
       "504  You are an expert in Cleaning Hospital Dataset...    NaN   \n",
       "505  You are an expert in Cleaning Hospital Dataset...    NaN   \n",
       "506  You are an expert in Cleaning Hospital Dataset...    NaN   \n",
       "507  You are an expert in Cleaning Hospital Dataset...    NaN   \n",
       "\n",
       "                                                output  \\\n",
       "0    {\"MeasureName\": \"surgery patients who were tak...   \n",
       "1                               {\"City\": \"birmingham\"}   \n",
       "2                               {\"City\": \"birmingham\"}   \n",
       "3                                {\"City\": \"sheffield\"}   \n",
       "4                          {\"ProviderNumber\": \"10019\"}   \n",
       "..                                                 ...   \n",
       "503           {\"HospitalType\": \"acute care hospitals\"}   \n",
       "504                                {\"City\": \"oneonta\"}   \n",
       "505                {\"Address1\": \"150 gilbreath drive\"}   \n",
       "506             {\"HospitalName\": \"st vincents blount\"}   \n",
       "507  {\"MeasureName\": \"surgery patients who were tak...   \n",
       "\n",
       "                                               predict  \n",
       "0    {\"MeasureName\": \"surgery patients who were tak...  \n",
       "1                               {\"City\": \"birmingham\"}  \n",
       "2                               {\"City\": \"birmingham\"}  \n",
       "3                                {\"City\": \"sheffield\"}  \n",
       "4                          {\"ProviderNumber\": \"10019\"}  \n",
       "..                                                 ...  \n",
       "503           {\"HospitalType\": \"acute care hospitals\"}  \n",
       "504                                {\"City\": \"oneonta\"}  \n",
       "505                {\"Address1\": \"150 gilbreath drive\"}  \n",
       "506              {\"HospitalName\": \"st vincents bount\"}  \n",
       "507  {\"MeasureName\": \"surgery patients who were tak...  \n",
       "\n",
       "[508 rows x 4 columns]"
      ]
     },
     "execution_count": 2249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hospital_result = pd.read_csv('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/hospital-test.csv',index_col=0)\n",
    "hospital_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "508"
      ]
     },
     "execution_count": 2251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.load('/home/yanmy/raha/raha-master/datasets/hospital/detector/detector_5.npy')\n",
    "len(np.argwhere(a.reshape((1000,-1))==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 2246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hospital_detector = np.load('/home/yanmy/raha/raha-master/datasets/hospital/detector/detection.npy').reshape((1000,-1))\n",
    "hospital_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "496"
      ]
     },
     "execution_count": 2248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(hospital_detector==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "hospital_correction = hospital_dirty.iloc[:,:-1]\n",
    "import ast\n",
    "for d in np.argwhere(hospital_detector==1):\n",
    "    i = d[0]\n",
    "    j = d[1]\n",
    "    predict = list(ast.literal_eval(hospital_result.iloc[count,-1]).values())[0]\n",
    "    hospital_correction.iloc[i,j] = predict\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp /home/yanmy/raha/raha-master/datasets/flights/vary_error_rate/50/flight_dirty_50_error_correction.csv flights/correction/result/correction-error-50.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_correction.to_csv('/home/yanmy/raha/raha-master/datasets/hospital/correct_result/hospital_correction_20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1624,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_correction = pd.read_csv('/home/yanmy/raha/raha-master/datasets/hospital/correct_result/hospital_correction_20.csv',index_col=0).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.where(input_matrix_hospital[hospital_label_index[:1]].sum(axis=0)!=0)\n",
    "b = np.where(input_matrix_hospital[hospital_label_index[:20]].sum(axis=0)!=0)\n",
    "[i for i in b[0] if i not in a[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1652,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8993839835728953, 0.8605108055009824, 0.8795180722891566)"
      ]
     },
     "execution_count": 1652,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hospital_correction = pd.read_csv('/home/yanmy/raha/raha-master/datasets/hospital/correct_result/hospital_correction_20.csv',index_col=0).astype(str)\n",
    "All_Data_Error = 0\n",
    "All_Fixed_Error = 0\n",
    "Correct_Fixed_Error = 0\n",
    "# hospital_correction.iloc[:,3] = hospital_dirty.iloc[:,3]\n",
    "# hospital_correction.iloc[:,14] = hospital_dirty.iloc[:,14]\n",
    "a = np.where(input_matrix_hospital[hospital_label_index[:9]].sum(axis=0)!=0)\n",
    "b = np.where(input_matrix_hospital[hospital_label_index[:20]].sum(axis=0)!=0)\n",
    "for h in [i for i in b[0] if i not in a[0]]:\n",
    "    hospital_correction.iloc[:,h] = hospital_dirty.iloc[:,h]\n",
    "\n",
    "for i in range(1000):\n",
    "    for j in range(20):\n",
    "        dirty_cell = hospital_dirty.iloc[i,j]\n",
    "        clean_cell = hospital_clean.iloc[i,j]\n",
    "        correct_cell = hospital_correction.iloc[i,j]\n",
    "        if(correct_cell!=dirty_cell):\n",
    "            All_Fixed_Error += 1\n",
    "        if(clean_cell!=dirty_cell):\n",
    "            All_Data_Error += 1\n",
    "            if(correct_cell==clean_cell):\n",
    "                Correct_Fixed_Error += 1\n",
    "Precision_hospital = Correct_Fixed_Error / All_Fixed_Error\n",
    "Recall_hospital = Correct_Fixed_Error / All_Data_Error\n",
    "F1_hospital = (2 * Precision_hospital * Recall_hospital) / (Precision_hospital + Recall_hospital)\n",
    "Precision_hospital,Recall_hospital,F1_hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.8461538461538461, 0.23772102161100198, 0.37116564417177916) 1\n",
    "(0.8942731277533039, 0.3988212180746562, 0.5516304347826088) 2\n",
    "(0.9025270758122743, 0.4911591355599214, 0.6361323155216284) 3\n",
    "(0.9095890410958904, 0.6522593320235757, 0.759725400457666) 4\n",
    "(0.892271662763466, 0.7485265225933202, 0.8141025641025642) 5\n",
    "(0.8971553610503282, 0.8055009823182712, 0.8488612836438924) 6 \n",
    "(0.8971553610503282, 0.8055009823182712, 0.8488612836438924) 7 \n",
    "(0.8971553610503282, 0.8055009823182712, 0.8488612836438924) 8\n",
    "(0.8993839835728953, 0.8605108055009824, 0.8795180722891566) 9\n",
    "(0.8993839835728953, 0.8605108055009824, 0.8795180722891566) 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1627,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([532, 157, 213, 228, 238, 256, 332, 386, 411, 493, 572, 698, 956,\n",
       "        22,  24,  42,  56,  57,  93,  94])"
      ]
     },
     "execution_count": 1627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hospital_label_index = np.load('datasets/hospital/detector/index.npy')\n",
    "hospital_label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1638,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1,  2,  8, 11, 12, 13, 16, 18, 19]),)"
      ]
     },
     "execution_count": 1638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_matrix_hospital = np.array(hospital_clean!=hospital_dirty).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1642,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 6, 7, 8, 9, 10, 12, 13, 14, 17, 19]"
      ]
     },
     "execution_count": 1642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.where(input_matrix_hospital[hospital_label_index[:1]].sum(axis=0)!=0)\n",
    "b = np.where(input_matrix_hospital[hospital_label_index[:20]].sum(axis=0)!=0)\n",
    "[i for i in b[0] if i not in a[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1551,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"abv\": \"0.09\"}</td>\n",
       "      <td>{\"abv\": \"0.09\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3352</th>\n",
       "      <td>3352</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3353</th>\n",
       "      <td>3353</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"abv\": \"0.055\"}</td>\n",
       "      <td>{\"abv\": \"0.055\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3354</th>\n",
       "      <td>3354</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3355</th>\n",
       "      <td>3355</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"abv\": \"0.055\"}</td>\n",
       "      <td>{\"abv\": \"0.055\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3356</th>\n",
       "      <td>3356</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3357 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                        instruction  input  \\\n",
       "0              0  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "1              1  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "2              2  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "3              3  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "4              4  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "...          ...                                                ...    ...   \n",
       "3352        3352  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "3353        3353  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "3354        3354  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "3355        3355  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "3356        3356  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "\n",
       "                output           predict  \n",
       "0     {\"ounces\": \"12\"}  {\"ounces\": \"12\"}  \n",
       "1     {\"ounces\": \"12\"}  {\"ounces\": \"12\"}  \n",
       "2     {\"ounces\": \"12\"}  {\"ounces\": \"12\"}  \n",
       "3     {\"ounces\": \"12\"}  {\"ounces\": \"12\"}  \n",
       "4      {\"abv\": \"0.09\"}   {\"abv\": \"0.09\"}  \n",
       "...                ...               ...  \n",
       "3352  {\"ounces\": \"12\"}  {\"ounces\": \"12\"}  \n",
       "3353  {\"abv\": \"0.055\"}  {\"abv\": \"0.055\"}  \n",
       "3354  {\"ounces\": \"12\"}  {\"ounces\": \"12\"}  \n",
       "3355  {\"abv\": \"0.055\"}  {\"abv\": \"0.055\"}  \n",
       "3356  {\"ounces\": \"12\"}  {\"ounces\": \"12\"}  \n",
       "\n",
       "[3357 rows x 5 columns]"
      ]
     },
     "execution_count": 1551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer_result = pd.read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"Stateavg\": \"al_scip-vte-2\"}</td>\n",
       "      <td>{\"Stateavg\": \"al_scipxvtex2\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"Stateavg\": \"al_scip-vte-1\"}</td>\n",
       "      <td>{\"Stateavg\": \"al_scipxvtex1\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"Stateavg\": \"al_scip-inf-2\"}</td>\n",
       "      <td>{\"Stateavg\": \"al_scipxinfx2\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"Stateavg\": \"al_scip-vte-1\"}</td>\n",
       "      <td>{\"Stateavg\": \"al_sci-vte-1\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"Stateavg\": \"al_pn-6\"}</td>\n",
       "      <td>{\"Stateavg\": \"al_pnx6\"}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           instruction  input  \\\n",
       "169  You are an expert in Cleaning Hospital Dataset...    NaN   \n",
       "293  You are an expert in Cleaning Hospital Dataset...    NaN   \n",
       "391  You are an expert in Cleaning Hospital Dataset...    NaN   \n",
       "394  You are an expert in Cleaning Hospital Dataset...    NaN   \n",
       "426  You are an expert in Cleaning Hospital Dataset...    NaN   \n",
       "\n",
       "                            output                        predict  \n",
       "169  {\"Stateavg\": \"al_scip-vte-2\"}  {\"Stateavg\": \"al_scipxvtex2\"}  \n",
       "293  {\"Stateavg\": \"al_scip-vte-1\"}  {\"Stateavg\": \"al_scipxvtex1\"}  \n",
       "391  {\"Stateavg\": \"al_scip-inf-2\"}  {\"Stateavg\": \"al_scipxinfx2\"}  \n",
       "394  {\"Stateavg\": \"al_scip-vte-1\"}   {\"Stateavg\": \"al_sci-vte-1\"}  \n",
       "426        {\"Stateavg\": \"al_pn-6\"}        {\"Stateavg\": \"al_pnx6\"}  "
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(hospital_result[hospital_result['output']!=hospital_result['predict'])\n",
    "hospital_result[(hospital_result['output'].str.contains('Stateavg')) & (hospital_result['output']!=hospital_result['predict'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "509"
      ]
     },
     "execution_count": 632,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum(input_matrix)\n",
    "input_matrix.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8850806451612904, 0.862475442043222, 0.87363184079602)"
      ]
     },
     "execution_count": 634,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Precision_hospital = (496-57) / 496\n",
    "Recall_hospital = (496-57) / input_matrix.sum()\n",
    "F1_hospital = (2 * Precision_hospital * Recall_hospital) / (Precision_hospital + Recall_hospital)\n",
    "Precision_hospital,Recall_hospital,F1_hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.8850806451612904, 0.862475442043222, 0.87363184079602) Error_Correction_HOSPITAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"MeasureName\": \"surgery patients who were taking heart blockers before coming to the hospital who were kept on the beta blockers during the period just before and after their surgery\"}'"
      ]
     },
     "execution_count": 600,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(detector_indice)\n",
    "hospital_result.iloc[495,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on Beers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    3],\n",
       "       [   1,    3],\n",
       "       [   2,    3],\n",
       "       ...,\n",
       "       [2408,    3],\n",
       "       [2408,    4],\n",
       "       [2409,    3]])"
      ]
     },
     "execution_count": 610,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer_detector_dirty = np.argwhere(beer_detector==1)\n",
    "beer_detector_dirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3342"
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(beer_detector_dirty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ICL_Beer(row):\n",
    "training_list_label = []\n",
    "for d in beer_detector_dirty:\n",
    "    index = d[0]\n",
    "    i = d[1] + 1 ## 重要，我们在detector中忽略了index列\n",
    "    dirty_cell = beer_dirty.iloc[index,i]\n",
    "    clean_cell = beer_clean.iloc[index,i]\n",
    "    col_name = beer_clean.columns[i]\n",
    "    template_dict = {}\n",
    "    template_dict[col_name] = ''\n",
    "    temp_dict = beer_dirty.iloc[index,1:].to_dict()\n",
    "    clean_dict = {}\n",
    "    clean_dict[col_name] = clean_cell\n",
    "    coreset_reference = np.random.choice([c for c in beer_label_index if c!=index],3,replace=False)\n",
    "    # if(dirty_cell!=clean_cell): ## 在LLM Inference中忽略\n",
    "    text_head = 'You are an expert in cleaning Beers Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\nTake these rows as reference:\\n\\n' % (col_name, json.dumps(template_dict), json.dumps(temp_dict))\n",
    "    dict_0 = beer_clean.iloc[coreset_reference[0],1:].to_dict()\n",
    "    dict_1 = beer_clean.iloc[coreset_reference[1],1:].to_dict()\n",
    "    dict_2 = beer_clean.iloc[coreset_reference[2],1:].to_dict()\n",
    "    ICL_text = '%s\\n\\n%s\\n\\n%s\\n\\n' % (json.dumps(dict_0),json.dumps(dict_1),json.dumps(dict_2))\n",
    "    training_list_label.append([text_head,ICL_text,'',json.dumps(clean_dict)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(training_list_label)\n",
    "training_list_pd = pd.DataFrame(training_list_label)\n",
    "training_list_pd['instruction'] = training_list_pd[0] + training_list_pd[1]\n",
    "training_list_pd['input'] = training_list_pd[2]\n",
    "training_list_pd['output'] = training_list_pd[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(training_list_pd.iloc[:,-3:].to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/beer/beer-test.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1552,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"abv\": \"0.09\"}</td>\n",
       "      <td>{\"abv\": \"0.09\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3359</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3360</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"abv\": \"0.055\"}</td>\n",
       "      <td>{\"abv\": \"0.055\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3361</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3362</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"abv\": \"0.055\"}</td>\n",
       "      <td>{\"abv\": \"0.055\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3363</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3364 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            instruction  input  \\\n",
       "0     You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "1     You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "2     You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "3     You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "4     You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "...                                                 ...    ...   \n",
       "3359  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "3360  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "3361  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "3362  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "3363  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "\n",
       "                output           predict  \n",
       "0     {\"ounces\": \"12\"}  {\"ounces\": \"12\"}  \n",
       "1     {\"ounces\": \"12\"}  {\"ounces\": \"12\"}  \n",
       "2     {\"ounces\": \"12\"}  {\"ounces\": \"12\"}  \n",
       "3     {\"ounces\": \"12\"}  {\"ounces\": \"12\"}  \n",
       "4      {\"abv\": \"0.09\"}   {\"abv\": \"0.09\"}  \n",
       "...                ...               ...  \n",
       "3359  {\"ounces\": \"12\"}  {\"ounces\": \"12\"}  \n",
       "3360  {\"abv\": \"0.055\"}  {\"abv\": \"0.055\"}  \n",
       "3361  {\"ounces\": \"12\"}  {\"ounces\": \"12\"}  \n",
       "3362  {\"abv\": \"0.055\"}  {\"abv\": \"0.055\"}  \n",
       "3363  {\"ounces\": \"12\"}  {\"ounces\": \"12\"}  \n",
       "\n",
       "[3364 rows x 4 columns]"
      ]
     },
     "execution_count": 1552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer_result = pd.read_csv('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/beer-test-20.csv',index_col=0)\n",
    "beer_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1553,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ast.literal_eval()\n",
    "import ast\n",
    "beer_result['item'] = ''\n",
    "beer_result['gt'] = ''\n",
    "for index,row in beer_result.iterrows():\n",
    "    temp_dict = ast.literal_eval(row[3])\n",
    "    gt_dict = ast.literal_eval(row[2])\n",
    "    beer_result.iloc[index,-2] = list(temp_dict.values())[0]\n",
    "    beer_result.iloc[index,-1] = list(gt_dict.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1576,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_beer.shape\n",
    "count = 0\n",
    "beer_correct_20 = beer_dirty.copy()\n",
    "for d in np.argwhere(detector_beer==1):\n",
    "    i = d[0] \n",
    "    j = d[1] + 2\n",
    "    predict = beer_result.iloc[count,-2]\n",
    "    beer_correct_20.iloc[i,j] = predict\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.09'"
      ]
     },
     "execution_count": 724,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# beer_result\n",
    "# float(0.055)<1\n",
    "def TypeTransfer(content):\n",
    "    # print(content)\n",
    "    try:\n",
    "        content_test = float(content)\n",
    "        if(content_test<1):\n",
    "            # print('case 0')\n",
    "            return str(content)\n",
    "        else:\n",
    "            # print('case 1')\n",
    "            return str(int(content_test))\n",
    "    except:\n",
    "        try:\n",
    "            content_test = int(content)\n",
    "            return str(content_test)\n",
    "        except:\n",
    "            print(content)\n",
    "            return content\n",
    "a = TypeTransfer(beer_result.iloc[4,-2])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_result['item_output'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TypeTransfer(beer_result.iloc[0,-2])\n",
    "# # beer_result.iloc[0,-2]\n",
    "# beer_result['item_output']  = beer_result['item'].apply(TypeTransfer,axis=1)\n",
    "for index,row in beer_result.iterrows():\n",
    "    output = TypeTransfer(beer_result.iloc[index,-3])\n",
    "    beer_result.iloc[index,-1] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 3310)"
      ]
     },
     "execution_count": 732,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# beer_result[beer_result['gt']!=beer_result['item_output']].iloc[100:150,-2:]\n",
    "len(beer_result[beer_result['gt']!=beer_result['item_output']].iloc[:,-2:]),len(beer_result[beer_result['gt']==beer_result['item_output']].iloc[:,-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3357"
      ]
     },
     "execution_count": 733,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Correct_Fixed_Error_beer = 3310\n",
    "All_Fixed_Error_beer = 3342\n",
    "All_Data_Error_beer = input_matrix_beer.sum()\n",
    "All_Data_Error_beer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.990424895272292, 0.9859994042299672, 0.9882071951037469)"
      ]
     },
     "execution_count": 735,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Precision = Correct_Fixed_Error_beer / All_Fixed_Error_beer\n",
    "Recall = Correct_Fixed_Error_beer / All_Data_Error_beer\n",
    "F1_beer = (2 * Precision * Recall) / (Precision + Recall)\n",
    "Precision,Recall,F1_beer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.990424895272292, 0.9859994042299672, 0.9882071951037469) Beers P/R/F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_dirty.iloc[beer_label_index,4].to_list(),beer_clean.iloc[beer_label_index,4].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_cell(cell):\n",
    "    if re.search(r'^\\d+(\\.?\\d+)? oz\\.?$', cell.value):\n",
    "        cleaned_value = float(re.search(r'^\\d+(\\.?\\d+)?', cell).group(0))\n",
    "        return cleaned_value\n",
    "    else:\n",
    "        return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/yanmy/raha/raha-master/detector.ipynb 单元格 139\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y255sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m clean_cell(\u001b[39m'\u001b[39;49m\u001b[39m12.0 oz\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/home/yanmy/raha/raha-master/detector.ipynb 单元格 139\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y255sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclean_cell\u001b[39m(cell):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y255sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mif\u001b[39;00m re\u001b[39m.\u001b[39msearch(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m^\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+(\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.?\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+)? oz\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.?$\u001b[39m\u001b[39m'\u001b[39m, cell\u001b[39m.\u001b[39;49mvalue):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y255sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m         cleaned_value \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(re\u001b[39m.\u001b[39msearch(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m^\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+(\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.?\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+)?\u001b[39m\u001b[39m'\u001b[39m, cell)\u001b[39m.\u001b[39mgroup(\u001b[39m0\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y255sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m cleaned_value\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "clean_cell('12.0 oz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/yanmy/raha/raha-master/detector.ipynb 单元格 130\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y330sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m dirty_cells \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39m16.0 oz.\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y330sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m'\u001b[39m\u001b[39m12.0 oz\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y330sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m'\u001b[39m\u001b[39m12.0 oz.\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y330sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m'\u001b[39m\u001b[39m16.0 oz.\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y330sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m'\u001b[39m\u001b[39m12.0 oz.\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y330sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m clean_cells \u001b[39m=\u001b[39m [clean_cell(cell) \u001b[39mfor\u001b[39;00m cell \u001b[39min\u001b[39;00m dirty_cells]\n",
      "\u001b[1;32m/home/yanmy/raha/raha-master/detector.ipynb 单元格 130\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y330sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m dirty_cells \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39m16.0 oz.\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y330sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m'\u001b[39m\u001b[39m12.0 oz\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y330sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m'\u001b[39m\u001b[39m12.0 oz.\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y330sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m'\u001b[39m\u001b[39m16.0 oz.\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y330sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m'\u001b[39m\u001b[39m12.0 oz.\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y330sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m clean_cells \u001b[39m=\u001b[39m [clean_cell(cell) \u001b[39mfor\u001b[39;00m cell \u001b[39min\u001b[39;00m dirty_cells]\n",
      "\u001b[1;32m/home/yanmy/raha/raha-master/detector.ipynb 单元格 130\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y330sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclean_cell\u001b[39m(cell):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y330sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mif\u001b[39;00m re\u001b[39m.\u001b[39msearch(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m^\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+(\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.?\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+)? oz\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.?$\u001b[39m\u001b[39m'\u001b[39m, cell\u001b[39m.\u001b[39;49mvalue):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y330sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m         cleaned_value \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(re\u001b[39m.\u001b[39msearch(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m^\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+(\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.?\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+)?\u001b[39m\u001b[39m'\u001b[39m, cell)\u001b[39m.\u001b[39mgroup(\u001b[39m0\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y330sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m cleaned_value\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "dirty_cells = ['16.0 oz.',\n",
    "'12.0 oz',\n",
    "'12.0 oz.',\n",
    "'16.0 oz.',\n",
    "'12.0 oz',\n",
    "'12.0 oz',\n",
    "'12.0 ounce',\n",
    "'12.0 ounce',\n",
    "'12.0 oz',\n",
    "'12.0 oz.',\n",
    "'12.0 oz.',\n",
    "'12.0 oz.',\n",
    "'12.0 oz.',\n",
    "'16.0 oz.',\n",
    "'12.0 oz',\n",
    "'12.0 ounce',\n",
    "'12.0 oz. Alumi-Tek',\n",
    "'12.0 oz.',\n",
    "'16.0 oz.',\n",
    "'12.0 oz.']\n",
    "\n",
    "clean_cells = [clean_cell(cell) for cell in dirty_cells]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input ['16.0 oz.', '12.0 oz', '12.0 oz.', '12.0 ounce',\n",
    "        '12.0 oz. Alumi-Tek'] are some dirty cells from table Beers-ounce, and the input ['16',\n",
    "'12',\n",
    "'12',\n",
    "'12',\n",
    "'12'] are corresponding clean ones, please write a function to correct dirty cell to clean ones, and correct the following cells: ['12.0 oz', '12.0 oz.', '12.0 ounce', '12.0 OZ.',\n",
    "'12.0 oz. Alumi-Tek', '8.4 ounce', '16.0 ounce', '16.0 oz.',\n",
    "'16.0 oz. Alumi-Tek', '16.0 oz', '16.0 OZ.', '12.0 oz. Silo Can',\n",
    "'16.0 oz. Silo Can', '24.0 oz.', '24.0 oz', '19.2 oz.', '19.2 oz',\n",
    "'24.0 ounce', '24.0 oz. Alumi-Tek', '32.0 oz. Alumi-Tek',\n",
    "'32.0 OZ.', '32.0 oz.', '19.2 oz. Alumi-Tek', '19.2 OZ.',\n",
    "'16.9 OZ.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 12.0 oz\n",
      "Corrected: 12\n",
      "\n",
      "Original: 12.0 oz.\n",
      "Corrected: 12\n",
      "\n",
      "Original: 12.0 ounce\n",
      "Corrected: 12\n",
      "\n",
      "Original: 12.0 OZ.\n",
      "Corrected: 12\n",
      "\n",
      "Original: 12.0 oz. Alumi-Tek\n",
      "Corrected: 12\n",
      "\n",
      "Original: 8.4 ounce\n",
      "Corrected: 8.4\n",
      "\n",
      "Original: 16.0 ounce\n",
      "Corrected: 16\n",
      "\n",
      "Original: 16.0 oz.\n",
      "Corrected: 16\n",
      "\n",
      "Original: 16.0 oz. Alumi-Tek\n",
      "Corrected: 16\n",
      "\n",
      "Original: 16.0 oz\n",
      "Corrected: 16\n",
      "\n",
      "Original: 16.0 OZ.\n",
      "Corrected: 16\n",
      "\n",
      "Original: 12.0 oz. Silo Can\n",
      "Corrected: 12\n",
      "\n",
      "Original: 16.0 oz. Silo Can\n",
      "Corrected: 16\n",
      "\n",
      "Original: 24.0 oz.\n",
      "Corrected: 24\n",
      "\n",
      "Original: 24.0 oz\n",
      "Corrected: 24\n",
      "\n",
      "Original: 19.2 oz.\n",
      "Corrected: 19.2\n",
      "\n",
      "Original: 19.2 oz\n",
      "Corrected: 19.2\n",
      "\n",
      "Original: 24.0 ounce\n",
      "Corrected: 24\n",
      "\n",
      "Original: 24.0 oz. Alumi-Tek\n",
      "Corrected: 24\n",
      "\n",
      "Original: 32.0 oz. Alumi-Tek\n",
      "Corrected: 32\n",
      "\n",
      "Original: 32.0 OZ.\n",
      "Corrected: 32\n",
      "\n",
      "Original: 32.0 oz.\n",
      "Corrected: 32\n",
      "\n",
      "Original: 19.2 oz. Alumi-Tek\n",
      "Corrected: 19.2\n",
      "\n",
      "Original: 19.2 OZ.\n",
      "Corrected: 19.2\n",
      "\n",
      "Original: 16.9 OZ.\n",
      "Corrected: 16.9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def correct_dirty_cell(cell):\n",
    "    # Regular expression to extract the numeric value from the cell\n",
    "    match = re.search(r'(\\d+(\\.\\d+)?)', cell)\n",
    "    if match:\n",
    "        value = match.group(1)\n",
    "        # Convert to integer string if the value ends with \".0\"\n",
    "        if value.endswith('.0'):\n",
    "            return str(int(float(value)))\n",
    "        return value\n",
    "    return cell  # If no match is found, return the original cell\n",
    "\n",
    "# Test\n",
    "cells_to_correct = ['12.0 oz', '12.0 oz.', '12.0 ounce', '12.0 OZ.',\n",
    "'12.0 oz. Alumi-Tek', '8.4 ounce', '16.0 ounce', '16.0 oz.',\n",
    "'16.0 oz. Alumi-Tek', '16.0 oz', '16.0 OZ.', '12.0 oz. Silo Can',\n",
    "'16.0 oz. Silo Can', '24.0 oz.', '24.0 oz', '19.2 oz.', '19.2 oz',\n",
    "'24.0 ounce', '24.0 oz. Alumi-Tek', '32.0 oz. Alumi-Tek',\n",
    "'32.0 OZ.', '32.0 oz.', '19.2 oz. Alumi-Tek', '19.2 OZ.',\n",
    "'16.9 OZ.']\n",
    "\n",
    "for cell in cells_to_correct:\n",
    "    print(f\"Original: {cell}\")\n",
    "    print(f\"Corrected: {correct_dirty_cell(cell)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_cells(cell):\n",
    "    # Remove any extraneous characters (e.g. periods, spaces)\n",
    "    cell = cell.strip()\n",
    "\n",
    "    # Convert to lowercase\n",
    "    cell = cell.lower()\n",
    "\n",
    "    # Remove any units that are not 'oz' or 'ounce'\n",
    "    cell = re.sub(r'(?<!\\boz\\b|\\bone\\b)(\\d+(\\.\\d+)?)', r'\\1', cell)\n",
    "\n",
    "    # Convert to integer\n",
    "    cell = int(cell)\n",
    "\n",
    "    return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[correct_cells(c) for c in ['12.0 oz', '12.0 oz.', '12.0 ounce', '12.0 OZ.',\n",
    "'12.0 oz. Alumi-Tek', '8.4 ounce', '16.0 ounce', '16.0 oz.',\n",
    "'16.0 oz. Alumi-Tek', '16.0 oz', '16.0 OZ.', '12.0 oz. Silo Can',\n",
    "'16.0 oz. Silo Can', '24.0 oz.', '24.0 oz', '19.2 oz.', '19.2 oz',\n",
    "'24.0 ounce', '24.0 oz. Alumi-Tek', '32.0 oz. Alumi-Tek',\n",
    "'32.0 OZ.', '32.0 oz.', '19.2 oz. Alumi-Tek', '19.2 OZ.',\n",
    "'16.9 OZ.']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rayyan\n",
    "rayyan_clean = pd.read_csv('datasets/rayyan/clean.csv').fillna('')\n",
    "rayyan_dirty = pd.read_csv('datasets/rayyan/dirty.csv').fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "rayyan_clean = pd.read_csv('datasets/rayyan/clean.csv').fillna('')\n",
    "rayyan_dirty = pd.read_csv('datasets/rayyan/dirty.csv').fillna('')\n",
    "def Str2Int(row):\n",
    "    for index in range(11):\n",
    "        temp = row[index]\n",
    "        try:\n",
    "            row[index] = str(int(temp))\n",
    "        except:\n",
    "            continue\n",
    "    return row\n",
    "rayyan_clean = rayyan_clean.apply(Str2Int,axis=1)\n",
    "rayyan_dirty = rayyan_dirty.apply(Str2Int,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                                 659297\n",
       "article_title           How assistive technology use by individuals wi...\n",
       "article_language                                                      eng\n",
       "journal_title           American Journal Of Physical Medicine & Rehabi...\n",
       "jounral_abbreviation                                Am J Phys Med Rehabil\n",
       "journal_issn                                                    1537-7385\n",
       "article_jvolumn                                                        91\n",
       "article_jissue                                                         11\n",
       "article_jcreated_at                                               1/12/11\n",
       "article_pagination                                                 984-98\n",
       "author_list             {\"James Lenker\",\"W Ben Mortenson\",\"Frank DeRuy...\n",
       "Name: 52, dtype: object"
      ]
     },
     "execution_count": 798,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_clean.iloc[52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 946,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix_rayyan = np.array(rayyan_dirty!=rayyan_clean).astype(int)\n",
    "input_matrix_rayyan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "948"
      ]
     },
     "execution_count": 947,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix_rayyan.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1191,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_matrix_tax = np.array(tax_clean!=tax_dirty).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所选行的索引: [19039, 3555, 4677, 4682, 4684, 4694, 5952, 9924, 10273, 10304, 10347, 12263, 17614, 17657, 17730]\n"
     ]
    }
   ],
   "source": [
    "### From here, we find the label index\n",
    "import numpy as np\n",
    "\n",
    "def find_max_coverage(matrix):\n",
    "    # 转置矩阵以便按列计算列和\n",
    "    transposed_matrix = np.transpose(matrix)\n",
    "    \n",
    "    # 初始化一个列表，用于记录每列的和以及列的索引\n",
    "    column_sums = [(sum(column), index) for index, column in enumerate(transposed_matrix)]\n",
    "    \n",
    "    # 按列和降序排序\n",
    "    column_sums.sort(reverse=True)\n",
    "    \n",
    "    selected_rows = []\n",
    "    selected_columns = set()\n",
    "    \n",
    "    for _, column_index in column_sums:\n",
    "        # 如果所选列已经包含了这一列，跳过\n",
    "        if column_index in selected_columns:\n",
    "            continue\n",
    "        \n",
    "        # 找到可以添加的行\n",
    "        best_row = None\n",
    "        best_row_sum = -1\n",
    "        \n",
    "        for row_index, row in enumerate(matrix):\n",
    "            if row_index in selected_rows:\n",
    "                continue\n",
    "            \n",
    "            # 计算将此行添加到已选行中后的行之和\n",
    "            new_row_sum = sum(row)\n",
    "            \n",
    "            if new_row_sum > best_row_sum:\n",
    "                best_row_sum = new_row_sum\n",
    "                best_row = row_index\n",
    "        \n",
    "        # 如果找到了合适的行，添加它\n",
    "        if best_row is not None:\n",
    "            selected_rows.append(best_row)\n",
    "            selected_columns.update([column_index])\n",
    "            \n",
    "            # 如果已经选择的行数超过20，停止\n",
    "            if len(selected_rows) >= 20:\n",
    "                break\n",
    "    \n",
    "    return selected_rows\n",
    "\n",
    "# 示例用法\n",
    "if __name__ == \"__main__\":\n",
    "    # matrix = np.random.randint(2, size=(1000, 20))  # 随机生成一个1000x20的二进制矩阵\n",
    "    selected_rows_tax = find_max_coverage(input_matrix_tax)\n",
    "    print(\"所选行的索引:\", selected_rows_tax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([357, 130, 532, 212, 694, 924, 918])"
      ]
     },
     "execution_count": 778,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(selected_rows_rayyan)\n",
    "np.random.choice(np.where((pd.Series(input_matrix_rayyan.sum(axis=1))==4)==1)[0],7,replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[158, 455] 6\n",
    "[85, 118, 322, 384, 392, 615, 656, 796, 862, 975, 979] 5\n",
    "[357, 130, 532, 212, 694, 924, 918] 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 801,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_label_index = [158, 455,85, 118, 322, 384, 392, 615, 656, 796, 862, 975, 979, 357, 130, 532, 212, 694, 924, 918]\n",
    "len(rayyan_label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [],
   "source": [
    "rayyan_dirty.to_csv('datasets/rayyan/dirty_process.csv')\n",
    "rayyan_clean.to_csv('datasets/rayyan/clean_process.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [],
   "source": [
    "rayyan_dirty_process = rayyan_dirty\n",
    "rayyan_clean_process = rayyan_clean\n",
    "rayyan_dirty_process['index'] = rayyan_dirty_process.index\n",
    "rayyan_clean_process['index'] = rayyan_clean_process.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rayyan_dirty['index'] = rayyan_dirty['index'].astype(int)\n",
    "# hospital_clean['index'] = hospital_clean['index'].astype(int)\n",
    "rayyan_dirty_dict = rayyan_dirty_process.set_index('index').to_dict('index')\n",
    "rayyan_clean_dict = rayyan_clean_process.set_index('index').to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rayyan Detector Training\n",
    "\n",
    "input_matrix_select_rayyan = input_matrix_rayyan[rayyan_label_index]\n",
    "detector_list_rayyan = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "for label_tuple in rayyan_label_index:\n",
    "    for i in range(1,len(rayyan_clean.columns),1):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = rayyan_clean.iloc[label_tuple]\n",
    "        dirty_context = rayyan_dirty.iloc[label_tuple]\n",
    "        clean_cell = rayyan_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = rayyan_dirty.iloc[label_tuple,i]\n",
    "        single_context_clean = 'COL %s VAL %s ' % (rayyan_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (rayyan_clean.columns[i],dirty_cell)\n",
    "        for c in range(1,11,1):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (rayyan_clean.columns[c],rayyan_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (rayyan_dirty.columns[c],rayyan_dirty.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            detector_list_rayyan.append([all_context_dirty,single_context_dirty,1])\n",
    "            detector_list_rayyan.append([all_context_clean,single_context_clean,0])\n",
    "            detector_list_rayyan.append([all_context_dirty,single_context_clean,0])\n",
    "            # detector_list.append([single_context_dirty,1])\n",
    "        else:\n",
    "            detector_list_rayyan.append([all_context_dirty,single_context_dirty,0])\n",
    "            detector_list_rayyan.append([all_context_clean,single_context_clean,0])\n",
    "            detector_list_rayyan.append([all_context_dirty,single_context_clean,0])\n",
    "        # else:\n",
    "        #     detector_list.append([all_context_dirty,1])\n",
    "        #     detector_list.append([single_context_dirty,1])\n",
    "            \n",
    "                # break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_rayyan).to_csv('datasets/rayyan/detector/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 963,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd98757b5da344f39959f45412686422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_matrix_select_rayyan = input_matrix_rayyan[rayyan_label_index]\n",
    "detector_list_rayyan = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "# for label_tuple in tqdm(selected_rows):\n",
    "for label_tuple in tqdm(range(len(rayyan_clean))):\n",
    "    for i in range(1,len(rayyan_clean.columns)-1,1):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = rayyan_clean.iloc[label_tuple]\n",
    "        dirty_context = rayyan_dirty.iloc[label_tuple]\n",
    "        clean_cell = rayyan_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = rayyan_dirty.iloc[label_tuple,i]\n",
    "        single_context_clean = 'COL %s VAL %s ' % (rayyan_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (rayyan_clean.columns[i],dirty_cell)\n",
    "        for c in range(1,11,1):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (rayyan_clean.columns[c],rayyan_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (rayyan_dirty.columns[c],rayyan_dirty.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            detector_list_rayyan.append([all_context_dirty,single_context_dirty,1])\n",
    "        else:\n",
    "            detector_list_rayyan.append([all_context_dirty,single_context_dirty,0])\n",
    "            # detector_list.append([single_context_dirty,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 962,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "948"
      ]
     },
     "execution_count": 962,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(detector_list_rayyan)[2].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_rayyan).to_csv('datasets/rayyan/detector/test_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [],
   "source": [
    "rayyan_label_dirty = rayyan_dirty.iloc[rayyan_label_index]\n",
    "rayyan_label_clean = rayyan_clean.iloc[rayyan_label_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Paliativn�_ Schanzova osteotomie p��i nereponibiln�_ luxaci ky��eln�_ho kloubu u pacient�� s d��tskou mozkovou obrnou v adolescentn�_m v��u',\n",
       " 'Nowoczesne biomateria��y jako opatrunki hemostatyczne w chirurgii oszczedzajacej miazsz nerki--model zwierzecy. Doniesienie wstepne.',\n",
       " 'G̩riatrie et Psychologie Neuropsychiatrie du Vieillissement',\n",
       " 'Actas espa̱olas de psiquiatr�_a',\n",
       " 'Cirug�_a Pedi��trica: Organo Oficial De La Sociedad Espa̱ola De Cirug�_a Pedi��trica']"
      ]
     },
     "execution_count": 836,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_label_dirty[rayyan_label_dirty['journal_title']!=rayyan_label_clean['journal_title']]['journal_title'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Paliativn_ Schanzova osteotomie p_i nereponibiln_ luxaci kyeln_ho kloubu u pacient s d_tskou mozkovou obrnou v adolescentn_m v_u',\n",
       " 'Nowoczesne biomateria_y jako opatrunki hemostatyczne w chirurgii oszczedzajacej miazsz nerki--model zwierzecy. Doniesienie wstepne.',\n",
       " 'Griatrie et Psychologie Neuropsychiatrie du Vieillissement',\n",
       " 'Actas espaolas de psiquiatr_a',\n",
       " 'Cirug_a Peditrica: Organo Oficial De La Sociedad Espaola De Cirug_a Peditrica']"
      ]
     },
     "execution_count": 837,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_label_clean[rayyan_label_dirty['journal_title']!=rayyan_label_clean['journal_title']]['journal_title'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input ['Paliativn�_ Schanzova osteotomie p��i nereponibiln�_ luxaci ky��eln�_ho kloubu u pacient�� s d��tskou mozkovou obrnou v adolescentn�_m v��u',\n",
    " 'Nowoczesne biomateria��y jako opatrunki hemostatyczne w chirurgii oszczedzajacej miazsz nerki--model zwierzecy. Doniesienie wstepne.',\n",
    " 'G̩riatrie et Psychologie Neuropsychiatrie du Vieillissement',\n",
    " 'Actas espa̱olas de psiquiatr�_a',\n",
    " 'Cirug�_a Pedi��trica: Organo Oficial De La Sociedad Espa̱ola De Cirug�_a Pedi��trica'] are some dirty cells from table rayyan column journal_title, and the input ['Paliativn_ Schanzova osteotomie p_i nereponibiln_ luxaci kyeln_ho kloubu u pacient s d_tskou mozkovou obrnou v adolescentn_m v_u',\n",
    " 'Nowoczesne biomateria_y jako opatrunki hemostatyczne w chirurgii oszczedzajacej miazsz nerki--model zwierzecy. Doniesienie wstepne.',\n",
    " 'Griatrie et Psychologie Neuropsychiatrie du Vieillissement',\n",
    " 'Actas espaolas de psiquiatr_a',\n",
    " 'Cirug_a Peditrica: Organo Oficial De La Sociedad Espaola De Cirug_a Peditrica'] are corresponding corrected ones, please write a general function to detect whether a given cell is dirty or not, and correct the following dirty cells: ['12.0 oz', '12.0 oz.', '12.0 ounce', '12.0 OZ.',\n",
    "'12.0 oz. Alumi-Tek', '8.4 ounce', '16.0 ounce', '16.0 oz.',\n",
    "'16.0 oz. Alumi-Tek', '16.0 oz', '16.0 OZ.', '12.0 oz. Silo Can',\n",
    "'16.0 oz. Silo Can', '24.0 oz.', '24.0 oz', '19.2 oz.', '19.2 oz',\n",
    "'24.0 ounce', '24.0 oz. Alumi-Tek', '32.0 oz. Alumi-Tek',\n",
    "'32.0 OZ.', '32.0 oz.', '19.2 oz. Alumi-Tek', '19.2 OZ.',\n",
    "'16.9 OZ.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 905,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "def is_dirty_cell(cell):\n",
    "    # cell = row['journal_title']\n",
    "    # 设置脏字符黑名单，这些字符不在黑名单中被认为是干净的\n",
    "    dirty_chars = set([\"�\", \"̩\", \"̨\", \"̦\", \"̬\", \"̯\", \"̺\", \"̖\", \"̗\", \"̪\", \"̟\", \"̥\", \"̟\", \"̝\", \"̞\", \"̙\", \"̜\", \"̲\", \"̯\", \"̼\", \"̩\"])\n",
    "    \n",
    "    # 检查单元格中是否包含脏字符\n",
    "    for char in cell:\n",
    "        if char in dirty_chars:\n",
    "            return True\n",
    "    return False\n",
    "is_dirty_cell(\"Paliativn�_ Schanzova osteotomie p��i nereponibiln�_ luxaci ky��eln�_ho kloubu u pacient�� s d��tskou mozkovou obrnou v adolescentn�_m v��u\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,  14,   0,   9,   0,  12,  22,  53, 722,  32,  84])"
      ]
     },
     "execution_count": 883,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix_rayyan.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1000,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['52',\n",
       " '28',\n",
       " '0',\n",
       " '904',\n",
       " '24',\n",
       " '3',\n",
       " '88',\n",
       " '64',\n",
       " '8',\n",
       " '15',\n",
       " '90',\n",
       " '34',\n",
       " '34',\n",
       " '27',\n",
       " '42',\n",
       " '71',\n",
       " '0',\n",
       " '10',\n",
       " '71',\n",
       " '21',\n",
       " '70',\n",
       " '76',\n",
       " '91',\n",
       " '13',\n",
       " '90',\n",
       " '71',\n",
       " '18',\n",
       " '7',\n",
       " '0',\n",
       " '22']"
      ]
     },
     "execution_count": 1000,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_dirty.iloc[30:60,6].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1051,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The input \\n\\n['51-5', '70-6', '93-6', '3111-9', '91-8', '72-7', '71-7', '3541-4']\\n\\nare some dirty cells from table rayyan column article_pagination, and the input \\n\\n['May-51', 'Jun-70', 'Jun-93', '11-Sep', 'Aug-91', 'Jul-72', 'Jul-71', 'Apr-41']\\n\\n are corrected clean cells, and ['256-62', '1937-1958', '323-330', '196-203', '167-72', '3017-28', '373-6', '1182-7', '80-96', '603-10', '248-54', '277-80'] are examples of corresponding corrected clean cells. please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\""
      ]
     },
     "execution_count": 1051,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_name = rayyan_label_clean.columns[9]\n",
    "clean_list = rayyan_label_clean[rayyan_label_dirty[col_name]!=rayyan_label_clean[col_name]][col_name].to_list()\n",
    "dirty_list = rayyan_label_dirty[rayyan_label_dirty[col_name]!=rayyan_label_clean[col_name]][col_name].to_list()\n",
    "clean_list_origin = rayyan_label_clean[rayyan_label_dirty[col_name]==rayyan_label_clean[col_name]][col_name].to_list()\n",
    "# clean_list = rayyan_label_clean[col_name].to_list()\n",
    "# dirty_list = rayyan_label_dirty[col_name].to_list()\n",
    "# clean_list\n",
    "# detector_inference = \"The input \\n\\n%s\\n\\nare some dirty cells from table rayyan column %s, and the input \\n\\n%s\\n\\n are corresponding clean ones, please write a general function to detect whether a given cell is dirty or not\" % (dirty_list,col_name,clean_list)\n",
    "detector_inference = \"The input \\n\\n%s\\n\\nare some dirty cells from table rayyan column %s, and the input \\n\\n%s\\n\\n are corrected clean cells, and %s are examples of corresponding corrected clean cells. please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (dirty_list,col_name,clean_list,clean_list_origin)\n",
    "# print(detector_inference)\n",
    "detector_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1068,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1068,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_dirty_cell('1937-1958')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1077,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'05-51'"
      ]
     },
     "execution_count": 1077,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = '51-5'\n",
    "clean_dirty_cell(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1070,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Cells:\n",
      "Cell 1: May-51\n",
      "Cell 2: Jun-70\n",
      "Cell 3: Jun-93\n",
      "Cell 4: 11-Sep\n",
      "Cell 5: Aug-91\n",
      "Cell 6: Jul-72\n",
      "Cell 7: Jul-71\n",
      "Cell 8: Apr-41\n",
      "Cell 9: 1937-1958\n",
      "Cell 10: 05-51\n",
      "Cell 11: 06-70\n",
      "Cell 12: 06-93\n",
      "Cell 13: 09-3111\n",
      "Cell 14: 08-91\n",
      "Cell 15: 07-72\n",
      "Cell 16: 07-71\n",
      "Cell 17: 04-3541\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def is_dirty_cell(cell):\n",
    "    # Define a regular expression pattern for dirty cells (digits-digits)\n",
    "    dirty_pattern = r'^\\d+-\\d+$'\n",
    "    \n",
    "    # Use the re.match function to check if the cell matches the dirty pattern\n",
    "    return bool(re.match(dirty_pattern, cell))\n",
    "\n",
    "def clean_dirty_cell(dirty_cell):\n",
    "    # Try to clean the dirty cell\n",
    "    if is_dirty_cell(dirty_cell):\n",
    "        parts = dirty_cell.split('-')\n",
    "        if len(parts) == 2:\n",
    "            if parts[0].isdigit() and parts[1].isdigit():\n",
    "                return f\"{parts[1].zfill(2)}-{parts[0].zfill(2)}\"\n",
    "    \n",
    "    # If it cannot be cleaned, it's considered clean\n",
    "    return dirty_cell\n",
    "\n",
    "# Example usage:\n",
    "dirty_cells = ['51-5', '70-6', '93-6', '3111-9', '91-8', '72-7', '71-7', '3541-4']\n",
    "clean_cells = ['May-51', 'Jun-70', 'Jun-93', '11-Sep', 'Aug-91', 'Jul-72', 'Jul-71', 'Apr-41', '1937-1958']\n",
    "\n",
    "# Clean the dirty cells and append them to the clean cells list\n",
    "for dirty_cell in dirty_cells:\n",
    "    cleaned_cell = clean_dirty_cell(dirty_cell)\n",
    "    clean_cells.append(cleaned_cell)\n",
    "\n",
    "print(\"Clean Cells:\")\n",
    "for i, cell in enumerate(clean_cells):\n",
    "    print(f'Cell {i + 1}: {cell}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1078,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>article_title</th>\n",
       "      <th>article_language</th>\n",
       "      <th>journal_title</th>\n",
       "      <th>jounral_abbreviation</th>\n",
       "      <th>journal_issn</th>\n",
       "      <th>article_jvolumn</th>\n",
       "      <th>article_jissue</th>\n",
       "      <th>article_jcreated_at</th>\n",
       "      <th>article_pagination</th>\n",
       "      <th>author_list</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>729712</td>\n",
       "      <td>Long-term effects of spontaneous breathing dur...</td>\n",
       "      <td>English; ABSTRACT LANGUAGE:English</td>\n",
       "      <td>American journal of respiratory and critical c...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>164</td>\n",
       "      <td>1</td>\n",
       "      <td>7/1/01</td>\n",
       "      <td>43-49</td>\n",
       "      <td>{\"St��_ber F\",\"Mutz N\",\"Wrigge H\",\"Putensen C\"...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>660168</td>\n",
       "      <td>Fat and neurosurgery: does obesity affect outc...</td>\n",
       "      <td>eng</td>\n",
       "      <td>Neurosurgery</td>\n",
       "      <td>Neurosurgery</td>\n",
       "      <td>1524-4040</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>2/1/09</td>\n",
       "      <td>316-26; discussion 326-7</td>\n",
       "      <td>{\"R Loch Macdonald\",\"Farbod Asgarzadie-Gadim\",...</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>318931</td>\n",
       "      <td>Community associated methicillin-resistant Sta...</td>\n",
       "      <td></td>\n",
       "      <td>AIDS Reviews</td>\n",
       "      <td></td>\n",
       "      <td>1139-6121</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>1/1/10</td>\n",
       "      <td>153-163</td>\n",
       "      <td>{\"M. Pujol\",\"P. Barrag��n\",\"J.M. Tiraboschi\",\"...</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>349150</td>\n",
       "      <td>The relationship between capsulorhexis size an...</td>\n",
       "      <td>eng</td>\n",
       "      <td>Ophthalmic surgery and lasers</td>\n",
       "      <td>Ophthalmic Surg Lasers</td>\n",
       "      <td>1082-3069</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>3/1/99</td>\n",
       "      <td>185-90</td>\n",
       "      <td>{\"O Ceki̤\",\"C Batman\"}</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>826030</td>\n",
       "      <td>A cognitive behavior intervention program in w...</td>\n",
       "      <td>eng</td>\n",
       "      <td>European journal of cardiovascular nursing : j...</td>\n",
       "      <td>Eur J Cardiovasc Nurs</td>\n",
       "      <td>1474-5151</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1/1/12</td>\n",
       "      <td>183-9</td>\n",
       "      <td>{\"Cecilia Bj̦rkelund\",\"Margaretha Jerlock\"}</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>83012</td>\n",
       "      <td>Type I and II endometrial cancers: have they d...</td>\n",
       "      <td>eng</td>\n",
       "      <td>Journal of clinical oncology : official journa...</td>\n",
       "      <td>J. Clin. Oncol.</td>\n",
       "      <td>1527-7755</td>\n",
       "      <td>31</td>\n",
       "      <td>20</td>\n",
       "      <td>7/10/13</td>\n",
       "      <td>2607-18</td>\n",
       "      <td>{\"Chu Chen\",\"Brian L Strom\",\"Susan E McCann\",\"...</td>\n",
       "      <td>931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>134738</td>\n",
       "      <td>Are stretches effective in the prevention and ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1/1/12</td>\n",
       "      <td>261-270 ST  - Are stretches effective in the p...</td>\n",
       "      <td>{\"Stuart Calver\",\"Julia Nichols\",\"Rachel C1  -...</td>\n",
       "      <td>949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>215820</td>\n",
       "      <td>Preval̻ncia de bruxismo e dist̼rbio do sono em...</td>\n",
       "      <td>pt</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>159-166</td>\n",
       "      <td>{\"Ana Paula de Lima Ferreira\",\"Marcelo de Souz...</td>\n",
       "      <td>952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>575935</td>\n",
       "      <td>The association between urinary cortisol excre...</td>\n",
       "      <td>eng</td>\n",
       "      <td>Steroids</td>\n",
       "      <td>Steroids</td>\n",
       "      <td>1878-5867</td>\n",
       "      <td>101</td>\n",
       "      <td></td>\n",
       "      <td>9/1/15</td>\n",
       "      <td>71-7</td>\n",
       "      <td>{\"Kerstin Landin-Wilhelmsen\",\"G̦ran Oler̦d\",\"O...</td>\n",
       "      <td>975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>84783</td>\n",
       "      <td>Mild endoplasmic reticulum stress augments the...</td>\n",
       "      <td>eng</td>\n",
       "      <td>Endocrinology</td>\n",
       "      <td>Endocrinology</td>\n",
       "      <td>1945-7170</td>\n",
       "      <td>153</td>\n",
       "      <td>7</td>\n",
       "      <td>7/1/12</td>\n",
       "      <td>3017-28</td>\n",
       "      <td>{\"Michela Miani\",\"Decio L Eizirik\",\"Laurence L...</td>\n",
       "      <td>979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                      article_title  \\\n",
       "7    729712  Long-term effects of spontaneous breathing dur...   \n",
       "37   660168  Fat and neurosurgery: does obesity affect outc...   \n",
       "67   318931  Community associated methicillin-resistant Sta...   \n",
       "69   349150  The relationship between capsulorhexis size an...   \n",
       "71   826030  A cognitive behavior intervention program in w...   \n",
       "..      ...                                                ...   \n",
       "931   83012  Type I and II endometrial cancers: have they d...   \n",
       "949  134738  Are stretches effective in the prevention and ...   \n",
       "952  215820  Preval̻ncia de bruxismo e dist̼rbio do sono em...   \n",
       "975  575935  The association between urinary cortisol excre...   \n",
       "979   84783  Mild endoplasmic reticulum stress augments the...   \n",
       "\n",
       "                       article_language  \\\n",
       "7    English; ABSTRACT LANGUAGE:English   \n",
       "37                                  eng   \n",
       "67                                        \n",
       "69                                  eng   \n",
       "71                                  eng   \n",
       "..                                  ...   \n",
       "931                                 eng   \n",
       "949                                       \n",
       "952                                  pt   \n",
       "975                                 eng   \n",
       "979                                 eng   \n",
       "\n",
       "                                         journal_title  \\\n",
       "7    American journal of respiratory and critical c...   \n",
       "37                                        Neurosurgery   \n",
       "67                                        AIDS Reviews   \n",
       "69                       Ophthalmic surgery and lasers   \n",
       "71   European journal of cardiovascular nursing : j...   \n",
       "..                                                 ...   \n",
       "931  Journal of clinical oncology : official journa...   \n",
       "949                                                      \n",
       "952                                                      \n",
       "975                                           Steroids   \n",
       "979                                      Endocrinology   \n",
       "\n",
       "       jounral_abbreviation journal_issn article_jvolumn article_jissue  \\\n",
       "7                                                    164              1   \n",
       "37             Neurosurgery    1524-4040              64              2   \n",
       "67                             1139-6121              12              3   \n",
       "69   Ophthalmic Surg Lasers    1082-3069              30              3   \n",
       "71    Eur J Cardiovasc Nurs    1474-5151              11              2   \n",
       "..                      ...          ...             ...            ...   \n",
       "931         J. Clin. Oncol.    1527-7755              31             20   \n",
       "949                                                   17              0   \n",
       "952                                                   26              1   \n",
       "975                Steroids    1878-5867             101                  \n",
       "979           Endocrinology    1945-7170             153              7   \n",
       "\n",
       "    article_jcreated_at                                 article_pagination  \\\n",
       "7                7/1/01                                              43-49   \n",
       "37               2/1/09                           316-26; discussion 326-7   \n",
       "67               1/1/10                                            153-163   \n",
       "69               3/1/99                                             185-90   \n",
       "71               1/1/12                                              183-9   \n",
       "..                  ...                                                ...   \n",
       "931             7/10/13                                            2607-18   \n",
       "949              1/1/12  261-270 ST  - Are stretches effective in the p...   \n",
       "952                                                                159-166   \n",
       "975              9/1/15                                               71-7   \n",
       "979              7/1/12                                            3017-28   \n",
       "\n",
       "                                           author_list  index  \n",
       "7    {\"St��_ber F\",\"Mutz N\",\"Wrigge H\",\"Putensen C\"...      7  \n",
       "37   {\"R Loch Macdonald\",\"Farbod Asgarzadie-Gadim\",...     37  \n",
       "67   {\"M. Pujol\",\"P. Barrag��n\",\"J.M. Tiraboschi\",\"...     67  \n",
       "69                              {\"O Ceki̤\",\"C Batman\"}     69  \n",
       "71         {\"Cecilia Bj̦rkelund\",\"Margaretha Jerlock\"}     71  \n",
       "..                                                 ...    ...  \n",
       "931  {\"Chu Chen\",\"Brian L Strom\",\"Susan E McCann\",\"...    931  \n",
       "949  {\"Stuart Calver\",\"Julia Nichols\",\"Rachel C1  -...    949  \n",
       "952  {\"Ana Paula de Lima Ferreira\",\"Marcelo de Souz...    952  \n",
       "975  {\"Kerstin Landin-Wilhelmsen\",\"G̦ran Oler̦d\",\"O...    975  \n",
       "979  {\"Michela Miani\",\"Decio L Eizirik\",\"Laurence L...    979  \n",
       "\n",
       "[84 rows x 12 columns]"
      ]
     },
     "execution_count": 1078,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_dirty[rayyan_dirty.iloc[:,-2]!=rayyan_clean.iloc[:,-2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1038,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 1: Dirty\n",
      "Row 2: Dirty\n",
      "Row 3: Dirty\n",
      "Row 4: Dirty\n",
      "Row 5: Dirty\n",
      "Row 6: Dirty\n",
      "Row 7: Dirty\n",
      "Row 8: Dirty\n",
      "Row 9: Dirty\n",
      "Row 10: Dirty\n",
      "Row 11: Dirty\n",
      "Row 12: Dirty\n",
      "Row 13: Dirty\n",
      "Row 14: Dirty\n",
      "Row 15: Dirty\n",
      "Row 16: Dirty\n",
      "Row 17: Dirty\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def is_dirty_cell(cell):\n",
    "    # Define a regular expression pattern for the dirty cell format (M/D/YY)\n",
    "    dirty_pattern = r'^\\d{1,2}/\\d{1,2}/\\d{2}$'\n",
    "    \n",
    "    # Use the re.match function to check if the cell matches the dirty pattern\n",
    "    if re.match(dirty_pattern, cell):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_clean_cell(cell):\n",
    "    # Define a regular expression pattern for the clean cell format (M/DD/YY)\n",
    "    clean_pattern = r'^\\d{1,2}/\\d{2}/\\d{2}$'\n",
    "    \n",
    "    # Use the re.match function to check if the cell matches the clean pattern\n",
    "    if re.match(clean_pattern, cell):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def judge_cells(cells_list):\n",
    "    result = []\n",
    "    for pair in cells_list:\n",
    "        dirty_cell, clean_cell = pair\n",
    "        if is_dirty_cell(dirty_cell):\n",
    "            result.append('Dirty')\n",
    "        elif is_clean_cell(clean_cell):\n",
    "            result.append('Clean')\n",
    "        else:\n",
    "            result.append('Invalid')\n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "cells_list = [['1/1/13', '1/13/01'], ['1/1/07', '1/7/01'], ['1/1/08', '1/8/01'], ['5/1/13', '1/13/05'], ['1/1/09', '1/9/01'], ['7/1/08', '1/8/07'], ['2/6/14', '6/14/02'], ['10/1/08', '1/8/10'], ['1/1/15', '1/15/01'], ['9/1/15', '1/15/09'], ['7/1/12', '1/12/07'], ['7/1/08', '1/8/07'], ['1/1/13', '1/13/01'], ['1/1/02', '1/2/01'], ['8/1/06', '1/6/08'], ['4/1/15', '1/15/04'], ['3/1/02', '1/2/03']]\n",
    "\n",
    "results = judge_cells(cells_list)\n",
    "for i, result in enumerate(results):\n",
    "    print(f'Row {i + 1}: {result}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1042,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1042,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_dirty_cell('1/13/01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1037,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['1/1/13', '1/13/01'],\n",
       "       ['1/1/07', '1/7/01'],\n",
       "       ['1/1/08', '1/8/01'],\n",
       "       ['5/1/13', '1/13/05'],\n",
       "       ['1/1/09', '1/9/01'],\n",
       "       ['7/1/08', '1/8/07'],\n",
       "       ['2/6/14', '6/14/02'],\n",
       "       ['10/1/08', '1/8/10'],\n",
       "       ['1/1/15', '1/15/01'],\n",
       "       ['9/1/15', '1/15/09'],\n",
       "       ['7/1/12', '1/12/07'],\n",
       "       ['7/1/08', '1/8/07'],\n",
       "       ['1/1/13', '1/13/01'],\n",
       "       ['1/1/02', '1/2/01'],\n",
       "       ['8/1/06', '1/6/08'],\n",
       "       ['4/1/15', '1/15/04'],\n",
       "       ['3/1/02', '1/2/03']], dtype='<U7')"
      ]
     },
     "execution_count": 1037,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([dirty_cells,clean_cells]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1029,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dirty Cells: []\n",
      "Clean Cells: ['1/1/13', '1/1/07', '1/1/08', '5/1/13', '1/1/09', '7/1/08', '2/6/14', '10/1/08', '1/1/15', '9/1/15', '7/1/12', '7/1/08', '1/1/13', '1/1/02', '8/1/06', '4/1/15', '3/1/02']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def is_clean_date(date):\n",
    "    # Define a regular expression pattern for the clean date format (MM/DD/YY)\n",
    "    clean_pattern = r'^\\d{1,2}\\/\\d{1,2}\\/\\d{2}$'\n",
    "    \n",
    "    # Use the re.match function to check if the date matches the pattern\n",
    "    if re.match(clean_pattern, date):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Example usage:\n",
    "dates = ['1/1/13', '1/1/07', '1/1/08', '5/1/13', '1/1/09', '7/1/08', '2/6/14', '10/1/08', '1/1/15', '9/1/15', '7/1/12', '7/1/08', '1/1/13', '1/1/02', '8/1/06', '4/1/15', '3/1/02']\n",
    "\n",
    "dirty_cells = [date for date in dates if not is_clean_date(date)]\n",
    "clean_cells = [date for date in dates if is_clean_date(date)]\n",
    "\n",
    "print(\"Dirty Cells:\", dirty_cells)\n",
    "print(\"Clean Cells:\", clean_cells)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_pattern = r\"^(\\d{1,2})/(\\d{2,4})/(\\d{1,2})$\"\n",
    "re.match(clean_pattern, '1/1/07')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1/1/13' 是 dirty cell\n",
      "'1/1/07' 是 dirty cell\n",
      "'1/1/08' 是 dirty cell\n",
      "'5/1/13' 是 dirty cell\n",
      "'1/1/09' 是 dirty cell\n",
      "'1/13/01' 是 dirty cell\n",
      "'1/7/01' 是 dirty cell\n",
      "'1/8/01' 是 dirty cell\n",
      "'1/13/05' 是 dirty cell\n",
      "'1/9/01' 是 dirty cell\n",
      "'12/19/13' 是 clean cell\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def is_dirty_or_clean(cell):\n",
    "    # 定义正则表达式模式\n",
    "    pattern = r'^\\d{1,2}/\\d{1,2}/\\d{2}$'\n",
    "\n",
    "    # 使用正则表达式匹配\n",
    "    if re.match(pattern, cell):\n",
    "        if cell[1] == '/':\n",
    "            return \"dirty\"\n",
    "        else:\n",
    "            return \"clean\"\n",
    "    else:\n",
    "        return \"invalid\"\n",
    "\n",
    "# 示例用法\n",
    "cells = ['1/1/13', '1/1/07', '1/1/08', '5/1/13', '1/1/09', '1/13/01', '1/7/01', '1/8/01', '1/13/05', '1/9/01', '12/19/13']\n",
    "\n",
    "for cell in cells:\n",
    "    result = is_dirty_or_clean(cell)\n",
    "    if result == \"dirty\":\n",
    "        print(f\"'{cell}' 是 dirty cell\")\n",
    "    elif result == \"clean\":\n",
    "        print(f\"'{cell}' 是 clean cell\")\n",
    "    else:\n",
    "        print(f\"'{cell}' 无效\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1/13/01' is dirty\n",
      "'1/7/01' is dirty\n",
      "'1/8/01' is dirty\n",
      "'1/13/05' is dirty\n",
      "'1/9/01' is dirty\n",
      "'1/8/07' is dirty\n",
      "'6/14/02' is dirty\n",
      "'1/8/10' is dirty\n",
      "'1/15/01' is dirty\n",
      "'1/15/09' is dirty\n",
      "'1/12/07' is dirty\n",
      "'1/8/07' is dirty\n",
      "'1/13/01' is dirty\n",
      "'1/2/01' is dirty\n",
      "'1/6/08' is dirty\n",
      "'1/15/04' is dirty\n",
      "'1/2/03' is dirty\n",
      "'1/1/13' is dirty\n",
      "'1/1/07' is dirty\n",
      "'1/1/08' is dirty\n",
      "'5/1/13' is dirty\n",
      "'1/1/09' is dirty\n",
      "'7/1/08' is dirty\n",
      "'2/6/14' is dirty\n",
      "'10/1/08' is dirty\n",
      "'1/1/15' is dirty\n",
      "'9/1/15' is dirty\n",
      "'7/1/12' is dirty\n",
      "'7/1/08' is dirty\n",
      "'1/1/13' is dirty\n",
      "'1/1/02' is dirty\n",
      "'8/1/06' is dirty\n",
      "'4/1/15' is dirty\n",
      "'3/1/02' is dirty\n",
      "'' is dirty\n",
      "'' is dirty\n",
      "'12/19/13' is clean\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def is_dirty_date_cell(cell):\n",
    "    # Define a regular expression pattern for flexible date cells\n",
    "    clean_pattern = r'^(0[1-9]|1[0-2])/(0[1-9]|[12][0-9]|3[01])/\\d{2,4}$'\n",
    "    \n",
    "    # Check if the cell matches the clean pattern\n",
    "    if re.match(clean_pattern, cell):\n",
    "        return False  # The cell is clean\n",
    "    else:\n",
    "        return True  # The cell is dirty\n",
    "\n",
    "# Example cells\n",
    "dirty_cells = ['1/1/13', '1/1/07', '1/1/08', '5/1/13', '1/1/09', '7/1/08', '2/6/14', '10/1/08', '1/1/15', '9/1/15', '7/1/12', '7/1/08', '1/1/13', '1/1/02', '8/1/06', '4/1/15', '3/1/02', '', '', '12/19/13']\n",
    "clean_cells = ['1/13/01', '1/7/01', '1/8/01', '1/13/05', '1/9/01', '1/8/07', '6/14/02', '1/8/10', '1/15/01', '1/15/09', '1/12/07', '1/8/07', '1/13/01', '1/2/01', '1/6/08', '1/15/04', '1/2/03']\n",
    "\n",
    "# Test the function for the provided clean cells and dirty cells\n",
    "for cell in clean_cells:\n",
    "    if is_dirty_date_cell(cell):\n",
    "        print(f\"'{cell}' is dirty\")\n",
    "    else:\n",
    "        print(f\"'{cell}' is clean\")\n",
    "\n",
    "for cell in dirty_cells:\n",
    "    if is_dirty_date_cell(cell):\n",
    "        print(f\"'{cell}' is dirty\")\n",
    "    else:\n",
    "        print(f\"'{cell}' is clean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 998,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'-1' is dirty\n",
      "'-1' is dirty\n",
      "'-1' is dirty\n",
      "'4' is clean\n",
      "'2' is clean\n",
      "'0' is clean\n",
      "'3' is clean\n",
      "'3' is clean\n",
      "'1' is clean\n",
      "'3' is clean\n",
      "'7486' is clean\n",
      "'10' is clean\n",
      "'2' is clean\n",
      "'7' is clean\n",
      "'4' is clean\n",
      "'7' is clean\n",
      "'27' is clean\n",
      "'3' is clean\n",
      "'4' is clean\n",
      "'3' is clean\n",
      "'6' is clean\n",
      "'17' is clean\n",
      "'0' is clean\n",
      "'' is dirty\n",
      "'2' is clean\n",
      "'3' is clean\n",
      "'2' is clean\n",
      "'2' is clean\n",
      "'10' is clean\n",
      "'3' is clean\n",
      "'1' is clean\n",
      "'10' is clean\n",
      "'7' is clean\n",
      "'1' is clean\n",
      "'12' is clean\n",
      "'10' is clean\n",
      "'0' is clean\n",
      "'' is dirty\n",
      "'11' is clean\n",
      "'0' is clean\n",
      "'2' is clean\n",
      "'5' is clean\n",
      "'11' is clean\n",
      "'6' is clean\n",
      "'3' is clean\n",
      "'' is dirty\n",
      "'6' is clean\n",
      "'0' is clean\n",
      "'0' is clean\n",
      "'12' is clean\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def is_dirty_cell(cell):\n",
    "    # Define a regular expression pattern for clean cells (numeric values)\n",
    "    clean_pattern = r'^\\d+$'\n",
    "    \n",
    "    # Check if the cell matches the clean pattern\n",
    "    if re.match(clean_pattern, cell):\n",
    "        return False  # The cell is clean\n",
    "    else:\n",
    "        return True  # The cell is dirty\n",
    "\n",
    "# Example cells\n",
    "dirty_cells = ['', '', '']\n",
    "clean_cells = ['-1', '-1', '-1']\n",
    "additional_clean_cells = ['4', '2', '0', '3', '3', '1', '3', '7486', '10', '2', '7', '4', '7', '27', '3', '4', '3']\n",
    "\n",
    "# Test the function for the provided clean cells and dirty cells\n",
    "for cell in clean_cells:\n",
    "    if is_dirty_cell(cell):\n",
    "        print(f\"'{cell}' is dirty\")\n",
    "    else:\n",
    "        print(f\"'{cell}' is clean\")\n",
    "\n",
    "for cell in additional_clean_cells:\n",
    "    if is_dirty_cell(cell):\n",
    "        print(f\"'{cell}' is dirty\")\n",
    "    else:\n",
    "        print(f\"'{cell}' is clean\")\n",
    "\n",
    "for cell in rayyan_dirty.iloc[30:60,7].to_list():\n",
    "    if is_dirty_cell(cell):\n",
    "        print(f\"'{cell}' is dirty\")\n",
    "    else:\n",
    "        print(f\"'{cell}' is clean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def is_dirty_issn(cell):\n",
    "    # Define a regular expression pattern to match clean cells in the 'DD-Mon' or 'DD-Mon-YY' format\n",
    "    clean_pattern = r'^\\d{1,2}-[A-Za-z]{3}(?:-\\d{2})?$'\n",
    "    \n",
    "    # Use the regular expression to check if the cell matches the clean pattern\n",
    "    if re.match(clean_pattern, cell):\n",
    "        return False  # Cell is clean\n",
    "    else:\n",
    "        return True   # Cell is dirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 980,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Jan-15' is dirty\n",
      "'0370-0747' is clean\n",
      "'9788480000000' is dirty\n",
      "'2115-7863(Electronic);2115-8789(Print)' is clean\n",
      "'1578-2735' is clean\n",
      "'1473-0480' is clean\n",
      "'0214-1221' is clean\n",
      "'1476-4687' is clean\n",
      "'1460-2385' is clean\n",
      "'' is clean\n",
      "'1699-5198' is clean\n",
      "'1878-5867' is clean\n",
      "'1945-7170' is clean\n",
      "'0041-4301' is clean\n",
      "'Feb-14' is dirty\n",
      "'1873-7544' is clean\n",
      "'0041-5782 (Print) 0041-5782' is clean\n",
      "'0214-9915' is clean\n",
      "'1365-2346' is clean\n",
      "'Mar-09' is dirty\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def is_clean_cell(cell):\n",
    "    # Define regular expression patterns for clean cells\n",
    "    clean_patterns = [\n",
    "        r'^\\s*$',  # Empty string pattern\n",
    "        r'^\\d{4}-\\d{4}(\\(Electronic\\))?(;\\d{4}-\\d{4}(\\([A-Za-z]+\\))?)*$',  # Pattern with hyphens and optional (Electronic)\n",
    "        r'^\\d{4}-\\d{4}(\\s*\\(Print\\)\\s*\\d{4}-\\d{4})*$'  # Pattern with (Print)\n",
    "    ]\n",
    "\n",
    "    # Check if the cell matches any of the clean patterns\n",
    "    for pattern in clean_patterns:\n",
    "        if re.match(pattern, cell):\n",
    "            return False  # The cell is clean\n",
    "\n",
    "    return True  # The cell is not clean\n",
    "\n",
    "# Example cell\n",
    "cell = '0370-0747'\n",
    "\n",
    "# Test the function\n",
    "for cell in ['Jan-15', '0370-0747', '9788480000000', '2115-7863(Electronic);2115-8789(Print)', '1578-2735', '1473-0480', '0214-1221', '1476-4687', '1460-2385', '', '1699-5198', '1878-5867', '1945-7170', '0041-4301', 'Feb-14', '1873-7544', '0041-5782 (Print) 0041-5782', '0214-9915', '1365-2346', 'Mar-09']:\n",
    "    if is_clean_cell(cell):\n",
    "        print(f\"'{cell}' is dirty\")\n",
    "    else:\n",
    "        print(f\"'{cell}' is clean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 983,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 983,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for index,row in rayyan_dirty.iterrows():\n",
    "    if (is_clean_cell(row[5])):\n",
    "        count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 969,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Jan-15' is a dirty cell.\n",
      "'0370-0747' is a clean cell.\n",
      "'9788480000000' is a dirty cell.\n",
      "'2115-7863(Electronic);2115-8789(Print)' is a dirty cell.\n",
      "'1578-2735' is a clean cell.\n",
      "'1473-0480' is a clean cell.\n",
      "'0214-1221' is a clean cell.\n",
      "'1476-4687' is a clean cell.\n",
      "'1460-2385' is a clean cell.\n",
      "'' is a dirty cell.\n",
      "'1699-5198' is a clean cell.\n",
      "'1878-5867' is a clean cell.\n",
      "'1945-7170' is a clean cell.\n",
      "'0041-4301' is a clean cell.\n",
      "'Feb-14' is a dirty cell.\n",
      "'1873-7544' is a clean cell.\n",
      "'0041-5782 (Print) 0041-5782' is a dirty cell.\n",
      "'0214-9915' is a clean cell.\n",
      "'1365-2346' is a clean cell.\n",
      "'Mar-09' is a dirty cell.\n",
      "'15-Jan' is a dirty cell.\n",
      "'0370-0747' is a clean cell.\n",
      "'9790000000000' is a dirty cell.\n",
      "'2115-7863(Electronic);2115-8789(Print)' is a dirty cell.\n",
      "'1578-2735' is a clean cell.\n",
      "'1473-0480' is a clean cell.\n",
      "'0214-1221' is a clean cell.\n",
      "'1476-4687' is a clean cell.\n",
      "'1460-2385' is a clean cell.\n",
      "'' is a dirty cell.\n",
      "'1699-5198' is a clean cell.\n",
      "'1878-5867' is a clean cell.\n",
      "'1945-7170' is a clean cell.\n",
      "'0041-4301' is a clean cell.\n",
      "'14-Feb' is a dirty cell.\n",
      "'1873-7544' is a clean cell.\n",
      "'0041-5782 (Print) 0041-5782' is a dirty cell.\n",
      "'0214-9915' is a clean cell.\n",
      "'1365-2346' is a clean cell.\n",
      "'9-Mar' is a dirty cell.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def is_dirty_issn(cell):\n",
    "    # Define a regular expression pattern to match clean cells\n",
    "    clean_pattern = r'^\\d{4}-\\d{4}(?:\\(Electronic\\);?\\d{4}\\(Print\\))?$'\n",
    "    \n",
    "    # Use the regular expression to check if the cell matches the clean pattern\n",
    "    if re.match(clean_pattern, cell):\n",
    "        return False  # Cell is clean\n",
    "    else:\n",
    "        return True   # Cell is dirty\n",
    "\n",
    "# Test the function with your input examples\n",
    "dirty_cells = ['Jan-15', '0370-0747', '9788480000000', '2115-7863(Electronic);2115-8789(Print)', '1578-2735', '1473-0480', '0214-1221', '1476-4687', '1460-2385', '', '1699-5198', '1878-5867', '1945-7170', '0041-4301', 'Feb-14', '1873-7544', '0041-5782 (Print) 0041-5782', '0214-9915', '1365-2346', 'Mar-09']\n",
    "clean_cells = ['15-Jan', '0370-0747', '9790000000000', '2115-7863(Electronic);2115-8789(Print)', '1578-2735', '1473-0480', '0214-1221', '1476-4687', '1460-2385', '', '1699-5198', '1878-5867', '1945-7170', '0041-4301', '14-Feb', '1873-7544', '0041-5782 (Print) 0041-5782', '0214-9915', '1365-2346', '9-Mar']\n",
    "\n",
    "for cell in dirty_cells:\n",
    "    if is_dirty_issn(cell):\n",
    "        print(f\"'{cell}' is a dirty cell.\")\n",
    "    else:\n",
    "        print(f\"'{cell}' is a clean cell.\")\n",
    "\n",
    "for cell in clean_cells:\n",
    "    if is_dirty_issn(cell):\n",
    "        print(f\"'{cell}' is a dirty cell.\")\n",
    "    else:\n",
    "        print(f\"'{cell}' is a clean cell.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_dirty_issn('14-Feb')\n",
    "# ['journal_issn'].unique()\n",
    "# rayyan_dirty[rayyan_clean['journal_issn']!=rayyan_dirty['journal_issn']]\n",
    "rayyan_dirty['journal_issn'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1537-7385', '1537-7385')"
      ]
     },
     "execution_count": 915,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_dirty.iloc[52,5],rayyan_clean.iloc[52,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1049,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,  14,   0,   9,   0,  12,  22,  53, 722,  32,  84,   0])"
      ]
     },
     "execution_count": 1049,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix_rayyan.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1050,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_jcreated_at\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "913"
      ]
     },
     "execution_count": 1050,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "col_name = rayyan_label_clean.columns[8]\n",
    "print(col_name)\n",
    "# def is_dirty_cell(cell):\n",
    "#     # Define a regular expression pattern to match clean cells\n",
    "#     clean_pattern = r'^[a-zA-Z0-9\\s\\-.,()\\'\"\\[\\]]+$'\n",
    "    \n",
    "#     # Use the regular expression to check if the cell matches the clean pattern\n",
    "#     if re.match(clean_pattern, cell):\n",
    "#         return False  # Cell is clean\n",
    "#     else:\n",
    "#         return True   # Cell is dirty\n",
    "# is_dirty_cell(dirty_list[4])\n",
    "count = 0 \n",
    "pred = []\n",
    "truth = []\n",
    "for index,row in rayyan_dirty.iterrows():\n",
    "    if(is_dirty_cell(row[col_name])):\n",
    "        count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,  14,   0,   9,   0,  12,  22,  53, 722,  32,  84])"
      ]
     },
     "execution_count": 904,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix_rayyan.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check the detection result of rayyan\n",
    "detector_rayyan = np.load('datasets/rayyan/detector/detection.npy')\n",
    "detector_rayyan = detector_rayyan.reshape((1000,10))\n",
    "for i in [0,2,4,6]:\n",
    "    detector_rayyan[:,i] = 0 ## Remove all non-labelled values in rayyan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  4,  0,  5,  0,  4,  0,  3, 17,  8, 17])"
      ]
     },
     "execution_count": 924,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix_rayyan[rayyan_label_index].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7676767676767676, 0.10526315789473684, 0.1851400730816078, 99, 722)"
      ]
     },
     "execution_count": 951,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_matrix_rayyan\n",
    "i = 8 ## col index(without id col)\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score\n",
    "precision_score(y_pred=detector_rayyan[:,i],y_true=input_matrix_rayyan[:,i]),recall_score(y_pred=detector_rayyan[:,i],y_true=input_matrix_rayyan[:,i]),f1_score(y_pred=detector_rayyan[:,i],y_true=input_matrix_rayyan[:,i]),sum(detector_rayyan[:,i]),sum(input_matrix_rayyan[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1081,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 1081,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.where(detector_rayyan[:,i]==1),np.where(input_matrix_rayyan[:,i]==1)\n",
    "detector_rayyan[:,8].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1103,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rayyan Detector Training\n",
    "\n",
    "input_matrix_select_rayyan = input_matrix_rayyan[rayyan_label_index]\n",
    "detector_list_rayyan = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "for label_tuple in rayyan_label_index:\n",
    "    for i in range(1,len(rayyan_clean.columns)-1,1):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        col_name = rayyan_clean.columns[i]\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = rayyan_clean.iloc[label_tuple]\n",
    "        dirty_context = rayyan_dirty.iloc[label_tuple]\n",
    "        clean_cell = rayyan_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = rayyan_dirty.iloc[label_tuple,i]\n",
    "        single_context_clean = 'COL %s VAL %s ' % (rayyan_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (rayyan_clean.columns[i],dirty_cell)\n",
    "        for c in range(1,11,1):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (rayyan_clean.columns[c],rayyan_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (rayyan_dirty.columns[c],rayyan_dirty.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            # detector_list_rayyan.append([all_context_dirty,single_context_dirty,1])\n",
    "            # detector_list_rayyan.append([all_context_clean,single_context_clean,0])\n",
    "            # detector_list_rayyan.append([all_context_dirty,single_context_clean,0])\n",
    "            detector_list_rayyan.append([col_name,dirty_cell,1])\n",
    "            detector_list_rayyan.append([col_name,clean_cell,0])\n",
    "            # detector_list.append([single_context_dirty,1])\n",
    "        else:\n",
    "            # detector_list_rayyan.append([all_context_dirty,single_context_dirty,0])\n",
    "            # detector_list_rayyan.append([all_context_clean,single_context_clean,0])\n",
    "            # detector_list_rayyan.append([all_context_dirty,single_context_clean,0])\n",
    "            detector_list_rayyan.append([col_name,dirty_cell,0])\n",
    "            detector_list_rayyan.append([col_name,clean_cell,0])\n",
    "        # else:\n",
    "        #     detector_list.append([all_context_dirty,1])\n",
    "        #     detector_list.append([single_context_dirty,1])\n",
    "            \n",
    "                # break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b274111091c7442593a2be32aa442ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_matrix_select_rayyan = input_matrix_rayyan[rayyan_label_index]\n",
    "detector_list_rayyan = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "# for label_tuple in tqdm(selected_rows):\n",
    "for label_tuple in tqdm(range(len(rayyan_clean))):\n",
    "    for i in range(1,len(rayyan_clean.columns)-1,1):\n",
    "        col_name = rayyan_clean.columns[i]\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = rayyan_clean.iloc[label_tuple]\n",
    "        dirty_context = rayyan_dirty.iloc[label_tuple]\n",
    "        clean_cell = rayyan_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = rayyan_dirty.iloc[label_tuple,i]\n",
    "        single_context_clean = 'COL %s VAL %s ' % (rayyan_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (rayyan_clean.columns[i],dirty_cell)\n",
    "        for c in range(1,11,1):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (rayyan_clean.columns[c],rayyan_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (rayyan_dirty.columns[c],rayyan_dirty.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            detector_list_rayyan.append([col_name,dirty_cell,1])\n",
    "        else:\n",
    "            detector_list_rayyan.append([col_name,dirty_cell,0])\n",
    "            # detector_list.append([single_context_dirty,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article_title           40\n",
       "article_language        40\n",
       "journal_title           40\n",
       "jounral_abbreviation    40\n",
       "journal_issn            40\n",
       "article_jvolumn         40\n",
       "article_jissue          40\n",
       "article_jcreated_at     40\n",
       "article_pagination      40\n",
       "author_list             40\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 1104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(detector_list_rayyan)[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1102,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_rayyan).to_csv('datasets/rayyan/detector/test_cell.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1124,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Only the Dirty Datas\n",
    "training_list_label = []\n",
    "for d in rayyan_label_index:\n",
    "    index = d\n",
    "    # i = d[1] + 1 ## 重要，我们在detector中忽略了index列\n",
    "    for i in range(1,11,1):\n",
    "        dirty_cell = rayyan_dirty.iloc[index,i]\n",
    "        clean_cell = rayyan_clean.iloc[index,i]\n",
    "        col_name = rayyan_clean.columns[i]\n",
    "        template_dict = {}\n",
    "        template_dict[col_name] = ''\n",
    "        temp_dict = rayyan_dirty.iloc[index,1:-1].to_dict()\n",
    "        clean_dict = {}\n",
    "        clean_dict[col_name] = clean_cell\n",
    "        coreset_reference = np.random.choice([c for c in rayyan_label_index if c!=index],3,replace=False)\n",
    "        # if(dirty_cell!=clean_cell): ## 在LLM Inference中忽略\n",
    "        text_head = 'You are an expert in cleaning Rayyan Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\nTake these rows as reference:\\n\\n' % (col_name, json.dumps(template_dict), json.dumps(temp_dict))\n",
    "        dict_0 = rayyan_clean.iloc[coreset_reference[0],1:].to_dict()\n",
    "        # dict_1 = rayyan_clean.iloc[coreset_reference[1],1:].to_dict()\n",
    "        # dict_2 = rayyan_clean.iloc[coreset_reference[2],1:].to_dict()\n",
    "        # ICL_text = '%s\\n\\n%s\\n\\n%s\\n\\n' % (json.dumps(dict_0),json.dumps(dict_1),json.dumps(dict_2))\n",
    "        ICL_text = '%s\\n\\n' % (json.dumps(dict_0))\n",
    "        training_list_label.append([text_head,ICL_text,'',json.dumps(clean_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1132,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For test in All Data Cells,\n",
    "training_list_label = []\n",
    "for d in range(1000):\n",
    "    index = d\n",
    "    # i = d[1] + 1 ## 重要，我们在detector中忽略了index列\n",
    "    for i in range(1,11,1):\n",
    "        dirty_cell = rayyan_dirty.iloc[index,i]\n",
    "        clean_cell = rayyan_clean.iloc[index,i]\n",
    "        col_name = rayyan_clean.columns[i]\n",
    "        template_dict = {}\n",
    "        template_dict[col_name] = ''\n",
    "        temp_dict = rayyan_dirty.iloc[index,1:-1].to_dict()\n",
    "        clean_dict = {}\n",
    "        clean_dict[col_name] = clean_cell\n",
    "        coreset_reference = np.random.choice([c for c in rayyan_label_index if c!=index],3,replace=False)\n",
    "        # if(dirty_cell!=clean_cell): ## 在LLM Inference中忽略\n",
    "        text_head = 'You are an expert in cleaning Rayyan Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\nTake these rows as reference:\\n\\n' % (col_name, json.dumps(template_dict), json.dumps(temp_dict))\n",
    "        dict_0 = rayyan_clean.iloc[coreset_reference[0],1:].to_dict()\n",
    "        # dict_1 = rayyan_clean.iloc[coreset_reference[1],1:].to_dict()\n",
    "        # dict_2 = rayyan_clean.iloc[coreset_reference[2],1:].to_dict()\n",
    "        # ICL_text = '%s\\n\\n%s\\n\\n%s\\n\\n' % (json.dumps(dict_0),json.dumps(dict_1),json.dumps(dict_2))\n",
    "        ICL_text = '%s\\n\\n' % (json.dumps(dict_0))\n",
    "        training_list_label.append([text_head,ICL_text,'',json.dumps(clean_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pd.DataFrame(training_list_label).iloc[49,0]),print(pd.DataFrame(training_list_label).iloc[49,-1])\n",
    "training_list_pd = pd.DataFrame(training_list_label)\n",
    "training_list_pd['instruction'] = training_list_pd[0] + training_list_pd[1]\n",
    "training_list_pd['input'] = training_list_pd[2]\n",
    "training_list_pd['output'] = training_list_pd[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td>{\"article_title\": \"Effet d'une intervention no...</td>\n",
       "      <td></td>\n",
       "      <td>{\"article_title\": \"Late repair of injuries of ...</td>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td></td>\n",
       "      <td>{\"article_title\": \"Late repair of injuries of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td>{\"article_title\": \"Amitriptyline, minocycline ...</td>\n",
       "      <td></td>\n",
       "      <td>{\"article_language\": \"eng\"}</td>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td></td>\n",
       "      <td>{\"article_language\": \"eng\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td>{\"article_title\": \"Amitriptyline, minocycline ...</td>\n",
       "      <td></td>\n",
       "      <td>{\"journal_title\": \"Proc R Soc Med\"}</td>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td></td>\n",
       "      <td>{\"journal_title\": \"Proc R Soc Med\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td>{\"article_title\": \"Child social skills trainin...</td>\n",
       "      <td></td>\n",
       "      <td>{\"jounral_abbreviation\": \"Proceedings of the R...</td>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td></td>\n",
       "      <td>{\"jounral_abbreviation\": \"Proceedings of the R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td>{\"article_title\": \"Osteoid osteoma in a 16-yea...</td>\n",
       "      <td></td>\n",
       "      <td>{\"journal_issn\": \"0035-9157 (Print) 0035-9157\"}</td>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td></td>\n",
       "      <td>{\"journal_issn\": \"0035-9157 (Print) 0035-9157\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td>{\"article_title\": \"[Osteoporosis prevention in...</td>\n",
       "      <td></td>\n",
       "      <td>{\"article_jvolumn\": \"13\"}</td>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td></td>\n",
       "      <td>{\"article_jvolumn\": \"13\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td>{\"article_title\": \"Bruxismo em crian_as Bruxis...</td>\n",
       "      <td></td>\n",
       "      <td>{\"article_jissue\": \"1\"}</td>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td></td>\n",
       "      <td>{\"article_jissue\": \"1\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td>{\"article_title\": \"Mild endoplasmic reticulum ...</td>\n",
       "      <td></td>\n",
       "      <td>{\"article_jcreated_at\": \"1/12/01\"}</td>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td></td>\n",
       "      <td>{\"article_jcreated_at\": \"1/12/01\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td>{\"article_title\": \"Bruxismo em crian_as Bruxis...</td>\n",
       "      <td></td>\n",
       "      <td>{\"article_pagination\": \"71-80\"}</td>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td></td>\n",
       "      <td>{\"article_pagination\": \"71-80\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td>{\"article_title\": \"Effet d'une intervention no...</td>\n",
       "      <td></td>\n",
       "      <td>{\"author_list\": \"{\\\"S. Iannazzo\\\",\\\"M. Miravit...</td>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td></td>\n",
       "      <td>{\"author_list\": \"{\\\"S. Iannazzo\\\",\\\"M. Miravit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0  \\\n",
       "0     You are an expert in cleaning Rayyan Dataset. ...   \n",
       "1     You are an expert in cleaning Rayyan Dataset. ...   \n",
       "2     You are an expert in cleaning Rayyan Dataset. ...   \n",
       "3     You are an expert in cleaning Rayyan Dataset. ...   \n",
       "4     You are an expert in cleaning Rayyan Dataset. ...   \n",
       "...                                                 ...   \n",
       "9995  You are an expert in cleaning Rayyan Dataset. ...   \n",
       "9996  You are an expert in cleaning Rayyan Dataset. ...   \n",
       "9997  You are an expert in cleaning Rayyan Dataset. ...   \n",
       "9998  You are an expert in cleaning Rayyan Dataset. ...   \n",
       "9999  You are an expert in cleaning Rayyan Dataset. ...   \n",
       "\n",
       "                                                      1 2   \\\n",
       "0     {\"article_title\": \"Effet d'une intervention no...      \n",
       "1     {\"article_title\": \"Amitriptyline, minocycline ...      \n",
       "2     {\"article_title\": \"Amitriptyline, minocycline ...      \n",
       "3     {\"article_title\": \"Child social skills trainin...      \n",
       "4     {\"article_title\": \"Osteoid osteoma in a 16-yea...      \n",
       "...                                                 ... ..   \n",
       "9995  {\"article_title\": \"[Osteoporosis prevention in...      \n",
       "9996  {\"article_title\": \"Bruxismo em crian_as Bruxis...      \n",
       "9997  {\"article_title\": \"Mild endoplasmic reticulum ...      \n",
       "9998  {\"article_title\": \"Bruxismo em crian_as Bruxis...      \n",
       "9999  {\"article_title\": \"Effet d'une intervention no...      \n",
       "\n",
       "                                                      3  \\\n",
       "0     {\"article_title\": \"Late repair of injuries of ...   \n",
       "1                           {\"article_language\": \"eng\"}   \n",
       "2                   {\"journal_title\": \"Proc R Soc Med\"}   \n",
       "3     {\"jounral_abbreviation\": \"Proceedings of the R...   \n",
       "4       {\"journal_issn\": \"0035-9157 (Print) 0035-9157\"}   \n",
       "...                                                 ...   \n",
       "9995                          {\"article_jvolumn\": \"13\"}   \n",
       "9996                            {\"article_jissue\": \"1\"}   \n",
       "9997                 {\"article_jcreated_at\": \"1/12/01\"}   \n",
       "9998                    {\"article_pagination\": \"71-80\"}   \n",
       "9999  {\"author_list\": \"{\\\"S. Iannazzo\\\",\\\"M. Miravit...   \n",
       "\n",
       "                                            instruction input  \\\n",
       "0     You are an expert in cleaning Rayyan Dataset. ...         \n",
       "1     You are an expert in cleaning Rayyan Dataset. ...         \n",
       "2     You are an expert in cleaning Rayyan Dataset. ...         \n",
       "3     You are an expert in cleaning Rayyan Dataset. ...         \n",
       "4     You are an expert in cleaning Rayyan Dataset. ...         \n",
       "...                                                 ...   ...   \n",
       "9995  You are an expert in cleaning Rayyan Dataset. ...         \n",
       "9996  You are an expert in cleaning Rayyan Dataset. ...         \n",
       "9997  You are an expert in cleaning Rayyan Dataset. ...         \n",
       "9998  You are an expert in cleaning Rayyan Dataset. ...         \n",
       "9999  You are an expert in cleaning Rayyan Dataset. ...         \n",
       "\n",
       "                                                 output  \n",
       "0     {\"article_title\": \"Late repair of injuries of ...  \n",
       "1                           {\"article_language\": \"eng\"}  \n",
       "2                   {\"journal_title\": \"Proc R Soc Med\"}  \n",
       "3     {\"jounral_abbreviation\": \"Proceedings of the R...  \n",
       "4       {\"journal_issn\": \"0035-9157 (Print) 0035-9157\"}  \n",
       "...                                                 ...  \n",
       "9995                          {\"article_jvolumn\": \"13\"}  \n",
       "9996                            {\"article_jissue\": \"1\"}  \n",
       "9997                 {\"article_jcreated_at\": \"1/12/01\"}  \n",
       "9998                    {\"article_pagination\": \"71-80\"}  \n",
       "9999  {\"author_list\": \"{\\\"S. Iannazzo\\\",\\\"M. Miravit...  \n",
       "\n",
       "[10000 rows x 7 columns]"
      ]
     },
     "execution_count": 1134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_list_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1135,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(training_list_pd.iloc[:,-3:].to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/rayyan/rayyan-test.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2118/4042012506.py:3: DtypeWarning: Columns (12,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tax_dirty = pd.read_csv('datasets/tax/dirty.csv').fillna('').astype(str)\n"
     ]
    }
   ],
   "source": [
    "### Tax Data\n",
    "tax_clean = pd.read_csv('datasets/tax/clean.csv').fillna('').astype(str)\n",
    "tax_dirty = pd.read_csv('datasets/tax/dirty.csv').fillna('').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0,1,5,6,7,8,9,12,14]\n",
    "0,1 ''\n",
    "5 6 -*\n",
    "7 zip-FD\n",
    "8,9 Marriage/Children not fixable?\n",
    "12,14 -*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_name</th>\n",
       "      <th>l_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>area_code</th>\n",
       "      <th>phone</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>has_child</th>\n",
       "      <th>salary</th>\n",
       "      <th>rate</th>\n",
       "      <th>single_exemp</th>\n",
       "      <th>married_exemp</th>\n",
       "      <th>child_exemp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17614</th>\n",
       "      <td>Peing</td>\n",
       "      <td>Bitter</td>\n",
       "      <td>M</td>\n",
       "      <td>251</td>\n",
       "      <td>848-9280</td>\n",
       "      <td>BIRMINGHAM</td>\n",
       "      <td>AL</td>\n",
       "      <td>35295</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>85000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17657</th>\n",
       "      <td>Chaomei</td>\n",
       "      <td>Bryce</td>\n",
       "      <td>F</td>\n",
       "      <td>334</td>\n",
       "      <td>518-3841</td>\n",
       "      <td>WOODLAND</td>\n",
       "      <td>AL</td>\n",
       "      <td>36280</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>25000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17727</th>\n",
       "      <td>Furio</td>\n",
       "      <td>Namjoshi</td>\n",
       "      <td>F</td>\n",
       "      <td>334</td>\n",
       "      <td>600-7468</td>\n",
       "      <td>PINCKARD</td>\n",
       "      <td>AL</td>\n",
       "      <td>36371</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17730</th>\n",
       "      <td>Atsuyuki</td>\n",
       "      <td>Gohring</td>\n",
       "      <td>F</td>\n",
       "      <td>334</td>\n",
       "      <td>739-5652</td>\n",
       "      <td>BIRMINGHAM</td>\n",
       "      <td>AL</td>\n",
       "      <td>35238</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>65000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17835</th>\n",
       "      <td>Suraj</td>\n",
       "      <td>Beilner</td>\n",
       "      <td>F</td>\n",
       "      <td>205</td>\n",
       "      <td>727-2792</td>\n",
       "      <td>ANNISTON</td>\n",
       "      <td>AL</td>\n",
       "      <td>36207</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>15000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39167</th>\n",
       "      <td>Munehiko</td>\n",
       "      <td>Bauknecht</td>\n",
       "      <td>M</td>\n",
       "      <td>205</td>\n",
       "      <td>495-7113</td>\n",
       "      <td>LAWLEY</td>\n",
       "      <td>AL</td>\n",
       "      <td>36793</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>85000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39173</th>\n",
       "      <td>Kimaya</td>\n",
       "      <td>Favero</td>\n",
       "      <td>M</td>\n",
       "      <td>256</td>\n",
       "      <td>398-3478</td>\n",
       "      <td>ELBERTA</td>\n",
       "      <td>AL</td>\n",
       "      <td>36530</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39180</th>\n",
       "      <td>Horia</td>\n",
       "      <td>Guth</td>\n",
       "      <td>M</td>\n",
       "      <td>256</td>\n",
       "      <td>383-8287</td>\n",
       "      <td>CHUNCHULA</td>\n",
       "      <td>AL</td>\n",
       "      <td>36521</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>10000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39182</th>\n",
       "      <td>Kern</td>\n",
       "      <td>Polt</td>\n",
       "      <td>F</td>\n",
       "      <td>334</td>\n",
       "      <td>908-2662</td>\n",
       "      <td>BIRMINGHAM</td>\n",
       "      <td>AL</td>\n",
       "      <td>35208</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>65000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39254</th>\n",
       "      <td>Benoit</td>\n",
       "      <td>Wadel</td>\n",
       "      <td>M</td>\n",
       "      <td>334</td>\n",
       "      <td>872-3922</td>\n",
       "      <td>WEAVER</td>\n",
       "      <td>AL</td>\n",
       "      <td>36277</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>95000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         f_name     l_name gender area_code     phone        city state  \\\n",
       "17614     Peing     Bitter      M       251  848-9280  BIRMINGHAM    AL   \n",
       "17657   Chaomei      Bryce      F       334  518-3841    WOODLAND    AL   \n",
       "17727     Furio   Namjoshi      F       334  600-7468    PINCKARD    AL   \n",
       "17730  Atsuyuki    Gohring      F       334  739-5652  BIRMINGHAM    AL   \n",
       "17835     Suraj    Beilner      F       205  727-2792    ANNISTON    AL   \n",
       "...         ...        ...    ...       ...       ...         ...   ...   \n",
       "39167  Munehiko  Bauknecht      M       205  495-7113      LAWLEY    AL   \n",
       "39173    Kimaya     Favero      M       256  398-3478     ELBERTA    AL   \n",
       "39180     Horia       Guth      M       256  383-8287   CHUNCHULA    AL   \n",
       "39182      Kern       Polt      F       334  908-2662  BIRMINGHAM    AL   \n",
       "39254    Benoit      Wadel      M       334  872-3922      WEAVER    AL   \n",
       "\n",
       "         zip marital_status has_child salary rate single_exemp married_exemp  \\\n",
       "17614  35295              M         N  85000  5.0         1500             0   \n",
       "17657  36280              M         N  25000  5.0         1500             0   \n",
       "17727  36371              M         N   5000  5.0         1500             0   \n",
       "17730  35238              M         N  65000  5.0         1500             0   \n",
       "17835  36207              M         N  15000  5.0         1500             0   \n",
       "...      ...            ...       ...    ...  ...          ...           ...   \n",
       "39167  36793              M         N  85000  5.0         1500             0   \n",
       "39173  36530              M         Y  40000  5.0         1500             0   \n",
       "39180  36521              M         N  10000  5.0         1500             0   \n",
       "39182  35208              M         N  65000  5.0         1500             0   \n",
       "39254  36277              M         N  95000  5.0         1500             0   \n",
       "\n",
       "      child_exemp  \n",
       "17614         300  \n",
       "17657         300  \n",
       "17727           0  \n",
       "17730         300  \n",
       "17835           0  \n",
       "...           ...  \n",
       "39167           0  \n",
       "39173         300  \n",
       "39180           0  \n",
       "39182           0  \n",
       "39254           0  \n",
       "\n",
       "[200 rows x 15 columns]"
      ]
     },
     "execution_count": 730,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 8\n",
    "tax_dirty[tax_clean.iloc[:,i]!=tax_dirty.iloc[:,i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AK-*    199\n",
       "AK      163\n",
       "NJ        1\n",
       "Name: state, dtype: int64"
      ]
     },
     "execution_count": 1373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_dirty[tax_dirty['city']=='ANCHORAGE']['state'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f_name            272\n",
       "l_name            694\n",
       "gender              0\n",
       "area_code           0\n",
       "phone               0\n",
       "city              200\n",
       "state             600\n",
       "zip               400\n",
       "marital_status    200\n",
       "has_child         200\n",
       "salary              0\n",
       "rate                0\n",
       "single_exemp      200\n",
       "married_exemp       0\n",
       "child_exemp       200\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tax_clean!=tax_dirty).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_name</th>\n",
       "      <th>l_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>area_code</th>\n",
       "      <th>phone</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>has_child</th>\n",
       "      <th>salary</th>\n",
       "      <th>rate</th>\n",
       "      <th>single_exemp</th>\n",
       "      <th>married_exemp</th>\n",
       "      <th>child_exemp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>Shalosh</td>\n",
       "      <td>Vrancken</td>\n",
       "      <td>F</td>\n",
       "      <td>205</td>\n",
       "      <td>471-4927</td>\n",
       "      <td>HATCHECHUBBEE</td>\n",
       "      <td>AL</td>\n",
       "      <td>36858</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>65000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2029</th>\n",
       "      <td>Jeane</td>\n",
       "      <td>Ballinger</td>\n",
       "      <td>F</td>\n",
       "      <td>251</td>\n",
       "      <td>997-4661</td>\n",
       "      <td>GROVE HILL</td>\n",
       "      <td>AL</td>\n",
       "      <td>36451</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>20000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2097</th>\n",
       "      <td>Rong</td>\n",
       "      <td>Cummings</td>\n",
       "      <td>M</td>\n",
       "      <td>251</td>\n",
       "      <td>952-3637</td>\n",
       "      <td>NOTASULGA</td>\n",
       "      <td>AL</td>\n",
       "      <td>36866</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2173</th>\n",
       "      <td>Lois</td>\n",
       "      <td>Fontana</td>\n",
       "      <td>M</td>\n",
       "      <td>334</td>\n",
       "      <td>869-6752</td>\n",
       "      <td>MILLRY</td>\n",
       "      <td>AL</td>\n",
       "      <td>36558</td>\n",
       "      <td>S</td>\n",
       "      <td>Y</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2273</th>\n",
       "      <td>Dannz</td>\n",
       "      <td>Bhattacharjee</td>\n",
       "      <td>M</td>\n",
       "      <td>251</td>\n",
       "      <td>195-7571</td>\n",
       "      <td>FLORENCE</td>\n",
       "      <td>AL</td>\n",
       "      <td>35633</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>75000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20793</th>\n",
       "      <td>Byoung</td>\n",
       "      <td>Lebah</td>\n",
       "      <td>F</td>\n",
       "      <td>205</td>\n",
       "      <td>773-5181</td>\n",
       "      <td>BIRMINGHAM</td>\n",
       "      <td>AL</td>\n",
       "      <td>35210</td>\n",
       "      <td>S</td>\n",
       "      <td>Y</td>\n",
       "      <td>25000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21180</th>\n",
       "      <td>Ross</td>\n",
       "      <td>Stround</td>\n",
       "      <td>F</td>\n",
       "      <td>334</td>\n",
       "      <td>516-7873</td>\n",
       "      <td>SLOCOMB</td>\n",
       "      <td>AL</td>\n",
       "      <td>36375</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>95000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21480</th>\n",
       "      <td>Mihir</td>\n",
       "      <td>Olech</td>\n",
       "      <td>M</td>\n",
       "      <td>256</td>\n",
       "      <td>737-7930</td>\n",
       "      <td>FOLEY</td>\n",
       "      <td>AL</td>\n",
       "      <td>36536</td>\n",
       "      <td>S</td>\n",
       "      <td>Y</td>\n",
       "      <td>100000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21678</th>\n",
       "      <td>Moheb</td>\n",
       "      <td>Doddapaneni</td>\n",
       "      <td>M</td>\n",
       "      <td>256</td>\n",
       "      <td>289-7064</td>\n",
       "      <td>GORDON</td>\n",
       "      <td>AL</td>\n",
       "      <td>36343</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>5000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21880</th>\n",
       "      <td>Dieter</td>\n",
       "      <td>Doroslovacki</td>\n",
       "      <td>M</td>\n",
       "      <td>251</td>\n",
       "      <td>253-5032</td>\n",
       "      <td>CUBA</td>\n",
       "      <td>AL</td>\n",
       "      <td>36907</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>60000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        f_name         l_name gender area_code     phone           city state  \\\n",
       "1991   Shalosh       Vrancken      F       205  471-4927  HATCHECHUBBEE    AL   \n",
       "2029     Jeane      Ballinger      F       251  997-4661     GROVE HILL    AL   \n",
       "2097      Rong       Cummings      M       251  952-3637      NOTASULGA    AL   \n",
       "2173      Lois        Fontana      M       334  869-6752         MILLRY    AL   \n",
       "2273     Dannz  Bhattacharjee      M       251  195-7571       FLORENCE    AL   \n",
       "...        ...            ...    ...       ...       ...            ...   ...   \n",
       "20793   Byoung          Lebah      F       205  773-5181     BIRMINGHAM    AL   \n",
       "21180     Ross        Stround      F       334  516-7873        SLOCOMB    AL   \n",
       "21480    Mihir          Olech      M       256  737-7930          FOLEY    AL   \n",
       "21678    Moheb    Doddapaneni      M       256  289-7064         GORDON    AL   \n",
       "21880   Dieter   Doroslovacki      M       251  253-5032           CUBA    AL   \n",
       "\n",
       "         zip marital_status has_child  salary rate single_exemp married_exemp  \\\n",
       "1991   36858              M         Y   65000  5.0            0          3000   \n",
       "2029   36451              M         Y   20000  5.0            0          3000   \n",
       "2097   36866              M         Y   40000  5.0            0          3000   \n",
       "2173   36558              S         Y   40000  5.0         1500             0   \n",
       "2273   35633              M         Y   75000  5.0            0          3000   \n",
       "...      ...            ...       ...     ...  ...          ...           ...   \n",
       "20793  35210              S         Y   25000  5.0         1500             0   \n",
       "21180  36375              M         Y   95000  5.0            0          3000   \n",
       "21480  36536              S         Y  100000  5.0         1500             0   \n",
       "21678  36343              M         Y    5000  5.0            0          3000   \n",
       "21880  36907              M         Y   60000  5.0            0          3000   \n",
       "\n",
       "      child_exemp  \n",
       "1991          300  \n",
       "2029          300  \n",
       "2097          300  \n",
       "2173          300  \n",
       "2273          300  \n",
       "...           ...  \n",
       "20793         300  \n",
       "21180         300  \n",
       "21480         300  \n",
       "21678         300  \n",
       "21880         300  \n",
       "\n",
       "[200 rows x 15 columns]"
      ]
     },
     "execution_count": 1181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_clean[tax_clean.iloc[:,i]!=tax_dirty.iloc[:,i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000,)"
      ]
     },
     "execution_count": 1196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix_tax.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tax_clean[tax_clean['city']=='BARING']\n",
    "tax_dirty_count = tax_dirty\n",
    "tax_dirty_count['count'] = input_matrix_tax.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 17730,  17853,   4694,  18600,  81673,  19800, 124538,   3555,\n",
       "            146948,  18960,  20793,  10347,  17614,  19250,  19694,  20191,\n",
       "             18764,  86465,  10273],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 1201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_dirty_count[tax_dirty_count['count']==2].sample(n=19).index[:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "选中的20行索引: [19039, 3555, 9924, 825, 2150, 2652, 2923, 3338, 3534, 3630, 5400, 9234, 10347, 11305, 12145, 14408, 15975, 16157, 16326, 17044]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def find_rows_to_cover(matrix):\n",
    "    # if matrix.shape != (1000, 15):\n",
    "    #     return \"输入矩阵的大小必须为1000x15。\"\n",
    "\n",
    "    num_rows, num_cols = matrix.shape\n",
    "    selected_rows = []\n",
    "\n",
    "    while len(selected_rows) < 20:\n",
    "        max_covered_count = 0\n",
    "        max_covered_row = None\n",
    "\n",
    "        for row in range(num_rows):\n",
    "            if row in selected_rows:\n",
    "                continue\n",
    "\n",
    "            covered_cols = [col for col in range(num_cols) if matrix[row, col] == 1]\n",
    "            covered_count = sum(1 for col in covered_cols if all(matrix[r, col] == 1 for r in selected_rows))\n",
    "\n",
    "            if covered_count > max_covered_count:\n",
    "                max_covered_count = covered_count\n",
    "                max_covered_row = row\n",
    "\n",
    "        if max_covered_row is not None:\n",
    "            selected_rows.append(max_covered_row)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return selected_rows\n",
    "\n",
    "# 创建一个示例的1000x15的NumPy数组（假设数组名为matrix）\n",
    "# 请替换这个示例数组为您自己的数据\n",
    "matrix = np.random.randint(2, size=(1000, 15))\n",
    "\n",
    "# 查找符合条件的20行\n",
    "selected_rows = find_rows_to_cover(input_matrix_tax)\n",
    "\n",
    "# 输出结果\n",
    "print(\"选中的20行索引:\", selected_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1316,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_label_index = [19039, 3555, 9924, 825, 2150, 2652, 2923, 3338, 3534, 3630, 5400, 9234, 10347, 11305, 12145, 14408, 15975, 16157, 16326, 17044]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20,  0,  0,  0,  0,  0,  1,  0,  1,  3,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 1317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix_tax[tax_label_index].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_name</th>\n",
       "      <th>l_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>area_code</th>\n",
       "      <th>phone</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>has_child</th>\n",
       "      <th>salary</th>\n",
       "      <th>rate</th>\n",
       "      <th>single_exemp</th>\n",
       "      <th>married_exemp</th>\n",
       "      <th>child_exemp</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3555</th>\n",
       "      <td>M''Lissa</td>\n",
       "      <td>Baumann</td>\n",
       "      <td>M</td>\n",
       "      <td>251</td>\n",
       "      <td>857-5924</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>AL</td>\n",
       "      <td>36314</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>60000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19039</th>\n",
       "      <td>Irs''hak</td>\n",
       "      <td>Guha</td>\n",
       "      <td>M</td>\n",
       "      <td>205</td>\n",
       "      <td>888-6448</td>\n",
       "      <td>BIRMINGHAM</td>\n",
       "      <td>AL</td>\n",
       "      <td>35205</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>10000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         f_name   l_name gender area_code     phone        city state    zip  \\\n",
       "3555   M''Lissa  Baumann      M       251  857-5924       BLACK    AL  36314   \n",
       "19039  Irs''hak     Guha      M       205  888-6448  BIRMINGHAM    AL  35205   \n",
       "\n",
       "      marital_status has_child salary rate single_exemp married_exemp  \\\n",
       "3555               S         N  60000  5.0         1500             0   \n",
       "19039              M         N  10000  5.0         1500             0   \n",
       "\n",
       "      child_exemp  count  \n",
       "3555          300      2  \n",
       "19039         300      3  "
      ]
     },
     "execution_count": 1226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 0\n",
    "tax_dirty_count[tax_clean.iloc[:,c]!=tax_dirty.iloc[:,c]].sort_values('count').iloc[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1227,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [0,1,5,6,7,8,9,12,14]\n",
    "index_all = []\n",
    "for c in col:\n",
    "    index_all.append(list(tax_dirty_count[tax_clean.iloc[:,c]!=tax_dirty.iloc[:,c]].sort_values('count').iloc[-3:].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1228,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_all = set(np.array(index_all).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[164643,\n",
       " 164166,\n",
       " 78015,\n",
       " 93823,\n",
       " 18960,\n",
       " 10273,\n",
       " 146948,\n",
       " 20793,\n",
       " 20663,\n",
       " 17853,\n",
       " 86465,\n",
       " 4677,\n",
       " 81673,\n",
       " 4682,\n",
       " 4694,\n",
       " 12263,\n",
       " 3555,\n",
       " 124538,\n",
       " 10347,\n",
       " 19039]"
      ]
     },
     "execution_count": 1231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_all = list(tax_dirty_count.iloc[list(index_all)].sort_values('count').iloc[-20:].index)\n",
    "index_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_matrix_tax[index_all].sum(axis=0)\n",
    "tax_label_index = index_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([272, 694,   0,   0,   0, 200, 600, 400, 200, 200,   0,   0, 200,\n",
       "          0, 200]),\n",
       " array([5, 7, 0, 0, 0, 2, 4, 3, 4, 8, 0, 0, 2, 0, 2]))"
      ]
     },
     "execution_count": 1320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix_tax.sum(axis=0),input_matrix_tax[index_all].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1243,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list_label = []\n",
    "for d in tax_label_index:\n",
    "    index = d\n",
    "    # i = d[1] + 1 ## 重要，我们在detector中忽略了index列\n",
    "    for i in range(15):\n",
    "        dirty_cell = tax_dirty.iloc[index,i]\n",
    "        clean_cell = tax_clean.iloc[index,i]\n",
    "        col_name = tax_clean.columns[i]\n",
    "        template_dict = {}\n",
    "        template_dict[col_name] = ''\n",
    "        temp_dict = tax_dirty.iloc[index,1:-1].to_dict()\n",
    "        clean_dict = {}\n",
    "        clean_dict[col_name] = clean_cell\n",
    "        coreset_reference = np.random.choice([c for c in tax_label_index if c!=index],3,replace=False)\n",
    "        # if(dirty_cell!=clean_cell): ## 在LLM Inference中忽略\n",
    "        text_head = 'You are an expert in cleaning Tax Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\nTake these rows as reference:\\n\\n' % (col_name, json.dumps(template_dict), json.dumps(temp_dict))\n",
    "        dict_0 = tax_clean.iloc[coreset_reference[0]].to_dict()\n",
    "        dict_1 = tax_clean.iloc[coreset_reference[1]].to_dict()\n",
    "        dict_2 = tax_clean.iloc[coreset_reference[2]].to_dict()\n",
    "        ICL_text = '%s\\n\\n%s\\n\\n%s\\n\\n' % (json.dumps(dict_0),json.dumps(dict_1),json.dumps(dict_2))\n",
    "        # ICL_text = '%s\\n\\n' % (json.dumps(dict_0))\n",
    "        training_list_label.append([text_head,ICL_text,'',json.dumps(clean_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"article_jcreated_at\": \"1/1/12\"}\n"
     ]
    }
   ],
   "source": [
    "rayyan_result = pd.read_csv('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/rayyan-test.csv',index_col=0)\n",
    "print(rayyan_result[rayyan_result['output']!=rayyan_result['predict']].iloc[-1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"M'hamed\", \"l_name\": \"O'Boyle\", \"ge...</td>\n",
       "      <td></td>\n",
       "      <td>{\"f_name\": \"Torbjorn\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"f_name\": \"Torbjorn\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"M'hamed\", \"l_name\": \"O'Boyle\", \"ge...</td>\n",
       "      <td></td>\n",
       "      <td>{\"l_name\": \"Kesselman\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"l_name\": \"Kesselman\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"Toni\", \"l_name\": \"McElwain\", \"gend...</td>\n",
       "      <td></td>\n",
       "      <td>{\"gender\": \"F\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"gender\": \"F\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"Irs'hak\", \"l_name\": \"Guha\", \"gende...</td>\n",
       "      <td></td>\n",
       "      <td>{\"area_code\": \"907\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"area_code\": \"907\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"Laurie\", \"l_name\": \"Fornaciari\", \"...</td>\n",
       "      <td></td>\n",
       "      <td>{\"phone\": \"861-3988\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"phone\": \"861-3988\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"Mehul\", \"l_name\": \"L'Excellent\", \"...</td>\n",
       "      <td></td>\n",
       "      <td>{\"salary\": \"10000\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"salary\": \"10000\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"Laurie\", \"l_name\": \"Fornaciari\", \"...</td>\n",
       "      <td></td>\n",
       "      <td>{\"rate\": \"5.0\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"rate\": \"5.0\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"Shibin\", \"l_name\": \"Raufmann\", \"ge...</td>\n",
       "      <td></td>\n",
       "      <td>{\"single_exemp\": \"1500\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"single_exemp\": \"1500\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"M'Lissa\", \"l_name\": \"Lugli\", \"gend...</td>\n",
       "      <td></td>\n",
       "      <td>{\"married_exemp\": \"0\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"married_exemp\": \"0\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"Emin\", \"l_name\": \"Fouks\", \"gender\"...</td>\n",
       "      <td></td>\n",
       "      <td>{\"child_exemp\": \"300\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"child_exemp\": \"300\"}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0  \\\n",
       "0    You are an expert in cleaning Tax Dataset. Giv...   \n",
       "1    You are an expert in cleaning Tax Dataset. Giv...   \n",
       "2    You are an expert in cleaning Tax Dataset. Giv...   \n",
       "3    You are an expert in cleaning Tax Dataset. Giv...   \n",
       "4    You are an expert in cleaning Tax Dataset. Giv...   \n",
       "..                                                 ...   \n",
       "295  You are an expert in cleaning Tax Dataset. Giv...   \n",
       "296  You are an expert in cleaning Tax Dataset. Giv...   \n",
       "297  You are an expert in cleaning Tax Dataset. Giv...   \n",
       "298  You are an expert in cleaning Tax Dataset. Giv...   \n",
       "299  You are an expert in cleaning Tax Dataset. Giv...   \n",
       "\n",
       "                                                     1 2   \\\n",
       "0    {\"f_name\": \"M'hamed\", \"l_name\": \"O'Boyle\", \"ge...      \n",
       "1    {\"f_name\": \"M'hamed\", \"l_name\": \"O'Boyle\", \"ge...      \n",
       "2    {\"f_name\": \"Toni\", \"l_name\": \"McElwain\", \"gend...      \n",
       "3    {\"f_name\": \"Irs'hak\", \"l_name\": \"Guha\", \"gende...      \n",
       "4    {\"f_name\": \"Laurie\", \"l_name\": \"Fornaciari\", \"...      \n",
       "..                                                 ... ..   \n",
       "295  {\"f_name\": \"Mehul\", \"l_name\": \"L'Excellent\", \"...      \n",
       "296  {\"f_name\": \"Laurie\", \"l_name\": \"Fornaciari\", \"...      \n",
       "297  {\"f_name\": \"Shibin\", \"l_name\": \"Raufmann\", \"ge...      \n",
       "298  {\"f_name\": \"M'Lissa\", \"l_name\": \"Lugli\", \"gend...      \n",
       "299  {\"f_name\": \"Emin\", \"l_name\": \"Fouks\", \"gender\"...      \n",
       "\n",
       "                            3  \\\n",
       "0      {\"f_name\": \"Torbjorn\"}   \n",
       "1     {\"l_name\": \"Kesselman\"}   \n",
       "2             {\"gender\": \"F\"}   \n",
       "3        {\"area_code\": \"907\"}   \n",
       "4       {\"phone\": \"861-3988\"}   \n",
       "..                        ...   \n",
       "295       {\"salary\": \"10000\"}   \n",
       "296           {\"rate\": \"5.0\"}   \n",
       "297  {\"single_exemp\": \"1500\"}   \n",
       "298    {\"married_exemp\": \"0\"}   \n",
       "299    {\"child_exemp\": \"300\"}   \n",
       "\n",
       "                                           instruction input  \\\n",
       "0    You are an expert in cleaning Tax Dataset. Giv...         \n",
       "1    You are an expert in cleaning Tax Dataset. Giv...         \n",
       "2    You are an expert in cleaning Tax Dataset. Giv...         \n",
       "3    You are an expert in cleaning Tax Dataset. Giv...         \n",
       "4    You are an expert in cleaning Tax Dataset. Giv...         \n",
       "..                                                 ...   ...   \n",
       "295  You are an expert in cleaning Tax Dataset. Giv...         \n",
       "296  You are an expert in cleaning Tax Dataset. Giv...         \n",
       "297  You are an expert in cleaning Tax Dataset. Giv...         \n",
       "298  You are an expert in cleaning Tax Dataset. Giv...         \n",
       "299  You are an expert in cleaning Tax Dataset. Giv...         \n",
       "\n",
       "                       output  \n",
       "0      {\"f_name\": \"Torbjorn\"}  \n",
       "1     {\"l_name\": \"Kesselman\"}  \n",
       "2             {\"gender\": \"F\"}  \n",
       "3        {\"area_code\": \"907\"}  \n",
       "4       {\"phone\": \"861-3988\"}  \n",
       "..                        ...  \n",
       "295       {\"salary\": \"10000\"}  \n",
       "296           {\"rate\": \"5.0\"}  \n",
       "297  {\"single_exemp\": \"1500\"}  \n",
       "298    {\"married_exemp\": \"0\"}  \n",
       "299    {\"child_exemp\": \"300\"}  \n",
       "\n",
       "[300 rows x 7 columns]"
      ]
     },
     "execution_count": 1244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_list_pd = pd.DataFrame(training_list_label)\n",
    "training_list_pd['instruction'] = training_list_pd[0] + training_list_pd[1]\n",
    "training_list_pd['input'] = training_list_pd[2]\n",
    "training_list_pd['output'] = training_list_pd[3]\n",
    "training_list_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1245,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(training_list_pd.iloc[:,-3:].to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/tax/tax-train.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 1257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "rayyan_result['item'] = ''\n",
    "rayyan_result['gt'] = ''\n",
    "for index,row in rayyan_result.iterrows():\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        temp_dict = ast.literal_eval(row[3])\n",
    "        rayyan_result.iloc[index,-2] = list(temp_dict.values())[0]\n",
    "    except:\n",
    "        print(index)\n",
    "    try:\n",
    "        gt_dict = ast.literal_eval(row[2])\n",
    "        rayyan_result.iloc[index,-1] = list(gt_dict.values())[0]\n",
    "    except:\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# rayyan_dirty.iloc[3,9],rayyan_clean.iloc[3,9]\n",
    "input_matrix_rayyan = input_matrix_rayyan[:,1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24178403755868544 0.10864978902953587 0.14992721979621543\n"
     ]
    }
   ],
   "source": [
    "# input_matrix_rayyan_flatten = input_matrix_rayyan.flatten()\n",
    "All_Data_Error_rayyan = input_matrix_rayyan.sum()\n",
    "Add_Fixed_Error_rayyan = 0\n",
    "Correct_Fixed_Error_rayyan = 0\n",
    "for d in np.argwhere(input_matrix_rayyan==1):\n",
    "    x = d[0]\n",
    "    y = d[1]\n",
    "    result_index = x * 10 + y\n",
    "    dirty_cell = rayyan_dirty.iloc[x,y+1]\n",
    "    correction_result = rayyan_result.iloc[result_index,-2]\n",
    "    \n",
    "    \n",
    "    # clean_cell = rayyan_clean.iloc[x,y+1] ## Ignore id\n",
    "    clean_cell = rayyan_result.iloc[result_index,-1]\n",
    "    if(dirty_cell!=correction_result):\n",
    "        assert dirty_cell!=clean_cell\n",
    "        Add_Fixed_Error_rayyan += 1\n",
    "    if(clean_cell==correction_result):\n",
    "        Correct_Fixed_Error_rayyan += 1\n",
    "\n",
    "Precision_rayyan = Correct_Fixed_Error_rayyan / Add_Fixed_Error_rayyan\n",
    "Recall_rayyan = Correct_Fixed_Error_rayyan / All_Data_Error_rayyan\n",
    "F1_rayyan = (2 * Precision_rayyan * Recall_rayyan) / (Precision_rayyan + Recall_rayyan)\n",
    "print(Precision_rayyan,Recall_rayyan,F1_rayyan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.7774261603375527 0.7774261603375527 0.7774261603375527 Rayyan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection for Tax Dataset, DO NOT CONCLUDE Violation of FD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1326,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tax Detector Training\n",
    "\n",
    "input_matrix_select_tax = input_matrix_tax[tax_label_index]\n",
    "detector_list_tax = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "for label_tuple in tax_label_index:\n",
    "    for i in range(15):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        col_name = tax_clean.columns[i]\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = tax_clean.iloc[label_tuple]\n",
    "        dirty_context = tax_dirty.iloc[label_tuple]\n",
    "        clean_cell = tax_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = tax_dirty.iloc[label_tuple,i]\n",
    "        single_context_clean = 'COL %s VAL %s ' % (tax_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (tax_clean.columns[i],dirty_cell)\n",
    "        for c in range(15):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (tax_clean.columns[c],tax_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (tax_dirty.columns[c],tax_dirty.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            detector_list_tax.append([all_context_dirty,single_context_dirty,1])\n",
    "            detector_list_tax.append([all_context_clean,single_context_clean,0])\n",
    "            detector_list_tax.append([all_context_dirty,single_context_clean,0])\n",
    "            # detector_list_tax.append([col_name,dirty_cell,1])\n",
    "            # detector_list_tax.append([col_name,clean_cell,0])\n",
    "            # detector_list.append([single_context_dirty,1])\n",
    "        else:\n",
    "            detector_list_tax.append([all_context_dirty,single_context_dirty,0])\n",
    "            detector_list_tax.append([all_context_clean,single_context_clean,0])\n",
    "            detector_list_tax.append([all_context_dirty,single_context_clean,0])\n",
    "            # detector_list_tax.append([col_name,dirty_cell,0])\n",
    "            # detector_list_tax.append([col_name,clean_cell,0])\n",
    "        # else:\n",
    "        #     detector_list.append([all_context_dirty,1])\n",
    "        #     detector_list.append([single_context_dirty,1])\n",
    "            \n",
    "                # break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(detector_list_tax)[2].value_counts()\n",
    "detector_list_tax_pd = pd.DataFrame(detector_list_tax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b595ddd1d0e044f6938f14e4ce1eb256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2929 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_matrix_select_tax = input_matrix_tax[tax_label_index]\n",
    "detector_list_tax = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "# for label_tuple in tqdm(selected_rows):\n",
    "# for label_tuple in tqdm(range(len(rayyan_clean))):\n",
    "for label_tuple in tqdm(selected_test_index): ## select 3000 rows to detect\n",
    "    for i in range(15):\n",
    "        col_name = tax_clean.columns[i]\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = tax_clean.iloc[label_tuple]\n",
    "        dirty_context = tax_dirty.iloc[label_tuple]\n",
    "        clean_cell = tax_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = tax_dirty.iloc[label_tuple,i]\n",
    "        single_context_clean = 'COL %s VAL %s ' % (tax_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (tax_dirty.columns[i],dirty_cell)\n",
    "        for c in range(15):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (tax_clean.columns[c],tax_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (tax_dirty.columns[c],tax_dirty.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            detector_list_tax.append([all_context_dirty,single_context_dirty,1])\n",
    "        else:\n",
    "            detector_list_tax.append([all_context_dirty,single_context_dirty,0])\n",
    "            # detector_list.append([single_context_dirty,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1301,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_test_index = np.where(input_matrix_tax.sum(axis=1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1311,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_tax).to_csv('datasets/tax/detector/test.csv')\n",
    "# pd.DataFrame(detector_list_tax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 1330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector_tax = np.load('datasets/tax/detector/detection_cell.npy')\n",
    "detector_tax = detector_tax.reshape((-1,15))\n",
    "detector_tax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix_tax[:,-2].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: married_exemp, dtype: int64)"
      ]
     },
     "execution_count": 1348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_dirty[(tax_dirty['marital_status']!=tax_clean['marital_status']) & (tax_dirty['marital_status']=='S')]['married_exemp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    print(tax_clean.columns[i])\n",
    "    print(i)\n",
    "    print(precision_score(y_pred = detector_tax[:,i],y_true = input_matrix_tax[selected_test_index,i]),recall_score(y_pred = detector_tax[:,i],y_true = input_matrix_tax[selected_test_index,i]),f1_score(y_pred = detector_tax[:,i],y_true = input_matrix_tax[selected_test_index,i]),input_matrix_tax[selected_test_index,i].sum()) ## selected_test_index is all dirty rows, for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input \n",
      "\n",
      "['AK-*', 'NJ', 'AK-*', 'NJ']\n",
      "\n",
      "are some dirty cells from table Tax column state, and the input \n",
      "\n",
      "['AK', 'NY', 'AK', 'LA']\n",
      "\n",
      " are corrected clean cells, and ['AK', 'AK', 'AK', 'CO', 'VA', 'AL', 'AL', 'AL', 'AL', 'AK', 'AL', 'MD', 'AL', 'AL', 'WA', 'AL'] are examples of corresponding corrected clean cells. please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28194/1479885975.py:6: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  clean_list = tax_label_clean[tax_dirty[col_name]!=tax_clean[col_name]][col_name].to_list()\n",
      "/tmp/ipykernel_28194/1479885975.py:7: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  dirty_list = tax_label_dirty[tax_dirty[col_name]!=tax_clean[col_name]][col_name].to_list()\n"
     ]
    }
   ],
   "source": [
    "## Loop Refinement\n",
    "i = 6\n",
    "tax_label_dirty = tax_dirty.iloc[tax_label_index]\n",
    "tax_label_clean = tax_clean.iloc[tax_label_index]\n",
    "col_name = tax_clean.columns[i]\n",
    "clean_list = tax_label_clean[tax_dirty[col_name]!=tax_clean[col_name]][col_name].to_list()\n",
    "dirty_list = tax_label_dirty[tax_dirty[col_name]!=tax_clean[col_name]][col_name].to_list()\n",
    "clean_list_origin = tax_label_clean[tax_label_dirty[col_name]==tax_label_clean[col_name]][col_name].to_list()\n",
    "# clean_list = rayyan_label_clean[col_name].to_list()\n",
    "# dirty_list = rayyan_label_dirty[col_name].to_list()\n",
    "# clean_list\n",
    "# detector_inference = \"The input \\n\\n%s\\n\\nare some dirty cells from table rayyan column %s, and the input \\n\\n%s\\n\\n are corresponding clean ones, please write a general function to detect whether a given cell is dirty or not\" % (dirty_list,col_name,clean_list)\n",
    "detector_inference = \"The input \\n\\n%s\\n\\nare some dirty cells from table Tax column %s, and the input \\n\\n%s\\n\\n are corrected clean cells, and %s are examples of corresponding corrected clean cells. please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (dirty_list,col_name,clean_list,clean_list_origin)\n",
    "# print(detector_inference)\n",
    "print(detector_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "694"
      ]
     },
     "execution_count": 1360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([is_dirty_cell(c) for c in tax_dirty.iloc[:,1].to_list()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Shin'ya\", \"M'Lissa\", \"M'hamed\", \"M'Lissa\", \"Irs'hak\"]"
      ]
     },
     "execution_count": 1368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    " ## f_name,l_name\n",
    "def is_dirty_cell(cell):\n",
    "    # 使用正则表达式来查找两个连续的单引号\n",
    "    dirty_pattern = re.compile(r\"''\")\n",
    "    \n",
    "    # 检查单元格是否包含脏模式\n",
    "    if re.search(dirty_pattern, cell):\n",
    "        return True\n",
    "    return False\n",
    "def clean_dirty_cell(dirty_cell):\n",
    "    # 使用字符串的替换函数将两个连续的单引号替换为一个单引号\n",
    "    clean_cell = dirty_cell.replace(\"''\", \"'\")\n",
    "    return clean_cell\n",
    "# 示例输入\n",
    "dirty_cells = [\"Shin''ya\", \"M''Lissa\", \"M''hamed\", \"M''Lissa\", \"Irs''hak\"]\n",
    "clean_cells = [\"Shin'ya\", \"M'Lissa\", \"M'hamed\", \"M'Lissa\", \"Irs'hak\"]\n",
    "other_clean_cells = ['Torbjorn', 'Emin', 'Toni', 'Jackson', 'Hendra', 'Mehul', 'Byoung', 'Iskender', 'Laurie', 'Seongtaek', 'Shibin', 'Leonor', 'Dentcho', 'Cedric', 'Roji']\n",
    "[clean_dirty_cell(c) for c in dirty_cells]\n",
    "# 使用函数检测脏单元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 1369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    " ## city\n",
    "def is_dirty_cell(cell):\n",
    "    # 使用正则表达式来查找以连字符（-*）结尾的单元格\n",
    "    dirty_pattern = re.compile(r'-\\*$')\n",
    "    \n",
    "    # 检查单元格是否匹配脏模式\n",
    "    if re.search(dirty_pattern, cell):\n",
    "        return True\n",
    "    return False\n",
    "def clean_dirty_cell(dirty_cell):\n",
    "    # 使用字符串的替换函数将连字符（-*）删除\n",
    "    clean_cell = dirty_cell.replace('-*', '')\n",
    "    return clean_cell\n",
    "# sum([is_dirty_cell(c) for c in tax_dirty.iloc[:,5].to_list()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1379,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('datasets/tax/detector/index.npy',tax_label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1380,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('datasets/rayyan/detector/index.npy',rayyan_label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1382,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('datasets/beers/detector/index.npy',beer_label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1385,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('datasets/hospital/detector/index.npy',selected_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1399,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('datasets/flights/detector/index.npy',flight_label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_label_index = np.load('datasets/beers/detector/index.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1389,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flight_unique\n",
    "flight_label_index = []\n",
    "for i in range(20):\n",
    "    flight_cluster_name = flight_unique[cluster_select_flight[i]]\n",
    "    flight_cluter_df = list(flight_dirty[flight_dirty['flight']==flight_cluster_name].index)[0]\n",
    "    flight_label_index.append(flight_cluter_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1400,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_detector = np.load('datasets/tax/detector/detection_cell.npy')\n",
    "tax_detector = tax_detector.reshape((-1,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    1],\n",
       "       [   1,    0],\n",
       "       [   2,    1],\n",
       "       ...,\n",
       "       [2926,    6],\n",
       "       [2927,    0],\n",
       "       [2928,    1]])"
      ]
     },
     "execution_count": 1402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_detector_indice = np.argwhere(tax_detector==1)\n",
    "tax_detector_indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1523,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_dirty_select = tax_dirty.iloc[selected_test_index].reset_index(drop=True)\n",
    "tax_clean_select = tax_clean.iloc[selected_test_index].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1407,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list_label = []\n",
    "for d in tax_detector_indice:\n",
    "    index = d[0]\n",
    "    i = d[1]\n",
    "    # i = d[1] + 1 ## 重要，我们在detector中忽略了index列\n",
    "    # for i in range(15):\n",
    "    dirty_cell = tax_dirty_select.iloc[index,i]\n",
    "    clean_cell = tax_clean_select.iloc[index,i]\n",
    "    col_name = tax_clean.columns[i]\n",
    "    template_dict = {}\n",
    "    template_dict[col_name] = ''\n",
    "    temp_dict = tax_dirty_select.iloc[index,:-1].to_dict()\n",
    "    clean_dict = {}\n",
    "    clean_dict[col_name] = clean_cell\n",
    "    coreset_reference = np.random.choice([c for c in tax_label_index if c!=index],3,replace=False)\n",
    "    # if(dirty_cell!=clean_cell): ## 在LLM Inference中忽略\n",
    "    text_head = 'You are an expert in cleaning Tax Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\nTake these rows as reference:\\n\\n' % (col_name, json.dumps(template_dict), json.dumps(temp_dict))\n",
    "    dict_0 = tax_clean.iloc[coreset_reference[0]].to_dict()\n",
    "    dict_1 = tax_clean.iloc[coreset_reference[1]].to_dict()\n",
    "    dict_2 = tax_clean.iloc[coreset_reference[2]].to_dict()\n",
    "    ICL_text = '%s\\n\\n%s\\n\\n%s\\n\\n' % (json.dumps(dict_0),json.dumps(dict_1),json.dumps(dict_2))\n",
    "    # ICL_text = '%s\\n\\n' % (json.dumps(dict_0))\n",
    "    training_list_label.append([text_head,ICL_text,'',json.dumps(clean_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"Hendra\", \"l_name\": \"O'Boyle\", \"gen...</td>\n",
       "      <td></td>\n",
       "      <td>{\"l_name\": \"d'Argence\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"l_name\": \"d'Argence\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"Hendra\", \"l_name\": \"O'Boyle\", \"gen...</td>\n",
       "      <td></td>\n",
       "      <td>{\"f_name\": \"Jun'ichi\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"f_name\": \"Jun'ichi\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"Toni\", \"l_name\": \"McElwain\", \"gend...</td>\n",
       "      <td></td>\n",
       "      <td>{\"l_name\": \"D'Amiano\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"l_name\": \"D'Amiano\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"Leonor\", \"l_name\": \"O'Young\", \"gen...</td>\n",
       "      <td></td>\n",
       "      <td>{\"state\": \"NJ\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"state\": \"NJ\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"Dentcho\", \"l_name\": \"Stancampiano\"...</td>\n",
       "      <td></td>\n",
       "      <td>{\"l_name\": \"Give'on\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"l_name\": \"Give'on\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"M'Lissa\", \"l_name\": \"Baumann\", \"ge...</td>\n",
       "      <td></td>\n",
       "      <td>{\"city\": \"ARCHBALD\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"city\": \"ARCHBALD\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"Torbjorn\", \"l_name\": \"Kesselman\", ...</td>\n",
       "      <td></td>\n",
       "      <td>{\"f_name\": \"Surveyors'\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"f_name\": \"Surveyors'\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"Shibin\", \"l_name\": \"Raufmann\", \"ge...</td>\n",
       "      <td></td>\n",
       "      <td>{\"state\": \"AK\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"state\": \"AK\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"Jackson\", \"l_name\": \"Froberg\", \"ge...</td>\n",
       "      <td></td>\n",
       "      <td>{\"f_name\": \"Irs'hak\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"f_name\": \"Irs'hak\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2471</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"M'hamed\", \"l_name\": \"O'Boyle\", \"ge...</td>\n",
       "      <td></td>\n",
       "      <td>{\"l_name\": \"L'Ecuyer\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"l_name\": \"L'Ecuyer\"}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2472 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0  \\\n",
       "0     You are an expert in cleaning Tax Dataset. Giv...   \n",
       "1     You are an expert in cleaning Tax Dataset. Giv...   \n",
       "2     You are an expert in cleaning Tax Dataset. Giv...   \n",
       "3     You are an expert in cleaning Tax Dataset. Giv...   \n",
       "4     You are an expert in cleaning Tax Dataset. Giv...   \n",
       "...                                                 ...   \n",
       "2467  You are an expert in cleaning Tax Dataset. Giv...   \n",
       "2468  You are an expert in cleaning Tax Dataset. Giv...   \n",
       "2469  You are an expert in cleaning Tax Dataset. Giv...   \n",
       "2470  You are an expert in cleaning Tax Dataset. Giv...   \n",
       "2471  You are an expert in cleaning Tax Dataset. Giv...   \n",
       "\n",
       "                                                      1 2   \\\n",
       "0     {\"f_name\": \"Hendra\", \"l_name\": \"O'Boyle\", \"gen...      \n",
       "1     {\"f_name\": \"Hendra\", \"l_name\": \"O'Boyle\", \"gen...      \n",
       "2     {\"f_name\": \"Toni\", \"l_name\": \"McElwain\", \"gend...      \n",
       "3     {\"f_name\": \"Leonor\", \"l_name\": \"O'Young\", \"gen...      \n",
       "4     {\"f_name\": \"Dentcho\", \"l_name\": \"Stancampiano\"...      \n",
       "...                                                 ... ..   \n",
       "2467  {\"f_name\": \"M'Lissa\", \"l_name\": \"Baumann\", \"ge...      \n",
       "2468  {\"f_name\": \"Torbjorn\", \"l_name\": \"Kesselman\", ...      \n",
       "2469  {\"f_name\": \"Shibin\", \"l_name\": \"Raufmann\", \"ge...      \n",
       "2470  {\"f_name\": \"Jackson\", \"l_name\": \"Froberg\", \"ge...      \n",
       "2471  {\"f_name\": \"M'hamed\", \"l_name\": \"O'Boyle\", \"ge...      \n",
       "\n",
       "                             3  \\\n",
       "0      {\"l_name\": \"d'Argence\"}   \n",
       "1       {\"f_name\": \"Jun'ichi\"}   \n",
       "2       {\"l_name\": \"D'Amiano\"}   \n",
       "3              {\"state\": \"NJ\"}   \n",
       "4        {\"l_name\": \"Give'on\"}   \n",
       "...                        ...   \n",
       "2467      {\"city\": \"ARCHBALD\"}   \n",
       "2468  {\"f_name\": \"Surveyors'\"}   \n",
       "2469           {\"state\": \"AK\"}   \n",
       "2470     {\"f_name\": \"Irs'hak\"}   \n",
       "2471    {\"l_name\": \"L'Ecuyer\"}   \n",
       "\n",
       "                                            instruction input  \\\n",
       "0     You are an expert in cleaning Tax Dataset. Giv...         \n",
       "1     You are an expert in cleaning Tax Dataset. Giv...         \n",
       "2     You are an expert in cleaning Tax Dataset. Giv...         \n",
       "3     You are an expert in cleaning Tax Dataset. Giv...         \n",
       "4     You are an expert in cleaning Tax Dataset. Giv...         \n",
       "...                                                 ...   ...   \n",
       "2467  You are an expert in cleaning Tax Dataset. Giv...         \n",
       "2468  You are an expert in cleaning Tax Dataset. Giv...         \n",
       "2469  You are an expert in cleaning Tax Dataset. Giv...         \n",
       "2470  You are an expert in cleaning Tax Dataset. Giv...         \n",
       "2471  You are an expert in cleaning Tax Dataset. Giv...         \n",
       "\n",
       "                        output  \n",
       "0      {\"l_name\": \"d'Argence\"}  \n",
       "1       {\"f_name\": \"Jun'ichi\"}  \n",
       "2       {\"l_name\": \"D'Amiano\"}  \n",
       "3              {\"state\": \"NJ\"}  \n",
       "4        {\"l_name\": \"Give'on\"}  \n",
       "...                        ...  \n",
       "2467      {\"city\": \"ARCHBALD\"}  \n",
       "2468  {\"f_name\": \"Surveyors'\"}  \n",
       "2469           {\"state\": \"AK\"}  \n",
       "2470     {\"f_name\": \"Irs'hak\"}  \n",
       "2471    {\"l_name\": \"L'Ecuyer\"}  \n",
       "\n",
       "[2472 rows x 7 columns]"
      ]
     },
     "execution_count": 1409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.DataFrame(training_list_label)\n",
    "training_list_pd = pd.DataFrame(training_list_label)\n",
    "training_list_pd['instruction'] = training_list_pd[0] + training_list_pd[1]\n",
    "training_list_pd['input'] = training_list_pd[2]\n",
    "training_list_pd['output'] = training_list_pd[3]\n",
    "training_list_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1410,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(training_list_pd.iloc[:,-3:].to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/tax/tax-test.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1411,
   "metadata": {},
   "outputs": [],
   "source": [
    "rayyan_clean.to_csv('datasets/rayyan/detector/clean.csv')\n",
    "rayyan_dirty.to_csv('datasets/rayyan/detector/dirty.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1412,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_clean.to_csv('datasets/flights/detector/clean.csv')\n",
    "flight_dirty.to_csv('datasets/flights/detector/dirty.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1413,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_clean.to_csv('datasets/hospital/detector/clean.csv')\n",
    "hospital_dirty.to_csv('datasets/hospital/detector/dirty.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1414,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_clean.to_csv('datasets/tax/detector/clean.csv')\n",
    "tax_dirty.to_csv('datasets/tax/detector/dirty.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1415,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_clean.to_csv('datasets/beers/detector/beer_clean.csv')\n",
    "beer_dirty.to_csv('datasets/beers/detector/beer_dirty.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>article_title</th>\n",
       "      <th>article_language</th>\n",
       "      <th>journal_title</th>\n",
       "      <th>jounral_abbreviation</th>\n",
       "      <th>journal_issn</th>\n",
       "      <th>article_jvolumn</th>\n",
       "      <th>article_jissue</th>\n",
       "      <th>article_jcreated_at</th>\n",
       "      <th>article_pagination</th>\n",
       "      <th>author_list</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>62306</td>\n",
       "      <td>Mandibular second molar with C-shaped canal mo...</td>\n",
       "      <td>eng</td>\n",
       "      <td>General dentistry</td>\n",
       "      <td>Gen Dent</td>\n",
       "      <td>0363-6771</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>5/1/04</td>\n",
       "      <td>253-4</td>\n",
       "      <td>{\"Fred W Benenati\"}</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                      article_title  \\\n",
       "149  62306  Mandibular second molar with C-shaped canal mo...   \n",
       "\n",
       "    article_language      journal_title jounral_abbreviation journal_issn  \\\n",
       "149              eng  General dentistry             Gen Dent    0363-6771   \n",
       "\n",
       "    article_jvolumn article_jissue article_jcreated_at article_pagination  \\\n",
       "149              52              3              5/1/04              253-4   \n",
       "\n",
       "             author_list  index  \n",
       "149  {\"Fred W Benenati\"}    149  "
      ]
     },
     "execution_count": 1428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_dirty[rayyan_dirty['article_jcreated_at']=='5/1/04']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>article_title</th>\n",
       "      <th>article_language</th>\n",
       "      <th>journal_title</th>\n",
       "      <th>jounral_abbreviation</th>\n",
       "      <th>journal_issn</th>\n",
       "      <th>article_jvolumn</th>\n",
       "      <th>article_jissue</th>\n",
       "      <th>article_jcreated_at</th>\n",
       "      <th>article_pagination</th>\n",
       "      <th>author_list</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>62306</td>\n",
       "      <td>Mandibular second molar with C-shaped canal mo...</td>\n",
       "      <td>eng</td>\n",
       "      <td>General dentistry</td>\n",
       "      <td>Gen Dent</td>\n",
       "      <td>0363-6771</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>5/1/04</td>\n",
       "      <td>253-4</td>\n",
       "      <td>{\"Fred W Benenati\"}</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                      article_title  \\\n",
       "149  62306  Mandibular second molar with C-shaped canal mo...   \n",
       "\n",
       "    article_language      journal_title jounral_abbreviation journal_issn  \\\n",
       "149              eng  General dentistry             Gen Dent    0363-6771   \n",
       "\n",
       "    article_jvolumn article_jissue article_jcreated_at article_pagination  \\\n",
       "149              52              3              5/1/04              253-4   \n",
       "\n",
       "             author_list  index  \n",
       "149  {\"Fred W Benenati\"}    149  "
      ]
     },
     "execution_count": 1439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tax_clean.columns\n",
    "# len(rayyan_clean['journal_title'].unique())\n",
    "rayyan_dirty[rayyan_clean['journal_title']=='General dentistry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>article_title</th>\n",
       "      <th>article_language</th>\n",
       "      <th>journal_title</th>\n",
       "      <th>jounral_abbreviation</th>\n",
       "      <th>journal_issn</th>\n",
       "      <th>article_jvolumn</th>\n",
       "      <th>article_jissue</th>\n",
       "      <th>article_jcreated_at</th>\n",
       "      <th>article_pagination</th>\n",
       "      <th>author_list</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>62306</td>\n",
       "      <td>Mandibular second molar with C-shaped canal mo...</td>\n",
       "      <td>eng</td>\n",
       "      <td>General dentistry</td>\n",
       "      <td>Gen Dent</td>\n",
       "      <td>0363-6771</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>1/4/05</td>\n",
       "      <td>253-4</td>\n",
       "      <td>{\"Fred W Benenati\"}</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                      article_title  \\\n",
       "149  62306  Mandibular second molar with C-shaped canal mo...   \n",
       "\n",
       "    article_language      journal_title jounral_abbreviation journal_issn  \\\n",
       "149              eng  General dentistry             Gen Dent    0363-6771   \n",
       "\n",
       "    article_jvolumn article_jissue article_jcreated_at article_pagination  \\\n",
       "149              52              3              1/4/05              253-4   \n",
       "\n",
       "             author_list  index  \n",
       "149  {\"Fred W Benenati\"}    149  "
      ]
     },
     "execution_count": 1440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_clean[rayyan_clean['journal_title']=='General dentistry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1584\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "tax_result = pd.read_csv('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/tax-test.csv',index_col=0)\n",
    "tax_result['item'] = ''\n",
    "tax_result['gt'] = ''\n",
    "for index,row in tax_result.iterrows():\n",
    "    try:\n",
    "        temp_dict = ast.literal_eval(row[3])\n",
    "    \n",
    "        tax_result.iloc[index,-2] = list(temp_dict.values())[0]\n",
    "    except:\n",
    "        print(index)\n",
    "    try:\n",
    "        gt_dict = ast.literal_eval(row[2])\n",
    "        tax_result.iloc[index,-1] = list(gt_dict.values())[0]\n",
    "    except:\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_result[tax_result['gt']!=tax_result['item']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_clean.columns\n",
    "pattern = tax_clean.iloc[:,-7] + '|' +  tax_clean.iloc[:,-3] +'|' + tax_clean.iloc[:,-2]\n",
    "pd.Series(pattern).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([272, 694,   0,   0,   0, 200, 600, 400, 200, 200,   0,   0, 200,\n",
       "         0, 200])"
      ]
     },
     "execution_count": 1458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix_tax[selected_test_index].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['f_name', 'l_name', 'city', 'state', 'zip', 'marital_status',\n",
       "       'has_child', 'single_exemp', 'child_exemp'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_col = [0,1,5,6,7,8,9,12,14]\n",
    "tax_clean.columns[tax_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1563,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_dirty_select = tax_dirty.iloc[selected_test_index].reset_index(drop=True)\n",
    "tax_clean_select = tax_clean.iloc[selected_test_index].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1564,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_dirty_correct = tax_dirty_select\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1565,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Special Correction\n",
    "# tax_dirty_correct.iloc[:,0] = tax_dirty_select.iloc[:,0].str.replace(\"''\",\"'\")\n",
    "# tax_dirty_correct.iloc[:,1] = tax_dirty_select.iloc[:,1].str.replace(\"''\",\"'\")\n",
    "# tax_dirty_correct.iloc[:,5] = tax_dirty_select.iloc[:,5].str.replace('*', '').replace('-', '')\n",
    "# tax_dirty_correct.iloc[:,6] = tax_dirty_select.iloc[:,6].str.replace('*', '').replace('-', '')\n",
    "# tax_dirty_correct.iloc[:,7] = tax_dirty_select.iloc[:,7].str.replace('*', '').replace('-', '')\n",
    "# tax_dirty_correct.iloc[:,8] = tax_dirty_select.iloc[:,8].str.replace('*', '').replace('-', '')\n",
    "# tax_dirty_correct.iloc[:,9] = tax_dirty_select.iloc[:,9].str.replace('*', '').replace('-', '')\n",
    "# tax_dirty_correct.iloc[:,12] = tax_dirty_select.iloc[:,12].replace('*', '').replace('-', '')\n",
    "# tax_dirty_correct.iloc[:,14] = tax_dirty_select.iloc[:,14].replace('*', '').replace('-', '')\n",
    "# tax_dirty_correct = tax_dirty_correct.str.replace(\"''\",\"'\").replace('*', '').replace('-', '')\n",
    "def FormatTax(row):\n",
    "    for x,y in row[[0,1,5,6,7,8,9,12,14]].items():\n",
    "        row[x] = y.replace(\"''\",\"'\").replace('-*', '')\n",
    "    return row\n",
    "tax_dirty_correct = tax_dirty_select.apply(FormatTax,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1566,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Graph_Dependency_State(row):\n",
    "    ## Dependencies for city-state\n",
    "    city = row['city']\n",
    "    replace = tax_dirty[tax_dirty['city']==city]['state'].mode().values[0]\n",
    "    return replace\n",
    "def Graph_Dependency_Zip(row):\n",
    "    ## Dependencies for city-state\n",
    "    if(row['zip']=='1907'):\n",
    "        city = row['city']\n",
    "        state = row['state']\n",
    "        replace = tax_dirty[(tax_dirty['city']==city) & (tax_dirty['state']==state)]['zip'].mode().values[0]\n",
    "        return replace\n",
    "    else:\n",
    "        return row['zip']\n",
    "# Graph_Dependency_Zip(tax_dirty.iloc[4549])\n",
    "tax_dirty_correct.iloc[:,7] = tax_dirty_correct.apply(Graph_Dependency_Zip,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1567,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Graph_Dependency_Marriage(row):\n",
    "    if(row['married_exemp']=='0') and (row['single_exemp']=='0'):\n",
    "        return row['marital_status']\n",
    "    else:\n",
    "        if(row['married_exemp']!='0'):\n",
    "            row['marital_status'] = 'M'\n",
    "        else:\n",
    "            row['marital_status'] = 'S'\n",
    "        return row['marital_status']\n",
    "tax_dirty_correct.iloc[:,8] = tax_dirty_correct.apply(Graph_Dependency_Marriage,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1568,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Graph_Dependency_Child(row):\n",
    "    if(row['child_exemp']!='0'):\n",
    "        row['has_child'] = 'Y'\n",
    "    return row['has_child']\n",
    "tax_dirty_correct.iloc[:,9] = tax_dirty_correct.apply(Graph_Dependency_Child,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Precision_tax = (496-57) / 496\n",
    "Recall_tax = (496-57) / input_matrix.sum()\n",
    "F1_hospital = (2 * Precision_hospital * Recall_hospital) / (Precision_hospital + Recall_hospital)\n",
    "Precision_hospital,Recall_hospital,F1_hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1585,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2966, 2438, 2550)"
      ]
     },
     "execution_count": 1585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_dirty_select = tax_dirty.iloc[selected_test_index].reset_index(drop=True)\n",
    "tax_clean_select = tax_clean.iloc[selected_test_index].reset_index(drop=True)\n",
    "All_Data_Error = 0\n",
    "Correct_Fixed_Error = 0\n",
    "All_Fixed_Error = 0\n",
    "list_wrong = []\n",
    "for x in range(2929):\n",
    "    for y in range(15):\n",
    "        dirty_cell = tax_dirty_select.iloc[x,y]\n",
    "        clean_cell = tax_clean_select.iloc[x,y]\n",
    "        corrected_cell = tax_dirty_correct.iloc[x,y]\n",
    "        if(clean_cell!=dirty_cell):\n",
    "            All_Data_Error += 1\n",
    "        if(dirty_cell!=corrected_cell):\n",
    "            All_Fixed_Error += 1\n",
    "            if(corrected_cell==clean_cell):\n",
    "                Correct_Fixed_Error += 1\n",
    "            else:\n",
    "                list_wrong.append([clean_cell,corrected_cell,x,y])\n",
    "All_Data_Error,Correct_Fixed_Error,All_Fixed_Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.956078431372549, 0.8219824679703304, 0.8839738941261784)"
      ]
     },
     "execution_count": 1587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Precision = Correct_Fixed_Error / All_Fixed_Error\n",
    "Recall = Correct_Fixed_Error / All_Data_Error\n",
    "F1_tax = (2 * Precision * Recall) / (Precision + Recall)\n",
    "Precision,Recall,F1_tax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.956078431372549, 0.8219824679703304, 0.8839738941261784) Tax_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'article_title', 'article_language', 'journal_title',\n",
       "       'jounral_abbreviation', 'journal_issn', 'article_jvolumn',\n",
       "       'article_jissue', 'article_jcreated_at', 'article_pagination',\n",
       "       'author_list', 'index'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 15\n",
    "tax_dirty_correct[tax_dirty_correct.iloc[:,i]!=tax_clean_select.iloc[:,i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1586,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95209</td>\n",
       "      <td>95207</td>\n",
       "      <td>58</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84157</td>\n",
       "      <td>84132</td>\n",
       "      <td>60</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99901</td>\n",
       "      <td>99950</td>\n",
       "      <td>61</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83606</td>\n",
       "      <td>83607</td>\n",
       "      <td>62</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68134</td>\n",
       "      <td>68122</td>\n",
       "      <td>63</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>25302</td>\n",
       "      <td>25304</td>\n",
       "      <td>1460</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>985</td>\n",
       "      <td>984</td>\n",
       "      <td>1461</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>27106</td>\n",
       "      <td>27198</td>\n",
       "      <td>1466</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>55573</td>\n",
       "      <td>55555</td>\n",
       "      <td>1467</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>19808</td>\n",
       "      <td>19892</td>\n",
       "      <td>1472</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1     2  3\n",
       "0    95209  95207    58  7\n",
       "1    84157  84132    60  7\n",
       "2    99901  99950    61  7\n",
       "3    83606  83607    62  7\n",
       "4    68134  68122    63  7\n",
       "..     ...    ...   ... ..\n",
       "107  25302  25304  1460  7\n",
       "108    985    984  1461  7\n",
       "109  27106  27198  1466  7\n",
       "110  55573  55555  1467  7\n",
       "111  19808  19892  1472  7\n",
       "\n",
       "[112 rows x 4 columns]"
      ]
     },
     "execution_count": 1586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(list_wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([158, 455,  85, 118, 322, 384, 392, 615, 656, 796, 862, 975, 979,\n",
       "       357, 130, 532, 212, 694, 924, 918])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_clean = pd.read_csv('datasets/rayyan/clean.csv').fillna('')\n",
    "rayyan_dirty = pd.read_csv('datasets/rayyan/dirty.csv').fillna('')\n",
    "rayyan_label = np.load('datasets/rayyan/detector/index.npy')\n",
    "rayyan_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hospital-Vary-LabelBudget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "509"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hospital_clean = pd.read_csv('datasets/hospital/clean.csv').astype(str)\n",
    "hospital_dirty = pd.read_csv('datasets/hospital/dirty.csv').astype(str)\n",
    "hospital_query = pd.read_csv('datasets/hospital/dirty_query.csv')\n",
    "hospital_dirty.columns = hospital_clean.columns\n",
    "# hospital_query\n",
    "input_matrix = np.array(hospital_clean!=hospital_dirty)\n",
    "input_matrix.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([532, 157, 213, 228, 238, 256, 332, 386, 411, 493, 572, 698, 956,\n",
       "        22,  24,  42,  56,  57,  93,  94])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_index_hospital = np.load('datasets/hospital/detector/index.npy')\n",
    "selected_index_hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([532, 157, 213, 228, 238])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_index_hospital_5 = selected_index_hospital[:10]\n",
    "selected_index_hospital_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 4, 0, 0, 3, 4, 3, 2, 5, 5, 7, 3, 4, 0, 4, 4, 1, 2]),\n",
       " array([ 0, 28, 24, 31,  0,  0, 33, 26, 30, 39, 34, 32, 27, 27, 32, 29, 36,\n",
       "        23, 31, 27]))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_index_hospital_5 = selected_index_hospital[:20]\n",
    "input_matrix[selected_index_hospital_5].sum(axis=0),input_matrix.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ProviderNumber</th>\n",
       "      <th>HospitalName</th>\n",
       "      <th>Address1</th>\n",
       "      <th>Address2</th>\n",
       "      <th>Address3</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>ZipCode</th>\n",
       "      <th>CountyName</th>\n",
       "      <th>PhoneNumber</th>\n",
       "      <th>HospitalType</th>\n",
       "      <th>HospitalOwner</th>\n",
       "      <th>EmergencyService</th>\n",
       "      <th>Condition</th>\n",
       "      <th>MeasureCode</th>\n",
       "      <th>MeasureName</th>\n",
       "      <th>Score</th>\n",
       "      <th>Sample</th>\n",
       "      <th>Stateavg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>10007</td>\n",
       "      <td>mizell memorial hospital</td>\n",
       "      <td>702 n main st</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>opp</td>\n",
       "      <td>al</td>\n",
       "      <td>36467</td>\n",
       "      <td>covington</td>\n",
       "      <td>3344933541</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>no</td>\n",
       "      <td>heart attack</td>\n",
       "      <td>axi-4</td>\n",
       "      <td>heart attack patients given smoking cessation ...</td>\n",
       "      <td>empty</td>\n",
       "      <td>0 patients</td>\n",
       "      <td>al_ami-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>102</td>\n",
       "      <td>10007</td>\n",
       "      <td>mizell memorial hospital</td>\n",
       "      <td>702 n main st</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>opp</td>\n",
       "      <td>al</td>\n",
       "      <td>36467</td>\n",
       "      <td>covington</td>\n",
       "      <td>3344x33541</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>no</td>\n",
       "      <td>heart failure</td>\n",
       "      <td>hfx1</td>\n",
       "      <td>heart failure patients given discharge instruc...</td>\n",
       "      <td>98%</td>\n",
       "      <td>59 patients</td>\n",
       "      <td>al_hf-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>109</td>\n",
       "      <td>10007</td>\n",
       "      <td>mizell memorial hospital</td>\n",
       "      <td>702 n main st</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>opp</td>\n",
       "      <td>al</td>\n",
       "      <td>36467</td>\n",
       "      <td>covington</td>\n",
       "      <td>3344933541</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>no</td>\n",
       "      <td>pneumonia</td>\n",
       "      <td>pnx5c</td>\n",
       "      <td>pneumonia patients given initial antibiotic(s)...</td>\n",
       "      <td>95%</td>\n",
       "      <td>87 patients</td>\n",
       "      <td>al_pn-5c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>121</td>\n",
       "      <td>10008</td>\n",
       "      <td>crenshaw community hospital</td>\n",
       "      <td>101 hospital circle</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>luverne</td>\n",
       "      <td>al</td>\n",
       "      <td>36049</td>\n",
       "      <td>crenshaw</td>\n",
       "      <td>3343353374</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>government - federal</td>\n",
       "      <td>yes</td>\n",
       "      <td>heart attack</td>\n",
       "      <td>axi-2</td>\n",
       "      <td>heart attack patients given aspirin at discharge</td>\n",
       "      <td>x00%</td>\n",
       "      <td>1 patients</td>\n",
       "      <td>al_ami-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>178</td>\n",
       "      <td>10010</td>\n",
       "      <td>marshall medical center north</td>\n",
       "      <td>8000 alabama highway 69</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>guntersville</td>\n",
       "      <td>al</td>\n",
       "      <td>35976</td>\n",
       "      <td>marshall</td>\n",
       "      <td>2565718000</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>government - hospital district or authority</td>\n",
       "      <td>yes</td>\n",
       "      <td>heart failure</td>\n",
       "      <td>hf-x</td>\n",
       "      <td>heart failure patients given an evaluation of ...</td>\n",
       "      <td>84%</td>\n",
       "      <td>80 patients</td>\n",
       "      <td>al_hf-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>191</td>\n",
       "      <td>10010</td>\n",
       "      <td>marshall medical center north</td>\n",
       "      <td>8000 alabama highway 69</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>guntersville</td>\n",
       "      <td>al</td>\n",
       "      <td>35976</td>\n",
       "      <td>marshall</td>\n",
       "      <td>2565718000</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>government - hospital district or authority</td>\n",
       "      <td>yes</td>\n",
       "      <td>surgical infection prevention</td>\n",
       "      <td>scip-inx-4</td>\n",
       "      <td>all heart surgery patients whose blood sugar (...</td>\n",
       "      <td>empty</td>\n",
       "      <td>0 patients</td>\n",
       "      <td>al_scip-inf-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>265</td>\n",
       "      <td>10015</td>\n",
       "      <td>southwest alabama medical center</td>\n",
       "      <td>33700 highway x3</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>thomasville</td>\n",
       "      <td>al</td>\n",
       "      <td>36784</td>\n",
       "      <td>clarke</td>\n",
       "      <td>3346366221</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>government - federal</td>\n",
       "      <td>yes</td>\n",
       "      <td>surgical infection prevention</td>\n",
       "      <td>scxp-xnf-3</td>\n",
       "      <td>surgery patients whose preventive antibiotics ...</td>\n",
       "      <td>86%</td>\n",
       "      <td>14 patients</td>\n",
       "      <td>al_scip-inf-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>350</td>\n",
       "      <td>10056</td>\n",
       "      <td>st vincents hospital</td>\n",
       "      <td>810 st vincents drive</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>birmingham</td>\n",
       "      <td>al</td>\n",
       "      <td>35205</td>\n",
       "      <td>jefferson</td>\n",
       "      <td>2059397000</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - other</td>\n",
       "      <td>yes</td>\n",
       "      <td>heart attack</td>\n",
       "      <td>amix2</td>\n",
       "      <td>heart attack patients given aspirin at discharge</td>\n",
       "      <td>98%</td>\n",
       "      <td>260 patients</td>\n",
       "      <td>al_ami-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>392</td>\n",
       "      <td>10087</td>\n",
       "      <td>univ of south alabama medical center</td>\n",
       "      <td>2451 fillingim street</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>mobile</td>\n",
       "      <td>al</td>\n",
       "      <td>36617</td>\n",
       "      <td>mobile</td>\n",
       "      <td>2514717110</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>government - state</td>\n",
       "      <td>yes</td>\n",
       "      <td>pneumonia</td>\n",
       "      <td>pn-xb</td>\n",
       "      <td>pneumonia patients whose initial emergency roo...</td>\n",
       "      <td>89%</td>\n",
       "      <td>35 patients</td>\n",
       "      <td>al_pn-3b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>399</td>\n",
       "      <td>10108</td>\n",
       "      <td>prattville baptist hospital</td>\n",
       "      <td>124 s memorial dr</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>prattville</td>\n",
       "      <td>al</td>\n",
       "      <td>36067</td>\n",
       "      <td>autauga</td>\n",
       "      <td>3343614267</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>yes</td>\n",
       "      <td>heart failure</td>\n",
       "      <td>hx-1</td>\n",
       "      <td>heart failure patients given discharge instruc...</td>\n",
       "      <td>96%</td>\n",
       "      <td>75 patients</td>\n",
       "      <td>al_hf-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>410</td>\n",
       "      <td>10108</td>\n",
       "      <td>prattville baptist hospital</td>\n",
       "      <td>124 s memorial dr</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>prattville</td>\n",
       "      <td>al</td>\n",
       "      <td>36067</td>\n",
       "      <td>autauga</td>\n",
       "      <td>3343614267</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>yes</td>\n",
       "      <td>surgical infection prevention</td>\n",
       "      <td>scipxinfx1</td>\n",
       "      <td>surgery patients who were given an antibiotic ...</td>\n",
       "      <td>90%</td>\n",
       "      <td>10 patients</td>\n",
       "      <td>al_scip-inf-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>465</td>\n",
       "      <td>10019</td>\n",
       "      <td>helen keller memorial hospital</td>\n",
       "      <td>1300 south montgomery avenue</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>sheffield</td>\n",
       "      <td>al</td>\n",
       "      <td>35660</td>\n",
       "      <td>jefferson</td>\n",
       "      <td>2563864556</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>government - hospital district or authority</td>\n",
       "      <td>yes</td>\n",
       "      <td>surgical infection prevention</td>\n",
       "      <td>sxip-vte-1</td>\n",
       "      <td>surgery patients whose doctors ordered treatme...</td>\n",
       "      <td>85%</td>\n",
       "      <td>144 patients</td>\n",
       "      <td>al_scip-vte-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>496</td>\n",
       "      <td>10022</td>\n",
       "      <td>cherokee medical center</td>\n",
       "      <td>400 northwood dr</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>centre</td>\n",
       "      <td>al</td>\n",
       "      <td>35960</td>\n",
       "      <td>cherokee</td>\n",
       "      <td>2569275531</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>yes</td>\n",
       "      <td>heart attack</td>\n",
       "      <td>ami-x</td>\n",
       "      <td>heart attack patients given beta blocker at di...</td>\n",
       "      <td>empty</td>\n",
       "      <td>0 patients</td>\n",
       "      <td>al_ami-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>581</td>\n",
       "      <td>10025</td>\n",
       "      <td>g h lanier memorial hospital</td>\n",
       "      <td>4800 48th st</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>valley</td>\n",
       "      <td>al</td>\n",
       "      <td>36854</td>\n",
       "      <td>chambers</td>\n",
       "      <td>3347561400</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - other</td>\n",
       "      <td>yes</td>\n",
       "      <td>pneumonia</td>\n",
       "      <td>px-5c</td>\n",
       "      <td>pneumonia patients given initial antibiotic(s)...</td>\n",
       "      <td>82%</td>\n",
       "      <td>50 pxtients</td>\n",
       "      <td>al_pn-5c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>640</td>\n",
       "      <td>1xx29</td>\n",
       "      <td>east alabama medical center and snf</td>\n",
       "      <td>2000 pepperell parkway</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>opelika</td>\n",
       "      <td>al</td>\n",
       "      <td>36801</td>\n",
       "      <td>lee</td>\n",
       "      <td>3347493411</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>government - hospital district or authority</td>\n",
       "      <td>yes</td>\n",
       "      <td>surgical infection prevention</td>\n",
       "      <td>scip-vtx-1</td>\n",
       "      <td>surgery patients whose doctors ordered treatme...</td>\n",
       "      <td>92%</td>\n",
       "      <td>473 patients</td>\n",
       "      <td>al_scip-vte-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>649</td>\n",
       "      <td>10032</td>\n",
       "      <td>wedowee hospital</td>\n",
       "      <td>209 north main street</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>wedowee</td>\n",
       "      <td>al</td>\n",
       "      <td>36278</td>\n",
       "      <td>randolph</td>\n",
       "      <td>2563572111</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>government - local</td>\n",
       "      <td>no</td>\n",
       "      <td>heart failure</td>\n",
       "      <td>hfx1</td>\n",
       "      <td>heart failure patients given discharge instruc...</td>\n",
       "      <td>0%</td>\n",
       "      <td>34 patients</td>\n",
       "      <td>al_hf-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>674</td>\n",
       "      <td>10033</td>\n",
       "      <td>university of alabama hospital</td>\n",
       "      <td>619 south 19th street</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>birmingham</td>\n",
       "      <td>al</td>\n",
       "      <td>35233</td>\n",
       "      <td>jefferson</td>\n",
       "      <td>2059344011</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>government - state</td>\n",
       "      <td>yes</td>\n",
       "      <td>heart failure</td>\n",
       "      <td>xf-1</td>\n",
       "      <td>heart failure patients given discharge instruc...</td>\n",
       "      <td>46%</td>\n",
       "      <td>618 xatients</td>\n",
       "      <td>al_hf-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>677</td>\n",
       "      <td>10033</td>\n",
       "      <td>university of alabama hospital</td>\n",
       "      <td>619 south 19th street</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>birmingham</td>\n",
       "      <td>al</td>\n",
       "      <td>35233</td>\n",
       "      <td>jefferson</td>\n",
       "      <td>2059344011</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>government - state</td>\n",
       "      <td>yes</td>\n",
       "      <td>heart failure</td>\n",
       "      <td>hfx4</td>\n",
       "      <td>heart failure patients given smoking cessation...</td>\n",
       "      <td>100%</td>\n",
       "      <td>130 patients</td>\n",
       "      <td>al_hf-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>692</td>\n",
       "      <td>10034</td>\n",
       "      <td>community hospital inc</td>\n",
       "      <td>805 friendship road</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>tallassee</td>\n",
       "      <td>al</td>\n",
       "      <td>36078</td>\n",
       "      <td>elmore</td>\n",
       "      <td>3342836541</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>no</td>\n",
       "      <td>heart attack</td>\n",
       "      <td>amix1</td>\n",
       "      <td>heart attack patients given aspirin at arrival</td>\n",
       "      <td>77%</td>\n",
       "      <td>13 patients</td>\n",
       "      <td>al_ami-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>695</td>\n",
       "      <td>10034</td>\n",
       "      <td>community hospital inc</td>\n",
       "      <td>805 friendship road</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>tallassee</td>\n",
       "      <td>al</td>\n",
       "      <td>36078</td>\n",
       "      <td>elmore</td>\n",
       "      <td>3342836541</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>no</td>\n",
       "      <td>heart attack</td>\n",
       "      <td>amx-4</td>\n",
       "      <td>heart attack patients given smoking cessation ...</td>\n",
       "      <td>empty</td>\n",
       "      <td>0 patients</td>\n",
       "      <td>al_ami-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>719</td>\n",
       "      <td>10035</td>\n",
       "      <td>cullman regional medical center</td>\n",
       "      <td>1912 alabama highway 157</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>cullman</td>\n",
       "      <td>al</td>\n",
       "      <td>35058</td>\n",
       "      <td>cullman</td>\n",
       "      <td>2567372000</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>government - hospital district or authority</td>\n",
       "      <td>yes</td>\n",
       "      <td>heart attack</td>\n",
       "      <td>amx-3</td>\n",
       "      <td>heart attack patients given ace inhibitor or a...</td>\n",
       "      <td>x7%</td>\n",
       "      <td>3 patients</td>\n",
       "      <td>al_ami-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>730</td>\n",
       "      <td>10035</td>\n",
       "      <td>cullman regional medical center</td>\n",
       "      <td>1912 alabama highway 157</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>cullman</td>\n",
       "      <td>al</td>\n",
       "      <td>35058</td>\n",
       "      <td>cullman</td>\n",
       "      <td>2567372000</td>\n",
       "      <td>acutexcarexhospitals</td>\n",
       "      <td>government - hospital district or authority</td>\n",
       "      <td>yes</td>\n",
       "      <td>pneumonia</td>\n",
       "      <td>px-4</td>\n",
       "      <td>pneumonia patients given smoking cessation adv...</td>\n",
       "      <td>100%</td>\n",
       "      <td>90 patients</td>\n",
       "      <td>al_pn-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>736</td>\n",
       "      <td>10035</td>\n",
       "      <td>cullman regional medical center</td>\n",
       "      <td>1912 alabama highway 157</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>cullman</td>\n",
       "      <td>al</td>\n",
       "      <td>35058</td>\n",
       "      <td>cullman</td>\n",
       "      <td>2567372000</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>government - hospital district or authority</td>\n",
       "      <td>yes</td>\n",
       "      <td>surgical infection prevention</td>\n",
       "      <td>scix-inf-2</td>\n",
       "      <td>surgery patients who were given the  right kin...</td>\n",
       "      <td>98%</td>\n",
       "      <td>417 patients</td>\n",
       "      <td>al_scip-inf-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>750</td>\n",
       "      <td>10036</td>\n",
       "      <td>andalusia regional hospital</td>\n",
       "      <td>849 south three notch street</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>andalusia</td>\n",
       "      <td>al</td>\n",
       "      <td>36420</td>\n",
       "      <td>covington</td>\n",
       "      <td>3342228466</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>proprietary</td>\n",
       "      <td>no</td>\n",
       "      <td>heart failure</td>\n",
       "      <td>hx-2</td>\n",
       "      <td>heart failure patients given an evaluation of ...</td>\n",
       "      <td>91%</td>\n",
       "      <td>139 patients</td>\n",
       "      <td>al_hf-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>775</td>\n",
       "      <td>10039</td>\n",
       "      <td>huntsville hospital</td>\n",
       "      <td>101 sivley rd</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>huntsville</td>\n",
       "      <td>al</td>\n",
       "      <td>35801</td>\n",
       "      <td>madison</td>\n",
       "      <td>2562651000</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>government - hospital district or authority</td>\n",
       "      <td>yes</td>\n",
       "      <td>children s asthma care</td>\n",
       "      <td>xax-1</td>\n",
       "      <td>children who received reliever medication whil...</td>\n",
       "      <td>100%</td>\n",
       "      <td>193 patients</td>\n",
       "      <td>al_cac-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>789</td>\n",
       "      <td>10039</td>\n",
       "      <td>huntsville hospital</td>\n",
       "      <td>101 sivley rd</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>huntsville</td>\n",
       "      <td>al</td>\n",
       "      <td>35801</td>\n",
       "      <td>madison</td>\n",
       "      <td>2562651000</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>government - hospital district or authority</td>\n",
       "      <td>yes</td>\n",
       "      <td>pneumonia</td>\n",
       "      <td>pn-x</td>\n",
       "      <td>pneumonia patients assessed and given pneumoco...</td>\n",
       "      <td>96%</td>\n",
       "      <td>509 patients</td>\n",
       "      <td>al_pn-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>843</td>\n",
       "      <td>10043</td>\n",
       "      <td>chilton medical center</td>\n",
       "      <td>1010 lay dam road</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>clanton</td>\n",
       "      <td>al</td>\n",
       "      <td>35045</td>\n",
       "      <td>chilton</td>\n",
       "      <td>2057552500</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>yes</td>\n",
       "      <td>pneumonia</td>\n",
       "      <td>pnx6</td>\n",
       "      <td>pneumonia patients given the most appropriate ...</td>\n",
       "      <td>95%</td>\n",
       "      <td>38 patients</td>\n",
       "      <td>al_pnx6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>866</td>\n",
       "      <td>10044</td>\n",
       "      <td>marion regional medical center</td>\n",
       "      <td>1256 military street south</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>hamilton</td>\n",
       "      <td>al</td>\n",
       "      <td>35570</td>\n",
       "      <td>marion</td>\n",
       "      <td>20x9216200</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>yes</td>\n",
       "      <td>pneumonia</td>\n",
       "      <td>px-4</td>\n",
       "      <td>pneumonia patients given smoking cessation adv...</td>\n",
       "      <td>90%</td>\n",
       "      <td>42 patients</td>\n",
       "      <td>al_pn-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>931</td>\n",
       "      <td>10047</td>\n",
       "      <td>georgiana hospital</td>\n",
       "      <td>515 miranda st</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>georgiana</td>\n",
       "      <td>al</td>\n",
       "      <td>36033</td>\n",
       "      <td>butler</td>\n",
       "      <td>3343762205</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>no</td>\n",
       "      <td>heart attack</td>\n",
       "      <td>amix1</td>\n",
       "      <td>heart attack patients given aspirin at arrival</td>\n",
       "      <td>53%</td>\n",
       "      <td>19 patients</td>\n",
       "      <td>al_ami-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index ProviderNumber                          HospitalName  \\\n",
       "97     98          10007              mizell memorial hospital   \n",
       "101   102          10007              mizell memorial hospital   \n",
       "108   109          10007              mizell memorial hospital   \n",
       "120   121          10008           crenshaw community hospital   \n",
       "177   178          10010         marshall medical center north   \n",
       "190   191          10010         marshall medical center north   \n",
       "264   265          10015      southwest alabama medical center   \n",
       "349   350          10056                  st vincents hospital   \n",
       "391   392          10087  univ of south alabama medical center   \n",
       "398   399          10108           prattville baptist hospital   \n",
       "409   410          10108           prattville baptist hospital   \n",
       "464   465          10019        helen keller memorial hospital   \n",
       "495   496          10022               cherokee medical center   \n",
       "580   581          10025          g h lanier memorial hospital   \n",
       "639   640          1xx29   east alabama medical center and snf   \n",
       "648   649          10032                      wedowee hospital   \n",
       "673   674          10033        university of alabama hospital   \n",
       "676   677          10033        university of alabama hospital   \n",
       "691   692          10034                community hospital inc   \n",
       "694   695          10034                community hospital inc   \n",
       "718   719          10035       cullman regional medical center   \n",
       "729   730          10035       cullman regional medical center   \n",
       "735   736          10035       cullman regional medical center   \n",
       "749   750          10036           andalusia regional hospital   \n",
       "774   775          10039                   huntsville hospital   \n",
       "788   789          10039                   huntsville hospital   \n",
       "842   843          10043                chilton medical center   \n",
       "865   866          10044        marion regional medical center   \n",
       "930   931          10047                    georgiana hospital   \n",
       "\n",
       "                         Address1 Address2 Address3          City State  \\\n",
       "97                  702 n main st    empty    empty           opp    al   \n",
       "101                 702 n main st    empty    empty           opp    al   \n",
       "108                 702 n main st    empty    empty           opp    al   \n",
       "120           101 hospital circle    empty    empty       luverne    al   \n",
       "177       8000 alabama highway 69    empty    empty  guntersville    al   \n",
       "190       8000 alabama highway 69    empty    empty  guntersville    al   \n",
       "264              33700 highway x3    empty    empty   thomasville    al   \n",
       "349         810 st vincents drive    empty    empty    birmingham    al   \n",
       "391         2451 fillingim street    empty    empty        mobile    al   \n",
       "398             124 s memorial dr    empty    empty    prattville    al   \n",
       "409             124 s memorial dr    empty    empty    prattville    al   \n",
       "464  1300 south montgomery avenue    empty    empty     sheffield    al   \n",
       "495              400 northwood dr    empty    empty        centre    al   \n",
       "580                  4800 48th st    empty    empty        valley    al   \n",
       "639        2000 pepperell parkway    empty    empty       opelika    al   \n",
       "648         209 north main street    empty    empty       wedowee    al   \n",
       "673         619 south 19th street    empty    empty    birmingham    al   \n",
       "676         619 south 19th street    empty    empty    birmingham    al   \n",
       "691           805 friendship road    empty    empty     tallassee    al   \n",
       "694           805 friendship road    empty    empty     tallassee    al   \n",
       "718      1912 alabama highway 157    empty    empty       cullman    al   \n",
       "729      1912 alabama highway 157    empty    empty       cullman    al   \n",
       "735      1912 alabama highway 157    empty    empty       cullman    al   \n",
       "749  849 south three notch street    empty    empty     andalusia    al   \n",
       "774                 101 sivley rd    empty    empty    huntsville    al   \n",
       "788                 101 sivley rd    empty    empty    huntsville    al   \n",
       "842             1010 lay dam road    empty    empty       clanton    al   \n",
       "865    1256 military street south    empty    empty      hamilton    al   \n",
       "930                515 miranda st    empty    empty     georgiana    al   \n",
       "\n",
       "    ZipCode CountyName PhoneNumber          HospitalType  \\\n",
       "97    36467  covington  3344933541  acute care hospitals   \n",
       "101   36467  covington  3344x33541  acute care hospitals   \n",
       "108   36467  covington  3344933541  acute care hospitals   \n",
       "120   36049   crenshaw  3343353374  acute care hospitals   \n",
       "177   35976   marshall  2565718000  acute care hospitals   \n",
       "190   35976   marshall  2565718000  acute care hospitals   \n",
       "264   36784     clarke  3346366221  acute care hospitals   \n",
       "349   35205  jefferson  2059397000  acute care hospitals   \n",
       "391   36617     mobile  2514717110  acute care hospitals   \n",
       "398   36067    autauga  3343614267  acute care hospitals   \n",
       "409   36067    autauga  3343614267  acute care hospitals   \n",
       "464   35660  jefferson  2563864556  acute care hospitals   \n",
       "495   35960   cherokee  2569275531  acute care hospitals   \n",
       "580   36854   chambers  3347561400  acute care hospitals   \n",
       "639   36801        lee  3347493411  acute care hospitals   \n",
       "648   36278   randolph  2563572111  acute care hospitals   \n",
       "673   35233  jefferson  2059344011  acute care hospitals   \n",
       "676   35233  jefferson  2059344011  acute care hospitals   \n",
       "691   36078     elmore  3342836541  acute care hospitals   \n",
       "694   36078     elmore  3342836541  acute care hospitals   \n",
       "718   35058    cullman  2567372000  acute care hospitals   \n",
       "729   35058    cullman  2567372000  acutexcarexhospitals   \n",
       "735   35058    cullman  2567372000  acute care hospitals   \n",
       "749   36420  covington  3342228466  acute care hospitals   \n",
       "774   35801    madison  2562651000  acute care hospitals   \n",
       "788   35801    madison  2562651000  acute care hospitals   \n",
       "842   35045    chilton  2057552500  acute care hospitals   \n",
       "865   35570     marion  20x9216200  acute care hospitals   \n",
       "930   36033     butler  3343762205  acute care hospitals   \n",
       "\n",
       "                                   HospitalOwner EmergencyService  \\\n",
       "97                voluntary non-profit - private               no   \n",
       "101               voluntary non-profit - private               no   \n",
       "108               voluntary non-profit - private               no   \n",
       "120                         government - federal              yes   \n",
       "177  government - hospital district or authority              yes   \n",
       "190  government - hospital district or authority              yes   \n",
       "264                         government - federal              yes   \n",
       "349                 voluntary non-profit - other              yes   \n",
       "391                           government - state              yes   \n",
       "398               voluntary non-profit - private              yes   \n",
       "409               voluntary non-profit - private              yes   \n",
       "464  government - hospital district or authority              yes   \n",
       "495               voluntary non-profit - private              yes   \n",
       "580                 voluntary non-profit - other              yes   \n",
       "639  government - hospital district or authority              yes   \n",
       "648                           government - local               no   \n",
       "673                           government - state              yes   \n",
       "676                           government - state              yes   \n",
       "691               voluntary non-profit - private               no   \n",
       "694               voluntary non-profit - private               no   \n",
       "718  government - hospital district or authority              yes   \n",
       "729  government - hospital district or authority              yes   \n",
       "735  government - hospital district or authority              yes   \n",
       "749                                  proprietary               no   \n",
       "774  government - hospital district or authority              yes   \n",
       "788  government - hospital district or authority              yes   \n",
       "842               voluntary non-profit - private              yes   \n",
       "865               voluntary non-profit - private              yes   \n",
       "930               voluntary non-profit - private               no   \n",
       "\n",
       "                         Condition MeasureCode  \\\n",
       "97                    heart attack       axi-4   \n",
       "101                  heart failure        hfx1   \n",
       "108                      pneumonia       pnx5c   \n",
       "120                   heart attack       axi-2   \n",
       "177                  heart failure        hf-x   \n",
       "190  surgical infection prevention  scip-inx-4   \n",
       "264  surgical infection prevention  scxp-xnf-3   \n",
       "349                   heart attack       amix2   \n",
       "391                      pneumonia       pn-xb   \n",
       "398                  heart failure        hx-1   \n",
       "409  surgical infection prevention  scipxinfx1   \n",
       "464  surgical infection prevention  sxip-vte-1   \n",
       "495                   heart attack       ami-x   \n",
       "580                      pneumonia       px-5c   \n",
       "639  surgical infection prevention  scip-vtx-1   \n",
       "648                  heart failure        hfx1   \n",
       "673                  heart failure        xf-1   \n",
       "676                  heart failure        hfx4   \n",
       "691                   heart attack       amix1   \n",
       "694                   heart attack       amx-4   \n",
       "718                   heart attack       amx-3   \n",
       "729                      pneumonia        px-4   \n",
       "735  surgical infection prevention  scix-inf-2   \n",
       "749                  heart failure        hx-2   \n",
       "774         children s asthma care       xax-1   \n",
       "788                      pneumonia        pn-x   \n",
       "842                      pneumonia        pnx6   \n",
       "865                      pneumonia        px-4   \n",
       "930                   heart attack       amix1   \n",
       "\n",
       "                                           MeasureName  Score        Sample  \\\n",
       "97   heart attack patients given smoking cessation ...  empty    0 patients   \n",
       "101  heart failure patients given discharge instruc...    98%   59 patients   \n",
       "108  pneumonia patients given initial antibiotic(s)...    95%   87 patients   \n",
       "120   heart attack patients given aspirin at discharge   x00%    1 patients   \n",
       "177  heart failure patients given an evaluation of ...    84%   80 patients   \n",
       "190  all heart surgery patients whose blood sugar (...  empty    0 patients   \n",
       "264  surgery patients whose preventive antibiotics ...    86%   14 patients   \n",
       "349   heart attack patients given aspirin at discharge    98%  260 patients   \n",
       "391  pneumonia patients whose initial emergency roo...    89%   35 patients   \n",
       "398  heart failure patients given discharge instruc...    96%   75 patients   \n",
       "409  surgery patients who were given an antibiotic ...    90%   10 patients   \n",
       "464  surgery patients whose doctors ordered treatme...    85%  144 patients   \n",
       "495  heart attack patients given beta blocker at di...  empty    0 patients   \n",
       "580  pneumonia patients given initial antibiotic(s)...    82%   50 pxtients   \n",
       "639  surgery patients whose doctors ordered treatme...    92%  473 patients   \n",
       "648  heart failure patients given discharge instruc...     0%   34 patients   \n",
       "673  heart failure patients given discharge instruc...    46%  618 xatients   \n",
       "676  heart failure patients given smoking cessation...   100%  130 patients   \n",
       "691     heart attack patients given aspirin at arrival    77%   13 patients   \n",
       "694  heart attack patients given smoking cessation ...  empty    0 patients   \n",
       "718  heart attack patients given ace inhibitor or a...    x7%    3 patients   \n",
       "729  pneumonia patients given smoking cessation adv...   100%   90 patients   \n",
       "735  surgery patients who were given the  right kin...    98%  417 patients   \n",
       "749  heart failure patients given an evaluation of ...    91%  139 patients   \n",
       "774  children who received reliever medication whil...   100%  193 patients   \n",
       "788  pneumonia patients assessed and given pneumoco...    96%  509 patients   \n",
       "842  pneumonia patients given the most appropriate ...    95%   38 patients   \n",
       "865  pneumonia patients given smoking cessation adv...    90%   42 patients   \n",
       "930     heart attack patients given aspirin at arrival    53%   19 patients   \n",
       "\n",
       "          Stateavg  \n",
       "97        al_ami-4  \n",
       "101        al_hf-1  \n",
       "108       al_pn-5c  \n",
       "120       al_ami-2  \n",
       "177        al_hf-2  \n",
       "190  al_scip-inf-4  \n",
       "264  al_scip-inf-3  \n",
       "349       al_ami-2  \n",
       "391       al_pn-3b  \n",
       "398        al_hf-1  \n",
       "409  al_scip-inf-1  \n",
       "464  al_scip-vte-1  \n",
       "495       al_ami-5  \n",
       "580       al_pn-5c  \n",
       "639  al_scip-vte-1  \n",
       "648        al_hf-1  \n",
       "673        al_hf-1  \n",
       "676        al_hf-4  \n",
       "691       al_ami-1  \n",
       "694       al_ami-4  \n",
       "718       al_ami-3  \n",
       "729        al_pn-4  \n",
       "735  al_scip-inf-2  \n",
       "749        al_hf-2  \n",
       "774       al_cac-1  \n",
       "788        al_pn-2  \n",
       "842        al_pnx6  \n",
       "865        al_pn-4  \n",
       "930       al_ami-1  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hospital_dirty[hospital_dirty.iloc[:,-5]!=hospital_clean.iloc[:,-5]]\n",
    "# hospital_dirty.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hospital_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/yanmy/raha/raha-master/detector.ipynb 单元格 298\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y604sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y604sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m hospital_clean_set \u001b[39m=\u001b[39m hospital_clean\u001b[39m.\u001b[39miloc[selected_index_hospital]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y604sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m hospital_dirty_set \u001b[39m=\u001b[39m hospital_dirty\u001b[39m.\u001b[39miloc[selected_index_hospital]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y604sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# hospital_dirty[hospital_dirty.iloc[:,i]!=hospital_clean.iloc[:,i]]\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y604sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# detector_inference = \"The input \\n\\n%s\\n\\nare some dirty cells from table Hospital column %s, and the input \\n\\n%s\\n\\n are corrected clean cells, and %s are examples of corresponding corrected clean cells. please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (dirty_list,col_name,clean_list,clean_list_origin)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y604sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# detector_inference = \"The input \\n\\n%s\\n\\nare [dirty,clean] cell pairs from table Hospital column %s, and %s are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (dirty_list,col_name,clean_list,clean_list_origin)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hospital_clean' is not defined"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "hospital_clean_set = hospital_clean.iloc[selected_index_hospital]\n",
    "hospital_dirty_set = hospital_dirty.iloc[selected_index_hospital]\n",
    "# hospital_dirty[hospital_dirty.iloc[:,i]!=hospital_clean.iloc[:,i]]\n",
    "# detector_inference = \"The input \\n\\n%s\\n\\nare some dirty cells from table Hospital column %s, and the input \\n\\n%s\\n\\n are corrected clean cells, and %s are examples of corresponding corrected clean cells. please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (dirty_list,col_name,clean_list,clean_list_origin)\n",
    "# detector_inference = \"The input \\n\\n%s\\n\\nare [dirty,clean] cell pairs from table Hospital column %s, and %s are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (dirty_list,col_name,clean_list,clean_list_origin)\n",
    "\n",
    "dirty_list = []\n",
    "col_name = hospital_clean.columns[i]\n",
    "clean_list = list(hospital_dirty.iloc[:,i].unique())\n",
    "for index,row in hospital_dirty_set[hospital_dirty_set.iloc[:,i]!=hospital_clean_set.iloc[:,i]].iterrows():\n",
    "    # print(hospital_clean.iloc[index,i],hospital_dirty.iloc[index,i])\n",
    "    dirty_list.append([hospital_clean.iloc[index,i],hospital_dirty.iloc[index,i]])\n",
    "clean_list_part = list(hospital_clean_set.iloc[:,i].unique())\n",
    "dirty_list_part = list(hospital_dirty_set[hospital_dirty_set.iloc[:,i]!=hospital_clean_set.iloc[:,i]].iloc[:,i].unique())\n",
    "    # print(index)\n",
    "detector_inference = \"The input \\n\\n%s\\n\\nare [clean,dirty] cell pairs from table Hospital column %s, and %s are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (dirty_list,col_name,clean_list[:15])\n",
    "# detector_inference = \"The input \\n\\n%s\\n\\nare clean cells, and \\n\\n%s\\n\\nare dirty cells from table Hospital column %s, and %s are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (clean_list_part,dirty_list_part,col_name,clean_list[:15])\n",
    "detector_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1791,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The input \\n\\n[['Hayward', 'Hayward WI'], ['Nellysford', 'Nellysford VA'], ['Duluth', 'Duluth MN'], ['Plainfield', 'Plainfield IN'], ['Stevens Point', 'Stevens Point WI'], ['Pittsburgh', 'Pittsburgh PA'], ['San Diego', 'San Diego CA'], ['Chicago', 'Chicago IL'], ['Canton', 'Canton MA'], ['Fort Worth', 'Fort Worth TX'], ['Idaho Springs', 'Idaho Springs CO'], ['Cincinnati', 'Cincinnati OH'], ['Utica', 'Utica NY'], ['Portland', 'Portland OR'], ['Athens', 'Athens GA'], ['North Woodstock', 'North Woodstock NH'], ['Pottstown', 'Pottstown PA'], ['Marietta', 'Marietta GA'], ['Portland', 'Portland OR'], ['Dillon', 'Dillon CO']]\\n\\nare [clean,dirty] cell pairs from table Beers column city, and ['Bend', 'Gary', 'Corvallis', 'San Francisco', 'San Francisco CA', 'St Petersburg', 'Saint Louis', 'Saint Louis MO', 'Columbus', 'Gig Harbor', 'Dunedin', 'Golden', 'Abita Springs', 'Louisville', 'Kent'] are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not. Input and output are all string format.\""
      ]
     },
     "execution_count": 1791,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 9\n",
    "beer_clean_set = beer_clean.iloc[beer_label_index]\n",
    "beer_dirty_set = beer_dirty.iloc[beer_label_index]\n",
    "# hospital_dirty[hospital_dirty.iloc[:,i]!=hospital_clean.iloc[:,i]]\n",
    "# detector_inference = \"The input \\n\\n%s\\n\\nare some dirty cells from table Hospital column %s, and the input \\n\\n%s\\n\\n are corrected clean cells, and %s are examples of corresponding corrected clean cells. please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (dirty_list,col_name,clean_list,clean_list_origin)\n",
    "# detector_inference = \"The input \\n\\n%s\\n\\nare [dirty,clean] cell pairs from table Hospital column %s, and %s are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (dirty_list,col_name,clean_list,clean_list_origin)\n",
    "\n",
    "dirty_list = []\n",
    "col_name = beer_clean.columns[i]\n",
    "clean_list = list(beer_dirty.iloc[:,i].unique())\n",
    "for index,row in beer_dirty_set[beer_dirty_set.iloc[:,i]!=beer_clean_set.iloc[:,i]].iterrows():\n",
    "    # print(hospital_clean.iloc[index,i],hospital_dirty.iloc[index,i])\n",
    "    dirty_list.append([beer_clean.iloc[index,i],beer_dirty.iloc[index,i]])\n",
    "# clean_list_part = list(beer_clean_set.iloc[:,i].unique())\n",
    "# dirty_list_part = list(beer_dirty[beer_dirty_set.iloc[:,i]!=beer_clean_set.iloc[:,i]].iloc[:,i].unique())\n",
    "#     # print(index)\n",
    "detector_inference = \"The input \\n\\n%s\\n\\nare [clean,dirty] cell pairs from table Beers column %s, and %s are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not. Input and output are all string format.\" % (dirty_list,col_name,clean_list[:15])\n",
    "# detector_inference = \"The input \\n\\n%s\\n\\nare clean cells, and \\n\\n%s\\n\\nare dirty cells from table Hospital column %s, and %s are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (clean_list_part,dirty_list_part,col_name,clean_list[:15])\n",
    "# print(detector_inference)\n",
    "detector_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2343,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandarallel.initialize(progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titleType</th>\n",
       "      <th>title</th>\n",
       "      <th>startYear</th>\n",
       "      <th>runtimeMinutes</th>\n",
       "      <th>director</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tvSpecial</td>\n",
       "      <td>Survivor Series</td>\n",
       "      <td>2008</td>\n",
       "      <td>180</td>\n",
       "      <td>Kevin Dunn</td>\n",
       "      <td>Sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tvSpecial</td>\n",
       "      <td>Summerslam</td>\n",
       "      <td>1998</td>\n",
       "      <td>166</td>\n",
       "      <td>Kevin Dunn</td>\n",
       "      <td>Sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tvSpecial</td>\n",
       "      <td>WrestleMania</td>\n",
       "      <td>2018</td>\n",
       "      <td>314</td>\n",
       "      <td>Kevin Dunn</td>\n",
       "      <td>Sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tvSpecial</td>\n",
       "      <td>Summerslam</td>\n",
       "      <td>1999</td>\n",
       "      <td>155</td>\n",
       "      <td>Kevin Dunn</td>\n",
       "      <td>Sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tvSpecial</td>\n",
       "      <td>WrestleMania X-Seven</td>\n",
       "      <td>2001</td>\n",
       "      <td>225</td>\n",
       "      <td>Kevin Dunn</td>\n",
       "      <td>Sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>movie</td>\n",
       "      <td>Making Maya</td>\n",
       "      <td>2003</td>\n",
       "      <td>85</td>\n",
       "      <td>Rolla Selbak</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>movie</td>\n",
       "      <td>Colors</td>\n",
       "      <td>1988</td>\n",
       "      <td>120</td>\n",
       "      <td>Dennis Hopper</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>movie</td>\n",
       "      <td>Legacy</td>\n",
       "      <td>2019</td>\n",
       "      <td>105</td>\n",
       "      <td>Dorian Boguta</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>movie</td>\n",
       "      <td>Heaven &amp; Earth</td>\n",
       "      <td>1993</td>\n",
       "      <td>140</td>\n",
       "      <td>Oliver Stone</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>movie</td>\n",
       "      <td>Chosen</td>\n",
       "      <td>2016</td>\n",
       "      <td>105</td>\n",
       "      <td>Jasmin Dizdar</td>\n",
       "      <td>War</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        titleType                 title startYear runtimeMinutes  \\\n",
       "0       tvSpecial       Survivor Series      2008            180   \n",
       "1       tvSpecial            Summerslam      1998            166   \n",
       "2       tvSpecial          WrestleMania      2018            314   \n",
       "3       tvSpecial            Summerslam      1999            155   \n",
       "4       tvSpecial  WrestleMania X-Seven      2001            225   \n",
       "...           ...                   ...       ...            ...   \n",
       "999995      movie           Making Maya      2003             85   \n",
       "999996      movie                Colors      1988            120   \n",
       "999997      movie                Legacy      2019            105   \n",
       "999998      movie        Heaven & Earth      1993            140   \n",
       "999999      movie                Chosen      2016            105   \n",
       "\n",
       "             director  genres  \n",
       "0          Kevin Dunn   Sport  \n",
       "1          Kevin Dunn   Sport  \n",
       "2          Kevin Dunn   Sport  \n",
       "3          Kevin Dunn   Sport  \n",
       "4          Kevin Dunn   Sport  \n",
       "...               ...     ...  \n",
       "999995   Rolla Selbak   Drama  \n",
       "999996  Dennis Hopper   Drama  \n",
       "999997  Dorian Boguta   Drama  \n",
       "999998   Oliver Stone  Action  \n",
       "999999  Jasmin Dizdar     War  \n",
       "\n",
       "[1000000 rows x 6 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2346,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3\n",
    "imdb_clean = pd.read_csv('datasets/imdb/clean.csv').fillna('')\n",
    "imdb_dirty = pd.read_csv('datasets/imdb/dirty.csv').fillna('')\n",
    "def Str2Int(row):\n",
    "    for index in range(6):\n",
    "        temp = row[index]\n",
    "        try:\n",
    "            row[index] = str(int(temp))\n",
    "        except:\n",
    "            continue\n",
    "    return row\n",
    "imdb_clean = imdb_clean.parallel_apply(Str2Int,axis=1)\n",
    "imdb_dirty = imdb_dirty.parallel_apply(Str2Int,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2008'"
      ]
     },
     "execution_count": 2344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imdb_dirty[imdb_dirty['startYear'].str.contains('.')]\n",
    "imdb_dirty.iloc[0,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2347,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_all = []\n",
    "for i in range(5):\n",
    "    select = np.random.choice(imdb_dirty[imdb_clean.iloc[:,i]!=imdb_dirty.iloc[:,i]].index,4)\n",
    "    select_all.extend(list(select))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12000, 12000, 11712,  9761, 12000,     0])"
      ]
     },
     "execution_count": 2348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(imdb_dirty!=imdb_clean).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2349,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('datasets/imdb/label.npy',np.array(select_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titleType</th>\n",
       "      <th>title</th>\n",
       "      <th>startYear</th>\n",
       "      <th>runtimeMinutes</th>\n",
       "      <th>director</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>353308</th>\n",
       "      <td>movie</td>\n",
       "      <td>Zansetsu</td>\n",
       "      <td>1968.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>Katsumi Nishikawa</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591277</th>\n",
       "      <td>movie</td>\n",
       "      <td>The Unknown</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>Henry Levin</td>\n",
       "      <td>Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203413</th>\n",
       "      <td>tvMovie</td>\n",
       "      <td>Shelter</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>Barbara Kopple</td>\n",
       "      <td>Documentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855452</th>\n",
       "      <td>tvMovie</td>\n",
       "      <td>The Big Easy</td>\n",
       "      <td>1982.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>Jud Taylor</td>\n",
       "      <td>Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498111</th>\n",
       "      <td>movie</td>\n",
       "      <td>Kokoro</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>Shô Tsukikawa</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       titleType         title startYear runtimeMinutes           director  \\\n",
       "353308     movie      Zansetsu    1968.0           94.0  Katsumi Nishikawa   \n",
       "591277     movie   The Unknown    1946.0           70.0        Henry Levin   \n",
       "203413   tvMovie       Shelter    2015.0           52.0     Barbara Kopple   \n",
       "855452   tvMovie  The Big Easy    1982.0           60.0         Jud Taylor   \n",
       "498111     movie        Kokoro    2007.0           64.0      Shô Tsukikawa   \n",
       "\n",
       "             genres  \n",
       "353308        Drama  \n",
       "591277        Crime  \n",
       "203413  Documentary  \n",
       "855452        Crime  \n",
       "498111        Drama  "
      ]
     },
     "execution_count": 2350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_clean_set[imdb_dirty_set.iloc[:,i]!=imdb_clean_set.iloc[:,i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The input \\n\\n[['2011', '11'], ['1919', '19'], ['2020', '20'], ['1968', '68']]\\n\\nare [clean,dirty] cell pairs from table IMDB column startYear, and ['2008', '1998', '2018', '1999', '2001', '2007', '2011', '2010', '2003', '2017', '2000', '2006', '2014', '2009', '1994'] are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not. Input and output are all string format.\""
      ]
     },
     "execution_count": 2354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 2\n",
    "imdb_label_index = select_all\n",
    "# imdb_clean = pd.read_csv('datasets/imdb/clean.csv').astype(str)\n",
    "# imdb_dirty = pd.read_csv('datasets/imdb/dirty.csv').astype(str)\n",
    "imdb_clean_set = imdb_clean.iloc[imdb_label_index]\n",
    "imdb_dirty_set = imdb_dirty.iloc[imdb_label_index]\n",
    "# hospital_dirty[hospital_dirty.iloc[:,i]!=hospital_clean.iloc[:,i]]\n",
    "# detector_inference = \"The input \\n\\n%s\\n\\nare some dirty cells from table Hospital column %s, and the input \\n\\n%s\\n\\n are corrected clean cells, and %s are examples of corresponding corrected clean cells. please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (dirty_list,col_name,clean_list,clean_list_origin)\n",
    "# detector_inference = \"The input \\n\\n%s\\n\\nare [dirty,clean] cell pairs from table Hospital column %s, and %s are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (dirty_list,col_name,clean_list,clean_list_origin)\n",
    "\n",
    "dirty_list = []\n",
    "col_name = imdb_clean.columns[i]\n",
    "clean_list = list(imdb_dirty.iloc[:,i].unique())\n",
    "for index,row in imdb_dirty_set[imdb_dirty_set.iloc[:,i]!=imdb_clean_set.iloc[:,i]].iterrows():\n",
    "    # print(hospital_clean.iloc[index,i],hospital_dirty.iloc[index,i])\n",
    "    dirty_list.append([imdb_clean.iloc[index,i],imdb_dirty.iloc[index,i]])\n",
    "# clean_list_part = list(beer_clean_set.iloc[:,i].unique())\n",
    "# dirty_list_part = list(beer_dirty[beer_dirty_set.iloc[:,i]!=beer_clean_set.iloc[:,i]].iloc[:,i].unique())\n",
    "#     # print(index)\n",
    "detector_inference = \"The input \\n\\n%s\\n\\nare [clean,dirty] cell pairs from table IMDB column %s, and %s are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not. Input and output are all string format.\" % (dirty_list,col_name,clean_list[:15])\n",
    "# detector_inference = \"The input \\n\\n%s\\n\\nare clean cells, and \\n\\n%s\\n\\nare dirty cells from table Hospital column %s, and %s are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (clean_list_part,dirty_list_part,col_name,clean_list[:15])\n",
    "# print(detector_inference)\n",
    "detector_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def is_dirty_cell(cell):\n",
    "    \"\"\"\n",
    "    Check if the given cell is dirty or not.\n",
    "\n",
    "    :param cell: A string representing a cell from the runtimeMinutes column.\n",
    "    :return: Boolean indicating whether the cell is dirty.\n",
    "    \"\"\"\n",
    "    # A regular expression to match only numeric strings\n",
    "    pattern = r'^\\d+$'\n",
    "\n",
    "    # Return False if cell matches the pattern (clean), otherwise True (dirty)\n",
    "    return not re.match(pattern, cell)\n",
    "def generate_dirty_cell(clean_cell):\n",
    "    \"\"\"\n",
    "    Generate a dirty cell from a clean one. The clean cell is assumed to be\n",
    "    a numeric string representing minutes. The dirty cell will represent\n",
    "    this in hours with a random level of precision.\n",
    "\n",
    "    :param clean_cell: A string representing a clean cell (numeric).\n",
    "    :return: A string representing a dirty cell.\n",
    "    \"\"\"\n",
    "    # Convert the clean cell to an integer\n",
    "    minutes = int(clean_cell)\n",
    "\n",
    "    # Convert minutes to hours and add a random level of decimal precision\n",
    "    hours = minutes / 60\n",
    "    precision = random.choice([1, 2, 3])  # Random precision level\n",
    "    formatted_hours = round(hours, precision)\n",
    "\n",
    "    # Format the dirty cell\n",
    "    dirty_cell = f\"{formatted_hours} h\"\n",
    "\n",
    "    return dirty_cell\n",
    "is_dirty_cell('180')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def is_dirty_cell(cell):\n",
    "    \"\"\"\n",
    "    Check if the given cell is dirty or not for the startYear column.\n",
    "\n",
    "    :param cell: A string representing a cell from the startYear column.\n",
    "    :return: Boolean indicating whether the cell is dirty.\n",
    "    \"\"\"\n",
    "    # Regular expression for a clean cell: exactly four digits\n",
    "    clean_pattern = r'^\\d{4}$'\n",
    "\n",
    "    # Return True (dirty) if cell does not match the clean pattern\n",
    "    return not re.match(clean_pattern, cell)\n",
    "def generate_dirty_cell(clean_cell):\n",
    "    \"\"\"\n",
    "    Generate a dirty cell from a clean one. The clean cell is assumed to be\n",
    "    a four-digit year. The dirty cell will be the last two digits of the year.\n",
    "\n",
    "    :param clean_cell: A string representing a clean cell (four-digit year).\n",
    "    :return: A string representing a dirty cell (last two digits of the year).\n",
    "    \"\"\"\n",
    "    # Extract the last two digits of the year\n",
    "    dirty_cell = clean_cell[-2:]\n",
    "\n",
    "    return dirty_cell\n",
    "\n",
    "# Example usage\n",
    "clean_cell_example = \"1999\"\n",
    "dirty_cell_example = generate_dirty_cell(clean_cell_example)\n",
    "dirty_cell_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1794,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The input \\n\\n['Hayward', 'Nellysford', 'Duluth', 'Plainfield', 'Stevens Point', 'Pittsburgh', 'San Diego', 'Chicago', 'Canton', 'Fort Worth', 'Idaho Springs', 'Cincinnati', 'Utica', 'Portland', 'Athens', 'North Woodstock', 'Pottstown', 'Marietta', 'Portland', 'Dillon']\\n\\nare clean cells from table Beers column city, and ['Hayward WI', 'Nellysford VA', 'Duluth MN', 'Plainfield IN', 'Stevens Point WI', 'Pittsburgh PA', 'San Diego CA', 'Chicago IL', 'Canton MA', 'Fort Worth TX', 'Idaho Springs CO', 'Cincinnati OH', 'Utica NY', 'Portland OR', 'Athens GA', 'North Woodstock NH', 'Pottstown PA', 'Marietta GA', 'Portland OR', 'Dillon CO'] are dirty cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not. Input and output are all string format.\""
      ]
     },
     "execution_count": 1794,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 9\n",
    "beer_clean_set = beer_clean.iloc[beer_label_index]\n",
    "beer_dirty_set = beer_dirty.iloc[beer_label_index]\n",
    "# hospital_dirty[hospital_dirty.iloc[:,i]!=hospital_clean.iloc[:,i]]\n",
    "# detector_inference = \"The input \\n\\n%s\\n\\nare some dirty cells from table Hospital column %s, and the input \\n\\n%s\\n\\n are corrected clean cells, and %s are examples of corresponding corrected clean cells. please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (dirty_list,col_name,clean_list,clean_list_origin)\n",
    "# detector_inference = \"The input \\n\\n%s\\n\\nare [dirty,clean] cell pairs from table Hospital column %s, and %s are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (dirty_list,col_name,clean_list,clean_list_origin)\n",
    "\n",
    "dirty_list = []\n",
    "col_name = beer_clean.columns[i]\n",
    "clean_list = list(beer_dirty.iloc[:,i].unique())\n",
    "for index,row in beer_dirty_set[beer_dirty_set.iloc[:,i]!=beer_clean_set.iloc[:,i]].iterrows():\n",
    "    # print(hospital_clean.iloc[index,i],hospital_dirty.iloc[index,i])\n",
    "    dirty_list.append([beer_clean.iloc[index,i],beer_dirty.iloc[index,i]])\n",
    "# clean_list_part = list(beer_clean_set.iloc[:,i].unique())\n",
    "# dirty_list_part = list(beer_dirty[beer_dirty_set.iloc[:,i]!=beer_clean_set.iloc[:,i]].iloc[:,i].unique())\n",
    "#     # print(index)\n",
    "detector_inference = \"The input \\n\\n%s\\n\\nare clean cells from table Beers column %s, and %s are dirty cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not. Input and output are all string format.\" % (list(np.array(dirty_list)[:,0]),col_name,list(np.array(dirty_list)[:,1]))\n",
    "# detector_inference = \"The input \\n\\n%s\\n\\nare clean cells, and \\n\\n%s\\n\\nare dirty cells from table Hospital column %s, and %s are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (clean_list_part,dirty_list_part,col_name,clean_list[:15])\n",
    "# print(detector_inference)\n",
    "detector_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_label_index = np.load('datasets/tax/detector/index.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S,1370,0'"
      ]
     },
     "execution_count": 919,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def concat_row(row):\n",
    "    text = ''\n",
    "    for x,y in row.items():\n",
    "        text += '%s,' % y\n",
    "    return text[:-1]\n",
    "concat_row(tax_dirty.iloc[8,[8,12,13]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'M,0,0',\n",
       " 'M,0,12000',\n",
       " 'M,0,1400',\n",
       " 'M,0,174',\n",
       " 'M,0,1800',\n",
       " 'M,0,2000',\n",
       " 'M,0,206',\n",
       " 'M,0,2080',\n",
       " 'M,0,220',\n",
       " 'M,0,24500',\n",
       " 'M,0,2600',\n",
       " 'M,0,2740',\n",
       " 'M,0,3000',\n",
       " 'M,0,318',\n",
       " 'M,0,3800',\n",
       " 'M,0,40',\n",
       " 'M,0,4000',\n",
       " 'M,0,4200',\n",
       " 'M,0,4500',\n",
       " 'M,0,4800',\n",
       " 'M,0,4950',\n",
       " 'M,0,5400',\n",
       " 'M,0,5700',\n",
       " 'M,0,6200',\n",
       " 'M,0,6600',\n",
       " 'M,0,7150',\n",
       " 'M,0,80',\n",
       " 'M,0,9000',\n",
       " 'M,0-*,0',\n",
       " 'M,1500,0',\n",
       " 'S,0,0',\n",
       " 'S,1000,0',\n",
       " 'S,103,0',\n",
       " 'S,1040,0',\n",
       " 'S,110,0',\n",
       " 'S,12750,0',\n",
       " 'S,1300,0',\n",
       " 'S,1370,0',\n",
       " 'S,1500,0',\n",
       " 'S,159,0',\n",
       " 'S,1900,0',\n",
       " 'S,20,0',\n",
       " 'S,2000,0',\n",
       " 'S,2100,0',\n",
       " 'S,2250,0',\n",
       " 'S,2400,0',\n",
       " 'S,2475,0',\n",
       " 'S,2700,0',\n",
       " 'S,2850,0',\n",
       " 'S,3100,0',\n",
       " 'S,3300,0',\n",
       " 'S,3575,0',\n",
       " 'S,40,0',\n",
       " 'S,4500,0',\n",
       " 'S,6000,0',\n",
       " 'S,700,0',\n",
       " 'S,87,0',\n",
       " 'S,900,0'}"
      ]
     },
     "execution_count": 923,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list = tax_dirty.iloc[:,[8,12,13]].apply(concat_row,axis=1)\n",
    "set(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_name</th>\n",
       "      <th>l_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>area_code</th>\n",
       "      <th>phone</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>has_child</th>\n",
       "      <th>salary</th>\n",
       "      <th>rate</th>\n",
       "      <th>single_exemp</th>\n",
       "      <th>married_exemp</th>\n",
       "      <th>child_exemp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20793</th>\n",
       "      <td>Byoung</td>\n",
       "      <td>Lebah</td>\n",
       "      <td>F</td>\n",
       "      <td>205</td>\n",
       "      <td>773-5181</td>\n",
       "      <td>BIRMINGHAM</td>\n",
       "      <td>AL</td>\n",
       "      <td>35210</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>25000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20663</th>\n",
       "      <td>Iskender</td>\n",
       "      <td>Hoest</td>\n",
       "      <td>F</td>\n",
       "      <td>251</td>\n",
       "      <td>786-4658</td>\n",
       "      <td>VALHERMOSO SPRINGS</td>\n",
       "      <td>AL</td>\n",
       "      <td>35775</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>55000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17853</th>\n",
       "      <td>Laurie</td>\n",
       "      <td>Fornaciari</td>\n",
       "      <td>F</td>\n",
       "      <td>205</td>\n",
       "      <td>556-3443</td>\n",
       "      <td>FAIRHOPE</td>\n",
       "      <td>AL</td>\n",
       "      <td>36532</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>20000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19039</th>\n",
       "      <td>Irs''hak</td>\n",
       "      <td>Guha</td>\n",
       "      <td>M</td>\n",
       "      <td>205</td>\n",
       "      <td>888-6448</td>\n",
       "      <td>BIRMINGHAM</td>\n",
       "      <td>AL</td>\n",
       "      <td>35205</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>10000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         f_name      l_name gender area_code     phone                city  \\\n",
       "20793    Byoung       Lebah      F       205  773-5181          BIRMINGHAM   \n",
       "20663  Iskender       Hoest      F       251  786-4658  VALHERMOSO SPRINGS   \n",
       "17853    Laurie  Fornaciari      F       205  556-3443            FAIRHOPE   \n",
       "19039  Irs''hak        Guha      M       205  888-6448          BIRMINGHAM   \n",
       "\n",
       "      state    zip marital_status has_child salary rate single_exemp  \\\n",
       "20793    AL  35210              M         N  25000  5.0         1500   \n",
       "20663    AL  35775              M         N  55000  5.0         1500   \n",
       "17853    AL  36532              M         N  20000  5.0         1500   \n",
       "19039    AL  35205              M         N  10000  5.0         1500   \n",
       "\n",
       "      married_exemp child_exemp  \n",
       "20793             0         300  \n",
       "20663             0         300  \n",
       "17853             0         300  \n",
       "19039             0         300  "
      ]
     },
     "execution_count": 926,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_dirty_set[tax_dirty_set.iloc[:,i]!=tax_clean_set.iloc[:,i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M    200\n",
       "Name: marital_status, dtype: int64"
      ]
     },
     "execution_count": 929,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_dirty[tax_dirty.iloc[:,i]!=tax_clean.iloc[:,i]]['marital_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/yanmy/raha/raha-master/detector.ipynb 单元格 301\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1302sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m clean_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(tax_dirty\u001b[39m.\u001b[39miloc[:,i]\u001b[39m.\u001b[39munique())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1302sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m index,row \u001b[39min\u001b[39;00m tax_dirty_set[tax_dirty_set\u001b[39m.\u001b[39miloc[:,i]\u001b[39m!=\u001b[39mtax_clean_set\u001b[39m.\u001b[39miloc[:,i]]\u001b[39m.\u001b[39miterrows():\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1302sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# print(hospital_clean.iloc[index,i],hospital_dirty.iloc[index,i])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1302sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# dirty_list.append([tax_clean.iloc[index,i],tax_dirty.iloc[index,i]])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1302sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m# dirty_list.append([[tax_clean.iloc[index,i-1],tax_clean.iloc[index,i]],[tax_clean.iloc[index,i-1],tax_dirty.iloc[index,i]]])\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1302sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     dirty_list\u001b[39m.\u001b[39mappend([concat_row(tax_clean_set\u001b[39m.\u001b[39;49miloc[index,[\u001b[39m8\u001b[39;49m,\u001b[39m12\u001b[39;49m,\u001b[39m13\u001b[39;49m]]),concat_row(tax_dirty_set\u001b[39m.\u001b[39miloc[index,[\u001b[39m8\u001b[39m,\u001b[39m12\u001b[39m,\u001b[39m13\u001b[39m]])])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1302sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# clean_list_part = list(beer_clean_set.iloc[:,i].unique())\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1302sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# dirty_list_part = list(beer_dirty[beer_dirty_set.iloc[:,i]!=beer_clean_set.iloc[:,i]].iloc[:,i].unique())\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1302sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m#     # print(index)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1302sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m col_name \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(tax_clean\u001b[39m.\u001b[39mcolumns[[\u001b[39m8\u001b[39m,\u001b[39m12\u001b[39m,\u001b[39m13\u001b[39m]])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1067\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1065\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m   1066\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_value(\u001b[39m*\u001b[39mkey, takeable\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_takeable)\n\u001b[0;32m-> 1067\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_tuple(key)\n\u001b[1;32m   1068\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1069\u001b[0m     \u001b[39m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m     axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1563\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1561\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_getitem_tuple\u001b[39m(\u001b[39mself\u001b[39m, tup: \u001b[39mtuple\u001b[39m):\n\u001b[0;32m-> 1563\u001b[0m     tup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_tuple_indexer(tup)\n\u001b[1;32m   1564\u001b[0m     \u001b[39mwith\u001b[39;00m suppress(IndexingError):\n\u001b[1;32m   1565\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_lowerdim(tup)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:873\u001b[0m, in \u001b[0;36m_LocationIndexer._validate_tuple_indexer\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[39mfor\u001b[39;00m i, k \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(key):\n\u001b[1;32m    872\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 873\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_key(k, i)\n\u001b[1;32m    874\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    875\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    876\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mLocation based indexing can only have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    877\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_valid_types\u001b[39m}\u001b[39;00m\u001b[39m] types\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    878\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1466\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_key\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1464\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1465\u001b[0m \u001b[39melif\u001b[39;00m is_integer(key):\n\u001b[0;32m-> 1466\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_integer(key, axis)\n\u001b[1;32m   1467\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m   1468\u001b[0m     \u001b[39m# a tuple should already have been caught by this point\u001b[39;00m\n\u001b[1;32m   1469\u001b[0m     \u001b[39m# so don't treat a tuple as a valid indexer\u001b[39;00m\n\u001b[1;32m   1470\u001b[0m     \u001b[39mraise\u001b[39;00m IndexingError(\u001b[39m\"\u001b[39m\u001b[39mToo many indexers\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1557\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1555\u001b[0m len_axis \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis(axis))\n\u001b[1;32m   1556\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m len_axis \u001b[39mor\u001b[39;00m key \u001b[39m<\u001b[39m \u001b[39m-\u001b[39mlen_axis:\n\u001b[0;32m-> 1557\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39msingle positional indexer is out-of-bounds\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "i = 8\n",
    "tax_clean_set = tax_clean.iloc[tax_label_index]\n",
    "tax_dirty_set = tax_dirty.iloc[tax_label_index]\n",
    "# hospital_dirty[hospital_dirty.iloc[:,i]!=hospital_clean.iloc[:,i]]\n",
    "# detector_inference = \"The input \\n\\n%s\\n\\nare some dirty cells from table Hospital column %s, and the input \\n\\n%s\\n\\n are corrected clean cells, and %s are examples of corresponding corrected clean cells. please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (dirty_list,col_name,clean_list,clean_list_origin)\n",
    "# detector_inference = \"The input \\n\\n%s\\n\\nare [dirty,clean] cell pairs from table Hospital column %s, and %s are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (dirty_list,col_name,clean_list,clean_list_origin)\n",
    "\n",
    "dirty_list = []\n",
    "col_name = tax_clean.columns[i]\n",
    "clean_list = list(tax_dirty.iloc[:,i].unique())\n",
    "for index,row in tax_dirty_set[tax_dirty_set.iloc[:,i]!=tax_clean_set.iloc[:,i]].iterrows():\n",
    "    # print(hospital_clean.iloc[index,i],hospital_dirty.iloc[index,i])\n",
    "    # dirty_list.append([tax_clean.iloc[index,i],tax_dirty.iloc[index,i]])\n",
    "    # dirty_list.append([[tax_clean.iloc[index,i-1],tax_clean.iloc[index,i]],[tax_clean.iloc[index,i-1],tax_dirty.iloc[index,i]]])\n",
    "    dirty_list.append([concat_row(tax_clean_set.iloc[index,[8,12,13]]),concat_row(tax_dirty_set.iloc[index,[8,12,13]])])\n",
    "# clean_list_part = list(beer_clean_set.iloc[:,i].unique())\n",
    "# dirty_list_part = list(beer_dirty[beer_dirty_set.iloc[:,i]!=beer_clean_set.iloc[:,i]].iloc[:,i].unique())\n",
    "#     # print(index)\n",
    "\n",
    "col_name = list(tax_clean.columns[[8,12,13]])\n",
    "\n",
    "detector_inference = \"The input \\n\\n%s\\n\\nare [clean,dirty] cell pairs from table Tax column %s, and %s are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not. Input and output are all string format.\" % (dirty_list,col_name,set(text_list))\n",
    "# detector_inference = \"The input \\n\\n%s\\n\\nare clean cells, and \\n\\n%s\\n\\nare dirty cells from table Hospital column %s, and %s are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (clean_list_part,dirty_list_part,col_name,clean_list[:15])\n",
    "print(detector_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([158, 455,  85, 118, 322, 384, 392, 615, 656, 796, 862, 975, 979,\n",
       "       357, 130, 532, 212, 694, 924, 918])"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_label_dirty = np.load('datasets/rayyan/detector/index.npy')\n",
    "rayyan_label_dirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_clean(cell):\n",
    "    # Regular expression to detect if the cell is in the format M/D/YY\n",
    "    # where M is 1-12 and D is 1-31\n",
    "    pattern = re.compile(r'^([1-9]|1[0-2])/([1-9]|[12][0-9]|3[01])/\\d{2}$')\n",
    "    return bool(pattern.match(cell))\n",
    "for c in rayyan_dirty[rayyan_dirty.iloc[:,8]!=rayyan_clean.iloc[:,8]].iloc[:,8].to_list():\n",
    "    if not is_clean(c):\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The input \\n\\n[]\\n\\nare [clean,dirty] cell pairs from table rayyan column article_jvolumn, and ['64', '', '54', '84', '33', '10', '29', '164', '25', '79', '14', '8', '92', '35', '140', '7', '0', '139', '39', '6', '40', '37', '27', '47', '69'] are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not. Input and output are all string format.\""
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 6\n",
    "rayyan_clean_set = rayyan_clean.iloc[rayyan_label_dirty]\n",
    "rayyan_dirty_set = rayyan_dirty.iloc[rayyan_label_dirty]\n",
    "\n",
    "\n",
    "dirty_list = []\n",
    "col_name = rayyan_clean.columns[i]\n",
    "clean_list = list(rayyan_dirty.iloc[:,i].unique())\n",
    "for index,row in rayyan_dirty_set[rayyan_dirty_set.iloc[:,i]!=rayyan_clean_set.iloc[:,i]].iterrows():\n",
    "    # print(hospital_clean.iloc[index,i],hospital_dirty.iloc[index,i])\n",
    "    dirty_list.append([rayyan_clean.iloc[index,i],rayyan_dirty.iloc[index,i]])\n",
    "# clean_list_part = list(beer_clean_set.iloc[:,i].unique())\n",
    "# dirty_list_part = list(beer_dirty[beer_dirty_set.iloc[:,i]!=beer_clean_set.iloc[:,i]].iloc[:,i].unique())\n",
    "#     # print(index)\n",
    "detector_inference = \"The input \\n\\n%s\\n\\nare [clean,dirty] cell pairs from table rayyan column %s, and %s are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not. Input and output are all string format.\" % (dirty_list,col_name,clean_list[:25])\n",
    "# detector_inference = \"The input \\n\\n%s\\n\\nare clean cells, and \\n\\n%s\\n\\nare dirty cells from table Hospital column %s, and %s are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (clean_list_part,dirty_list_part,col_name,clean_list[:15])\n",
    "detector_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>article_title</th>\n",
       "      <th>article_language</th>\n",
       "      <th>journal_title</th>\n",
       "      <th>jounral_abbreviation</th>\n",
       "      <th>journal_issn</th>\n",
       "      <th>article_jvolumn</th>\n",
       "      <th>article_jissue</th>\n",
       "      <th>article_jcreated_at</th>\n",
       "      <th>article_pagination</th>\n",
       "      <th>author_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>498345</td>\n",
       "      <td>Ebola Virus GP Gene Polyadenylation Versus RNA...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>The Journal of infectious diseases</td>\n",
       "      <td>J. Infect. Dis.</td>\n",
       "      <td>1537-6613</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4/2/15</td>\n",
       "      <td></td>\n",
       "      <td>{\"Valentina A Volchkova\",\"Jaroslav Vorac\",\"Phi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>255432</td>\n",
       "      <td>Pool fencing for preventing drowning in children.</td>\n",
       "      <td>eng</td>\n",
       "      <td>The Cochrane Database Of Systematic Reviews</td>\n",
       "      <td>Cochrane Database Syst Rev</td>\n",
       "      <td>1469-493X</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>1/1/00</td>\n",
       "      <td>CD001047</td>\n",
       "      <td>{\"D C Thompson\",\"F P Rivara\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>657210</td>\n",
       "      <td>From task characteristics to learning: A syste...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>Scandinavian journal of psychology</td>\n",
       "      <td>Scand J Psychol</td>\n",
       "      <td>1467-9450</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2/17/10</td>\n",
       "      <td></td>\n",
       "      <td>{\"Michiel A J Kompier\",\"Etty G A Wielenga-Meij...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>143029</td>\n",
       "      <td>Steroids and antihistamines synergize to inhib...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>European archives of oto-rhino-laryngology : o...</td>\n",
       "      <td>Eur Arch Otorhinolaryngol</td>\n",
       "      <td>1434-4726</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>8/13/14</td>\n",
       "      <td></td>\n",
       "      <td>{\"Shao-Cheng Liu\",\"Chi-Chung Wu\",\"Hsing-Won Wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>979059</td>\n",
       "      <td>Difference in fascicle behaviors between super...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>Muscle &amp; nerve</td>\n",
       "      <td>Muscle Nerve</td>\n",
       "      <td>1097-4598</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>9/10/15</td>\n",
       "      <td></td>\n",
       "      <td>{\"Hiroshi Akima\",\"Kazunori Nosaka\",\"Aya Tomita...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>735083</td>\n",
       "      <td>Muscle variables of importance for physiologic...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>European journal of applied physiology</td>\n",
       "      <td>Eur. J. Appl. Physiol.</td>\n",
       "      <td>1439-6327</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>10/8/15</td>\n",
       "      <td></td>\n",
       "      <td>{\"Sebastien Racinais\",\"Olivier Girard\",\"Lars N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>226419</td>\n",
       "      <td>Anterior approach unilateral right sacrospinou...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>International journal of colorectal disease</td>\n",
       "      <td>Int J Colorectal Dis</td>\n",
       "      <td>1432-1262</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1/7/15</td>\n",
       "      <td></td>\n",
       "      <td>{\"Enie Akhtar Bt Nawawi\",\"Ahlam M Al-Kharabshe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>734870</td>\n",
       "      <td>Stress across the life course and depression i...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>International journal of geriatric psychiatry</td>\n",
       "      <td>Int J Geriatr Psychiatry</td>\n",
       "      <td>1099-1166</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>10/9/15</td>\n",
       "      <td></td>\n",
       "      <td>{\"Stephen E Gilman\",\"Chaoqiang Jiang\",\"Kar Keu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>232896</td>\n",
       "      <td>Detecting and monitoring the symptoms of Parki...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>Parkinsonism &amp; related disorders</td>\n",
       "      <td>Parkinsonism Relat. Disord.</td>\n",
       "      <td>1873-5126</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>3/7/15</td>\n",
       "      <td></td>\n",
       "      <td>{\"S Arora\",\"A Zhan\",\"S Donohue\",\"E R Dorsey\",\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>81959</td>\n",
       "      <td>Disruption of the gastroesophageal junction by...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>Diseases of the esophagus : official journal o...</td>\n",
       "      <td>Dis. Esophagus</td>\n",
       "      <td>1442-2050</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2/28/14</td>\n",
       "      <td></td>\n",
       "      <td>{\"Y Y Lee\",\"K E L McColl\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>98562</td>\n",
       "      <td>Prevalence of Masked Hypertension in African A...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>Journal of clinical hypertension (Greenwich, C...</td>\n",
       "      <td>J Clin Hypertens (Greenwich)</td>\n",
       "      <td>1751-7176</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>10/20/14</td>\n",
       "      <td></td>\n",
       "      <td>{\"Frances Williams\",\"Alehegn Gelaye\",\"Susan St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>600827</td>\n",
       "      <td>Nutritional supplementation for hip fracture a...</td>\n",
       "      <td>eng</td>\n",
       "      <td>Cochrane Database Syst. Rev.</td>\n",
       "      <td></td>\n",
       "      <td>1469-493X</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>1/1/00</td>\n",
       "      <td>CD001880</td>\n",
       "      <td>{\"A Avenell\",\"H H Handoll\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>30272</td>\n",
       "      <td>Glial cells in amyotrophic lateral sclerosis.</td>\n",
       "      <td>ENG</td>\n",
       "      <td>Experimental neurology</td>\n",
       "      <td>Exp. Neurol.</td>\n",
       "      <td>1090-2430</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>5/22/14</td>\n",
       "      <td></td>\n",
       "      <td>{\"T Philips\",\"J D Rothstein\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>26475</td>\n",
       "      <td>Situating universal design architecture: desig...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>Disability and rehabilitation</td>\n",
       "      <td>Disabil Rehabil</td>\n",
       "      <td>1464-5165</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>7/28/14</td>\n",
       "      <td>6-Jan</td>\n",
       "      <td>{\"Paul Jones\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>476191</td>\n",
       "      <td>Endovascular Treatment of Ruptured Large or Wi...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>Journal of stroke and cerebrovascular diseases...</td>\n",
       "      <td>J Stroke Cerebrovasc Dis</td>\n",
       "      <td>1532-8511</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>7/24/15</td>\n",
       "      <td></td>\n",
       "      <td>{\"Yibin Fang\",\"Pengfei Yang\",\"Jianmin Liu\",\"Yi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>847600</td>\n",
       "      <td>Experiences of African-American Women with Sma...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>Public health nursing (Boston, Mass.)</td>\n",
       "      <td>Public Health Nurs</td>\n",
       "      <td>1525-1446</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>11/4/15</td>\n",
       "      <td></td>\n",
       "      <td>{\"Ashley McDonald\",\"Shannon N Zenk\",\"Colleen C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>18298</td>\n",
       "      <td>Ulcer Healing After Peripheral Intervention.</td>\n",
       "      <td>ENG</td>\n",
       "      <td>Circulation journal : official journal of the ...</td>\n",
       "      <td>Circ. J.</td>\n",
       "      <td>1347-4820</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>7/4/14</td>\n",
       "      <td></td>\n",
       "      <td>{\"Daiki Uchida\",\"Yukihiro Saito\",\"Hisashi Uchi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>100986</td>\n",
       "      <td>Association of bleeding, mortality and sex in ...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>Journal of cardiovascular medicine (Hagerstown...</td>\n",
       "      <td>J Cardiovasc Med (Hagerstown)</td>\n",
       "      <td>1558-2035</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>9/23/14</td>\n",
       "      <td></td>\n",
       "      <td>{\"Rossana De Palma\",\"Paolo Ortolani\",\"Caterina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>71351</td>\n",
       "      <td>Differential dendritic cell-mediated activatio...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>Oral diseases</td>\n",
       "      <td>Oral Dis</td>\n",
       "      <td>1601-0825</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>3/22/14</td>\n",
       "      <td></td>\n",
       "      <td>{\"Nk Shukla\",\"Sn Das\",\"P Gaur\",\"Ak Singh\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>603381</td>\n",
       "      <td>WITHDRAWN: Zinc for the common cold.</td>\n",
       "      <td>eng</td>\n",
       "      <td>Cochrane Database Syst. Rev.</td>\n",
       "      <td></td>\n",
       "      <td>1469-493X</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>1/1/06</td>\n",
       "      <td>CD001364</td>\n",
       "      <td>{\"I Marshall\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>98479</td>\n",
       "      <td>The ROX coupler: Creation of a fixed iliofemor...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>Catheterization and cardiovascular interventio...</td>\n",
       "      <td>Catheter Cardiovasc Interv</td>\n",
       "      <td>1522-726X</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>10/24/14</td>\n",
       "      <td></td>\n",
       "      <td>{\"Adam Witkowski\",\"Ajay K Jain\",\"Ivan Casserly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>226417</td>\n",
       "      <td>Correct Performance of Pelvic Muscle Exercises...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>Female pelvic medicine &amp; reconstructive surgery</td>\n",
       "      <td>Female Pelvic Med Reconstr Surg</td>\n",
       "      <td>2154-4212</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>10/27/14</td>\n",
       "      <td></td>\n",
       "      <td>{\"Katharine O'Dell\",\"Padma Kandadai\",\"Jyot Sai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                      article_title  \\\n",
       "1    498345  Ebola Virus GP Gene Polyadenylation Versus RNA...   \n",
       "109  255432  Pool fencing for preventing drowning in children.   \n",
       "181  657210  From task characteristics to learning: A syste...   \n",
       "196  143029  Steroids and antihistamines synergize to inhib...   \n",
       "209  979059  Difference in fascicle behaviors between super...   \n",
       "312  735083  Muscle variables of importance for physiologic...   \n",
       "323  226419  Anterior approach unilateral right sacrospinou...   \n",
       "340  734870  Stress across the life course and depression i...   \n",
       "356  232896  Detecting and monitoring the symptoms of Parki...   \n",
       "360   81959  Disruption of the gastroesophageal junction by...   \n",
       "445   98562  Prevalence of Masked Hypertension in African A...   \n",
       "522  600827  Nutritional supplementation for hip fracture a...   \n",
       "533   30272      Glial cells in amyotrophic lateral sclerosis.   \n",
       "549   26475  Situating universal design architecture: desig...   \n",
       "550  476191  Endovascular Treatment of Ruptured Large or Wi...   \n",
       "642  847600  Experiences of African-American Women with Sma...   \n",
       "768   18298       Ulcer Healing After Peripheral Intervention.   \n",
       "791  100986  Association of bleeding, mortality and sex in ...   \n",
       "861   71351  Differential dendritic cell-mediated activatio...   \n",
       "866  603381               WITHDRAWN: Zinc for the common cold.   \n",
       "887   98479  The ROX coupler: Creation of a fixed iliofemor...   \n",
       "997  226417  Correct Performance of Pelvic Muscle Exercises...   \n",
       "\n",
       "    article_language                                      journal_title  \\\n",
       "1                ENG                 The Journal of infectious diseases   \n",
       "109              eng        The Cochrane Database Of Systematic Reviews   \n",
       "181              ENG                 Scandinavian journal of psychology   \n",
       "196              ENG  European archives of oto-rhino-laryngology : o...   \n",
       "209              ENG                                     Muscle & nerve   \n",
       "312              ENG             European journal of applied physiology   \n",
       "323              ENG        International journal of colorectal disease   \n",
       "340              ENG      International journal of geriatric psychiatry   \n",
       "356              ENG                   Parkinsonism & related disorders   \n",
       "360              ENG  Diseases of the esophagus : official journal o...   \n",
       "445              ENG  Journal of clinical hypertension (Greenwich, C...   \n",
       "522              eng                       Cochrane Database Syst. Rev.   \n",
       "533              ENG                             Experimental neurology   \n",
       "549              ENG                      Disability and rehabilitation   \n",
       "550              ENG  Journal of stroke and cerebrovascular diseases...   \n",
       "642              ENG              Public health nursing (Boston, Mass.)   \n",
       "768              ENG  Circulation journal : official journal of the ...   \n",
       "791              ENG  Journal of cardiovascular medicine (Hagerstown...   \n",
       "861              ENG                                      Oral diseases   \n",
       "866              eng                       Cochrane Database Syst. Rev.   \n",
       "887              ENG  Catheterization and cardiovascular interventio...   \n",
       "997              ENG    Female pelvic medicine & reconstructive surgery   \n",
       "\n",
       "                jounral_abbreviation journal_issn article_jvolumn  \\\n",
       "1                    J. Infect. Dis.    1537-6613                   \n",
       "109       Cochrane Database Syst Rev    1469-493X                   \n",
       "181                  Scand J Psychol    1467-9450                   \n",
       "196        Eur Arch Otorhinolaryngol    1434-4726                   \n",
       "209                     Muscle Nerve    1097-4598                   \n",
       "312           Eur. J. Appl. Physiol.    1439-6327                   \n",
       "323             Int J Colorectal Dis    1432-1262                   \n",
       "340         Int J Geriatr Psychiatry    1099-1166                   \n",
       "356      Parkinsonism Relat. Disord.    1873-5126                   \n",
       "360                   Dis. Esophagus    1442-2050                   \n",
       "445     J Clin Hypertens (Greenwich)    1751-7176                   \n",
       "522                                     1469-493X                   \n",
       "533                     Exp. Neurol.    1090-2430                   \n",
       "549                  Disabil Rehabil    1464-5165                   \n",
       "550         J Stroke Cerebrovasc Dis    1532-8511                   \n",
       "642               Public Health Nurs    1525-1446                   \n",
       "768                         Circ. J.    1347-4820                   \n",
       "791    J Cardiovasc Med (Hagerstown)    1558-2035                   \n",
       "861                         Oral Dis    1601-0825                   \n",
       "866                                     1469-493X                   \n",
       "887       Catheter Cardiovasc Interv    1522-726X                   \n",
       "997  Female Pelvic Med Reconstr Surg    2154-4212                   \n",
       "\n",
       "    article_jissue article_jcreated_at article_pagination  \\\n",
       "1                               4/2/15                      \n",
       "109              2              1/1/00           CD001047   \n",
       "181                            2/17/10                      \n",
       "196                            8/13/14                      \n",
       "209                            9/10/15                      \n",
       "312                            10/8/15                      \n",
       "323                             1/7/15                      \n",
       "340                            10/9/15                      \n",
       "356                             3/7/15                      \n",
       "360                            2/28/14                      \n",
       "445                           10/20/14                      \n",
       "522              4              1/1/00           CD001880   \n",
       "533                            5/22/14                      \n",
       "549                            7/28/14              6-Jan   \n",
       "550                            7/24/15                      \n",
       "642                            11/4/15                      \n",
       "768                             7/4/14                      \n",
       "791                            9/23/14                      \n",
       "861                            3/22/14                      \n",
       "866              3              1/1/06           CD001364   \n",
       "887                           10/24/14                      \n",
       "997                           10/27/14                      \n",
       "\n",
       "                                           author_list  \n",
       "1    {\"Valentina A Volchkova\",\"Jaroslav Vorac\",\"Phi...  \n",
       "109                      {\"D C Thompson\",\"F P Rivara\"}  \n",
       "181  {\"Michiel A J Kompier\",\"Etty G A Wielenga-Meij...  \n",
       "196  {\"Shao-Cheng Liu\",\"Chi-Chung Wu\",\"Hsing-Won Wa...  \n",
       "209  {\"Hiroshi Akima\",\"Kazunori Nosaka\",\"Aya Tomita...  \n",
       "312  {\"Sebastien Racinais\",\"Olivier Girard\",\"Lars N...  \n",
       "323  {\"Enie Akhtar Bt Nawawi\",\"Ahlam M Al-Kharabshe...  \n",
       "340  {\"Stephen E Gilman\",\"Chaoqiang Jiang\",\"Kar Keu...  \n",
       "356  {\"S Arora\",\"A Zhan\",\"S Donohue\",\"E R Dorsey\",\"...  \n",
       "360                         {\"Y Y Lee\",\"K E L McColl\"}  \n",
       "445  {\"Frances Williams\",\"Alehegn Gelaye\",\"Susan St...  \n",
       "522                        {\"A Avenell\",\"H H Handoll\"}  \n",
       "533                      {\"T Philips\",\"J D Rothstein\"}  \n",
       "549                                     {\"Paul Jones\"}  \n",
       "550  {\"Yibin Fang\",\"Pengfei Yang\",\"Jianmin Liu\",\"Yi...  \n",
       "642  {\"Ashley McDonald\",\"Shannon N Zenk\",\"Colleen C...  \n",
       "768  {\"Daiki Uchida\",\"Yukihiro Saito\",\"Hisashi Uchi...  \n",
       "791  {\"Rossana De Palma\",\"Paolo Ortolani\",\"Caterina...  \n",
       "861         {\"Nk Shukla\",\"Sn Das\",\"P Gaur\",\"Ak Singh\"}  \n",
       "866                                     {\"I Marshall\"}  \n",
       "887  {\"Adam Witkowski\",\"Ajay K Jain\",\"Ivan Casserly...  \n",
       "997  {\"Katharine O'Dell\",\"Padma Kandadai\",\"Jyot Sai...  "
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_dirty[rayyan_dirty.iloc[:,6]!=rayyan_clean.iloc[:,6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_MODE=disabled accelerate launch src/train_bash.py     --stage sft     --model_name_or_path /data/yanmengyi/huggingface/vicuna-13b-1.3     --do_train     --finetuning_type lora     --dataset beer-train-15     --output_dir lora_weight/beer/beer-train-15 --overwrite_output_dir     --lr_scheduler_type cosine     --num_train_epochs 15.0     --gradient_accumulation_steps 8     --per_device_eval_batch_size 8     --fp16     --template vicuna     --lora_r 16     --quantization_bit 8 --logging_steps 5 --plot_loss  --lora_target q_proj,v_proj --save_steps 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean: 1/2/03\n",
    "dirty: 3/1/02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>article_title</th>\n",
       "      <th>article_language</th>\n",
       "      <th>journal_title</th>\n",
       "      <th>jounral_abbreviation</th>\n",
       "      <th>journal_issn</th>\n",
       "      <th>article_jvolumn</th>\n",
       "      <th>article_jissue</th>\n",
       "      <th>article_jcreated_at</th>\n",
       "      <th>article_pagination</th>\n",
       "      <th>author_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>589465</td>\n",
       "      <td>Effect of a cooling hood on physiological resp...</td>\n",
       "      <td>eng</td>\n",
       "      <td>J Appl Physiol</td>\n",
       "      <td>Journal of applied physiology</td>\n",
       "      <td>0021-8987 (Print)     0021-8987</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>1/1/70</td>\n",
       "      <td>36-9</td>\n",
       "      <td>{\"E. Shvartz\"}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                      article_title  \\\n",
       "705  589465  Effect of a cooling hood on physiological resp...   \n",
       "\n",
       "    article_language   journal_title           jounral_abbreviation  \\\n",
       "705              eng  J Appl Physiol  Journal of applied physiology   \n",
       "\n",
       "                        journal_issn article_jvolumn article_jissue  \\\n",
       "705  0021-8987 (Print)     0021-8987              29              1   \n",
       "\n",
       "    article_jcreated_at article_pagination     author_list  \n",
       "705              1/1/70               36-9  {\"E. Shvartz\"}  "
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_dirty[rayyan_dirty['article_jcreated_at']=='1/1/70']\n",
    "# rayyan_dirty[rayyan_dirty['journal_title']=='Anaesthesia']\n",
    "# rayyan_clean.iloc[:,8].value_counts()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell: 9788480000000\n",
      "Status: Clean\n",
      "\n",
      "Cell: Mar-22\n",
      "Status: Dirty\n",
      "\n",
      "Cell: Feb-14\n",
      "Status: Dirty\n",
      "\n",
      "Cell: Jan-15\n",
      "Status: Dirty\n",
      "\n",
      "Cell: Mar-17\n",
      "Status: Dirty\n",
      "\n",
      "Cell: Feb-14\n",
      "Status: Dirty\n",
      "\n",
      "Cell: Jan-14\n",
      "Status: Dirty\n",
      "\n",
      "Cell: Sep-22\n",
      "Status: Dirty\n",
      "\n",
      "Cell: Sep-22\n",
      "Status: Dirty\n",
      "\n",
      "Cell: Sep-22\n",
      "Status: Dirty\n",
      "\n",
      "Cell: Mar-22\n",
      "Status: Dirty\n",
      "\n",
      "Cell: Mar-09\n",
      "Status: Dirty\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def is_dirty(cell):\n",
    "    # Regular expression to detect if the cell is in the format Mon-DD\n",
    "    date_pattern = re.compile(r'^[A-Za-z]{3}-\\d{1,2}$')\n",
    "    \n",
    "    # Regular expression to detect if the ISSN starts with a number other than 9\n",
    "    issn_pattern = re.compile(r'^[^9]\\d{12}$')\n",
    "    \n",
    "    return bool(date_pattern.match(cell) or issn_pattern.match(cell))\n",
    "def generate_dirty_cell(cell):\n",
    "    # If the cell matches the format DD-Mon, reverse it to Mon-DD\n",
    "    date_match = re.match(r'(\\d{1,2})-([A-Za-z]{3})', cell)\n",
    "    if date_match:\n",
    "        return f\"{date_match.group(2)}-{date_match.group(1)}\"\n",
    "    \n",
    "    # If the cell matches the ISSN format starting with '9', change the leading '9' to another number\n",
    "    issn_match = re.match(r'^9\\d{12}$', cell)\n",
    "    if issn_match:\n",
    "        return f\"{random.choice(['0', '1', '2', '3', '4', '5', '6', '7', '8'])}{cell[1:]}\"\n",
    "    \n",
    "    return cell\n",
    "# Test\n",
    "cells_to_check = ['0035-9157 (Print) 0035-9157', '1537-6613', '0301-4738', '', '1558-0520', '0007-1420', '1385-4046', '1361-9209', '1940-5901', '0001-6349 (Print) 0001-6349', '1547-5271', '0007-0963', '1178-1998', '1469-493X', 'Jan-68', '0195-9131', '1940-6215', '1432-086X', '1527-7755', '1916-9736', '1555-2101', '1932-6203', '15265161 (ISSN)', '1598-9992', '0195-668X']\n",
    "\n",
    "for cells in rayyan_dirty[rayyan_dirty.iloc[:,5]!=rayyan_clean.iloc[:,5]].iloc[:,5].to_list():\n",
    "    cell = cells\n",
    "    print(f\"Cell: {cell}\")\n",
    "    print(f\"Status: {'Dirty' if is_dirty(cell) else 'Clean'}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def is_dirty(cell):\n",
    "    # Regular expression to detect if the cell contains special characters or combining diacritical marks\n",
    "    pattern = re.compile(r'[�\\u0300-\\u036F]')\n",
    "    return bool(pattern.search(cell))\n",
    "\n",
    "# Test\n",
    "# cells_to_check = ['Late repair of injuries of the anal sphincter', 'Ebola Virus GP Gene Polyadenylation Versus RNA Editing.', 'Duane retraction syndrome associated with oculocutaneous albinism: an ocular miswiring.', '[Noninvasive prenatal diagnosis of trisomy 21, 18 and 13 using cell-free fetal DNA]', 'Diagnosis and Management of Cutaneous B-cell Lymphoma.']\n",
    "\n",
    "# for cells in rayyan_dirty[rayyan_dirty.iloc[:,1]!=rayyan_clean.iloc[:,1]].iloc[:,1].to_list():\n",
    "#     cell = cells\n",
    "#     print(f\"Cell: {cell}\")\n",
    "#     print(f\"Status: {'Dirty' if is_dirty(cell) else 'Clean'}\\n\")\n",
    "import re\n",
    "\n",
    "def clean_cell(cell):\n",
    "    # Remove special characters like �\n",
    "    cleaned = re.sub(r'�', '', cell)\n",
    "    \n",
    "    # Remove combining diacritical marks (from Unicode range U+0300 to U+036F)\n",
    "    cleaned = re.sub(r'[\\u0300-\\u036F]', '', cleaned)\n",
    "    \n",
    "    return cleaned\n",
    "def generate_dirty_cell(cell):\n",
    "    # List of special characters and combining diacritical marks\n",
    "    special_chars = ['�', '\\u0301', '\\u0300', '\\u0302', '\\u0303', '\\u0304']\n",
    "    \n",
    "    # Randomly choose a special character\n",
    "    char = random.choice(special_chars)\n",
    "    \n",
    "    # Randomly choose a position to insert the special character\n",
    "    position = random.randint(0, len(cell))\n",
    "    \n",
    "    # Insert the special character at the chosen position\n",
    "    dirty_cell = cell[:position] + char + cell[position:]\n",
    "    \n",
    "    return dirty_cell\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/beer-test-20.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[result['predict']!=result['output']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def is_dirty(cell):\n",
    "    # Regular expression to detect the character 'x' in places where it's likely an error\n",
    "    pattern = re.compile(r'\\bx|\\bx\\b|[^a-z]x[^a-z]', re.IGNORECASE)\n",
    "    \n",
    "    if pattern.search(cell):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Test\n",
    "cells =clean_list\n",
    "\n",
    "for cell in cells:\n",
    "    print(f\"{cell}: {'Dirty' if is_dirty(cell) else 'Clean'}\")\n",
    "    print(cell.__contains__('x'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['10018', '10019', '10001', '10005', '10006', '10007', '10008', '10009', '10010', '10011', '10012', '10015', '10016', '10038', '10055', '10056', '10085', '10086', '10087', '10108', '10158', '10164', '20017', '20018', '10021', '10022', '10023', '10024', '10025', '10027', '10029', '10032', '10033', '10034', '10035', '10036', '10039', '10040', '10043', '10044', '10045', '10046', '10047', '10049', '10050'] are clean cells with pattern re.match(r'^\\\\d{5}$', and ['1xx19', 'x0005', '1000x', 'x00xx', 'x00x5', '1xx15', '1xx16', '100x8', '100x6', 'x0x08', '1xx24', 'x0027', 'x0029', '1xx29', '1xx32', '100x4', '100x5', '1xx35', '1xx36', '1003x', '1xx39', '100x9', '1xx44', '1xx45', 'x0045', '1xx47', '1004x'] are dirty cells. Please conclude a general pattern for dirty and clean cells, and write a general Python function to transfer a given clean cell to dirty cell.\""
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_dirty(cell):\n",
    "    \"\"\"Return True if the cell is dirty, otherwise False.\"\"\"\n",
    "    # If the cell matches the pattern of 5 digits, it's clean.\n",
    "    # If not, it's dirty.\n",
    "    return not bool(re.match(r'^\\d{5}$', cell))\n",
    "dirty = [c for c in clean_list if is_dirty(c)]\n",
    "clean = [c for c in clean_list if not is_dirty(c)]\n",
    "query = '%s are clean cells with pattern %s, and %s are dirty cells. Please conclude a general pattern for dirty and clean cells, and write a general Python function to transfer a given clean cell to dirty cell.' % (clean,\"re.match(r'^\\d{5}$'\",dirty)\n",
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From Here, We Try to write a function to construct Multi-view Framework for the Plain Version and Augmented Version of Critic LM Model\n",
    "- Single-View: COL ProviderID VAL 1001x SEP COL ProviderID VAL 1001x\n",
    "- Multi-View: COL A VAL ValueA COL B VAL ValueB SEP COL A VAL ValueA\n",
    "- Column-View: Extract from Same Cluster\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([532, 157, 213, 228, 238])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hospital_dirty.shape\n",
    "selected_index_hospital_5 = selected_index_hospital[:5]\n",
    "# selected_index_hospital_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Construct the Init Multi-view Training Set\n",
    "## 找到选中的20个tuple\n",
    "selected_index_hospital_5 = selected_index_hospital[:20]\n",
    "input_matrix_select = input_matrix[selected_index_hospital_5]\n",
    "row_list = []\n",
    "single_list = []\n",
    "column_list = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "for label_tuple in selected_index_hospital_5:\n",
    "    cluster_index = hospital_clean.iloc[label_tuple,1]\n",
    "    cluster_set_index = hospital_clean[hospital_clean['ProviderNumber']==cluster_index].index\n",
    "    cluster_set = hospital_dirty.iloc[cluster_set_index]\n",
    "    for i in range(len(hospital_clean.columns)):\n",
    "        columns_unique = list(cluster_set.iloc[:,i].unique())\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = hospital_clean.iloc[label_tuple]\n",
    "        dirty_context = hospital_dirty.iloc[label_tuple]\n",
    "        clean_cell = hospital_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = hospital_dirty.iloc[label_tuple,i]\n",
    "        single_context_clean = 'COL %s VAL %s ' % (hospital_dirty.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (hospital_dirty.columns[i],dirty_cell)\n",
    "        column_context = 'COL %s VAL %s ' % (hospital_dirty.columns[i],columns_unique)\n",
    "        for c in range(20):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (hospital_clean.columns[c],hospital_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (hospital_dirty.columns[c],hospital_dirty.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            row_list.append([all_context_dirty,single_context_dirty,1])\n",
    "            row_list.append([all_context_clean,single_context_clean,0])\n",
    "            single_list.append([single_context_dirty,single_context_dirty,1])\n",
    "            single_list.append([single_context_clean,single_context_clean,0])\n",
    "            column_list.append([column_context,single_context_dirty,1])\n",
    "            column_list.append([column_context,single_context_clean,0])\n",
    "            # detector_list.append([all_context_dirty,single_context_clean,0])\n",
    "            # detector_list.append([single_context_dirty,1])\n",
    "        else:\n",
    "            row_list.append([all_context_dirty,single_context_dirty,0])\n",
    "            row_list.append([all_context_clean,single_context_clean,0])\n",
    "            single_list.append([single_context_clean,single_context_clean,0])\n",
    "            column_list.append([column_context,single_context_clean,0])\n",
    "            # detector_list.append([all_context_dirty,single_context_clean,0])\n",
    "        # else:\n",
    "        #     detector_list.append([all_context_dirty,1])\n",
    "        #     detector_list.append([single_context_dirty,1])\n",
    "            \n",
    "                # break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.concatenate([single_list,row_list,column_list])).to_csv('/home/yanmy/raha/raha-master/datasets/hospital/detector/multi-view/train_20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def insert_randomly(lst, element):\n",
    "    # 选择一个随机的索引\n",
    "    index = random.randint(0, len(lst))\n",
    "    \n",
    "    # 在随机位置插入新元素\n",
    "    lst.insert(index, element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27048/2524762027.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dirty_row[i] = dirty_cell\n"
     ]
    }
   ],
   "source": [
    "# input_matrix_select = input_matrix[selected_index_hospital_5]\n",
    "## Coreset Generation, used to augment Critic Model \n",
    "hospital_coreset = hospital_dirty[hospital_dirty['count']==0].index\n",
    "noise_col = np.where(input_matrix[selected_index_hospital_5].sum(axis=0)!=0)[0] ## 不使用不可靠的detector\n",
    "row_list = []\n",
    "single_list = []\n",
    "column_list = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "for label_tuple in hospital_coreset:\n",
    "    cluster_index = hospital_clean.iloc[label_tuple,1]\n",
    "    cluster_set_index = hospital_clean[hospital_clean['ProviderNumber']==cluster_index].index\n",
    "    cluster_set = hospital_dirty.iloc[cluster_set_index]\n",
    "    noise_col_subset = np.random.choice(noise_col,5,replace=False) ## 取5个col作为注入噪声的对象\n",
    "    for i in noise_col_subset:\n",
    "        original_row = hospital_dirty.iloc[label_tuple]\n",
    "        dirty_row = original_row\n",
    "        clean_cell = hospital_dirty.iloc[label_tuple,i]\n",
    "        dirty_cell = replace_random_char_with_x(clean_cell) ## Inject Noise\n",
    "        columns_unique = list(cluster_set.iloc[:,i].unique())\n",
    "        insert_randomly(columns_unique,dirty_cell)\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        dirty_row[i] = dirty_cell\n",
    "        single_context_clean = 'COL %s VAL %s ' % (hospital_dirty.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (hospital_dirty.columns[i],dirty_cell)\n",
    "        column_context = 'COL %s VAL %s ' % (hospital_dirty.columns[i],columns_unique)\n",
    "        for c in range(20):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (hospital_dirty.columns[c],original_row[c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (hospital_dirty.columns[c],dirty_row[c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            row_list.append([all_context_dirty,single_context_dirty,1])\n",
    "            row_list.append([all_context_clean,single_context_clean,0])\n",
    "            single_list.append([single_context_dirty,single_context_dirty,1])\n",
    "            single_list.append([single_context_clean,single_context_clean,0])\n",
    "            column_list.append([column_context,single_context_dirty,1])\n",
    "            column_list.append([column_context,single_context_clean,0])\n",
    "            # detector_list.append([all_context_dirty,single_context_clean,0])\n",
    "            # detector_list.append([single_context_dirty,1])\n",
    "        else:\n",
    "            row_list.append([all_context_dirty,single_context_dirty,0])\n",
    "            row_list.append([all_context_clean,single_context_clean,0])\n",
    "            single_list.append([single_context_clean,single_context_clean,0])\n",
    "            column_list.append([column_context,single_context_clean,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.concatenate([single_list,row_list,column_list])).to_csv('/home/yanmy/raha/raha-master/datasets/hospital/detector/multi-view/train_augment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Construct the Init Multi-view Training Set\n",
    "## 找到选中的20个tuple\n",
    "# input_matrix_select = input_matrix[selected_index_hospital]\n",
    "row_list = []\n",
    "single_list = []\n",
    "column_list = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "for label_tuple in range(len(hospital_dirty)):\n",
    "    cluster_index = hospital_clean.iloc[label_tuple,1]\n",
    "    cluster_set_index = hospital_clean[hospital_clean['ProviderNumber']==cluster_index].index\n",
    "    cluster_set = hospital_dirty.iloc[cluster_set_index]\n",
    "    for i in range(len(hospital_clean.columns)):\n",
    "        columns_unique = list(cluster_set.iloc[:,i].unique())\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = hospital_clean.iloc[label_tuple]\n",
    "        dirty_context = hospital_dirty.iloc[label_tuple]\n",
    "        clean_cell = hospital_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = hospital_dirty.iloc[label_tuple,i]\n",
    "        single_context_clean = 'COL %s VAL %s ' % (hospital_dirty.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (hospital_dirty.columns[i],dirty_cell)\n",
    "        column_context = 'COL %s VAL %s ' % (hospital_dirty.columns[i],columns_unique)\n",
    "        for c in range(20):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (hospital_clean.columns[c],hospital_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (hospital_dirty.columns[c],hospital_dirty.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            row_list.append([all_context_dirty,single_context_dirty,1])\n",
    "            # row_list.append([all_context_clean,single_context_clean,0])\n",
    "            single_list.append([single_context_dirty,single_context_dirty,1])\n",
    "            # single_list.append([single_context_clean,single_context_clean,0])\n",
    "            column_list.append([column_context,single_context_dirty,1])\n",
    "            # column_list.append([column_context,single_context_clean,0])\n",
    "            # detector_list.append([all_context_dirty,single_context_clean,0])\n",
    "            # detector_list.append([single_context_dirty,1])\n",
    "        else:\n",
    "            row_list.append([all_context_dirty,single_context_dirty,0])\n",
    "            # row_list.append([all_context_clean,single_context_clean,0])\n",
    "            single_list.append([single_context_clean,single_context_clean,0])\n",
    "            column_list.append([column_context,single_context_clean,0])\n",
    "            # detector_list.append([all_context_dirty,single_context_clean,0])\n",
    "        # else:\n",
    "        #     detector_list.append([all_context_dirty,1])\n",
    "        #     detector_list.append([single_context_dirty,1])\n",
    "            \n",
    "                # break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.concatenate([single_list,row_list,column_list])).to_csv('/home/yanmy/raha/raha-master/datasets/hospital/detector/multi-view/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hospital_dirty\n",
    "def is_dirty(cell):\n",
    "    # Regular expression to detect the character 'x'\n",
    "    pattern = re.compile(r'x', re.IGNORECASE)\n",
    "    \n",
    "    if pattern.search(cell):\n",
    "        return True\n",
    "    return False\n",
    "def generate_dirty_cell(cells):\n",
    "    \"\"\"Generate a random dirty cell based on the input cells.\"\"\"\n",
    "    # Randomly select a cell from the input cells.\n",
    "    cell = random.choice([c for c in cells if not is_dirty(c)])\n",
    "    \n",
    "    # Randomly select a position in the cell.\n",
    "    position = random.randint(0, len(cell) - 1)\n",
    "    \n",
    "    # Replace the character at that position with 'x'.\n",
    "    dirty_cell = cell[:position] + 'x' + cell[position+1:]\n",
    "    \n",
    "    return cell,dirty_cell\n",
    "def function_set_value_count(row):\n",
    "    count = 0\n",
    "    for x,y in row.items():\n",
    "        if(is_dirty(y)):\n",
    "            count += 1\n",
    "    return count\n",
    "hospital_dirty['count'] = hospital_dirty.apply(function_set_value_count,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([532, 157, 213, 228, 238, 256, 332, 386, 411, 493, 572, 698, 956,\n",
       "        22,  24,  42,  56,  57,  93,  94])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_index_hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 20)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector = np.load('datasets/hospital/detector/detector_5.npy').reshape((1000,-1))\n",
    "detector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coreset_detect = detector\n",
    "coreset_detect = np.where(detector.sum(axis=1)==0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,  16],\n",
       "       [  3,   6],\n",
       "       [  7,   6],\n",
       "       ...,\n",
       "       [986,   3],\n",
       "       [987,   2],\n",
       "       [997,  16]])"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(detector==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:00<00:00, 66.69it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "noise_col = np.where(detector.sum(axis=0)>0)[0]\n",
    "header = list(hospital_dirty.columns)\n",
    "safe_value = ['empty'] ## 不注入噪声的类型\n",
    "training_list = []\n",
    "for h in tqdm(hospital_cluster): ## 对比学习的采样策略在同cluster内进行\n",
    "    coreset_subset = [n for n in hospital_cluster[h] if n in coreset_detect]\n",
    "    for coreset_tuple in coreset_subset: ## 取遍coreset所有的tuple，剩下的n-1 tuple是参考对象,coreset_tuple是index\n",
    "        noise_col_subset = np.random.choice(noise_col,5,replace=False) ## 取5个col作为注入噪声的对象\n",
    "        for noise_col_single in noise_col_subset:\n",
    "            col_name = header[noise_col_single] ## 从index转成列名\n",
    "            if(hospital_dirty_dict[coreset_tuple][col_name] not in safe_value and len(coreset_subset)>2):\n",
    "                temp_dict = hospital_dirty.iloc[coreset_tuple,1:].to_dict() ## coreset tuple字典\n",
    "                clean_cell = temp_dict[col_name]\n",
    "                dirty_cell = replace_random_char_with_x(clean_cell) ## 注入噪声\n",
    "                temp_dict[col_name] = dirty_cell\n",
    "                coreset_reference = select_two_different_elements(coreset_subset,coreset_tuple)\n",
    "                template_dict = {}\n",
    "                clean_dict = {}\n",
    "                template_dict[col_name] = ''\n",
    "                clean_dict[col_name] = clean_cell\n",
    "                text_head = 'You are an expert in Cleaning Hospital Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\nTake these rows as reference:\\n\\n' % (col_name, json.dumps(template_dict), json.dumps(temp_dict))\n",
    "                \n",
    "                ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]), json.dumps(hospital_dirty_dict[coreset_reference[1]]))\n",
    "                \n",
    "                # ICL_text = '%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]))\n",
    "                \n",
    "                training_list.append([text_head,ICL_text,'',json.dumps(clean_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(training_list).to_csv('datasets/hospital/detector/correction_aug.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Labeling Data\n",
    "training_list_label = []\n",
    "# for label_tuple in tqdm(selected_index_hospital):\n",
    "#     for noise_col_single in range(len(header)):\n",
    "for label_tuple,noise_col_single in np.argwhere(input_matrix==1):\n",
    "    if(label_tuple in selected_index_hospital):\n",
    "        col_name = header[noise_col_single]\n",
    "        template_dict = {}\n",
    "        template_dict[col_name] = ''\n",
    "        clean_dict = {}\n",
    "        temp_dict = hospital_dirty.iloc[label_tuple,1:].to_dict()\n",
    "        clean_cell = hospital_clean.iloc[label_tuple,noise_col_single]\n",
    "        dirty_cell = hospital_dirty.iloc[label_tuple,noise_col_single]\n",
    "        clean_dict[col_name] = clean_cell\n",
    "        # if(clean_cell!=dirty_cell):\n",
    "        text_head = 'You are an expert in Cleaning Hospital Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\nTake these rows as reference:\\n\\n' % (col_name, json.dumps(template_dict), json.dumps(temp_dict))\n",
    "        cluster = [hospital_cluster[h] for h in hospital_cluster if hospital_cluster[h].__contains__(label_tuple)][0] ## 找到label tuple所在的簇\n",
    "        cluster_coreset = [c for c in cluster if c in coreset_detect] ## 找到簇内coreset元素\n",
    "        try:\n",
    "                coreset_reference = np.random.choice(cluster_coreset,2) ## 取两个\n",
    "                ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]), json.dumps(hospital_dirty_dict[coreset_reference[1]]))\n",
    "                # ICL_text = '%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]))\n",
    "        except:\n",
    "                coreset_reference = np.random.choice(cluster_coreset,1) ## 取两个\n",
    "                # ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]), json.dumps(hospital_dirty_dict[coreset_reference[1]]))\n",
    "                ICL_text = '%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]))\n",
    "        training_list_label.append([text_head,ICL_text,'',json.dumps(clean_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test\n",
    "training_list_label = []\n",
    "# for label_tuple in tqdm(selected_index_hospital):\n",
    "#     for noise_col_single in range(len(header)):\n",
    "for label_tuple,noise_col_single in np.argwhere(detector==1):\n",
    "        col_name = header[noise_col_single]\n",
    "        template_dict = {}\n",
    "        template_dict[col_name] = ''\n",
    "        clean_dict = {}\n",
    "        temp_dict = hospital_dirty.iloc[label_tuple,1:].to_dict()\n",
    "        clean_cell = hospital_clean.iloc[label_tuple,noise_col_single]\n",
    "        dirty_cell = hospital_dirty.iloc[label_tuple,noise_col_single]\n",
    "        clean_dict[col_name] = clean_cell\n",
    "        # if(clean_cell!=dirty_cell):\n",
    "        text_head = 'You are an expert in Cleaning Hospital Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\nTake these rows as reference:\\n\\n' % (col_name, json.dumps(template_dict), json.dumps(temp_dict))\n",
    "        cluster = [hospital_cluster[h] for h in hospital_cluster if hospital_cluster[h].__contains__(label_tuple)][0] ## 找到label tuple所在的簇\n",
    "        cluster_coreset = [c for c in cluster if c in coreset_detect] ## 找到簇内coreset元素\n",
    "        try:\n",
    "                coreset_reference = np.random.choice(cluster_coreset,2) ## 取两个\n",
    "                ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]), json.dumps(hospital_dirty_dict[coreset_reference[1]]))\n",
    "                # ICL_text = '%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]))\n",
    "        except:\n",
    "                coreset_reference = np.random.choice(cluster_coreset,1) ## 取两个\n",
    "                # ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]), json.dumps(hospital_dirty_dict[coreset_reference[1]]))\n",
    "                ICL_text = '%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]))\n",
    "        training_list_label.append([text_head,ICL_text,'',json.dumps(clean_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...</td>\n",
       "      <td></td>\n",
       "      <td>{\"MeasureCode\": \"scip-inf-1\"}</td>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td></td>\n",
       "      <td>{\"MeasureCode\": \"scip-inf-1\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...</td>\n",
       "      <td></td>\n",
       "      <td>{\"EmergencyService\": \"yes\"}</td>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td></td>\n",
       "      <td>{\"EmergencyService\": \"yes\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...</td>\n",
       "      <td></td>\n",
       "      <td>{\"PhoneNumber\": \"2053258100\"}</td>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td></td>\n",
       "      <td>{\"PhoneNumber\": \"2053258100\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ZipCode\": \"35233\"}</td>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ZipCode\": \"35233\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...</td>\n",
       "      <td></td>\n",
       "      <td>{\"State\": \"al\"}</td>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td></td>\n",
       "      <td>{\"State\": \"al\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10034\", \"HospitalName\": \"c...</td>\n",
       "      <td></td>\n",
       "      <td>{\"Condition\": \"heart failure\"}</td>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td></td>\n",
       "      <td>{\"Condition\": \"heart failure\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10034\", \"HospitalName\": \"c...</td>\n",
       "      <td></td>\n",
       "      <td>{\"Score\": \"88%\"}</td>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td></td>\n",
       "      <td>{\"Score\": \"88%\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10049\", \"HospitalName\": \"m...</td>\n",
       "      <td></td>\n",
       "      <td>{\"HospitalName\": \"medical center enterprise\"}</td>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td></td>\n",
       "      <td>{\"HospitalName\": \"medical center enterprise\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10049\", \"HospitalName\": \"m...</td>\n",
       "      <td></td>\n",
       "      <td>{\"Address1\": \"400 n edwards street\"}</td>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td></td>\n",
       "      <td>{\"Address1\": \"400 n edwards street\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10049\", \"HospitalName\": \"m...</td>\n",
       "      <td></td>\n",
       "      <td>{\"HospitalType\": \"acute care hospitals\"}</td>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td></td>\n",
       "      <td>{\"HospitalType\": \"acute care hospitals\"}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3055 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0  \\\n",
       "0   You are an expert in Cleaning Hospital Dataset...   \n",
       "1   You are an expert in Cleaning Hospital Dataset...   \n",
       "2   You are an expert in Cleaning Hospital Dataset...   \n",
       "3   You are an expert in Cleaning Hospital Dataset...   \n",
       "4   You are an expert in Cleaning Hospital Dataset...   \n",
       "..                                                ...   \n",
       "49  You are an expert in Cleaning Hospital Dataset...   \n",
       "50  You are an expert in Cleaning Hospital Dataset...   \n",
       "51  You are an expert in Cleaning Hospital Dataset...   \n",
       "52  You are an expert in Cleaning Hospital Dataset...   \n",
       "53  You are an expert in Cleaning Hospital Dataset...   \n",
       "\n",
       "                                                    1 2   \\\n",
       "0   {\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...      \n",
       "1   {\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...      \n",
       "2   {\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...      \n",
       "3   {\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...      \n",
       "4   {\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...      \n",
       "..                                                ... ..   \n",
       "49  {\"ProviderNumber\": \"10034\", \"HospitalName\": \"c...      \n",
       "50  {\"ProviderNumber\": \"10034\", \"HospitalName\": \"c...      \n",
       "51  {\"ProviderNumber\": \"10049\", \"HospitalName\": \"m...      \n",
       "52  {\"ProviderNumber\": \"10049\", \"HospitalName\": \"m...      \n",
       "53  {\"ProviderNumber\": \"10049\", \"HospitalName\": \"m...      \n",
       "\n",
       "                                                3  \\\n",
       "0                   {\"MeasureCode\": \"scip-inf-1\"}   \n",
       "1                     {\"EmergencyService\": \"yes\"}   \n",
       "2                   {\"PhoneNumber\": \"2053258100\"}   \n",
       "3                            {\"ZipCode\": \"35233\"}   \n",
       "4                                 {\"State\": \"al\"}   \n",
       "..                                            ...   \n",
       "49                 {\"Condition\": \"heart failure\"}   \n",
       "50                               {\"Score\": \"88%\"}   \n",
       "51  {\"HospitalName\": \"medical center enterprise\"}   \n",
       "52           {\"Address1\": \"400 n edwards street\"}   \n",
       "53       {\"HospitalType\": \"acute care hospitals\"}   \n",
       "\n",
       "                                          instruction input  \\\n",
       "0   You are an expert in Cleaning Hospital Dataset...         \n",
       "1   You are an expert in Cleaning Hospital Dataset...         \n",
       "2   You are an expert in Cleaning Hospital Dataset...         \n",
       "3   You are an expert in Cleaning Hospital Dataset...         \n",
       "4   You are an expert in Cleaning Hospital Dataset...         \n",
       "..                                                ...   ...   \n",
       "49  You are an expert in Cleaning Hospital Dataset...         \n",
       "50  You are an expert in Cleaning Hospital Dataset...         \n",
       "51  You are an expert in Cleaning Hospital Dataset...         \n",
       "52  You are an expert in Cleaning Hospital Dataset...         \n",
       "53  You are an expert in Cleaning Hospital Dataset...         \n",
       "\n",
       "                                           output  \n",
       "0                   {\"MeasureCode\": \"scip-inf-1\"}  \n",
       "1                     {\"EmergencyService\": \"yes\"}  \n",
       "2                   {\"PhoneNumber\": \"2053258100\"}  \n",
       "3                            {\"ZipCode\": \"35233\"}  \n",
       "4                                 {\"State\": \"al\"}  \n",
       "..                                            ...  \n",
       "49                 {\"Condition\": \"heart failure\"}  \n",
       "50                               {\"Score\": \"88%\"}  \n",
       "51  {\"HospitalName\": \"medical center enterprise\"}  \n",
       "52           {\"Address1\": \"400 n edwards street\"}  \n",
       "53       {\"HospitalType\": \"acute care hospitals\"}  \n",
       "\n",
       "[3055 rows x 7 columns]"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_list = pd.concat([pd.DataFrame(training_list),pd.DataFrame(training_list_label)]).drop_duplicates()\n",
    "training_list['instruction'] = training_list[0] + training_list[1]\n",
    "training_list['input'] = training_list[2]\n",
    "training_list['output'] = training_list[3]\n",
    "training_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list_pd = pd.DataFrame(training_list_label)\n",
    "training_list_pd['instruction'] = training_list_pd[0] + training_list_pd[1]\n",
    "training_list_pd['input'] = training_list_pd[2]\n",
    "training_list_pd['output'] = training_list_pd[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json.dump(training_list_pd.iloc[:,-3:].to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/hospital/hospital-train-5.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(training_list.iloc[:,-3:].to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/hospital/hospital-train-20.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,    0,    0,    0, 2410,  693,    0,    0,    0,  127,  127]),\n",
       " array([0, 0, 0, 0, 5, 5, 0, 0, 0, 5, 5]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Beer Detection Function By Rows\n",
    "input_matrix_beer = np.array(beer_clean!=beer_dirty).astype(int)\n",
    "input_matrix_beer.sum(axis=0),input_matrix_beer[beer_label_index[:5]].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ounces', 'abv', 'city', 'state'], dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer_clean.columns[[4,5,-2,-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_dirty(cell):\n",
    "    # Regular expression to detect if the cell contains anything other than whole numbers represented as strings\n",
    "    pattern = re.compile(r'^\\d+$')\n",
    "    return not bool(pattern.match(cell))\n",
    "is_dirty('19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2410, 2410)"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for c in beer_dirty[beer_dirty.iloc[:,i]!=beer_clean.iloc[:,i]].iloc[:,i].to_list():\n",
    "    if(Beer_Detection_Ounces(c)):\n",
    "        count += 1\n",
    "count,len(beer_dirty[beer_dirty.iloc[:,i]!=beer_clean.iloc[:,i]].iloc[:,i].to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'16 OZ.'"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Beer Detection Function By Rows\n",
    "import random\n",
    "def Beer_Detection_Ounces(cell):\n",
    "    pattern = re.compile(r'^\\d+$')\n",
    "    return not bool(pattern.match(cell))\n",
    "def correct_dirty_cell(cell):\n",
    "    # Regular expression to extract the numeric value from the cell\n",
    "    match = re.search(r'(\\d+(\\.\\d+)?)', cell)\n",
    "    if match:\n",
    "        value = match.group(1)\n",
    "        # Convert to integer string if the value ends with \".0\"\n",
    "        if value.endswith('.0'):\n",
    "            return str(int(float(value)))\n",
    "        return value\n",
    "    return cell\n",
    "def Beer_Generation_Ounces(cell):\n",
    "    descriptors = [\" oz.\", \" ounce\", \" OZ.\", \" oz. Alumi-Tek\", \" oz. Silo Can\"]\n",
    "    value = correct_dirty_cell(cell)\n",
    "    descriptor = random.choice(descriptors)\n",
    "    \n",
    "    # Append the descriptor to the cell to make it dirty\n",
    "    dirty_cell = value + descriptor\n",
    "    \n",
    "    return dirty_cell\n",
    "\n",
    "def Beer_Detection_abv(cell):\n",
    "    # Regular expression to detect if the cell contains \"%\" or has more than three decimal places\n",
    "    pattern = re.compile(r'%|^\\d+\\.\\d{4,}$')\n",
    "    return bool(pattern.search(cell))\n",
    "def Beer_Generation_abv(cell):\n",
    "    choices = [\"append_percent\"]\n",
    "    action = random.choice(choices)\n",
    "    \n",
    "    if action == \"append_percent\":\n",
    "        return cell + \"%\"\n",
    "    elif action == \"alter_float\" and \".\" in cell:\n",
    "        # Introduce a small random change to the floating point\n",
    "        parts = cell.split(\".\")\n",
    "        if len(parts[1]) == 3:\n",
    "            last_digit = str(int(parts[1][2]) + random.choice([-1, 1]) % 10)  # Increment or decrement the last digit\n",
    "            return parts[0] + \".\" + parts[1][:2] + last_digit + \"%\"\n",
    "    \n",
    "def Beer_Detection_city(cell):\n",
    "    pattern = re.compile(r'\\b[A-Z]{2}$')\n",
    "    return bool(pattern.search(cell))\n",
    "def Beer_Generation_city(cell):\n",
    "    state_abbreviations = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']\n",
    "    \n",
    "    # Randomly choose a state abbreviation\n",
    "    state = random.choice(state_abbreviations)\n",
    "    \n",
    "    # Append the state abbreviation to the cell to make it dirty\n",
    "    dirty_cell = cell + \" \" + state\n",
    "    \n",
    "    return dirty_cell\n",
    "def Beer_Detection_state(cell):\n",
    "    return cell == \"\"\n",
    "def Beer_Generation_state(cell):\n",
    "    return \"\"\n",
    "def Beer_Row_Detection(x,y):\n",
    "    # for x,y in row.items():\n",
    "        if(x=='ounces'):\n",
    "            return Beer_Detection_Ounces(y)\n",
    "        elif(x=='abv'):\n",
    "            return Beer_Detection_abv(y)\n",
    "        elif(x=='city'):\n",
    "            return Beer_Detection_city(y)\n",
    "        elif(x=='state'):\n",
    "            return Beer_Detection_state(y)\n",
    "        else:\n",
    "            return False\n",
    "def Beer_Row_Generation(x,y):\n",
    "    # for x,y in row.items():\n",
    "        if(x=='ounces'): ## try to Correct Ounces\n",
    "            return Beer_Generation_Ounces(y)\n",
    "            # return correct_dirty_cell(y)\n",
    "        elif(x=='abv'):\n",
    "            return Beer_Generation_abv(y)\n",
    "        elif(x=='city'):\n",
    "            return Beer_Generation_city(y)\n",
    "        elif(x=='state'):\n",
    "            return Beer_Generation_state(y)\n",
    "        else:\n",
    "            return False\n",
    "# def Beer_Row_Generation(x,y):\n",
    "    \n",
    "# Beer_Row_Detection('abv','0.599') ## True Means Error\n",
    "# Beer_Generation_city('Washington')\n",
    "Beer_Row_Generation('ounces','16.0 ounces')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_clean = pd.read_csv('datasets/beers/clean.csv').fillna('')\n",
    "beer_dirty = pd.read_csv('datasets/beers/dirty.csv').fillna('')\n",
    "beer_dirty.columns = beer_clean.columns\n",
    "def try_convert_to_int(row):\n",
    "    for x,y in row.items():\n",
    "        if(x in ['ounces','ibu']):\n",
    "            try:\n",
    "                row[x] = int(y)\n",
    "            except:\n",
    "                row[x] = y\n",
    "    return row\n",
    "beer_clean = beer_clean.apply(try_convert_to_int,axis=1).astype(str)\n",
    "beer_dirty = beer_dirty.apply(try_convert_to_int,axis=1).astype(str)\n",
    "beer_dirty_augment = beer_dirty.copy()\n",
    "beer_dirty_augment['ounces'] = beer_dirty_augment['ounces'].apply(correct_dirty_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beers Dataset Correction \n",
    "# beer_dirty_augment = beer_dirty\n",
    "# beer_dirty_augment['ounces'] = beer_dirty_augment['ounces'].apply(correct_dirty_cell)\n",
    "## Coreset Generation, used to augment Critic Model \n",
    "# hospital_coreset = hospital_dirty[hospital_dirty['count']==0].index\n",
    "# noise_col = np.where(input_matrix[selected_index_hospital_5].sum(axis=0)!=0)[0] ## 不使用不可靠的detector\n",
    "row_list = []\n",
    "single_list = []\n",
    "column_list = []\n",
    "# ## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "#     # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "#     #     print(hospital_clean.columns[i])\n",
    "# for label_tuple in hospital_coreset:\n",
    "#     cluster_index = hospital_clean.iloc[label_tuple,1]\n",
    "#     cluster_set_index = hospital_clean[hospital_clean['ProviderNumber']==cluster_index].index\n",
    "#     cluster_set = hospital_dirty.iloc[cluster_set_index]\n",
    "#     noise_col_subset = np.random.choice(noise_col,5,replace=False) ## 取5个col作为注入噪声的对象\n",
    "#     for i in noise_col_subset:\n",
    "#         original_row = hospital_dirty.iloc[label_tuple]\n",
    "#         dirty_row = original_row\n",
    "#         clean_cell = hospital_dirty.iloc[label_tuple,i]\n",
    "#         dirty_cell = replace_random_char_with_x(clean_cell) ## Inject Noise\n",
    "#         columns_unique = list(cluster_set.iloc[:,i].unique())\n",
    "#         insert_randomly(columns_unique,dirty_cell)\n",
    "#         # if(input_matrix[label_tuple,i]==1):\n",
    "#         all_context_clean = ''\n",
    "#         all_context_dirty = ''\n",
    "#         dirty_row[i] = dirty_cell\n",
    "#         single_context_clean = 'COL %s VAL %s ' % (hospital_dirty.columns[i],clean_cell)\n",
    "#         single_context_dirty = 'COL %s VAL %s ' % (hospital_dirty.columns[i],dirty_cell)\n",
    "#         column_context = 'COL %s VAL %s ' % (hospital_dirty.columns[i],columns_unique)\n",
    "#         for c in range(20):\n",
    "#             all_context_clean += 'COL %s VAL %s ' % (hospital_dirty.columns[c],original_row[c])\n",
    "#             all_context_dirty += 'COL %s VAL %s ' % (hospital_dirty.columns[c],dirty_row[c])\n",
    "#         # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "#         # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "#         # detector_list.append([single_context_clean,0])        \n",
    "#         # detector_list.append([all_context_clean,0])\n",
    "#         if(dirty_cell!=clean_cell):\n",
    "#             row_list.append([all_context_dirty,single_context_dirty,1])\n",
    "#             row_list.append([all_context_clean,single_context_clean,0])\n",
    "#             single_list.append([single_context_dirty,single_context_dirty,1])\n",
    "#             single_list.append([single_context_clean,single_context_clean,0])\n",
    "#             column_list.append([column_context,single_context_dirty,1])\n",
    "#             column_list.append([column_context,single_context_clean,0])\n",
    "#             # detector_list.append([all_context_dirty,single_context_clean,0])\n",
    "#             # detector_list.append([single_context_dirty,1])\n",
    "#         else:\n",
    "#             row_list.append([all_context_dirty,single_context_dirty,0])\n",
    "#             row_list.append([all_context_clean,single_context_clean,0])\n",
    "#             single_list.append([single_context_clean,single_context_clean,0])\n",
    "#             column_list.append([column_context,single_context_clean,0])\n",
    "\n",
    "for index,row in beer_dirty_augment.iterrows():\n",
    "    for x,y in row.items():\n",
    "        if(x in ['ounces', 'abv', 'city', 'state']):\n",
    "            detection = Beer_Row_Detection(x,y) \n",
    "            if not detection: ## No Outlier in Given Rows, inject noise\n",
    "                all_context_clean = ''\n",
    "                all_context_dirty = ''\n",
    "                dirty_row = row.copy()\n",
    "                original_row = row.copy()\n",
    "                if(x in ['ounces']): ## Correct Ounces Error, since it occurs in all values\n",
    "                    clean_cell = correct_dirty_cell(y)\n",
    "                    original_row[x] = clean_cell\n",
    "                else:\n",
    "                    clean_cell = y ## identical value\n",
    "                dirty_cell = Beer_Row_Generation(x,clean_cell)\n",
    "                # if(dirty_cell!='') and (dirty_cell!=clean_cell):\n",
    "                dirty_row[x] = dirty_cell\n",
    "                # columns_unique = np.array(beer_dirty[x].unique())\n",
    "                # columns_unique = list(np.random.choice(columns_unique,min(15,len(columns_unique)))) ## 截断\n",
    "                # insert_randomly(columns_unique,dirty_cell) \n",
    "                single_context_clean = 'COL %s VAL %s ' % (x,clean_cell)\n",
    "                single_context_dirty = 'COL %s VAL %s ' % (x,dirty_cell)\n",
    "                # column_context = 'COL %s VAL %s ' % (x,columns_unique)\n",
    "                for i in range(2,11,1):\n",
    "                    attr = beer_dirty_augment.columns[i]\n",
    "                    all_context_clean += 'COL %s VAL %s ' % (attr,original_row[attr]) ## Clean Result\n",
    "                    all_context_dirty += 'COL %s VAL %s ' % (attr,dirty_row[attr])\n",
    "                # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "                # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "                row_list.append([all_context_dirty,single_context_dirty,1])\n",
    "                row_list.append([all_context_clean,single_context_clean,0])\n",
    "                single_list.append([single_context_dirty,single_context_dirty,1])\n",
    "                single_list.append([single_context_clean,single_context_clean,0])\n",
    "                    # column_list.append([column_context,single_context_dirty,1])\n",
    "                    # column_list.append([column_context,single_context_clean,0])\n",
    "        else:\n",
    "            all_context_clean = ''    \n",
    "            original_row = row.copy()\n",
    "            clean_cell = y\n",
    "            single_context_clean = 'COL %s VAL %s ' % (x,clean_cell)\n",
    "            for i in range(2,11,1):\n",
    "                attr = beer_dirty.columns[i]\n",
    "                all_context_clean += 'COL %s VAL %s ' % (attr,original_row[attr]) ## Clean Result \n",
    "            all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "            row_list.append([all_context_clean,single_context_clean,0])\n",
    "            single_list.append([single_context_clean,single_context_clean,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_list = []\n",
    "single_list = []\n",
    "column_list = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "for label_tuple in hospital_coreset:\n",
    "    cluster_index = hospital_clean.iloc[label_tuple,1]\n",
    "    cluster_set_index = hospital_clean[hospital_clean['ProviderNumber']==cluster_index].index\n",
    "    cluster_set = hospital_dirty.iloc[cluster_set_index]\n",
    "    noise_col_subset = np.random.choice(noise_col,5,replace=False) ## 取5个col作为注入噪声的对象\n",
    "    for i in noise_col_subset:\n",
    "        original_row = hospital_dirty.iloc[label_tuple]\n",
    "        dirty_row = original_row\n",
    "        clean_cell = hospital_dirty.iloc[label_tuple,i]\n",
    "        dirty_cell = replace_random_char_with_x(clean_cell) ## Inject Noise\n",
    "        columns_unique = list(cluster_set.iloc[:,i].unique())\n",
    "        insert_randomly(columns_unique,dirty_cell)\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        dirty_row[i] = dirty_cell\n",
    "        single_context_clean = 'COL %s VAL %s ' % (hospital_dirty.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (hospital_dirty.columns[i],dirty_cell)\n",
    "        column_context = 'COL %s VAL %s ' % (hospital_dirty.columns[i],columns_unique)\n",
    "        for c in range(20):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (hospital_dirty.columns[c],original_row[c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (hospital_dirty.columns[c],dirty_row[c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            row_list.append([all_context_dirty,single_context_dirty,1])\n",
    "            row_list.append([all_context_clean,single_context_clean,0])\n",
    "            single_list.append([single_context_dirty,single_context_dirty,1])\n",
    "            single_list.append([single_context_clean,single_context_clean,0])\n",
    "            column_list.append([column_context,single_context_dirty,1])\n",
    "            column_list.append([column_context,single_context_clean,0])\n",
    "            # detector_list.append([all_context_dirty,single_context_clean,0])\n",
    "            # detector_list.append([single_context_dirty,1])\n",
    "        else:\n",
    "            row_list.append([all_context_dirty,single_context_dirty,0])\n",
    "            row_list.append([all_context_clean,single_context_clean,0])\n",
    "            single_list.append([single_context_clean,single_context_clean,0])\n",
    "            column_list.append([column_context,single_context_clean,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Beer_Row_Detection('ounces','16.0 ounces'),Beer_Row_Detection('city','Washington')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beer_dirty.iloc[0\n",
    "pd.DataFrame(row_list).to_csv('/home/yanmy/raha/raha-master/datasets/beers/detector/multi-view/train_aug.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(row_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 28/2410 [00:00<00:08, 270.32it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2410/2410 [00:09<00:00, 266.37it/s]\n"
     ]
    }
   ],
   "source": [
    "## Detector Inference\n",
    "# input_matrix_select_beer = input_matrix_beer[selected_rows_beer]\n",
    "detector_list_beer = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "# for label_tuple in tqdm(selected_rows):\n",
    "for label_tuple in tqdm(range(len(beer_clean))):\n",
    "    for i in range(2,len(beer_clean.columns),1):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = beer_clean.iloc[label_tuple]\n",
    "        dirty_context = beer_dirty.iloc[label_tuple]\n",
    "        clean_cell = beer_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = beer_dirty.iloc[label_tuple,i]\n",
    "        single_context_clean = 'COL %s VAL %s ' % (beer_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (beer_clean.columns[i],dirty_cell)\n",
    "        for c in range(2,11,1):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (beer_clean.columns[c],beer_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (beer_dirty.columns[c],beer_dirty.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            detector_list_beer.append([all_context_dirty,single_context_dirty,1])\n",
    "        else:\n",
    "            detector_list_beer.append([all_context_dirty,single_context_dirty,0])\n",
    "            # detector_list.append([single_context_dirty,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_beer).to_csv('/home/yanmy/raha/raha-master/datasets/beers/detector/multi-view/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Beer Training List\n",
    "import json\n",
    "noise_col = np.where(detector.sum(axis=0)>0)[0]\n",
    "header = list(hospital_dirty.columns)\n",
    "safe_value = ['empty'] ## 不注入噪声的类型\n",
    "training_list = []\n",
    "for h in tqdm(hospital_cluster): ## 对比学习的采样策略在同cluster内进行\n",
    "    coreset_subset = [n for n in hospital_cluster[h] if n in coreset_detect]\n",
    "    for coreset_tuple in coreset_subset: ## 取遍coreset所有的tuple，剩下的n-1 tuple是参考对象,coreset_tuple是index\n",
    "        noise_col_subset = np.random.choice(noise_col,5,replace=False) ## 取5个col作为注入噪声的对象\n",
    "        for noise_col_single in noise_col_subset:\n",
    "            col_name = header[noise_col_single] ## 从index转成列名\n",
    "            if(hospital_dirty_dict[coreset_tuple][col_name] not in safe_value and len(coreset_subset)>2):\n",
    "                temp_dict = hospital_dirty.iloc[coreset_tuple,1:].to_dict() ## coreset tuple字典\n",
    "                clean_cell = temp_dict[col_name]\n",
    "                dirty_cell = replace_random_char_with_x(clean_cell) ## 注入噪声\n",
    "                temp_dict[col_name] = dirty_cell\n",
    "                coreset_reference = select_two_different_elements(coreset_subset,coreset_tuple)\n",
    "                template_dict = {}\n",
    "                clean_dict = {}\n",
    "                template_dict[col_name] = ''\n",
    "                clean_dict[col_name] = clean_cell\n",
    "                text_head = 'You are an expert in Cleaning Hospital Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\nTake these rows as reference:\\n\\n' % (col_name, json.dumps(template_dict), json.dumps(temp_dict))\n",
    "                \n",
    "                ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]), json.dumps(hospital_dirty_dict[coreset_reference[1]]))\n",
    "                \n",
    "                # ICL_text = '%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]))\n",
    "                \n",
    "                training_list.append([text_head,ICL_text,'',json.dumps(clean_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'article_title', 'article_language', 'journal_title',\n",
       "       'jounral_abbreviation', 'journal_issn', 'article_jvolumn',\n",
       "       'article_jissue', 'article_jcreated_at', 'article_pagination',\n",
       "       'author_list'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 659,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Augmented Training Data in LLM Generation, size controlled into 3000-4000 pairs\n",
    "import json\n",
    "\n",
    "beer_label_index_select = beer_label_index[:15]\n",
    "dirty_example_dict = {}\n",
    "dirty_example_dict_index = {'ounces':4, 'abv':5, 'city':9, 'state':10}\n",
    "for x in ['ounces', 'abv', 'city', 'state']:\n",
    "    dirty_example = []\n",
    "    j = dirty_example_dict_index[x]\n",
    "    for i in beer_label_index_select:\n",
    "        dirty_example.append([beer_dirty.iloc[i,j],beer_clean.iloc[i,j]])\n",
    "    dirty_example_dict[x] = dirty_example\n",
    "    # print(\"The input \\n\\n%s\\n\\nare [dirty,clean] cell pairs\" % str(dirty_example))\n",
    "training_list = []\n",
    "for index,row in beer_dirty_augment.iterrows():\n",
    "    for x,y in row.items():\n",
    "        if(x in ['ounces', 'abv', 'city', 'state']):\n",
    "            detection = Beer_Row_Detection(x,y) \n",
    "            if not detection: ## No Outlier in Given Rows, inject noise\n",
    "                dirty_example = dirty_example_dict[x]\n",
    "                all_context_clean = ''\n",
    "                all_context_dirty = ''\n",
    "                dirty_row = row.copy()\n",
    "                original_row = row.copy()\n",
    "                if(x in ['ounces']): ## Correct Ounces Error, since it occurs in all values\n",
    "                    clean_cell = correct_dirty_cell(y)\n",
    "                    original_row[x] = clean_cell\n",
    "                else:\n",
    "                    clean_cell = y ## identical value\n",
    "                dirty_cell = Beer_Row_Generation(x,clean_cell)\n",
    "                # if(dirty_cell!='') and (dirty_cell!=clean_cell):\n",
    "                dirty_row[x] = dirty_cell\n",
    "                # columns_unique = np.array(beer_dirty[x].unique())\n",
    "                # columns_unique = list(np.random.choice(columns_unique,min(15,len(columns_unique)))) ## 截断\n",
    "                # insert_randomly(columns_unique,dirty_cell) \n",
    "                template_dict = {}\n",
    "                temp_dict = dirty_row[2:].to_dict()\n",
    "                template_dict[x] = ''\n",
    "                coreset_reference = np.random.choice(beer_label_index_select,2,replace=False)\n",
    "                # clean_dict = original_row[2:].to_dict()\n",
    "                clean_dict = {}\n",
    "                clean_dict[x] = clean_cell\n",
    "                Correction_format = \"The input \\n\\n%s\\n\\nare [dirty,clean] cell pairs for value %s\" % (str(dirty_example),x)\n",
    "                text_head = 'You are an expert in Cleaning Beers Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\n%s\\n\\nTake these clean rows as reference:\\n\\n' % (x, json.dumps(template_dict), json.dumps(temp_dict),Correction_format)\n",
    "                ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(beer_clean.iloc[coreset_reference[0],2:].to_dict()), json.dumps(beer_clean.iloc[coreset_reference[1],2:].to_dict()))\n",
    "                \n",
    "                training_list.append([text_head,ICL_text,'',json.dumps(clean_dict)])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Labeling Result\n",
    "import json\n",
    "\n",
    "beer_label_index_select = beer_label_index[:15]\n",
    "dirty_example_dict = {}\n",
    "dirty_example_dict_index = {'ounces':4, 'abv':5, 'city':9, 'state':10}\n",
    "for x in ['ounces', 'abv', 'city', 'state']:\n",
    "    dirty_example = []\n",
    "    j = dirty_example_dict_index[x]\n",
    "    for i in beer_label_index_select:\n",
    "        dirty_example.append([beer_dirty.iloc[i,j],beer_clean.iloc[i,j]])\n",
    "    dirty_example_dict[x] = dirty_example\n",
    "    # print(\"The input \\n\\n%s\\n\\nare [dirty,clean] cell pairs\" % str(dirty_example))\n",
    "training_list_label = []\n",
    "for index,row in beer_dirty.iterrows():\n",
    "    if(index in beer_label_index_select):\n",
    "        for i in range(11):\n",
    "            if(beer_clean.iloc[index,i]!=beer_dirty.iloc[index,i]):\n",
    "                clean_cell = beer_clean.iloc[index,i]\n",
    "                x = beer_clean.columns[i]\n",
    "                dirty_example = dirty_example_dict[x]\n",
    "                # if(dirty_cell!='') and (dirty_cell!=clean_cell):\n",
    "\n",
    "                # columns_unique = np.array(beer_dirty[x].unique())\n",
    "                # columns_unique = list(np.random.choice(columns_unique,min(15,len(columns_unique)))) ## 截断\n",
    "                # insert_randomly(columns_unique,dirty_cell) \n",
    "                template_dict = {}\n",
    "                temp_dict = beer_dirty.iloc[index,2:].to_dict()\n",
    "                template_dict[x] = ''\n",
    "                coreset_reference = np.random.choice(beer_label_index_select,2,replace=False)\n",
    "                # clean_dict = original_row[2:].to_dict()\n",
    "                clean_dict = {}\n",
    "                clean_dict[x] = clean_cell\n",
    "                Correction_format = \"The input \\n\\n%s\\n\\nare [dirty,clean] cell pairs for value %s\" % (str(dirty_example),x)\n",
    "                text_head = 'You are an expert in Cleaning Beers Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\n%s\\n\\nTake these clean rows as reference:\\n\\n' % (x, json.dumps(template_dict), json.dumps(temp_dict),Correction_format)\n",
    "                ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(beer_clean.iloc[coreset_reference[0],2:].to_dict()), json.dumps(beer_clean.iloc[coreset_reference[1],2:].to_dict()))   \n",
    "                training_list_label.append([text_head,ICL_text,'',json.dumps(clean_dict)])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input \n",
      "\n",
      "[['16.0 oz.', '16'], ['12.0 oz', '12'], ['12.0 oz.', '12'], ['16.0 oz.', '16'], ['12.0 oz', '12']]\n",
      "\n",
      "are [dirty,clean] cell pairs\n",
      "The input \n",
      "\n",
      "[['0.047%', '0.047'], ['0.068%', '0.068'], ['0.06%', '0.06'], ['0.057999999999999996%', '0.058'], ['0.06%', '0.06']]\n",
      "\n",
      "are [dirty,clean] cell pairs\n",
      "The input \n",
      "\n",
      "[['Hayward WI', 'Hayward'], ['Nellysford VA', 'Nellysford'], ['Duluth MN', 'Duluth'], ['Plainfield IN', 'Plainfield'], ['Stevens Point WI', 'Stevens Point']]\n",
      "\n",
      "are [dirty,clean] cell pairs\n",
      "The input \n",
      "\n",
      "[['', 'WI'], ['', 'VA'], ['', 'MN'], ['', 'IN'], ['', 'WI']]\n",
      "\n",
      "are [dirty,clean] cell pairs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Testing Result\n",
    "import json\n",
    "\n",
    "beer_label_index_select = beer_label_index[:5]\n",
    "dirty_example_dict = {}\n",
    "dirty_example_dict_index = {'ounces':4, 'abv':5, 'city':9, 'state':10}\n",
    "for x in ['ounces', 'abv', 'city', 'state']:\n",
    "    dirty_example = []\n",
    "    j = dirty_example_dict_index[x]\n",
    "    for i in beer_label_index_select:\n",
    "        dirty_example.append([beer_dirty.iloc[i,j],beer_clean.iloc[i,j]])\n",
    "    dirty_example_dict[x] = dirty_example\n",
    "    print(\"The input \\n\\n%s\\n\\nare [dirty,clean] cell pairs\" % str(dirty_example))\n",
    "training_list_label = []\n",
    "for index,i in np.argwhere(detector_beer==1):\n",
    "    # if(index in beer_label_index_select):\n",
    "        # for i in range(11):\n",
    "                i = i + 2 ## ignore index and id\n",
    "            # if(beer_clean.iloc[index,i]!=beer_dirty.iloc[index,i]):\n",
    "                clean_cell = beer_clean.iloc[index,i]\n",
    "                x = beer_clean.columns[i]\n",
    "                dirty_example = dirty_example_dict[x]\n",
    "                # if(dirty_cell!='') and (dirty_cell!=clean_cell):\n",
    "\n",
    "                # columns_unique = np.array(beer_dirty[x].unique())\n",
    "                # columns_unique = list(np.random.choice(columns_unique,min(15,len(columns_unique)))) ## 截断\n",
    "                # insert_randomly(columns_unique,dirty_cell) \n",
    "                template_dict = {}\n",
    "                temp_dict = beer_dirty.iloc[index,2:].to_dict()\n",
    "                template_dict[x] = ''\n",
    "                coreset_reference = np.random.choice(beer_label_index_select,2,replace=False)\n",
    "                # clean_dict = original_row[2:].to_dict()\n",
    "                clean_dict = {}\n",
    "                clean_dict[x] = clean_cell\n",
    "                Correction_format = \"The input \\n\\n%s\\n\\nare [dirty,clean] cell pairs for value %s\" % (str(dirty_example),x)\n",
    "                text_head = 'You are an expert in Cleaning Beers Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\n%s\\n\\nTake these clean rows as reference:\\n\\n' % (x, json.dumps(template_dict), json.dumps(temp_dict),Correction_format)\n",
    "                ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(beer_clean.iloc[coreset_reference[0],2:].to_dict()), json.dumps(beer_clean.iloc[coreset_reference[1],2:].to_dict()))   \n",
    "                training_list_label.append([text_head,ICL_text,'',json.dumps(clean_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"beer-name\": \"Blonde Hunny\", \"style\": \"Belgia...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"beer-name\": \"Blonde Hunny\", \"style\": \"Belgia...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"beer-name\": \"Insert Hop Reference\", \"style\":...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"beer-name\": \"Bent Paddle Black Ale\", \"style\"...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"beer-name\": \"Bent Paddle Black Ale\", \"style\"...</td>\n",
       "      <td></td>\n",
       "      <td>{\"abv\": \"0.09\"}</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"abv\": \"0.09\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3359</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"beer-name\": \"Bent Paddle Black Ale\", \"style\"...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3360</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"beer-name\": \"Insert Hop Reference\", \"style\":...</td>\n",
       "      <td></td>\n",
       "      <td>{\"abv\": \"0.055\"}</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"abv\": \"0.055\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3361</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"beer-name\": \"JP's Ould Sod Irish Red IPA\", \"...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3362</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"beer-name\": \"Insert Hop Reference\", \"style\":...</td>\n",
       "      <td></td>\n",
       "      <td>{\"abv\": \"0.055\"}</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"abv\": \"0.055\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3363</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"beer-name\": \"Blonde Hunny\", \"style\": \"Belgia...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3364 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0  \\\n",
       "0     You are an expert in Cleaning Beers Dataset. G...   \n",
       "1     You are an expert in Cleaning Beers Dataset. G...   \n",
       "2     You are an expert in Cleaning Beers Dataset. G...   \n",
       "3     You are an expert in Cleaning Beers Dataset. G...   \n",
       "4     You are an expert in Cleaning Beers Dataset. G...   \n",
       "...                                                 ...   \n",
       "3359  You are an expert in Cleaning Beers Dataset. G...   \n",
       "3360  You are an expert in Cleaning Beers Dataset. G...   \n",
       "3361  You are an expert in Cleaning Beers Dataset. G...   \n",
       "3362  You are an expert in Cleaning Beers Dataset. G...   \n",
       "3363  You are an expert in Cleaning Beers Dataset. G...   \n",
       "\n",
       "                                                      1 2                  3  \\\n",
       "0     {\"beer-name\": \"Blonde Hunny\", \"style\": \"Belgia...     {\"ounces\": \"12\"}   \n",
       "1     {\"beer-name\": \"Blonde Hunny\", \"style\": \"Belgia...     {\"ounces\": \"12\"}   \n",
       "2     {\"beer-name\": \"Insert Hop Reference\", \"style\":...     {\"ounces\": \"12\"}   \n",
       "3     {\"beer-name\": \"Bent Paddle Black Ale\", \"style\"...     {\"ounces\": \"12\"}   \n",
       "4     {\"beer-name\": \"Bent Paddle Black Ale\", \"style\"...      {\"abv\": \"0.09\"}   \n",
       "...                                                 ... ..               ...   \n",
       "3359  {\"beer-name\": \"Bent Paddle Black Ale\", \"style\"...     {\"ounces\": \"12\"}   \n",
       "3360  {\"beer-name\": \"Insert Hop Reference\", \"style\":...     {\"abv\": \"0.055\"}   \n",
       "3361  {\"beer-name\": \"JP's Ould Sod Irish Red IPA\", \"...     {\"ounces\": \"12\"}   \n",
       "3362  {\"beer-name\": \"Insert Hop Reference\", \"style\":...     {\"abv\": \"0.055\"}   \n",
       "3363  {\"beer-name\": \"Blonde Hunny\", \"style\": \"Belgia...     {\"ounces\": \"12\"}   \n",
       "\n",
       "                                            instruction input  \\\n",
       "0     You are an expert in Cleaning Beers Dataset. G...         \n",
       "1     You are an expert in Cleaning Beers Dataset. G...         \n",
       "2     You are an expert in Cleaning Beers Dataset. G...         \n",
       "3     You are an expert in Cleaning Beers Dataset. G...         \n",
       "4     You are an expert in Cleaning Beers Dataset. G...         \n",
       "...                                                 ...   ...   \n",
       "3359  You are an expert in Cleaning Beers Dataset. G...         \n",
       "3360  You are an expert in Cleaning Beers Dataset. G...         \n",
       "3361  You are an expert in Cleaning Beers Dataset. G...         \n",
       "3362  You are an expert in Cleaning Beers Dataset. G...         \n",
       "3363  You are an expert in Cleaning Beers Dataset. G...         \n",
       "\n",
       "                output  \n",
       "0     {\"ounces\": \"12\"}  \n",
       "1     {\"ounces\": \"12\"}  \n",
       "2     {\"ounces\": \"12\"}  \n",
       "3     {\"ounces\": \"12\"}  \n",
       "4      {\"abv\": \"0.09\"}  \n",
       "...                ...  \n",
       "3359  {\"ounces\": \"12\"}  \n",
       "3360  {\"abv\": \"0.055\"}  \n",
       "3361  {\"ounces\": \"12\"}  \n",
       "3362  {\"abv\": \"0.055\"}  \n",
       "3363  {\"ounces\": \"12\"}  \n",
       "\n",
       "[3364 rows x 7 columns]"
      ]
     },
     "execution_count": 799,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_list_label_pd = pd.DataFrame(training_list_label)\n",
    "training_list_label_pd['instruction'] = training_list_label_pd[0] + training_list_label_pd[1]\n",
    "training_list_label_pd['input'] = training_list_label_pd[2]\n",
    "training_list_label_pd['output'] = training_list_label_pd[3]\n",
    "training_list_label_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list_pd = pd.DataFrame(training_list).sample(n=(150 * len(beer_label_index_select)))\n",
    "training_list_pd['instruction'] = training_list_pd[0] + training_list_pd[1]\n",
    "training_list_pd['input'] = training_list_pd[2]\n",
    "training_list_pd['output'] = training_list_pd[3]\n",
    "training_list_label_pd = pd.DataFrame(training_list_label)\n",
    "training_list_label_pd['instruction'] = training_list_label_pd[0] + training_list_label_pd[1]\n",
    "training_list_label_pd['input'] = training_list_label_pd[2]\n",
    "training_list_label_pd['output'] = training_list_label_pd[3]\n",
    "training_list_output = pd.concat([training_list_pd,training_list_label_pd])\n",
    "training_list_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(np.concatenate([training_list,training_list_label]))\n",
    "# json.dump(training_list_output.iloc[:,-3:].to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/beer/beer-train-15.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
    "json.dump(training_list_label_pd.iloc[:,-3:].to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/beer/beer-test-5.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1559,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "detector_beer = np.load('datasets/beers/detector/multi-view/detection_cell_5.npy').reshape((-1,9))\n",
    "## Only the labelled attributes is dirty\n",
    "# detector_beer.sum(axis=0)\n",
    "detector_beer[:,0] = 0\n",
    "detector_beer[:,1] = 0\n",
    "detector_beer[:,4] = 0\n",
    "detector_beer[:,5] = 0\n",
    "detector_beer[:,6] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3364"
      ]
     },
     "execution_count": 793,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(detector_beer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4259489363.py, line 184)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[812], line 184\u001b[0;36m\u001b[0m\n\u001b[0;31m    if(cell!='')\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### Try Rayyan Dataset\n",
    "def Rayyan_Detect_atitle(cell):\n",
    "    # Regular expression to detect if the cell contains special characters or combining diacritical marks\n",
    "    pattern = re.compile(r'[�\\u0300-\\u036F]')\n",
    "    return bool(pattern.search(cell))\n",
    "def Rayyan_Generate_atitle(cell):\n",
    "    # List of special characters and combining diacritical marks\n",
    "    special_chars = ['�', '\\u0301', '\\u0300', '\\u0302', '\\u0303', '\\u0304']\n",
    "    \n",
    "    # Randomly choose a special character\n",
    "    char = random.choice(special_chars)\n",
    "    \n",
    "    # Randomly choose a position to insert the special character\n",
    "    position = random.randint(0, len(cell))\n",
    "    \n",
    "    # Insert the special character at the chosen position\n",
    "    dirty_cell = cell[:position] + char + cell[position:]\n",
    "    \n",
    "    return dirty_cell\n",
    "def Rayyan_Clean_atitle(cell):\n",
    "    # Remove special characters like �\n",
    "    cleaned = re.sub(r'�', '', cell)\n",
    "    \n",
    "    # Remove combining diacritical marks (from Unicode range U+0300 to U+036F)\n",
    "    cleaned = re.sub(r'[\\u0300-\\u036F]', '', cleaned)\n",
    "    \n",
    "    return cleaned\n",
    "def Rayyan_Detect_jtitle(cell):\n",
    "    # Regular expression to detect if the cell contains special characters or combining diacritical marks\n",
    "    pattern = re.compile(r'[�\\u0300-\\u036F]')\n",
    "    return bool(pattern.search(cell))\n",
    "def Rayyan_Generate_jtitle(cell):\n",
    "    # List of special characters and combining diacritical marks\n",
    "    special_chars = ['�', '\\u0301', '\\u0300', '\\u0302', '\\u0303', '\\u0304']\n",
    "    \n",
    "    # Randomly choose a special character\n",
    "    char = random.choice(special_chars)\n",
    "    \n",
    "    # Randomly choose a position to insert the special character\n",
    "    position = random.randint(0, len(cell))\n",
    "    \n",
    "    # Insert the special character at the chosen position\n",
    "    dirty_cell = cell[:position] + char + cell[position:]\n",
    "    \n",
    "    return dirty_cell\n",
    "def Rayyan_Clean_jtitle(cell):\n",
    "    # Remove special characters like �\n",
    "    cleaned = re.sub(r'�', '', cell)\n",
    "    \n",
    "    # Remove combining diacritical marks (from Unicode range U+0300 to U+036F)\n",
    "    cleaned = re.sub(r'[\\u0300-\\u036F]', '', cleaned)\n",
    "    \n",
    "    return cleaned\n",
    "def Rayyan_Detect_author(cell):\n",
    "    # Regular expression to detect if the cell contains special characters or combining diacritical marks\n",
    "    pattern = re.compile(r'[�\\u0300-\\u036F]')\n",
    "    return bool(pattern.search(cell))\n",
    "def Rayyan_Generate_author(cell):\n",
    "    # List of special characters and combining diacritical marks\n",
    "    special_chars = ['�', '\\u0301', '\\u0300', '\\u0302', '\\u0303', '\\u0304']\n",
    "    \n",
    "    # Randomly choose a special character\n",
    "    char = random.choice(special_chars)\n",
    "    \n",
    "    # Randomly choose a position to insert the special character\n",
    "    position = random.randint(0, len(cell))\n",
    "    \n",
    "    # Insert the special character at the chosen position\n",
    "    dirty_cell = cell[:position] + char + cell[position:]\n",
    "    \n",
    "    return dirty_cell\n",
    "def Rayyan_Detect_issn(cell):\n",
    "    # Regular expression to detect if the cell is in the format Mon-DD\n",
    "    date_pattern = re.compile(r'^[A-Za-z]{3}-\\d{1,2}$')\n",
    "    \n",
    "    # Regular expression to detect if the ISSN starts with a number other than 9\n",
    "    issn_pattern = re.compile(r'^[^9]\\d{12}$')\n",
    "    \n",
    "    return bool(date_pattern.match(cell) or issn_pattern.match(cell))\n",
    "def Rayyan_Correct_issn(cell):\n",
    "    date_match = re.match(r'([A-Za-z]{3})-(\\d{1,2})', cell)\n",
    "    if date_match:\n",
    "        return f\"{date_match.group(2)}-{date_match.group(1)}\"\n",
    "    \n",
    "    # If the cell matches the ISSN format not starting with '9', replace the leading digit with '9'\n",
    "    issn_match = re.match(r'^[^9]\\d{12}$', cell)\n",
    "    if issn_match:\n",
    "        return f\"9{cell[1:]}\"\n",
    "    \n",
    "    return cell  # If no patterns match, return the original cel\n",
    "def Rayyan_Generate_issn(cell):\n",
    "    # If the cell matches the format DD-Mon, reverse it to Mon-DD\n",
    "    date_match = re.match(r'(\\d{1,2})-([A-Za-z]{3})', cell)\n",
    "    if date_match:\n",
    "        return f\"{date_match.group(2)}-{date_match.group(1)}\"\n",
    "    \n",
    "    # If the cell matches the ISSN format starting with '9', change the leading '9' to another number\n",
    "    issn_match = re.match(r'^9\\d{12}$', cell)\n",
    "    if issn_match:\n",
    "        return f\"{random.choice(['0', '1', '2', '3', '4', '5', '6', '7', '8'])}{cell[1:]}\"\n",
    "    \n",
    "    return cell\n",
    "def Rayyan_Detect_jissue(cell):\n",
    "    pattern = re.compile(r'^\\s*$')\n",
    "    return bool(pattern.match(cell))\n",
    "def Rayyan_Generate_jissue(cell):\n",
    "    return \"\"\n",
    "def Rayyan_Correct_jissue(cell):\n",
    "    return \"-1\"\n",
    "def Rayyan_Detect_pagination(cell):\n",
    "    clean_pattern = re.compile(r'^(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)-\\d{2}$')\n",
    "    \n",
    "    # Dirty pattern\n",
    "    dirty_pattern1 = re.compile(r'^\\d{2}-(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)$')\n",
    "    dirty_pattern2 = re.compile(r'^\\d{2}-\\d$')\n",
    "    \n",
    "    if clean_pattern.match(cell):\n",
    "        return False\n",
    "    elif dirty_pattern1.match(cell) or dirty_pattern2.match(cell):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def Rayyan_Correct_pagination(cell):\n",
    "    clean_pattern = re.compile(r'^(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)-\\d{2}$')\n",
    "    \n",
    "    # Dirty pattern\n",
    "    dirty_pattern1 = re.compile(r'^(\\d{2})-(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)$')\n",
    "    dirty_pattern2 = re.compile(r'^(\\d{2})-(\\d)$')\n",
    "    \n",
    "    # If the cell matches the clean pattern, return it as is\n",
    "    if clean_pattern.match(cell):\n",
    "        return cell\n",
    "    \n",
    "    # If the cell matches the first dirty pattern, reverse month and year\n",
    "    match = dirty_pattern1.match(cell)\n",
    "    if match:\n",
    "        return f\"{match.group(2)}-{match.group(1)}\"\n",
    "    \n",
    "    # If the cell matches the second dirty pattern, reverse month and year\n",
    "    match = dirty_pattern2.match(cell)\n",
    "    if match:\n",
    "        month_map = {\n",
    "            '1': 'Jan', '2': 'Feb', '3': 'Mar', '4': 'Apr', '5': 'May', '6': 'Jun',\n",
    "            '7': 'Jul', '8': 'Aug', '9': 'Sep', '10': 'Oct', '11': 'Nov', '12': 'Dec'\n",
    "        }\n",
    "        return f\"{month_map[match.group(2)]}-{match.group(1)}\"\n",
    "    \n",
    "    # If the cell doesn't match any pattern, return it as is\n",
    "    return cell\n",
    "def Rayyan_Generate_pagination(cell):\n",
    "    # Extract month and year from the clean format\n",
    "    match = re.match(r'([A-Za-z]{3})-(\\d{2})', cell)\n",
    "    if not match:\n",
    "        return cell  # Return the original cell if it doesn't match the clean format\n",
    "    \n",
    "    month, year = match.groups()\n",
    "    \n",
    "    # Convert month to its corresponding number\n",
    "    month_to_num = {\n",
    "        'Jan': '1', 'Feb': '2', 'Mar': '3', 'Apr': '4', 'May': '5', 'Jun': '6',\n",
    "        'Jul': '7', 'Aug': '8', 'Sep': '9', 'Oct': '10', 'Nov': '11', 'Dec': '12'\n",
    "    }\n",
    "    month_num = month_to_num.get(month, '')\n",
    "    \n",
    "    # Randomly decide to add extra numbers or not\n",
    "    if random.choice([True, False]):\n",
    "        year = str(random.randint(1000, 9999)) + year\n",
    "    \n",
    "    # Return the dirty format\n",
    "    return f\"{year}-{month_num}\"\n",
    "def Rayyan_Detect_jcreate(cell):\n",
    "    try:\n",
    "        YY,MM,DD = cell.split('/')\n",
    "        YY = int(YY)\n",
    "        MM = int(MM)\n",
    "        DD = int(DD)\n",
    "        if (1 <= MM <= 12) and (1 <= DD <= 31):\n",
    "            return True ## Satisfied Dirty Pattern, maybe all value qualified dirty pattern\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return False \n",
    "def Rayyan_Correct_jcreate(cell): ## Dirty -> Clean\n",
    "    if(cell!=''):\n",
    "        YY,MM,DD = cell.split('/')\n",
    "        YY = int(YY)\n",
    "        MM = int(MM)\n",
    "        DD = int(DD)\n",
    "        YY = \"{:02}\".format(YY)\n",
    "        MM = str(MM)\n",
    "        DD = str(DD)\n",
    "        return '%s/%s/%s' % (MM,DD,YY)\n",
    "    else:\n",
    "        return ''\n",
    "def Rayyan_Generate_jcreate(cell): ## Clean -> Dirty\n",
    "    MM,DD,YY = cell.split('/')\n",
    "    YY = int(YY)\n",
    "    MM = int(MM)\n",
    "    DD = int(DD)\n",
    "    DD = \"{:02}\".format(DD)\n",
    "    MM = str(MM)\n",
    "    YY = str(YY)\n",
    "    return '%s/%s/%s' % (YY,MM,DD)\n",
    "def Rayyan_Row_Detect(x,cell):\n",
    "    if(x=='article_title'):\n",
    "        return Rayyan_Detect_atitle(cell)\n",
    "    elif(x=='journal_title'):\n",
    "        return Rayyan_Detect_jtitle(cell)\n",
    "    elif(x=='journal_issn'):\n",
    "        return Rayyan_Detect_issn(cell)\n",
    "    elif(x=='article_jvolumn') or (x=='article_jissue'): ## Modify when label budget shrink\n",
    "        return Rayyan_Detect_jissue(cell)\n",
    "    elif(x=='article_jcreated_at'):\n",
    "        return Rayyan_Detect_jcreate(cell)\n",
    "    elif(x=='article_pagination'):\n",
    "        return Rayyan_Detect_pagination(cell)\n",
    "    elif(x=='author_list'):\n",
    "        return Rayyan_Detect_author(cell)\n",
    "    else:\n",
    "        return False\n",
    "# Rayyan_Row_Detect('article_jissue','')\n",
    "def Rayyan_Row_Generate(x,cell): ## Input should be detected to clean, except jcreate_at\n",
    "    if(x=='article_title'):\n",
    "        return Rayyan_Generate_atitle(cell)\n",
    "    elif(x=='journal_title'):\n",
    "        return Rayyan_Generate_jtitle(cell)\n",
    "    elif(x=='journal_issn'):\n",
    "        return Rayyan_Generate_issn(cell)\n",
    "    elif(x=='article_jvolumn') or (x=='article_jissue'): ## Modify when label budget shrink\n",
    "        return Rayyan_Generate_jissue(cell)\n",
    "    elif(x=='article_jcreated_at'):\n",
    "        return Rayyan_Generate_jcreate(cell) ## Input Should Be Clean\n",
    "    elif(x=='article_pagination'):\n",
    "        return Rayyan_Generate_pagination(cell)\n",
    "    elif(x=='author_list'):\n",
    "        return Rayyan_Generate_author(cell)\n",
    "    else:\n",
    "        return False\n",
    "def Rayyan_Row_Correction(x,cell): ## Input should be detected to clean, except jcreate_at\n",
    "    if(x=='journal_issn'):\n",
    "        return Rayyan_Correct_issn(cell)\n",
    "    elif(x=='article_jvolumn') or (x=='article_jissue'): ## Modify when label budget shrink\n",
    "        return Rayyan_Correct_jissue(cell)\n",
    "    elif(x=='article_jcreated_at'):\n",
    "        return Rayyan_Correct_jcreate(cell) ## Input Should Be Clean\n",
    "    elif(x=='article_pagination'):\n",
    "        return Rayyan_Correct_pagination(cell)\n",
    "    elif(x in ['article_title','journal_title','author_list']):\n",
    "        return Rayyan_Clean_atitle(cell)\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 14)"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = -10\n",
    "count = 0\n",
    "for c in rayyan_dirty[rayyan_dirty.iloc[:,i]!=rayyan_clean.iloc[:,i]].iloc[:,i].to_list():\n",
    "    # print(c,Rayyan_Row_Detect(rayyan_dirty.columns[i],c))\n",
    "    if(Rayyan_Row_Detect(rayyan_dirty.columns[i],c)):\n",
    "        count += 1\n",
    "    # else:\n",
    "        # print(c)\n",
    "count,len(rayyan_dirty[rayyan_dirty.iloc[:,i]!=rayyan_clean.iloc[:,i]].iloc[:,i].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,  14,   0,   9,   0,  12,  22,  53, 722,  32,  84])"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix_rayyan = np.array(rayyan_clean!=rayyan_dirty).astype(int)\n",
    "input_matrix_rayyan.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['id', 'article_title', 'article_language', 'journal_title',\n",
    "       'jounral_abbreviation', 'journal_issn', 'article_jvolumn',\n",
    "       'article_jissue', 'article_jcreated_at', 'article_pagination',\n",
    "       'author_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['article_title', 'journal_title', 'journal_issn', 'article_jvolumn',\n",
       "       'article_jissue', 'article_jcreated_at', 'article_pagination',\n",
       "       'author_list'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_clean.columns[[1,3,5,6,7,8,9,10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rayyan_pagination: Pagination detect then correct as wrong-clean pairs\n",
    "Rayyan_issn: ISSN detect then correct as wrong-clean pairs\n",
    "Rayyan_jcreate: jcreate detect then correct as wrong-clean pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "rayyan_clean = pd.read_csv('datasets/rayyan/clean.csv').fillna('')\n",
    "rayyan_dirty = pd.read_csv('datasets/rayyan/dirty.csv').fillna('')\n",
    "def Str2Int(row):\n",
    "    for index in range(11):\n",
    "        temp = row[index]\n",
    "        try:\n",
    "            row[index] = str(int(temp))\n",
    "        except:\n",
    "            continue\n",
    "    return row\n",
    "rayyan_clean = rayyan_clean.apply(Str2Int,axis=1)\n",
    "rayyan_dirty = rayyan_dirty.apply(Str2Int,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "rayyan_dirty_augment = rayyan_dirty\n",
    "# rayyan_dirty_augment['article_jcreated_at'] = rayyan_dirty_augment['article_jcreated_at'].apply(Rayyan_Correct_jcreate,axis=1)\n",
    "for r in range(len(rayyan_dirty_augment)):\n",
    "    if(rayyan_dirty.iloc[r,8]!=''):\n",
    "        if(Rayyan_Detect_jcreate(rayyan_dirty.iloc[r,8])):\n",
    "            rayyan_dirty_augment.iloc[r,8] = Rayyan_Correct_jcreate(rayyan_dirty.iloc[r,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rayyan_Augmented_Data\n",
    "rayyan_clean = pd.read_csv('datasets/rayyan/clean.csv').fillna('')\n",
    "rayyan_dirty = pd.read_csv('datasets/rayyan/dirty.csv').fillna('')\n",
    "def Str2Int(row):\n",
    "    for index in range(11):\n",
    "        temp = row[index]\n",
    "        try:\n",
    "            row[index] = str(int(temp))\n",
    "        except:\n",
    "            continue\n",
    "    return row\n",
    "rayyan_clean = rayyan_clean.apply(Str2Int,axis=1)\n",
    "rayyan_dirty = rayyan_dirty.apply(Str2Int,axis=1)\n",
    "row_list = []\n",
    "single_list = []\n",
    "column_list = []\n",
    "rayyan_dirty_augment = rayyan_dirty.copy()\n",
    "\n",
    "for index,row in rayyan_dirty_augment.iterrows():\n",
    "    for x,y in row[1:].items():\n",
    "        if(x in ['article_title', 'journal_title', 'article_jvolumn',\n",
    "       'article_jissue',\n",
    "       'author_list']):\n",
    "            detection = Rayyan_Row_Detect(x,y) \n",
    "            \n",
    "            all_context_clean = ''\n",
    "            all_context_dirty = ''\n",
    "            dirty_row = row.copy()\n",
    "            original_row = row.copy()\n",
    "            # if(x in ['article_jcreated_at']): ## Correct Ounces Error, since it occurs in all values\n",
    "            #     clean_cell = correct_dirty_cell(y)\n",
    "            #     original_row[x] = clean_cell\n",
    "            # else:\n",
    "            clean_cell = y ## identical value\n",
    "            dirty_cell = Rayyan_Row_Generate(x,clean_cell)\n",
    "            # if(dirty_cell!='') and (dirty_cell!=clean_cell):\n",
    "            dirty_row[x] = dirty_cell\n",
    "            # columns_unique = np.array(beer_dirty[x].unique())\n",
    "            # columns_unique = list(np.random.choice(columns_unique,min(15,len(columns_unique)))) ## 截断\n",
    "            # insert_randomly(columns_unique,dirty_cell) \n",
    "            single_context_clean = 'COL %s VAL %s ' % (x,clean_cell)\n",
    "            single_context_dirty = 'COL %s VAL %s ' % (x,dirty_cell)\n",
    "            # column_context = 'COL %s VAL %s ' % (x,columns_unique)\n",
    "            for i in range(1,11,1):\n",
    "                attr = rayyan_dirty_augment.columns[i]\n",
    "                all_context_clean += 'COL %s VAL %s ' % (attr,original_row[attr]) ## Clean Result\n",
    "                all_context_dirty += 'COL %s VAL %s ' % (attr,dirty_row[attr])\n",
    "            # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "            # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "            if (not detection) and (single_context_dirty!=single_context_clean) : ## No Outlier in Given Rows, inject noise\n",
    "                row_list.append([all_context_dirty,single_context_dirty,1])\n",
    "                row_list.append([all_context_clean,single_context_clean,0])\n",
    "                single_list.append([single_context_dirty,single_context_dirty,1])\n",
    "                single_list.append([single_context_clean,single_context_clean,0])\n",
    "            else:\n",
    "                row_list.append([all_context_clean,single_context_clean,1])\n",
    "                single_list.append([single_context_clean,single_context_clean,1])\n",
    "                    # column_list.append([column_context,single_context_dirty,1])\n",
    "                    # column_list.append([column_context,single_context_clean,0])\n",
    "        elif(x in ['article_jcreated_at'] and y!=''):\n",
    "            detection = Rayyan_Row_Detect(x,y) \n",
    "            \n",
    "            all_context_clean = ''\n",
    "            all_context_dirty = ''\n",
    "            dirty_row = row.copy()\n",
    "            original_row = row.copy()\n",
    "            # if(x in ['article_jcreated_at']): ## Correct Ounces Error, since it occurs in all values\n",
    "            #     clean_cell = correct_dirty_cell(y)\n",
    "            #     original_row[x] = clean_cell\n",
    "            # else:\n",
    "            clean_cell = Rayyan_Correct_jcreate(y) ## identical value\n",
    "            dirty_cell = y\n",
    "            # if(dirty_cell!='') and (dirty_cell!=clean_cell):\n",
    "            dirty_row[x] = dirty_cell\n",
    "            original_row[x] = clean_cell\n",
    "            # columns_unique = np.array(beer_dirty[x].unique())\n",
    "            # columns_unique = list(np.random.choice(columns_unique,min(15,len(columns_unique)))) ## 截断\n",
    "            # insert_randomly(columns_unique,dirty_cell) \n",
    "            single_context_clean = 'COL %s VAL %s ' % (x,clean_cell)\n",
    "            single_context_dirty = 'COL %s VAL %s ' % (x,dirty_cell)\n",
    "            # column_context = 'COL %s VAL %s ' % (x,columns_unique)\n",
    "            for i in range(1,11,1):\n",
    "                attr = rayyan_dirty_augment.columns[i]\n",
    "                all_context_clean += 'COL %s VAL %s ' % (attr,original_row[attr]) ## Clean Result\n",
    "                all_context_dirty += 'COL %s VAL %s ' % (attr,dirty_row[attr])\n",
    "            # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "            # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "            if detection:\n",
    "                row_list.append([all_context_dirty,single_context_dirty,1])\n",
    "                row_list.append([all_context_clean,single_context_clean,0])\n",
    "                single_list.append([single_context_dirty,single_context_dirty,1])\n",
    "                single_list.append([single_context_clean,single_context_clean,0])                \n",
    "            else:\n",
    "                row_list.append([all_context_clean,single_context_clean,0])\n",
    "                single_list.append([single_context_clean,single_context_clean,0])\n",
    "        elif(x in ['article_pagination']):\n",
    "            detection = Rayyan_Row_Detect(x,y) \n",
    "            \n",
    "            all_context_clean = ''\n",
    "            all_context_dirty = ''\n",
    "            dirty_row = row.copy()\n",
    "            original_row = row.copy()\n",
    "            # if(x in ['article_jcreated_at']): ## Correct Ounces Error, since it occurs in all values\n",
    "            #     clean_cell = correct_dirty_cell(y)\n",
    "            #     original_row[x] = clean_cell\n",
    "            # else:\n",
    "            clean_cell = Rayyan_Correct_pagination(y) ## identical value\n",
    "            dirty_cell = y\n",
    "            # if(dirty_cell!='') and (dirty_cell!=clean_cell):\n",
    "            dirty_row[x] = dirty_cell\n",
    "            original_row[x] = clean_cell\n",
    "            # columns_unique = np.array(beer_dirty[x].unique())\n",
    "            # columns_unique = list(np.random.choice(columns_unique,min(15,len(columns_unique)))) ## 截断\n",
    "            # insert_randomly(columns_unique,dirty_cell) \n",
    "            single_context_clean = 'COL %s VAL %s ' % (x,clean_cell)\n",
    "            single_context_dirty = 'COL %s VAL %s ' % (x,dirty_cell)\n",
    "            # column_context = 'COL %s VAL %s ' % (x,columns_unique)\n",
    "            for i in range(1,11,1):\n",
    "                attr = rayyan_dirty_augment.columns[i]\n",
    "                all_context_clean += 'COL %s VAL %s ' % (attr,original_row[attr]) ## Clean Result\n",
    "                all_context_dirty += 'COL %s VAL %s ' % (attr,dirty_row[attr])\n",
    "            # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "            # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "            if detection and (single_context_dirty!=single_context_clean):\n",
    "                print(single_context_dirty)\n",
    "                row_list.append([all_context_dirty,single_context_dirty,1])\n",
    "                row_list.append([all_context_clean,single_context_clean,0])\n",
    "                single_list.append([single_context_dirty,single_context_dirty,1])\n",
    "                single_list.append([single_context_clean,single_context_clean,0])                \n",
    "            else:\n",
    "                row_list.append([all_context_clean,single_context_clean,0])\n",
    "                single_list.append([single_context_clean,single_context_clean,0])\n",
    "        elif(x in ['journal_issn']):\n",
    "            detection = Rayyan_Row_Detect(x,y) \n",
    "            \n",
    "            all_context_clean = ''\n",
    "            all_context_dirty = ''\n",
    "            dirty_row = row.copy()\n",
    "            original_row = row.copy()\n",
    "            # if(x in ['article_jcreated_at']): ## Correct Ounces Error, since it occurs in all values\n",
    "            #     clean_cell = correct_dirty_cell(y)\n",
    "            #     original_row[x] = clean_cell\n",
    "            # else:\n",
    "            clean_cell = Rayyan_Correct_issn(y) ## identical value\n",
    "            dirty_cell = y\n",
    "            # if(dirty_cell!='') and (dirty_cell!=clean_cell):\n",
    "            dirty_row[x] = dirty_cell\n",
    "            original_row[x] = clean_cell\n",
    "            # columns_unique = np.array(beer_dirty[x].unique())\n",
    "            # columns_unique = list(np.random.choice(columns_unique,min(15,len(columns_unique)))) ## 截断\n",
    "            # insert_randomly(columns_unique,dirty_cell) \n",
    "            single_context_clean = 'COL %s VAL %s ' % (x,clean_cell)\n",
    "            single_context_dirty = 'COL %s VAL %s ' % (x,dirty_cell)\n",
    "            # column_context = 'COL %s VAL %s ' % (x,columns_unique)\n",
    "            for i in range(1,11,1):\n",
    "                attr = rayyan_dirty_augment.columns[i]\n",
    "                all_context_clean += 'COL %s VAL %s ' % (attr,original_row[attr]) ## Clean Result\n",
    "                all_context_dirty += 'COL %s VAL %s ' % (attr,dirty_row[attr])\n",
    "            # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "            # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "            if detection:\n",
    "                print(single_context_dirty)\n",
    "                row_list.append([all_context_dirty,single_context_dirty,1])\n",
    "                row_list.append([all_context_clean,single_context_clean,0])\n",
    "                single_list.append([single_context_dirty,single_context_dirty,1])\n",
    "                single_list.append([single_context_clean,single_context_clean,0])                \n",
    "            else:\n",
    "                row_list.append([all_context_clean,single_context_clean,0])\n",
    "                single_list.append([single_context_clean,single_context_clean,0])\n",
    "        else:\n",
    "            all_context_clean = ''    \n",
    "            original_row = row.copy()\n",
    "            clean_cell = y\n",
    "            single_context_clean = 'COL %s VAL %s ' % (x,clean_cell)\n",
    "            for i in range(1,11,1):\n",
    "                attr = rayyan_dirty.columns[i]\n",
    "                all_context_clean += 'COL %s VAL %s ' % (attr,original_row[attr]) ## Clean Result \n",
    "            # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "            row_list.append([all_context_clean,single_context_clean,0])\n",
    "            single_list.append([single_context_clean,single_context_clean,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'article_title', 'article_language', 'journal_title',\n",
       "       'jounral_abbreviation', 'journal_issn', 'article_jvolumn',\n",
       "       'article_jissue', 'article_jcreated_at', 'article_pagination',\n",
       "       'author_list'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'article_title', 'article_language', 'journal_title',\n",
       "       'jounral_abbreviation', 'journal_issn', 'article_jvolumn',\n",
       "       'article_jissue', 'article_jcreated_at', 'article_pagination',\n",
       "       'author_list'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rayyan_Row_Detect('journal_issn','1551-1553')\n",
    "rayyan_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rayyan_Detect_pagination('835-40')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(row_list).to_csv('datasets/rayyan/detector/multi-view/train_aug.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(single_list).to_csv('datasets/rayyan/detector/multi-view/train_aug_single.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([158, 455,  85, 118, 322, 384, 392, 615, 656, 796, 862, 975, 979,\n",
       "       357, 130, 532, 212, 694, 924, 918])"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_label_index = np.load('datasets/rayyan/detector/index.npy')\n",
    "rayyan_label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71ccf007f1c414c9c43cc5ebae34cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Rayyan_Augmented_Data\n",
    "rayyan_clean = pd.read_csv('datasets/rayyan/clean.csv').fillna('')\n",
    "rayyan_dirty = pd.read_csv('datasets/rayyan/dirty.csv').fillna('')\n",
    "def Str2Int(row):\n",
    "    for index in range(11):\n",
    "        temp = row[index]\n",
    "        try:\n",
    "            row[index] = str(int(temp))\n",
    "        except:\n",
    "            continue\n",
    "    return row\n",
    "rayyan_clean = rayyan_clean.apply(Str2Int,axis=1)\n",
    "rayyan_dirty = rayyan_dirty.apply(Str2Int,axis=1)\n",
    "row_list = []\n",
    "single_list = []\n",
    "column_list = []\n",
    "rayyan_dirty_augment = rayyan_dirty.copy()\n",
    "detector_list_rayyan = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "# for label_tuple in tqdm(rayyan_label_index[:20]):\n",
    "for label_tuple in tqdm(range(len(rayyan_dirty))):\n",
    "    for i in range(1,len(rayyan_dirty.columns),1):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = rayyan_clean.iloc[label_tuple]\n",
    "        dirty_context = rayyan_dirty.iloc[label_tuple]\n",
    "        clean_cell = rayyan_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = rayyan_dirty.iloc[label_tuple,i]\n",
    "        single_context_clean = 'COL %s VAL %s ' % (rayyan_dirty.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (rayyan_dirty.columns[i],dirty_cell)\n",
    "        for c in range(1,11,1):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (rayyan_clean.columns[c],rayyan_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (rayyan_dirty.columns[c],rayyan_dirty.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            detector_list_rayyan.append([all_context_dirty,single_context_dirty,1])\n",
    "            # detector_list_rayyan.append([all_context_clean,single_context_clean,0])\n",
    "        else:\n",
    "            # detector_list_rayyan.append([all_context_clean,single_context_clean,0])\n",
    "            detector_list_rayyan.append([all_context_dirty,single_context_dirty,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_rayyan).to_csv('datasets/rayyan/detector/multi-view/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_rayyan).to_csv('datasets/rayyan/detector/multi-view/train_20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'article_title', 'article_language', 'journal_title',\n",
       "       'jounral_abbreviation', 'journal_issn', 'article_jvolumn',\n",
       "       'article_jissue', 'article_jcreated_at', 'article_pagination',\n",
       "       'author_list'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_dirty.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rayyan_Row_Augment(row,x): ## if cannot make augmentation, return [False,'',''], if we can make augmentation, return [True,dirty_row,dirty_cell,clean_cell], 'article_jvolumn','article_jissue' do not envolve in augmentation\n",
    "    original_row = row.copy()\n",
    "    dirty_row = row.copy()\n",
    "    y = original_row[x]\n",
    "    if(x in ['article_title', 'journal_title', \n",
    "'author_list']):\n",
    "        detection = Rayyan_Row_Detect(x,y)\n",
    "        clean_cell = y\n",
    "        dirty_cell = Rayyan_Row_Generate(x,clean_cell)\n",
    "        dirty_row[x] = dirty_cell\n",
    "        if (not detection) and(clean_cell!=dirty_cell) : ## Valid Augmentation\n",
    "            return [True,dirty_row,dirty_cell,clean_cell]\n",
    "        else:\n",
    "            return [False,'','','']\n",
    "    elif(x in ['article_jcreated_at','article_pagination','journal_issn'] and y!=''):\n",
    "        detection = Rayyan_Row_Detect(x,y)\n",
    "        clean_cell = Rayyan_Row_Correction(x,y)\n",
    "        dirty_cell = y\n",
    "        dirty_row[x] = dirty_cell\n",
    "        original_row[x] = clean_cell\n",
    "        if(detection):\n",
    "            return [True,dirty_row,dirty_cell,clean_cell]\n",
    "        else:\n",
    "            return [False,'','','']\n",
    "    elif(x in ['article_jvolumn','article_jissue']):\n",
    "        detection = Rayyan_Row_Detect(x,y)\n",
    "        clean_cell = Rayyan_Row_Correction(x,y)\n",
    "        dirty_cell = y\n",
    "        dirty_row[x] = dirty_cell\n",
    "        original_row[x] = clean_cell\n",
    "        if(detection):\n",
    "            return [True,dirty_row,dirty_cell,clean_cell]\n",
    "        else:\n",
    "            return [False,'','','']\n",
    "    else:\n",
    "        return [False,'','','']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Augmented Training Data in LLM Generation, size controlled into 3000-4000 pairs\n",
    "import json\n",
    "rayyan_clean = pd.read_csv('datasets/rayyan/clean.csv').fillna('')\n",
    "rayyan_dirty = pd.read_csv('datasets/rayyan/dirty.csv').fillna('')\n",
    "def Str2Int(row):\n",
    "    for index in range(11):\n",
    "        temp = row[index]\n",
    "        try:\n",
    "            row[index] = str(int(temp))\n",
    "        except:\n",
    "            continue\n",
    "    return row\n",
    "rayyan_clean = rayyan_clean.apply(Str2Int,axis=1)\n",
    "rayyan_dirty = rayyan_dirty.apply(Str2Int,axis=1)\n",
    "row_list = []\n",
    "single_list = []\n",
    "column_list = []\n",
    "rayyan_dirty_augment = rayyan_dirty.copy()\n",
    "detector_list_rayyan = []\n",
    "rayyan_label_index_select = rayyan_label_index[:5]\n",
    "dirty_example_dict = {}\n",
    "dirty_example_dict_index = {'article_title':1, 'journal_title':3, 'journal_issn':5, 'article_jvolumn':6,'article_jissue':7,'article_jcreated_at':8,'article_pagination':9,'author_list':10}\n",
    "for x in dirty_example_dict_index.keys():\n",
    "    dirty_example = []\n",
    "    j = dirty_example_dict_index[x]\n",
    "    for i in rayyan_label_index_select:\n",
    "        if rayyan_dirty.iloc[i,j]!=rayyan_clean.iloc[i,j]:\n",
    "            dirty_example.append([rayyan_dirty.iloc[i,j],rayyan_clean.iloc[i,j]]) \n",
    "    dirty_example_dict[x] = dirty_example\n",
    "    # print(\"The input \\n\\n%s\\n\\nare [dirty,clean] cell pairs\" % str(dirty_example))\n",
    "training_list = []\n",
    "for index,row in rayyan_dirty_augment.iterrows():\n",
    "    row_input = row.copy()\n",
    "\n",
    "    for x,y in row.items():\n",
    "        Indicator,row_output,dirty_cell,clean_cell = Rayyan_Row_Augment(row_input,x)\n",
    "        if(Indicator):\n",
    "                dirty_example = dirty_example_dict[x]\n",
    "                if(x in ['article_title','journal_title','author_list']):\n",
    "                    dirty_example = random.sample(dirty_example,min(2,len(dirty_example)))\n",
    "                else:\n",
    "                    dirty_example = random.sample(dirty_example,min(5,len(dirty_example)))\n",
    "                dirty_row = row_output.copy()\n",
    "                original_row = row_output.copy()\n",
    "                dirty_row[x] = dirty_cell\n",
    "                # columns_unique = np.array(beer_dirty[x].unique())\n",
    "                # columns_unique = list(np.random.choice(columns_unique,min(15,len(columns_unique)))) ## 截断\n",
    "                # insert_randomly(columns_unique,dirty_cell) \n",
    "                template_dict = {}\n",
    "                temp_dict = dirty_row[1:].to_dict()\n",
    "                template_dict[x] = ''\n",
    "                coreset_reference = np.random.choice(rayyan_label_index_select,2,replace=False)\n",
    "                # clean_dict = original_row[2:].to_dict()\n",
    "                clean_dict = {}\n",
    "                clean_dict[x] = clean_cell\n",
    "                Correction_format = \"The input \\n\\n%s\\n\\nare [dirty,clean] cell pairs for value %s\" % (str(dirty_example),x)\n",
    "                text_head = 'You are an expert in Cleaning Rayyan Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\n%s\\n\\nTake these clean rows as reference:\\n\\n' % (x, json.dumps(template_dict), json.dumps(temp_dict),Correction_format)\n",
    "                ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(rayyan_clean.iloc[coreset_reference[0],1:].to_dict()), json.dumps(rayyan_clean.iloc[coreset_reference[1],1:].to_dict()))\n",
    "                \n",
    "                training_list.append([text_head,ICL_text,'',json.dumps(clean_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Augmented Training Data in LLM Generation, size controlled into 3000-4000 pairs\n",
    "import json\n",
    "rayyan_clean = pd.read_csv('datasets/rayyan/clean.csv').fillna('')\n",
    "rayyan_dirty = pd.read_csv('datasets/rayyan/dirty.csv').fillna('')\n",
    "def Str2Int(row):\n",
    "    for index in range(11):\n",
    "        temp = row[index]\n",
    "        try:\n",
    "            row[index] = str(int(temp))\n",
    "        except:\n",
    "            continue\n",
    "    return row\n",
    "rayyan_clean = rayyan_clean.apply(Str2Int,axis=1)\n",
    "rayyan_dirty = rayyan_dirty.apply(Str2Int,axis=1)\n",
    "row_list = []\n",
    "single_list = []\n",
    "column_list = []\n",
    "rayyan_dirty_augment = rayyan_dirty.copy()\n",
    "detector_list_rayyan = []\n",
    "rayyan_label_index_select = rayyan_label_index[:5]\n",
    "dirty_example_dict = {}\n",
    "dirty_example_dict_index = {'article_title':1, 'journal_title':3, 'journal_issn':5, 'article_jvolumn':6,'article_jissue':7,'article_jcreated_at':8,'article_pagination':9,'author_list':10}\n",
    "for x in dirty_example_dict_index.keys():\n",
    "    dirty_example = []\n",
    "    j = dirty_example_dict_index[x]\n",
    "    for i in rayyan_label_index_select:\n",
    "        if rayyan_dirty.iloc[i,j]!=rayyan_clean.iloc[i,j]:\n",
    "            dirty_example.append([rayyan_dirty.iloc[i,j],rayyan_clean.iloc[i,j]]) \n",
    "    dirty_example_dict[x] = dirty_example\n",
    "    # print(\"The input \\n\\n%s\\n\\nare [dirty,clean] cell pairs\" % str(dirty_example))\n",
    "training_list_label = []\n",
    "for index,row in rayyan_dirty.iterrows():\n",
    "    if(index in rayyan_label_index_select):\n",
    "        for i in range(11):\n",
    "            if(rayyan_clean.iloc[index,i]!=rayyan_dirty.iloc[index,i]):\n",
    "                clean_cell = rayyan_clean.iloc[index,i]\n",
    "                x = rayyan_clean.columns[i]\n",
    "                dirty_example = dirty_example_dict[x]\n",
    "                # if(dirty_cell!='') and (dirty_cell!=clean_cell):\n",
    "                if(x in ['article_title','journal_title','author_list']):\n",
    "                    dirty_example = random.sample(dirty_example,min(2,len(dirty_example)))\n",
    "                else:\n",
    "                    dirty_example = random.sample(dirty_example,min(5,len(dirty_example)))\n",
    "                # columns_unique = np.array(beer_dirty[x].unique())\n",
    "                # columns_unique = list(np.random.choice(columns_unique,min(15,len(columns_unique)))) ## 截断\n",
    "                # insert_randomly(columns_unique,dirty_cell) \n",
    "                template_dict = {}\n",
    "                temp_dict = rayyan_dirty.iloc[index,1:].to_dict()\n",
    "                template_dict[x] = ''\n",
    "                coreset_reference = np.random.choice(rayyan_label_index_select,2,replace=False)\n",
    "                # clean_dict = original_row[2:].to_dict()\n",
    "                clean_dict = {}\n",
    "                clean_dict[x] = clean_cell\n",
    "                Correction_format = \"The input \\n\\n%s\\n\\nare [dirty,clean] cell pairs for value %s\" % (str(dirty_example),x)\n",
    "                text_head = 'You are an expert in Cleaning Rayyan Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\n%s\\n\\nTake these clean rows as reference:\\n\\n' % (x, json.dumps(template_dict), json.dumps(temp_dict),Correction_format)\n",
    "                ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(rayyan_clean.iloc[coreset_reference[0],1:].to_dict()), json.dumps(rayyan_clean.iloc[coreset_reference[1],1:].to_dict()))   \n",
    "                training_list_label.append([text_head,ICL_text,'',json.dumps(clean_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Augmented Training Data in LLM Generation, size controlled into 3000-4000 pairs\n",
    "import json\n",
    "rayyan_clean = pd.read_csv('datasets/rayyan/clean.csv').fillna('')\n",
    "rayyan_dirty = pd.read_csv('datasets/rayyan/dirty.csv').fillna('')\n",
    "def Str2Int(row):\n",
    "    for index in range(11):\n",
    "        temp = row[index]\n",
    "        try:\n",
    "            row[index] = str(int(temp))\n",
    "        except:\n",
    "            continue\n",
    "    return row\n",
    "rayyan_clean = rayyan_clean.apply(Str2Int,axis=1)\n",
    "rayyan_dirty = rayyan_dirty.apply(Str2Int,axis=1)\n",
    "row_list = []\n",
    "single_list = []\n",
    "column_list = []\n",
    "rayyan_dirty_augment = rayyan_dirty.copy()\n",
    "detector_list_rayyan = []\n",
    "rayyan_label_index_select = rayyan_label_index[:20]\n",
    "dirty_example_dict = {}\n",
    "dirty_example_dict_index = {'article_title':1, 'journal_title':3, 'journal_issn':5, 'article_jvolumn':6,'article_jissue':7,'article_jcreated_at':8,'article_pagination':9,'author_list':10}\n",
    "for x in dirty_example_dict_index.keys():\n",
    "    dirty_example = []\n",
    "    j = dirty_example_dict_index[x]\n",
    "    for i in rayyan_label_index_select:\n",
    "        if rayyan_dirty.iloc[i,j]!=rayyan_clean.iloc[i,j]:\n",
    "            dirty_example.append([rayyan_dirty.iloc[i,j],rayyan_clean.iloc[i,j]]) \n",
    "    dirty_example_dict[x] = dirty_example\n",
    "    # print(\"The input \\n\\n%s\\n\\nare [dirty,clean] cell pairs\" % str(dirty_example))\n",
    "training_list_label = []\n",
    "for index,i in np.argwhere(rayyan_detector==1):\n",
    "    # if(index in rayyan_label_index_select):\n",
    "        # for i in range(11):\n",
    "            i = i + 1 ## Exclude id\n",
    "            # if(rayyan_clean.iloc[index,i]!=rayyan_dirty.iloc[index,i]):\n",
    "            clean_cell = rayyan_clean.iloc[index,i]\n",
    "            x = rayyan_clean.columns[i]\n",
    "            dirty_example = dirty_example_dict[x]\n",
    "            if(x in ['article_title','journal_title','author_list']):\n",
    "                dirty_example = random.sample(dirty_example,min(2,len(dirty_example)))\n",
    "            else:\n",
    "                dirty_example = random.sample(dirty_example,min(5,len(dirty_example)))\n",
    "            # columns_unique = np.array(beer_dirty[x].unique())\n",
    "            # columns_unique = list(np.random.choice(columns_unique,min(15,len(columns_unique)))) ## 截断\n",
    "            # insert_randomly(columns_unique,dirty_cell) \n",
    "            template_dict = {}\n",
    "            temp_dict = rayyan_dirty.iloc[index,1:].to_dict()\n",
    "            template_dict[x] = ''\n",
    "            coreset_reference = np.random.choice(rayyan_label_index_select,2,replace=False)\n",
    "            # clean_dict = original_row[2:].to_dict()\n",
    "            clean_dict = {}\n",
    "            clean_dict[x] = clean_cell\n",
    "            Correction_format = \"The input \\n\\n%s\\n\\nare [dirty,clean] cell pairs for value %s\" % (str(dirty_example),x)\n",
    "            text_head = 'You are an expert in Cleaning Rayyan Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\n%s\\n\\nTake these clean rows as reference:\\n\\n' % (x, json.dumps(template_dict), json.dumps(temp_dict),Correction_format)\n",
    "            ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(rayyan_clean.iloc[coreset_reference[0],1:].to_dict()), json.dumps(rayyan_clean.iloc[coreset_reference[1],1:].to_dict()))   \n",
    "            training_list_label.append([text_head,ICL_text,'',json.dumps(clean_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list_label_pd = pd.DataFrame(training_list_label)\n",
    "training_list_label_pd['instruction'] = training_list_label_pd[0] + training_list_label_pd[1]\n",
    "training_list_label_pd['input'] = training_list_label_pd[2]\n",
    "training_list_label_pd['output'] = training_list_label_pd[3]\n",
    "# training_list_label_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(training_list_label_pd.iloc[:,-3:].to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/rayyan/rayyan-test-20.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_list_pd = pd.DataFrame(training_list).sample(n=(150 * len(beer_label_index_select)))\n",
    "training_list_pd = pd.DataFrame(training_list)\n",
    "training_list_pd['instruction'] = training_list_pd[0] + training_list_pd[1]\n",
    "training_list_pd['input'] = training_list_pd[2]\n",
    "training_list_pd['output'] = training_list_pd[3]\n",
    "training_list_label_pd = pd.DataFrame(training_list_label)\n",
    "training_list_label_pd['instruction'] = training_list_label_pd[0] + training_list_label_pd[1]\n",
    "training_list_label_pd['input'] = training_list_label_pd[2]\n",
    "training_list_label_pd['output'] = training_list_label_pd[3]\n",
    "training_list_output = pd.concat([training_list_pd,training_list_label_pd])\n",
    "training_list_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(training_list_output.iloc[:,-3:].to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/rayyan/rayyan-train-5.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_detector = np.load('datasets/rayyan/detector/multi-view/detector_20.npy').reshape((1000,10))\n",
    "rayyan_detector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_label_index = np.load('datasets/tax/detector/index.npy')\n",
    "example = []\n",
    "for t in tax_label_index[:10]:\n",
    "    example.append([tax_clean.iloc[t].to_dict(),tax_dirty.iloc[t].to_dict()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0, 0, 0, 2, 2, 0, 3, 3, 0, 0, 2, 0, 1])"
      ]
     },
     "execution_count": 737,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_set = tax_clean.iloc[tax_label_index[:10]]\n",
    "dirty_set = tax_dirty.iloc[tax_label_index[:10]]\n",
    "np.array(clean_set!=dirty_set).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try to test recall on Rayyan Dataset\n",
    "import json\n",
    "rayyan_clean = pd.read_csv('datasets/rayyan/clean.csv').fillna('')\n",
    "rayyan_dirty = pd.read_csv('datasets/rayyan/dirty.csv').fillna('')\n",
    "def Str2Int(row):\n",
    "    for index in range(11):\n",
    "        temp = row[index]\n",
    "        try:\n",
    "            row[index] = str(int(temp))\n",
    "        except:\n",
    "            continue\n",
    "    return row\n",
    "rayyan_clean = rayyan_clean.apply(Str2Int,axis=1)\n",
    "rayyan_dirty = rayyan_dirty.apply(Str2Int,axis=1)\n",
    "count = 0\n",
    "valid_count = 0\n",
    "rayyan_correction = rayyan_dirty.copy()\n",
    "import ast\n",
    "for d in np.argwhere(rayyan_detector==1):\n",
    "    i = d[0]\n",
    "    j = d[1] + 1 ## Ignore Index\n",
    "    try:\n",
    "        predict = list(ast.literal_eval(result.iloc[count,-1]).values())[0]\n",
    "        rayyan_correction.iloc[i,j] = predict\n",
    "        valid_count += 1\n",
    "    except:\n",
    "        predict = result.iloc[count,-1]\n",
    "        rayyan_correction.iloc[i,j] = predict\n",
    "    count += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state - zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_name</th>\n",
       "      <th>l_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>area_code</th>\n",
       "      <th>phone</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>has_child</th>\n",
       "      <th>salary</th>\n",
       "      <th>rate</th>\n",
       "      <th>single_exemp</th>\n",
       "      <th>married_exemp</th>\n",
       "      <th>child_exemp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pengyuan</td>\n",
       "      <td>Zendler</td>\n",
       "      <td>F</td>\n",
       "      <td>508</td>\n",
       "      <td>744-9007</td>\n",
       "      <td>SWAMPSCOTT</td>\n",
       "      <td>MA</td>\n",
       "      <td>1907</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>90000</td>\n",
       "      <td>5.3</td>\n",
       "      <td>0</td>\n",
       "      <td>7150</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4411</th>\n",
       "      <td>Sonja</td>\n",
       "      <td>Fullerton</td>\n",
       "      <td>F</td>\n",
       "      <td>339</td>\n",
       "      <td>672-1352</td>\n",
       "      <td>SWAMPSCOTT</td>\n",
       "      <td>MA</td>\n",
       "      <td>1907</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>100000</td>\n",
       "      <td>5.3</td>\n",
       "      <td>0</td>\n",
       "      <td>7150</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27163</th>\n",
       "      <td>Piera</td>\n",
       "      <td>Brodie</td>\n",
       "      <td>F</td>\n",
       "      <td>617</td>\n",
       "      <td>533-3461</td>\n",
       "      <td>SWAMPSCOTT</td>\n",
       "      <td>MA</td>\n",
       "      <td>1907</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>10000</td>\n",
       "      <td>5.3</td>\n",
       "      <td>0</td>\n",
       "      <td>7150</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77457</th>\n",
       "      <td>Premachandran</td>\n",
       "      <td>Nepomnjashchaja</td>\n",
       "      <td>M</td>\n",
       "      <td>978</td>\n",
       "      <td>571-1299</td>\n",
       "      <td>SWAMPSCOTT</td>\n",
       "      <td>MA</td>\n",
       "      <td>1907</td>\n",
       "      <td>S</td>\n",
       "      <td>Y</td>\n",
       "      <td>75000</td>\n",
       "      <td>5.3</td>\n",
       "      <td>3575</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101971</th>\n",
       "      <td>Arden</td>\n",
       "      <td>Leach</td>\n",
       "      <td>M</td>\n",
       "      <td>617</td>\n",
       "      <td>953-8814</td>\n",
       "      <td>SWAMPSCOTT</td>\n",
       "      <td>MA</td>\n",
       "      <td>1907</td>\n",
       "      <td>S</td>\n",
       "      <td>Y</td>\n",
       "      <td>45000</td>\n",
       "      <td>5.3</td>\n",
       "      <td>3575</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125068</th>\n",
       "      <td>Mohua</td>\n",
       "      <td>Ducret</td>\n",
       "      <td>M</td>\n",
       "      <td>351</td>\n",
       "      <td>516-5755</td>\n",
       "      <td>SWAMPSCOTT</td>\n",
       "      <td>MA</td>\n",
       "      <td>1907</td>\n",
       "      <td>S</td>\n",
       "      <td>Y</td>\n",
       "      <td>30000</td>\n",
       "      <td>5.3</td>\n",
       "      <td>3575</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128619</th>\n",
       "      <td>Yasuaki</td>\n",
       "      <td>Glodjo</td>\n",
       "      <td>M</td>\n",
       "      <td>413</td>\n",
       "      <td>313-1978</td>\n",
       "      <td>SWAMPSCOTT</td>\n",
       "      <td>MA</td>\n",
       "      <td>1907</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>30000</td>\n",
       "      <td>5.3</td>\n",
       "      <td>3575</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163559</th>\n",
       "      <td>Rushikesh</td>\n",
       "      <td>Kaka</td>\n",
       "      <td>M</td>\n",
       "      <td>857</td>\n",
       "      <td>384-7061</td>\n",
       "      <td>SWAMPSCOTT</td>\n",
       "      <td>MA</td>\n",
       "      <td>1907</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>35000</td>\n",
       "      <td>5.3</td>\n",
       "      <td>3575</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191027</th>\n",
       "      <td>Klichiro</td>\n",
       "      <td>Davy</td>\n",
       "      <td>M</td>\n",
       "      <td>413</td>\n",
       "      <td>110-1863</td>\n",
       "      <td>SWAMPSCOTT</td>\n",
       "      <td>MA</td>\n",
       "      <td>1907</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>65000</td>\n",
       "      <td>5.3</td>\n",
       "      <td>0</td>\n",
       "      <td>7150</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               f_name           l_name gender area_code     phone        city  \\\n",
       "0            Pengyuan          Zendler      F       508  744-9007  SWAMPSCOTT   \n",
       "4411            Sonja        Fullerton      F       339  672-1352  SWAMPSCOTT   \n",
       "27163           Piera           Brodie      F       617  533-3461  SWAMPSCOTT   \n",
       "77457   Premachandran  Nepomnjashchaja      M       978  571-1299  SWAMPSCOTT   \n",
       "101971          Arden            Leach      M       617  953-8814  SWAMPSCOTT   \n",
       "125068          Mohua           Ducret      M       351  516-5755  SWAMPSCOTT   \n",
       "128619        Yasuaki           Glodjo      M       413  313-1978  SWAMPSCOTT   \n",
       "163559      Rushikesh             Kaka      M       857  384-7061  SWAMPSCOTT   \n",
       "191027       Klichiro             Davy      M       413  110-1863  SWAMPSCOTT   \n",
       "\n",
       "       state   zip marital_status has_child  salary rate single_exemp  \\\n",
       "0         MA  1907              M         N   90000  5.3            0   \n",
       "4411      MA  1907              M         Y  100000  5.3            0   \n",
       "27163     MA  1907              M         N   10000  5.3            0   \n",
       "77457     MA  1907              S         Y   75000  5.3         3575   \n",
       "101971    MA  1907              S         Y   45000  5.3         3575   \n",
       "125068    MA  1907              S         Y   30000  5.3         3575   \n",
       "128619    MA  1907              S         N   30000  5.3         3575   \n",
       "163559    MA  1907              S         N   35000  5.3         3575   \n",
       "191027    MA  1907              M         N   65000  5.3            0   \n",
       "\n",
       "       married_exemp child_exemp  \n",
       "0               7150           0  \n",
       "4411            7150        1000  \n",
       "27163           7150           0  \n",
       "77457              0        1000  \n",
       "101971             0        1000  \n",
       "125068             0        1000  \n",
       "128619             0           0  \n",
       "163559             0           0  \n",
       "191027          7150           0  "
      ]
     },
     "execution_count": 839,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_clean[tax_dirty['city']=='SWAMPSCOTT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "rayyan_clean = pd.read_csv('datasets/rayyan/clean.csv').fillna('')\n",
    "rayyan_dirty = pd.read_csv('datasets/rayyan/dirty.csv').fillna('')\n",
    "def Str2Int(row):\n",
    "    for index in range(11):\n",
    "        temp = row[index]\n",
    "        try:\n",
    "            row[index] = str(int(temp))\n",
    "        except:\n",
    "            continue\n",
    "    return row\n",
    "rayyan_clean = rayyan_clean.apply(Str2Int,axis=1)\n",
    "rayyan_dirty = rayyan_dirty.apply(Str2Int,axis=1)\n",
    "rayyan_correction_function = rayyan_dirty.copy()\n",
    "# if(x in ['journal_issn','article_jvolumn','article_jissue','article_jcreated_at','article_pagination']):\n",
    "for i,j in np.argwhere(rayyan_detector==1):\n",
    "    j = j + 1 ## Skip Index\n",
    "    dirty_cell = rayyan_dirty.iloc[i,j]\n",
    "    x = rayyan_dirty.columns[j]\n",
    "    correction = Rayyan_Row_Correction(x,dirty_cell)\n",
    "    if correction!=False:\n",
    "        rayyan_correction_function.iloc[i,j] = correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [],
   "source": [
    "rayyan_correction.to_csv('datasets/rayyan/detector/multi-view/correction_20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [],
   "source": [
    "rayyan_correction_function.to_csv('datasets/rayyan/detector/multi-view/correction_function_20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correction_list = []\n",
    "correction_pd = rayyan_correction_function.copy()\n",
    "for i,j in np.argwhere(rayyan_detector==1):\n",
    "    all_context_clean = ''\n",
    "    j = j + 1 ## Skip Index\n",
    "    x = rayyan_dirty.columns[j]\n",
    "    clean_cell = correction_pd.iloc[i,j]\n",
    "    single_context_clean = 'COL %s VAL %s ' % (x,clean_cell)\n",
    "    for c in range(1,11,1):\n",
    "        all_context_clean += 'COL %s VAL %s ' % (rayyan_clean.columns[c],correction_pd.iloc[i,c])\n",
    "    if(clean_cell!=rayyan_clean.iloc[i,j]):\n",
    "        correction_list.append([all_context_clean,single_context_clean,1])\n",
    "    else:\n",
    "        correction_list.append([all_context_clean,single_context_clean,0])\n",
    "pd.DataFrame(correction_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(correction_list).to_csv('datasets/rayyan/detector/multi-view/correction_LLM_20.csv')\n",
    "pd.DataFrame(correction_list).to_csv('datasets/rayyan/detector/multi-view/correction_function_20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [],
   "source": [
    "rayyan_correction_final = rayyan_dirty.copy()\n",
    "correction_function = np.load('/home/yanmy/raha/raha-master/datasets/rayyan/detector/multi-view/correction_function_20.npy')\n",
    "correction_LLM = np.load('/home/yanmy/raha/raha-master/datasets/rayyan/detector/multi-view/correction_LLM_20.npy')\n",
    "count = 0\n",
    "for i,j in np.argwhere(rayyan_detector==1):\n",
    "    j = j + 1 ## Skip Index\n",
    "    LLM = correction_LLM[count]\n",
    "    function = correction_function[count]\n",
    "    LLM_output = rayyan_correction.iloc[i,j]\n",
    "    function_output = rayyan_correction_function.iloc[i,j]\n",
    "    if(LLM<function): ## the small predict score, indicate that the value is more normal, and should be accelpted as correction value\n",
    "        rayyan_correction_final.iloc[i,j] = LLM_output\n",
    "    else:\n",
    "        rayyan_correction_final.iloc[i,j] = function_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1694,
   "metadata": {},
   "outputs": [],
   "source": [
    "rayyan_correction = rayyan_correction_final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1716,
   "metadata": {},
   "outputs": [],
   "source": [
    "rayyan_correction = rayyan_correction_final.copy()\n",
    "input_matrix_rayyan = np.array(rayyan_clean!=rayyan_dirty).astype(int)\n",
    "rayyan_label_index_select = rayyan_label_index[3]\n",
    "# rayyan_label_index_select = [924]\n",
    "a = np.where(input_matrix_rayyan[rayyan_label_index_select].sum(axis=0)!=0)\n",
    "b = np.where(input_matrix_rayyan[rayyan_label_index[:20]].sum(axis=0)!=0)\n",
    "for h in [i for i in b[0] if i not in a[0]]:\n",
    "    rayyan_correction.iloc[:,h] = rayyan_dirty.iloc[:,h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1710,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([158, 455,  85, 118, 322, 384, 392, 615, 656, 796, 862, 975, 979,\n",
       "       357, 130, 532, 212, 694, 924, 918])"
      ]
     },
     "execution_count": 1710,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1717,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9090909090909091, 0.02109704641350211, 0.041237113402061855)"
      ]
     },
     "execution_count": 1717,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "rayyan_clean = pd.read_csv('datasets/rayyan/clean.csv').fillna('')\n",
    "rayyan_dirty = pd.read_csv('datasets/rayyan/dirty.csv').fillna('')\n",
    "def Str2Int(row):\n",
    "    for index in range(11):\n",
    "        temp = row[index]\n",
    "        try:\n",
    "            row[index] = str(int(temp))\n",
    "        except:\n",
    "            continue\n",
    "    return row\n",
    "rayyan_clean = rayyan_clean.apply(Str2Int,axis=1)\n",
    "rayyan_dirty = rayyan_dirty.apply(Str2Int,axis=1)\n",
    "All_Data_Error = 0\n",
    "All_Fixed_Error = 0\n",
    "Correct_Fixed_Error = 0\n",
    "for i in range(1000):\n",
    "    for j in range(11):\n",
    "        dirty_cell = rayyan_dirty.iloc[i,j]\n",
    "        clean_cell = rayyan_clean.iloc[i,j]\n",
    "        correct_cell = rayyan_correction.iloc[i,j]\n",
    "        if(correct_cell!=dirty_cell):\n",
    "            All_Fixed_Error += 1\n",
    "        if(clean_cell!=dirty_cell):\n",
    "            All_Data_Error += 1\n",
    "            if(correct_cell==clean_cell):\n",
    "                Correct_Fixed_Error += 1\n",
    "Precision_hospital = Correct_Fixed_Error / All_Fixed_Error\n",
    "Recall_hospital = Correct_Fixed_Error / All_Data_Error\n",
    "F1_hospital = (2 * Precision_hospital * Recall_hospital) / (Precision_hospital + Recall_hospital)\n",
    "Precision_hospital,Recall_hospital,F1_hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1741,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_correction = beer_correct_20.copy()\n",
    "a = np.where(input_matrix_beer[beer_label_index[:1]].sum(axis=0)!=0)\n",
    "b = np.where(input_matrix_beer[beer_label_index[:20]].sum(axis=0)!=0)\n",
    "# for h in [9]:\n",
    "#     beer_correction.iloc[:,h] = beer_dirty.iloc[:,h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1727,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1]])"
      ]
     },
     "execution_count": 1727,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix_beer[beer_label_index[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Beer-Vary-Label-budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1742,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9770696843359142, 0.9773607387548406, 0.9772151898734178)"
      ]
     },
     "execution_count": 1742,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_Data_Error = 0\n",
    "All_Fixed_Error = 0\n",
    "Correct_Fixed_Error = 0\n",
    "for i in range(len(beer_clean)):\n",
    "    for j in range(11):\n",
    "        dirty_cell = beer_dirty.iloc[i,j]\n",
    "        clean_cell = beer_clean.iloc[i,j]\n",
    "        correct_cell = beer_correction.iloc[i,j]\n",
    "        if(correct_cell!=dirty_cell):\n",
    "            All_Fixed_Error += 1\n",
    "        if(clean_cell!=dirty_cell):\n",
    "            All_Data_Error += 1\n",
    "            if(correct_cell==clean_cell):\n",
    "                Correct_Fixed_Error += 1\n",
    "Precision_hospital = Correct_Fixed_Error / All_Fixed_Error\n",
    "Recall_hospital = Correct_Fixed_Error / All_Data_Error\n",
    "F1_hospital = (2 * Precision_hospital * Recall_hospital) / (Precision_hospital + Recall_hospital)\n",
    "Precision_hospital,Recall_hospital,F1_hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1746,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['f_name', 'l_name', 'gender', 'area_code', 'phone', 'city', 'state',\n",
       "       'zip', 'marital_status', 'has_child', 'salary', 'rate', 'single_exemp',\n",
       "       'married_exemp', 'child_exemp'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1746,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_dirty.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1765,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_correction = tax_correct_15_3.copy()\n",
    "a = np.where(input_matrix_tax[tax_label_index[:1]].sum(axis=0)!=0)\n",
    "b = np.where(input_matrix_tax[tax_label_index[:20]].sum(axis=0)!=0)\n",
    "for h in [i for i in b[0] if i not in a[0] and i not in [8,9]]:\n",
    "    tax_correction.iloc[:,h] = tax_dirty.iloc[:,h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1777,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2118/2675795260.py:10: DtypeWarning: Columns (12,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tax_dirty = pd.read_csv('datasets/tax/dirty.csv').fillna('').astype(str)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9705581157194061, 0.9602330293819655, 0.9653679653679654)"
      ]
     },
     "execution_count": 1777,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_correction = tax_correct_15_3.copy()\n",
    "a = np.where(input_matrix_tax[tax_label_index[:15]].sum(axis=0)!=0)\n",
    "b = np.where(input_matrix_tax[tax_label_index[:20]].sum(axis=0)!=0)\n",
    "for h in [i for i in b[0] if i not in a[0] and i not in [8,9]]:\n",
    "    tax_correction.iloc[:,h] = tax_dirty.iloc[:,h]\n",
    "All_Data_Error = 0\n",
    "All_Fixed_Error = 0\n",
    "Correct_Fixed_Error = 0\n",
    "tax_clean = pd.read_csv('datasets/tax/clean.csv').fillna('').astype(str)\n",
    "tax_dirty = pd.read_csv('datasets/tax/dirty.csv').fillna('').astype(str)\n",
    "for i in tax_error:\n",
    "    for j in range(len(tax_clean.columns)):\n",
    "        dirty_cell = tax_dirty.iloc[i,j]\n",
    "        clean_cell = tax_clean.iloc[i,j]\n",
    "        correct_cell = tax_correction.iloc[i,j]\n",
    "        if(correct_cell!=dirty_cell):\n",
    "            All_Fixed_Error += 1\n",
    "        if(clean_cell!=dirty_cell):\n",
    "            All_Data_Error += 1\n",
    "            if(correct_cell==clean_cell):\n",
    "                Correct_Fixed_Error += 1\n",
    "Precision_hospital = Correct_Fixed_Error / All_Fixed_Error\n",
    "Recall_hospital = Correct_Fixed_Error / All_Data_Error\n",
    "F1_hospital = (2 * Precision_hospital * Recall_hospital) / (Precision_hospital + Recall_hospital)\n",
    "Precision_hospital,Recall_hospital,F1_hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1.0, 0.1534954407294833, 0.2661396574440053) 1\n",
    "(1.0, 0.1534954407294833, 0.2661396574440053) 2 \n",
    "(1.0, 0.20440729483282674, 0.3394321766561514) 3\n",
    "(1.0, 0.2553191489361702, 0.4067796610169491) 4\n",
    "(1.0, 0.5395136778115501, 0.700888450148075) 5\n",
    "(1.0, 0.8913373860182371, 0.9425472077139414) 6 \n",
    "(1.0, 0.8913373860182371, 0.9425472077139414) 7 \n",
    "(1.0, 0.8913373860182371, 0.9425472077139414) 8\n",
    "(1.0, 0.8913373860182371, 0.9425472077139414) 9\n",
    "(1.0, 0.8913373860182371, 0.9425472077139414) 10\n",
    "(0.9705581157194061, 0.9602330293819655, 0.9653679653679654)\n",
    "(0.9705581157194061, 0.9602330293819655, 0.9653679653679654) 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_result = pd.read_csv('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/beer-test-20.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"abv\": \"0.09\"}</td>\n",
       "      <td>{\"abv\": \"0.09\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3359</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3360</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"abv\": \"0.055\"}</td>\n",
       "      <td>{\"abv\": \"0.055\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3361</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3362</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"abv\": \"0.055\"}</td>\n",
       "      <td>{\"abv\": \"0.055\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3363</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3364 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            instruction  input  \\\n",
       "0     You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "1     You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "2     You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "3     You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "4     You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "...                                                 ...    ...   \n",
       "3359  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "3360  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "3361  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "3362  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "3363  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "\n",
       "                output           predict  \n",
       "0     {\"ounces\": \"12\"}  {\"ounces\": \"12\"}  \n",
       "1     {\"ounces\": \"12\"}  {\"ounces\": \"12\"}  \n",
       "2     {\"ounces\": \"12\"}  {\"ounces\": \"12\"}  \n",
       "3     {\"ounces\": \"12\"}  {\"ounces\": \"12\"}  \n",
       "4      {\"abv\": \"0.09\"}   {\"abv\": \"0.09\"}  \n",
       "...                ...               ...  \n",
       "3359  {\"ounces\": \"12\"}  {\"ounces\": \"12\"}  \n",
       "3360  {\"abv\": \"0.055\"}  {\"abv\": \"0.055\"}  \n",
       "3361  {\"ounces\": \"12\"}  {\"ounces\": \"12\"}  \n",
       "3362  {\"abv\": \"0.055\"}  {\"abv\": \"0.055\"}  \n",
       "3363  {\"ounces\": \"12\"}  {\"ounces\": \"12\"}  \n",
       "\n",
       "[3364 rows x 4 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer_detector = np.load('/home/yanmy/raha/raha-master/datasets/beers/detector/detection.npy')\n",
    "beer_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3357"
      ]
     },
     "execution_count": 792,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(beer_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_clean = pd.read_csv('datasets/beers/clean.csv').fillna('')\n",
    "beer_dirty = pd.read_csv('datasets/beers/dirty.csv').fillna('')\n",
    "beer_dirty.columns = beer_clean.columns\n",
    "def try_convert_to_int(row):\n",
    "    for x,y in row.items():\n",
    "        if(x in ['ounces','ibu']):\n",
    "            try:\n",
    "                row[x] = int(y)\n",
    "            except:\n",
    "                row[x] = y\n",
    "    return row\n",
    "beer_clean = beer_clean.apply(try_convert_to_int,axis=1).astype(str)\n",
    "beer_dirty = beer_dirty.apply(try_convert_to_int,axis=1).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>beer-name</th>\n",
       "      <th>style</th>\n",
       "      <th>ounces</th>\n",
       "      <th>abv</th>\n",
       "      <th>ibu</th>\n",
       "      <th>brewery_id</th>\n",
       "      <th>brewery-name</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1436</td>\n",
       "      <td>Pub Beer</td>\n",
       "      <td>American Pale Lager</td>\n",
       "      <td>12.0 oz</td>\n",
       "      <td>0.05</td>\n",
       "      <td></td>\n",
       "      <td>408</td>\n",
       "      <td>10 Barrel Brewing Company</td>\n",
       "      <td>Bend</td>\n",
       "      <td>OR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2265</td>\n",
       "      <td>Devil's Cup</td>\n",
       "      <td>American Pale Ale (APA)</td>\n",
       "      <td>12.0 oz.</td>\n",
       "      <td>0.066</td>\n",
       "      <td></td>\n",
       "      <td>177</td>\n",
       "      <td>18th Street Brewery</td>\n",
       "      <td>Gary</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2264</td>\n",
       "      <td>Rise of the Phoenix</td>\n",
       "      <td>American IPA</td>\n",
       "      <td>12.0 ounce</td>\n",
       "      <td>0.071</td>\n",
       "      <td></td>\n",
       "      <td>177</td>\n",
       "      <td>18th Street Brewery</td>\n",
       "      <td>Gary</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2263</td>\n",
       "      <td>Sinister</td>\n",
       "      <td>American Double / Imperial IPA</td>\n",
       "      <td>12.0 oz</td>\n",
       "      <td>0.09%</td>\n",
       "      <td></td>\n",
       "      <td>177</td>\n",
       "      <td>18th Street Brewery</td>\n",
       "      <td>Gary</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2262</td>\n",
       "      <td>Sex and Candy</td>\n",
       "      <td>American IPA</td>\n",
       "      <td>12.0 OZ.</td>\n",
       "      <td>0.075</td>\n",
       "      <td></td>\n",
       "      <td>177</td>\n",
       "      <td>18th Street Brewery</td>\n",
       "      <td>Gary</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2405</th>\n",
       "      <td>2406</td>\n",
       "      <td>928</td>\n",
       "      <td>Belgorado</td>\n",
       "      <td>Belgian IPA</td>\n",
       "      <td>12.0 oz.</td>\n",
       "      <td>0.067</td>\n",
       "      <td>45</td>\n",
       "      <td>424</td>\n",
       "      <td>Wynkoop Brewing Company</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2406</th>\n",
       "      <td>2407</td>\n",
       "      <td>807</td>\n",
       "      <td>Rail Yard Ale</td>\n",
       "      <td>American Amber / Red Ale</td>\n",
       "      <td>12.0 oz. Alumi-Tek</td>\n",
       "      <td>0.052</td>\n",
       "      <td></td>\n",
       "      <td>424</td>\n",
       "      <td>Wynkoop Brewing Company</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2407</th>\n",
       "      <td>2408</td>\n",
       "      <td>620</td>\n",
       "      <td>B3K Black Lager</td>\n",
       "      <td>Schwarzbier</td>\n",
       "      <td>12.0 oz.</td>\n",
       "      <td>0.055%</td>\n",
       "      <td></td>\n",
       "      <td>424</td>\n",
       "      <td>Wynkoop Brewing Company</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2408</th>\n",
       "      <td>2409</td>\n",
       "      <td>145</td>\n",
       "      <td>Silverback Pale Ale</td>\n",
       "      <td>American Pale Ale (APA)</td>\n",
       "      <td>12.0 ounce</td>\n",
       "      <td>0.055%</td>\n",
       "      <td>40</td>\n",
       "      <td>424</td>\n",
       "      <td>Wynkoop Brewing Company</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2409</th>\n",
       "      <td>2410</td>\n",
       "      <td>84</td>\n",
       "      <td>Rail Yard Ale (2009)</td>\n",
       "      <td>American Amber / Red Ale</td>\n",
       "      <td>12.0 oz.</td>\n",
       "      <td>0.052</td>\n",
       "      <td></td>\n",
       "      <td>424</td>\n",
       "      <td>Wynkoop Brewing Company</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2410 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index    id             beer-name                           style  \\\n",
       "0        1  1436              Pub Beer             American Pale Lager   \n",
       "1        2  2265           Devil's Cup         American Pale Ale (APA)   \n",
       "2        3  2264   Rise of the Phoenix                    American IPA   \n",
       "3        4  2263              Sinister  American Double / Imperial IPA   \n",
       "4        5  2262         Sex and Candy                    American IPA   \n",
       "...    ...   ...                   ...                             ...   \n",
       "2405  2406   928             Belgorado                     Belgian IPA   \n",
       "2406  2407   807         Rail Yard Ale        American Amber / Red Ale   \n",
       "2407  2408   620       B3K Black Lager                     Schwarzbier   \n",
       "2408  2409   145   Silverback Pale Ale         American Pale Ale (APA)   \n",
       "2409  2410    84  Rail Yard Ale (2009)        American Amber / Red Ale   \n",
       "\n",
       "                  ounces     abv ibu brewery_id               brewery-name  \\\n",
       "0                12.0 oz    0.05            408  10 Barrel Brewing Company   \n",
       "1               12.0 oz.   0.066            177        18th Street Brewery   \n",
       "2             12.0 ounce   0.071            177        18th Street Brewery   \n",
       "3                12.0 oz   0.09%            177        18th Street Brewery   \n",
       "4               12.0 OZ.   0.075            177        18th Street Brewery   \n",
       "...                  ...     ...  ..        ...                        ...   \n",
       "2405            12.0 oz.   0.067  45        424    Wynkoop Brewing Company   \n",
       "2406  12.0 oz. Alumi-Tek   0.052            424    Wynkoop Brewing Company   \n",
       "2407            12.0 oz.  0.055%            424    Wynkoop Brewing Company   \n",
       "2408          12.0 ounce  0.055%  40        424    Wynkoop Brewing Company   \n",
       "2409            12.0 oz.   0.052            424    Wynkoop Brewing Company   \n",
       "\n",
       "        city state  \n",
       "0       Bend    OR  \n",
       "1       Gary    IN  \n",
       "2       Gary    IN  \n",
       "3       Gary    IN  \n",
       "4       Gary    IN  \n",
       "...      ...   ...  \n",
       "2405  Denver    CO  \n",
       "2406  Denver    CO  \n",
       "2407  Denver    CO  \n",
       "2408  Denver    CO  \n",
       "2409  Denver    CO  \n",
       "\n",
       "[2410 rows x 11 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer_dirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0, 2410,  700,    0,    0,    0,  127,  127])"
      ]
     },
     "execution_count": 790,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(detector_beer==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_clean = pd.read_csv('datasets/beers/clean.csv').fillna('')\n",
    "beer_dirty = pd.read_csv('datasets/beers/dirty.csv').fillna('')\n",
    "beer_dirty.columns = beer_clean.columns\n",
    "def try_convert_to_int(row):\n",
    "    for x,y in row.items():\n",
    "        if(x in ['ounces','ibu']):\n",
    "            try:\n",
    "                row[x] = int(y)\n",
    "            except:\n",
    "                row[x] = y\n",
    "    return row\n",
    "beer_clean = beer_clean.apply(try_convert_to_int,axis=1).astype(str)\n",
    "beer_dirty = beer_dirty.apply(try_convert_to_int,axis=1).astype(str)\n",
    "count = 0\n",
    "valid_count = 0\n",
    "beer_correction = beer_dirty.copy()\n",
    "import ast\n",
    "for d in np.argwhere(rayyan_detector==1):\n",
    "    i = d[0]\n",
    "    j = d[1] + 1 ## Ignore Index\n",
    "    try:\n",
    "        predict = list(ast.literal_eval(result.iloc[count,-1]).values())[0]\n",
    "        rayyan_correction.iloc[i,j] = predict\n",
    "        valid_count += 1\n",
    "    except:\n",
    "        predict = result.iloc[count,-1]\n",
    "        rayyan_correction.iloc[i,j] = predict\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From Here We Tackle with Tax Dataset Function Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tax_Detect_fname(cell):\n",
    "    return bool(re.search(r\"''\", cell))\n",
    "def Tax_Generate_fname(cell):\n",
    "    # Find all positions of single quotes in the cell\n",
    "    positions = [i for i, char in enumerate(cell) if char == \"'\"]\n",
    "    \n",
    "    # If there's no single quote, return the original cell\n",
    "    if not positions:\n",
    "        return cell\n",
    "    \n",
    "    # Randomly choose a position from the positions of single quotes\n",
    "    chosen_position = random.choice(positions)\n",
    "    \n",
    "    # Insert an additional single quote at the chosen position\n",
    "    dirty_cell = cell[:chosen_position] + \"'\" + cell[chosen_position:]\n",
    "    \n",
    "    return dirty_cell\n",
    "def Tax_Correct_fname(dirty_cell):\n",
    "    return dirty_cell.replace(\"''\", \"'\")\n",
    "def Tax_Detect_city(cell):\n",
    "    return bool(re.search(r'-\\*$', cell))\n",
    "def Tax_Generate_city(cell,num_samples=5):\n",
    "    dirty_cell = cell + '-*'\n",
    "    return dirty_cell\n",
    "def Tax_Correct_city(cell):\n",
    "    # Use regex to remove the '-*' pattern from the end of the string\n",
    "    corrected_value = re.sub('-\\*$', '', cell)\n",
    "    return corrected_value\n",
    "def Tax_Detect_zip(cell):\n",
    "    return cell == '1907'\n",
    "def Tax_Generate_zip(cell):\n",
    "    return '1907'\n",
    "def Tax_Row_Detection(x,cell):\n",
    "    if(x in ['f_name','l_name']):\n",
    "        return Tax_Detect_fname(cell)\n",
    "    elif(x in ['city','state','single_exemp','child_exemp']):\n",
    "        return Tax_Detect_city(cell)\n",
    "    elif(x in ['zip']):\n",
    "        return Tax_Detect_zip(cell)\n",
    "    else:\n",
    "        return False\n",
    "def Tax_Row_Generate(x,cell):\n",
    "    if(x in ['f_name','l_name']):\n",
    "        return Tax_Generate_fname(cell)\n",
    "    elif(x in ['city','state','single_exemp','child_exemp']):\n",
    "        return Tax_Generate_city(cell)\n",
    "    elif(x in ['zip']):\n",
    "        return Tax_Generate_zip(cell)\n",
    "    else:\n",
    "        return False\n",
    "def Tax_Row_Correction(x,cell):\n",
    "    if(x in ['f_name','l_name']):\n",
    "        return Tax_Correct_fname(cell)\n",
    "    elif(x in ['city','state','single_exemp','child_exemp']):\n",
    "        return Tax_Correct_city(cell)\n",
    "    else:\n",
    "        return False   \n",
    "def Tax_Row_Correction_pd(row):\n",
    "    \n",
    "    if(x in ['f_name','l_name']):\n",
    "        return Tax_Correct_fname(cell)\n",
    "    elif(x in ['city','state','single_exemp','child_exemp']):\n",
    "        return Tax_Correct_city(cell)\n",
    "    else:\n",
    "        return False   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_error = np.where(np.array(tax_dirty!=tax_clean).sum(axis=1)!=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2526,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_error = np.where(np.array(tax_dirty!=tax_clean).sum(axis=1)!=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2929,)"
      ]
     },
     "execution_count": 2528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_error.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2318f083db848879fb6a879f236df71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2929 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "detector_list_tax = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "# for label_tuple in tqdm(tax_label_index[:5]):\n",
    "# for label_tuple in tqdm(tax_label_index[:5]):\n",
    "for label_tuple in tqdm(tax_error):\n",
    "    for i in range(len(tax_dirty.columns)):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = tax_clean.iloc[label_tuple]\n",
    "        dirty_context = tax_dirty.iloc[label_tuple]\n",
    "        clean_cell = tax_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = tax_dirty.iloc[label_tuple,i]\n",
    "        single_context_clean = 'COL %s VAL %s ' % (tax_dirty.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (tax_dirty.columns[i],dirty_cell)\n",
    "        for c in range(len(tax_dirty.columns)):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (tax_clean.columns[c],tax_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (tax_dirty.columns[c],tax_dirty.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        \n",
    "        if(dirty_cell!=clean_cell):\n",
    "            detector_list_tax.append([all_context_dirty,single_context_dirty,1])\n",
    "        else:\n",
    "            detector_list_tax.append([all_context_dirty,single_context_dirty,0])\n",
    "        \n",
    "        # if(dirty_cell!=clean_cell):\n",
    "        #     detector_list_tax.append([all_context_dirty,single_context_dirty,1])\n",
    "        #     detector_list_tax.append([all_context_clean,single_context_clean,0])\n",
    "        # else:\n",
    "        #     detector_list_tax.append([all_context_dirty,single_context_dirty,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07c343c4540d48a2b09b78d4277b16f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2929 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Test Data Generation for Tax\n",
    "detector_list_tax = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "# for label_tuple in tqdm(tax_label_index[:5]):\n",
    "# for label_tuple in tqdm(tax_label_index[:5]):\n",
    "for label_tuple in tqdm(tax_error):\n",
    "    for i in range(len(tax_dirty.columns)):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = tax_dirty.iloc[label_tuple].copy()\n",
    "        dirty_context = tax_dirty.iloc[label_tuple].copy()\n",
    "        clean_cell = tax_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = tax_dirty.iloc[label_tuple,i]\n",
    "        single_context_clean = 'COL %s VAL %s ' % (tax_dirty.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (tax_dirty.columns[i],dirty_cell)\n",
    "        for c in range(len(tax_dirty.columns)):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (tax_clean.columns[c],tax_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (tax_dirty.columns[c],tax_dirty.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        \n",
    "        if(dirty_cell!=clean_cell):\n",
    "            detector_list_tax.append([all_context_dirty,single_context_dirty,1])\n",
    "        else:\n",
    "            detector_list_tax.append([all_context_dirty,single_context_dirty,0])\n",
    "        \n",
    "        # if(dirty_cell!=clean_cell):\n",
    "        #     detector_list_tax.append([all_context_dirty,single_context_dirty,1])\n",
    "        #     detector_list_tax.append([all_context_clean,single_context_clean,0])\n",
    "        # else:\n",
    "        #     detector_list_tax.append([all_context_dirty,single_context_dirty,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COL f_name VAL Jun''ichi COL l_name VAL Kur''i...</td>\n",
       "      <td>COL l_name VAL Kur''ita</td>\n",
       "      <td>1</td>\n",
       "      <td>l_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COL f_name VAL Jun''ichi COL l_name VAL Kur'it...</td>\n",
       "      <td>COL l_name VAL Kur'ita</td>\n",
       "      <td>0</td>\n",
       "      <td>l_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COL f_name VAL Jun''ichi COL l_name VAL Kurita...</td>\n",
       "      <td>COL city VAL BALTIC-*</td>\n",
       "      <td>1</td>\n",
       "      <td>city</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COL f_name VAL Jun''ichi COL l_name VAL Kurita...</td>\n",
       "      <td>COL city VAL BALTIC</td>\n",
       "      <td>0</td>\n",
       "      <td>city</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COL f_name VAL Jun''ichi COL l_name VAL Kurita...</td>\n",
       "      <td>COL state VAL CT-*</td>\n",
       "      <td>1</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62667</th>\n",
       "      <td>COL titleType VAL tvMovie COL title VAL Charac...</td>\n",
       "      <td>COL genres VAL Comedy</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62668</th>\n",
       "      <td>COL titleType VAL txMoxie COL title VAL Passio...</td>\n",
       "      <td>COL titleType VAL txMoxie</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62669</th>\n",
       "      <td>COL titleType VAL tvMovie COL title VAL Passio...</td>\n",
       "      <td>COL titleType VAL tvMovie</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62670</th>\n",
       "      <td>COL titleType VAL txMoxie COL title VAL Passio...</td>\n",
       "      <td>COL title VAL Passion</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62671</th>\n",
       "      <td>COL titleType VAL txMoxie COL title VAL Passio...</td>\n",
       "      <td>COL startYear VAL 1991</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62672 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0  \\\n",
       "0      COL f_name VAL Jun''ichi COL l_name VAL Kur''i...   \n",
       "1      COL f_name VAL Jun''ichi COL l_name VAL Kur'it...   \n",
       "2      COL f_name VAL Jun''ichi COL l_name VAL Kurita...   \n",
       "3      COL f_name VAL Jun''ichi COL l_name VAL Kurita...   \n",
       "4      COL f_name VAL Jun''ichi COL l_name VAL Kurita...   \n",
       "...                                                  ...   \n",
       "62667  COL titleType VAL tvMovie COL title VAL Charac...   \n",
       "62668  COL titleType VAL txMoxie COL title VAL Passio...   \n",
       "62669  COL titleType VAL tvMovie COL title VAL Passio...   \n",
       "62670  COL titleType VAL txMoxie COL title VAL Passio...   \n",
       "62671  COL titleType VAL txMoxie COL title VAL Passio...   \n",
       "\n",
       "                                1  2       3  \n",
       "0        COL l_name VAL Kur''ita   1  l_name  \n",
       "1         COL l_name VAL Kur'ita   0  l_name  \n",
       "2          COL city VAL BALTIC-*   1    city  \n",
       "3            COL city VAL BALTIC   0    city  \n",
       "4             COL state VAL CT-*   1   state  \n",
       "...                           ... ..     ...  \n",
       "62667      COL genres VAL Comedy   0    None  \n",
       "62668  COL titleType VAL txMoxie   1    None  \n",
       "62669  COL titleType VAL tvMovie   0    None  \n",
       "62670      COL title VAL Passion   0    None  \n",
       "62671     COL startYear VAL 1991   0    None  \n",
       "\n",
       "[62672 rows x 4 columns]"
      ]
     },
     "execution_count": 2525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(detector_list_tax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 980,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87bcefd232cd4e43971cf1e70431a6f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3895 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Data Augmentation for Tax\n",
    "detector_list_tax = []\n",
    "def Rand_Insert(cell):\n",
    "    length = len(cell)\n",
    "    a = np.random.choice(length)\n",
    "    return cell[:a] + \"'\" + cell[a:]\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "# for label_tuple in tqdm(tax_label_index[:5]):\n",
    "# for label_tuple in tqdm(tax_label_index[:5]):\n",
    "for label_tuple in tqdm(tax_error):\n",
    "    for i in range(len(tax_dirty.columns)):\n",
    "        x = tax_dirty.columns[i] \n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = tax_dirty.iloc[label_tuple].copy()\n",
    "        dirty_context = tax_dirty.iloc[label_tuple].copy()\n",
    "        clean_cell = tax_dirty.iloc[label_tuple,i]\n",
    "        if(not Tax_Row_Detection(x,clean_cell)) and (x in ['city','state','single_exemp','child_exemp','f_name','l_name']):\n",
    "            if(x in ['f_name','l_name']):\n",
    "                clean_cell = Rand_Insert(clean_cell)\n",
    "            dirty_cell = Tax_Row_Generate(x,clean_cell)\n",
    "            # if(clean_cell!=dirty_cell):\n",
    "            # dirty_cell = tax_dirty.iloc[label_tuple,i]\n",
    "            single_context_clean = 'COL %s VAL %s ' % (x,clean_cell)\n",
    "            single_context_dirty = 'COL %s VAL %s ' % (x,dirty_cell)\n",
    "            clean_context[x] = clean_cell\n",
    "            dirty_context[x] = dirty_cell\n",
    "            for c in range(len(tax_dirty.columns)):\n",
    "                all_context_clean += 'COL %s VAL %s ' % (tax_dirty.columns[c],clean_context[c])\n",
    "                all_context_dirty += 'COL %s VAL %s ' % (tax_dirty.columns[c],dirty_context[c])\n",
    "            # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "            # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "            # detector_list.append([single_context_clean,0])        \n",
    "            # detector_list.append([all_context_clean,0])\n",
    "            \n",
    "            if(dirty_cell!=clean_cell) and (clean_cell!=False):\n",
    "                detector_list_tax.append([all_context_dirty,single_context_dirty,1,x])\n",
    "                detector_list_tax.append([all_context_clean,single_context_clean,0,x])\n",
    "        \n",
    "        # if(dirty_cell!=clean_cell):\n",
    "        #     detector_list_tax.append([all_context_dirty,single_context_dirty,1])\n",
    "        #     detector_list_tax.append([all_context_clean,single_context_clean,0])\n",
    "        # else:\n",
    "        #     detector_list_tax.append([all_context_dirty,single_context_dirty,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 969,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 969,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tax_Row_Detection('f_name',\"Eli''sa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 978,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "single_exemp    7390\n",
       "city            7388\n",
       "child_exemp     7388\n",
       "state           7386\n",
       "f_name          6700\n",
       "l_name          5012\n",
       "Name: 3, dtype: int64"
      ]
     },
     "execution_count": 978,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(detector_list_tax)[3].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 981,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2716</th>\n",
       "      <td>COL f_name VAL Elis''a COL l_name VAL d''Argen...</td>\n",
       "      <td>COL f_name VAL Elis''a</td>\n",
       "      <td>1</td>\n",
       "      <td>f_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2726</th>\n",
       "      <td>COL f_name VAL Ja''ehyung COL l_name VAL D''Am...</td>\n",
       "      <td>COL f_name VAL Ja''ehyung</td>\n",
       "      <td>1</td>\n",
       "      <td>f_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2736</th>\n",
       "      <td>COL f_name VAL ''Oystein COL l_name VAL Give''...</td>\n",
       "      <td>COL f_name VAL ''Oystein</td>\n",
       "      <td>1</td>\n",
       "      <td>f_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>COL f_name VAL ''Fukumi COL l_name VAL O''Boyl...</td>\n",
       "      <td>COL f_name VAL ''Fukumi</td>\n",
       "      <td>1</td>\n",
       "      <td>f_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756</th>\n",
       "      <td>COL f_name VAL Yen''nun COL l_name VAL O''Dono...</td>\n",
       "      <td>COL f_name VAL Yen''nun</td>\n",
       "      <td>1</td>\n",
       "      <td>f_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41194</th>\n",
       "      <td>COL f_name VAL Just''in COL l_name VAL d''Acie...</td>\n",
       "      <td>COL f_name VAL Just''in</td>\n",
       "      <td>1</td>\n",
       "      <td>f_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41204</th>\n",
       "      <td>COL f_name VAL ''Tanka COL l_name VAL Narin''a...</td>\n",
       "      <td>COL f_name VAL ''Tanka</td>\n",
       "      <td>1</td>\n",
       "      <td>f_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41214</th>\n",
       "      <td>COL f_name VAL Ce''zar COL l_name VAL Giese CO...</td>\n",
       "      <td>COL f_name VAL Ce''zar</td>\n",
       "      <td>1</td>\n",
       "      <td>f_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41234</th>\n",
       "      <td>COL f_name VAL Th''om COL l_name VAL Weinert C...</td>\n",
       "      <td>COL f_name VAL Th''om</td>\n",
       "      <td>1</td>\n",
       "      <td>f_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41254</th>\n",
       "      <td>COL f_name VAL Wahee''d COL l_name VAL L''Ecuy...</td>\n",
       "      <td>COL f_name VAL Wahee''d</td>\n",
       "      <td>1</td>\n",
       "      <td>f_name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3350 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0  \\\n",
       "2716   COL f_name VAL Elis''a COL l_name VAL d''Argen...   \n",
       "2726   COL f_name VAL Ja''ehyung COL l_name VAL D''Am...   \n",
       "2736   COL f_name VAL ''Oystein COL l_name VAL Give''...   \n",
       "2746   COL f_name VAL ''Fukumi COL l_name VAL O''Boyl...   \n",
       "2756   COL f_name VAL Yen''nun COL l_name VAL O''Dono...   \n",
       "...                                                  ...   \n",
       "41194  COL f_name VAL Just''in COL l_name VAL d''Acie...   \n",
       "41204  COL f_name VAL ''Tanka COL l_name VAL Narin''a...   \n",
       "41214  COL f_name VAL Ce''zar COL l_name VAL Giese CO...   \n",
       "41234  COL f_name VAL Th''om COL l_name VAL Weinert C...   \n",
       "41254  COL f_name VAL Wahee''d COL l_name VAL L''Ecuy...   \n",
       "\n",
       "                                1  2       3  \n",
       "2716      COL f_name VAL Elis''a   1  f_name  \n",
       "2726   COL f_name VAL Ja''ehyung   1  f_name  \n",
       "2736    COL f_name VAL ''Oystein   1  f_name  \n",
       "2746     COL f_name VAL ''Fukumi   1  f_name  \n",
       "2756     COL f_name VAL Yen''nun   1  f_name  \n",
       "...                           ... ..     ...  \n",
       "41194    COL f_name VAL Just''in   1  f_name  \n",
       "41204     COL f_name VAL ''Tanka   1  f_name  \n",
       "41214     COL f_name VAL Ce''zar   1  f_name  \n",
       "41234      COL f_name VAL Th''om   1  f_name  \n",
       "41254    COL f_name VAL Wahee''d   1  f_name  \n",
       "\n",
       "[3350 rows x 4 columns]"
      ]
     },
     "execution_count": 981,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pd.DataFrame(detector_list_tax)\n",
    "output[(output[1].str.contains('f_name')) & output[2]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 983,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_tax).sample(n=1000).to_csv('datasets/tax/detector/multi-view/train_aug_few.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['marital_status', 'single_exemp', 'married_exemp'], dtype='object')"
      ]
     },
     "execution_count": 915,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_dirty.columns[[9,14]]\n",
    "tax_dirty.columns[[8,12,13]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_tax).to_csv('datasets/tax/detector/multi-view/train_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_tax).to_csv('datasets/tax/detector/multi-view/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From Here We Tackle With Flight Dataset Function Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 990,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['f_name', 'l_name', 'gender', 'area_code', 'phone', 'city', 'state',\n",
       "       'zip', 'marital_status', 'has_child', 'salary', 'rate', 'single_exemp',\n",
       "       'married_exemp', 'child_exemp'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 990,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_clean.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zip,city \n",
    "zip,state \n",
    "[city,state],zip \n",
    "[single_exemp,married_exemp],zip\n",
    "child_exemp,has_child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 993,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_matrix_tax = np.array(tax_clean!=tax_dirty).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 997,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  5,  6, 12, 14])"
      ]
     },
     "execution_count": 997,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(input_matrix_tax[tax_label_index[:5]].sum(axis=0)!=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 996,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['zip', 'marital_status', 'has_child'], dtype='object')"
      ]
     },
     "execution_count": 996,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_clean.columns[[7,8,9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 7, 0, 0, 0, 2, 4, 3, 4, 8, 0, 0, 2, 0, 2])"
      ]
     },
     "execution_count": 994,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix_tax[tax_label_index].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1015,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d3f3de452c4fefbc4c03357eb734e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dirty_index_col = np.where(input_matrix_tax[tax_label_index].sum(axis=0)!=0)[0]\n",
    "tax_correct_15 = tax_dirty.copy()\n",
    "correct_index_col = np.where(input_matrix_tax[tax_label_index[:15]].sum(axis=0)!=0)[0]\n",
    "for c in tqdm(correct_index_col):\n",
    "    x = tax_dirty.columns[c]\n",
    "    if x in (['f_name','l_name','city','state','single_exemp','child_exemp']):\n",
    "        for i in range(200000):\n",
    "            value = tax_dirty.iloc[i,c]\n",
    "            tax_correct_15.iloc[i,c] = Tax_Row_Correction(x,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1017,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0, 400, 400, 200, 200,   0,   0,   0,\n",
       "         0,   0])"
      ]
     },
     "execution_count": 1017,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tax_correct_15!=tax_clean).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1002,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_name</th>\n",
       "      <th>l_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>area_code</th>\n",
       "      <th>phone</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>has_child</th>\n",
       "      <th>salary</th>\n",
       "      <th>rate</th>\n",
       "      <th>single_exemp</th>\n",
       "      <th>married_exemp</th>\n",
       "      <th>child_exemp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [f_name, l_name, gender, area_code, phone, city, state, zip, marital_status, has_child, salary, rate, single_exemp, married_exemp, child_exemp]\n",
       "Index: []"
      ]
     },
     "execution_count": 1002,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_correct_5[tax_correct_5['f_name'].str.contains(\"''\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1991,  2029,  2097, ..., 50757, 50758, 50759])"
      ]
     },
     "execution_count": 1011,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.array(tax_correct_10!=tax_clean).sum(axis=1)!=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1531,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_correct_15_3.iloc[:,:-1].to_csv('datasets/tax/FD/correction_20_FD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1530,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2951/2951 [00:02<00:00, 1402.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9538357094365241, 0.947403910991234, 0.9506089309878214)"
      ]
     },
     "execution_count": 1530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_Data_Error = 0\n",
    "All_Fixed_Error = 0\n",
    "Correct_Fixed_Error = 0\n",
    "tax_correct = tax_correct_15_3.iloc[:,:-1]\n",
    "ineq_0 = np.where(np.array(tax_correct!=tax_clean).sum(axis=1)!=0)[0]\n",
    "ineq_1 = np.where(np.array(tax_dirty!=tax_clean).sum(axis=1)!=0)[0]\n",
    "ineq = list(set(np.concatenate([ineq_0,ineq_1])))\n",
    "for i in tqdm(ineq):\n",
    "    for j in range(15):\n",
    "        dirty_cell = tax_dirty.iloc[i,j]\n",
    "        clean_cell = tax_clean.iloc[i,j]\n",
    "        correct_cell = tax_correct.iloc[i,j]\n",
    "        if(correct_cell!=dirty_cell):\n",
    "            All_Fixed_Error += 1\n",
    "        if(clean_cell!=dirty_cell):\n",
    "            All_Data_Error += 1\n",
    "            if(correct_cell==clean_cell):\n",
    "                Correct_Fixed_Error += 1\n",
    "Precision_hospital = Correct_Fixed_Error / All_Fixed_Error\n",
    "Recall_hospital = Correct_Fixed_Error / All_Data_Error\n",
    "F1_hospital = (2 * Precision_hospital * Recall_hospital) / (Precision_hospital + Recall_hospital)\n",
    "Precision_hospital,Recall_hospital,F1_hospital"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on Tax Dataset Repair on T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_Data_Error = 0\n",
    "All_Fixed_Error = 0\n",
    "Correct_Fixed_Error = 0\n",
    "tax_correct = tax_correct_15_3.iloc[:,:-1]\n",
    "ineq_0 = np.where(np.array(tax_correct!=tax_clean).sum(axis=1)!=0)[0]\n",
    "ineq_1 = np.where(np.array(tax_dirty!=tax_clean).sum(axis=1)!=0)[0]\n",
    "ineq = list(set(np.concatenate([ineq_0,ineq_1])))\n",
    "for i in tqdm(ineq):\n",
    "    for j in range(15):\n",
    "        dirty_cell = tax_dirty.iloc[i,j]\n",
    "        clean_cell = tax_clean.iloc[i,j]\n",
    "        correct_cell = tax_correct.iloc[i,j]\n",
    "        if(correct_cell!=dirty_cell):\n",
    "            All_Fixed_Error += 1\n",
    "        if(clean_cell!=dirty_cell):\n",
    "            All_Data_Error += 1\n",
    "            if(correct_cell==clean_cell):\n",
    "                Correct_Fixed_Error += 1\n",
    "Precision_hospital = Correct_Fixed_Error / All_Fixed_Error\n",
    "Recall_hospital = Correct_Fixed_Error / All_Data_Error\n",
    "F1_hospital = (2 * Precision_hospital * Recall_hospital) / (Precision_hospital + Recall_hospital)\n",
    "Precision_hospital,Recall_hospital,F1_hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1533,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:26<00:00,  1.78s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0009887282973972044, 1.0, 0.001975503358722037)"
      ]
     },
     "execution_count": 1533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_error_detection_metrics(clean_table, dirty_table, predict_table):\n",
    "    # 初始化计数器\n",
    "    TP = FP = FN = 0\n",
    "    \n",
    "    # 遍历表格的每个单元格\n",
    "    for column in tqdm(clean_table.columns):\n",
    "        for row in clean_table.index:\n",
    "            clean_value = clean_table.at[row, column]\n",
    "            dirty_value = dirty_table.at[row, column]\n",
    "            predicted_error = predict_table.at[row, column]\n",
    "            \n",
    "            # 检查真正的错误（dirty和clean不同）\n",
    "            actual_error = clean_value != dirty_value\n",
    "            \n",
    "            # 更新计数器\n",
    "            if predicted_error and actual_error:\n",
    "                TP += 1\n",
    "            elif predicted_error and not actual_error:\n",
    "                FP += 1\n",
    "            elif not predicted_error and actual_error:\n",
    "                FN += 1\n",
    "    \n",
    "    # 计算指标\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1\n",
    "calculate_error_detection_metrics(tax_clean,tax_dirty,tax_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9925322471147319"
      ]
     },
     "execution_count": 1537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = np.array(tax_dirty.iloc[ineq]!=tax_correct.iloc[ineq]).flatten()\n",
    "gt = np.array(tax_dirty.iloc[ineq]!=tax_clean.iloc[ineq]).flatten()\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "precision_score(y_true=gt,y_pred=predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input \n",
      "\n",
      "[['May-51', '51-5'], ['Jun-70', '70-6'], ['Jun-93', '93-6'], ['11-Sep', '3111-9'], ['Aug-91', '91-8'], ['Jul-72', '72-7'], ['Jul-71', '71-7'], ['Apr-41', '3541-4']]\n",
      "\n",
      "are [clean,dirty] cell pairs from table rayyan column article_pagination, and ['1187-9', '', '283-4', '714-9 ST  - [Noninvasive prenatal diagnosis of trisomy 21, 18 and 13 using cell-free fetal DNA]-', '835-40', '185-91', '163-7', '43-49', '1158-78', '158-61', '317-325', '10213-10224', '711-5', 'S40', '1245-50', '919-21', '185-90', 'Cd004797', '257-8', '991-5', '1304-16', '512-9 ST  - A performance improvement process to tackle tachysystole-', '1530-9', '2496-502', '785-794'] are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not. Input and output are all string format.\n"
     ]
    }
   ],
   "source": [
    "print(\"The input \\n\\n[['May-51', '51-5'], ['Jun-70', '70-6'], ['Jun-93', '93-6'], ['11-Sep', '3111-9'], ['Aug-91', '91-8'], ['Jul-72', '72-7'], ['Jul-71', '71-7'], ['Apr-41', '3541-4']]\\n\\nare [clean,dirty] cell pairs from table rayyan column article_pagination, and ['1187-9', '', '283-4', '714-9 ST  - [Noninvasive prenatal diagnosis of trisomy 21, 18 and 13 using cell-free fetal DNA]-', '835-40', '185-91', '163-7', '43-49', '1158-78', '158-61', '317-325', '10213-10224', '711-5', 'S40', '1245-50', '919-21', '185-90', 'Cd004797', '257-8', '991-5', '1304-16', '512-9 ST  - A performance improvement process to tackle tachysystole-', '1530-9', '2496-502', '785-794'] are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not. Input and output are all string format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1579,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_correct_20.to_csv('datasets/beers/detector/beer_correction_20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1549,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8411552346570397 0.9831223628691983 0.9066147859922179\n"
     ]
    }
   ],
   "source": [
    "predict = np.array(rayyan_dirty!=rayyan_correction_final).flatten().astype(int)\n",
    "gt = np.array(rayyan_dirty!=rayyan_clean).flatten().astype(int)\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "print(precision_score(y_true=gt,y_pred=predict),recall_score(y_true=gt,y_pred=predict),f1_score(y_true=gt,y_pred=predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1577,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9979154258487195 0.998212689901698 0.9980640357408788\n"
     ]
    }
   ],
   "source": [
    "predict = np.array(beer_dirty!=beer_correct_20).flatten().astype(int)\n",
    "gt = np.array(beer_dirty!=beer_clean).flatten().astype(int)\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "print(precision_score(y_true=gt,y_pred=predict),recall_score(y_true=gt,y_pred=predict),f1_score(y_true=gt,y_pred=predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1550,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'beer_correction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/yanmy/raha/raha-master/detector.ipynb 单元格 448\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1650sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m predict \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(beer_dirty\u001b[39m!=\u001b[39mbeer_correction)\u001b[39m.\u001b[39mflatten()\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1650sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m gt \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(beer_dirty\u001b[39m!=\u001b[39mbeer_clean)\u001b[39m.\u001b[39mflatten()\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1650sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m precision_score,recall_score,f1_score\n",
      "\u001b[0;31mNameError\u001b[0m: name 'beer_correction' is not defined"
     ]
    }
   ],
   "source": [
    "predict = np.array(beer_dirty!=beer_correction).flatten().astype(int)\n",
    "gt = np.array(beer_dirty!=beer_clean).flatten().astype(int)\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "print(precision_score(y_true=gt,y_pred=predict),recall_score(y_true=gt,y_pred=predict),f1_score(y_true=gt,y_pred=predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1617,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  2,  4,  3,  0,  0,  3,  0,  0,  0,  8,  2,  2,  0,  3,  6, 14,\n",
       "       13,  5,  6])"
      ]
     },
     "execution_count": 1617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(hospital_correction.astype(str)!=hospital_clean).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1618,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.9567779960707269 0.9779116465863453\n"
     ]
    }
   ],
   "source": [
    "predict = np.array(hospital_dirty!=hospital_correction.astype(str)).flatten().astype(int)\n",
    "gt = np.array(hospital_dirty!=hospital_clean).flatten().astype(int)\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "print(precision_score(y_true=gt,y_pred=predict),recall_score(y_true=gt,y_pred=predict),f1_score(y_true=gt,y_pred=predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1581,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5357507082152975 0.9609756097560975 0.687959257911968\n"
     ]
    }
   ],
   "source": [
    "predict = np.array(flight_dirty!=flight_correct_20).flatten().astype(int)\n",
    "gt = np.array(flight_dirty!=flight_clean).flatten().astype(int)\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "print(precision_score(y_true=gt,y_pred=predict),recall_score(y_true=gt,y_pred=predict),f1_score(y_true=gt,y_pred=predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1605,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tuple_id</th>\n",
       "      <th>src</th>\n",
       "      <th>flight</th>\n",
       "      <th>sched_dep_time</th>\n",
       "      <th>act_dep_time</th>\n",
       "      <th>sched_arr_time</th>\n",
       "      <th>act_arr_time</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:16 a.m.</td>\n",
       "      <td>9:40 a.m.</td>\n",
       "      <td>9:32 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-1733-ORD-PHX</td>\n",
       "      <td>7:45 p.m.</td>\n",
       "      <td>7:58 p.m.</td>\n",
       "      <td>10:30 p.m.</td>\n",
       "      <td>10:30 p.m.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-1640-MIA-MCO</td>\n",
       "      <td>6:30 p.m.</td>\n",
       "      <td>6:30 p.m.</td>\n",
       "      <td>7:25 p.m.</td>\n",
       "      <td>7:25 p.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-518-MIA-JFK</td>\n",
       "      <td>6:40 a.m.</td>\n",
       "      <td>6:54 a.m.</td>\n",
       "      <td>9:25 a.m.</td>\n",
       "      <td>9:28 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-3756-ORD-SLC</td>\n",
       "      <td>12:15 p.m.</td>\n",
       "      <td>12:41 p.m.</td>\n",
       "      <td>2:45 p.m.</td>\n",
       "      <td>2:50 p.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2371</th>\n",
       "      <td>2372</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>UA-3099-PHX-PHL</td>\n",
       "      <td>11:55 a.m.</td>\n",
       "      <td>11:55 a.m.</td>\n",
       "      <td>6:17 p.m.</td>\n",
       "      <td>5:38 p.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2372</th>\n",
       "      <td>2373</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-4198-ORD-CLE</td>\n",
       "      <td>10:40 a.m.</td>\n",
       "      <td>10:54 a.m.</td>\n",
       "      <td>12:55 p.m.</td>\n",
       "      <td>12:50 p.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2373</th>\n",
       "      <td>2374</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>CO-45-EWR-MIA</td>\n",
       "      <td>4:00 p.m.</td>\n",
       "      <td>3:58 p.m.</td>\n",
       "      <td>7:05 p.m.</td>\n",
       "      <td>6:36 p.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2374</th>\n",
       "      <td>2375</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-3809-PHX-LAX</td>\n",
       "      <td>6:00 a.m.</td>\n",
       "      <td>6:10 a.m.</td>\n",
       "      <td>6:40 a.m.</td>\n",
       "      <td>6:19 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2375</th>\n",
       "      <td>2376</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-59-JFK-SFO</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:39 a.m.</td>\n",
       "      <td>10:45 a.m.</td>\n",
       "      <td>11:12 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2376 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     tuple_id                   src           flight sched_dep_time  \\\n",
       "0           1                    aa  AA-3859-IAH-ORD      7:10 a.m.   \n",
       "1           2                    aa  AA-1733-ORD-PHX      7:45 p.m.   \n",
       "2           3                    aa  AA-1640-MIA-MCO      6:30 p.m.   \n",
       "3           4                    aa   AA-518-MIA-JFK      6:40 a.m.   \n",
       "4           5                    aa  AA-3756-ORD-SLC     12:15 p.m.   \n",
       "...       ...                   ...              ...            ...   \n",
       "2371     2372  world-flight-tracker  UA-3099-PHX-PHL     11:55 a.m.   \n",
       "2372     2373  world-flight-tracker  AA-4198-ORD-CLE     10:40 a.m.   \n",
       "2373     2374  world-flight-tracker    CO-45-EWR-MIA      4:00 p.m.   \n",
       "2374     2375  world-flight-tracker  AA-3809-PHX-LAX      6:00 a.m.   \n",
       "2375     2376  world-flight-tracker    AA-59-JFK-SFO      7:10 a.m.   \n",
       "\n",
       "     act_dep_time sched_arr_time act_arr_time  count  \n",
       "0       7:16 a.m.      9:40 a.m.    9:32 a.m.      0  \n",
       "1       7:58 p.m.     10:30 p.m.   10:30 p.m.      1  \n",
       "2       6:30 p.m.      7:25 p.m.    7:25 p.m.      2  \n",
       "3       6:54 a.m.      9:25 a.m.    9:28 a.m.      0  \n",
       "4      12:41 p.m.      2:45 p.m.    2:50 p.m.      0  \n",
       "...           ...            ...          ...    ...  \n",
       "2371   11:55 a.m.      6:17 p.m.    5:38 p.m.      0  \n",
       "2372   10:54 a.m.     12:55 p.m.   12:50 p.m.      0  \n",
       "2373    3:58 p.m.      7:05 p.m.    6:36 p.m.      0  \n",
       "2374    6:10 a.m.      6:40 a.m.    6:19 a.m.      0  \n",
       "2375    7:39 a.m.     10:45 a.m.   11:12 a.m.      0  \n",
       "\n",
       "[2376 rows x 8 columns]"
      ]
     },
     "execution_count": 1605,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_dirty_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1606,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9940132122213047 0.9786585365853658 0.9862761163457598\n"
     ]
    }
   ],
   "source": [
    "predict = np.array(flight_dirty!=flight_dirty_clean.iloc[:,:-1]).flatten().astype(int)\n",
    "gt = np.array(flight_dirty!=flight_clean).flatten().astype(int)\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "print(precision_score(y_true=gt,y_pred=predict),recall_score(y_true=gt,y_pred=predict),f1_score(y_true=gt,y_pred=predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1607,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hospital_correction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/yanmy/raha/raha-master/detector.ipynb 单元格 459\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z2002sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m hospital_correction\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hospital_correction' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9925322471147319 0.98583951449764 0.9891745602165087\n"
     ]
    }
   ],
   "source": [
    "print(precision_score(y_true=gt,y_pred=predict),recall_score(y_true=gt,y_pred=predict),f1_score(y_true=gt,y_pred=predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1.0, 0.3614295347269049, 0.5309559187716691) 5 label no FD\n",
    "(0.9396092362344582, 0.7134187457855697, 0.8110387121502491) 5 label with FD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_correct_15.to_csv('datasets/tax/FD/correct_15.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head</th>\n",
       "      <th>relation</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [head, relation, target]\n",
       "Index: []"
      ]
     },
     "execution_count": 1021,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_subgraph_marriaga = pd.DataFrame(columns=['head','relation','target'])\n",
    "tax_subgraph_marriaga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['f_name', 'l_name', 'gender', 'area_code', 'phone', 'city', 'state',\n",
       "       'zip', 'marital_status', 'has_child', 'salary', 'rate', 'single_exemp',\n",
       "       'married_exemp', 'child_exemp'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1022,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1186,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_correct_15['index'] = tax_correct_15.index\n",
    "tax_subgraph_marriaga = pd.DataFrame(columns=['head','relation','target'])\n",
    "for x in ['marital_status','single_exemp','married_exemp','child_exemp','has_child']:\n",
    "    add = pd.DataFrame(columns=['head','relation','target'])\n",
    "    add['head'] = tax_correct_15['index']\n",
    "    add['relation'] = x\n",
    "    add['target'] = tax_correct_15[x]\n",
    "    tax_subgraph_marriaga = pd.concat([tax_subgraph_marriaga,add])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tax_correct_15['index'] = tax_correct_15.index\n",
    "tax_subgraph_marriaga = pd.DataFrame(columns=['head','relation','target'])\n",
    "for x in ['city','state','zip']:\n",
    "    add = pd.DataFrame(columns=['head','relation','target'])\n",
    "    add['head'] = tax_correct_15['index']\n",
    "    add['relation'] = x\n",
    "    add['target'] = tax_correct_15[x]\n",
    "    tax_subgraph_marriaga = pd.concat([tax_subgraph_marriaga,add])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1025,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_subgraph_marriaga.to_csv('datasets/tax/FD/tax_marriage.txt',sep='\\t',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1029,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_subgraph_marriaga.to_csv('datasets/tax/FD/tax_zip.txt',sep='\\t',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1189,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_subgraph_marriaga = tax_subgraph_marriaga.sample(frac=1,random_state=42)\n",
    "tax_subgraph_marriaga.to_csv('/home/yanmy/PyTorch-BigGraph-main/data/tax-marriage/tax-marriage-train.txt',sep='\\t',header=None)\n",
    "tax_subgraph_marriaga.iloc[700000:800000].to_csv('/home/yanmy/PyTorch-BigGraph-main/data/tax-marriage/tax-marriage-valid.txt',sep='\\t',header=None)\n",
    "tax_subgraph_marriaga.iloc[800000:].to_csv('/home/yanmy/PyTorch-BigGraph-main/data/tax-marriage/tax-marriage-test.txt',sep='\\t',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1178,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_subgraph_marriaga['target'] = tax_subgraph_marriaga['target'].str.replace(' ','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1185,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_subgraph_marriaga = tax_subgraph_marriaga.sample(frac=1,random_state=42)\n",
    "tax_subgraph_marriaga.to_csv('/home/yanmy/PyTorch-BigGraph-main/data/tax-zip/tax-zip-train.txt',sep='\\t',header=None,index=False)\n",
    "tax_subgraph_marriaga.iloc[420000:480000].to_csv('/home/yanmy/PyTorch-BigGraph-main/data/tax-zip/tax-zip-valid.txt',sep='\\t',header=None,index=False)\n",
    "tax_subgraph_marriaga.iloc[480000:].to_csv('/home/yanmy/PyTorch-BigGraph-main/data/tax-zip/tax-zip-test.txt',sep='\\t',header=None,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1077,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flight_dirty.columns[-4:]\n",
    "flight_correct = flight_dirty.copy()\n",
    "for i in range(len(flight_correct)):\n",
    "    for j in [-4,-3,-2,-1]:\n",
    "        value = flight_dirty.iloc[i,j]\n",
    "        if not is_clean_time_format(value):\n",
    "            flight_correct.iloc[i,j] = 'empty'\n",
    "        else:\n",
    "            flight_correct.iloc[i,j] = value.replace(' ','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2, ..., 2373, 2374, 2375])"
      ]
     },
     "execution_count": 1446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1447,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tax_correct_15['index'] = tax_correct_15.index\n",
    "flight_correct_sub = flight_correct.iloc[clean_index]\n",
    "flight_subgraph = pd.DataFrame(columns=['head','relation','target'])\n",
    "for c in range(1,7,1):\n",
    "    x = flight_dirty.columns[c]\n",
    "    add = pd.DataFrame(columns=['head','relation','target'])\n",
    "    add['head'] = flight_correct_sub['tuple_id']\n",
    "    add['relation'] = x\n",
    "    add['target'] = flight_correct_sub[x]\n",
    "    flight_subgraph = pd.concat([flight_subgraph,add])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1448,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in range(3,7,1):\n",
    "    x = flight_dirty.columns[c]\n",
    "    add = pd.DataFrame(columns=['head','relation','target'])\n",
    "    add['head'] = flight_correct_sub['flight']\n",
    "    add['relation'] = x\n",
    "    add['target'] = flight_correct_sub[x]\n",
    "    flight_subgraph = pd.concat([flight_subgraph,add])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head</th>\n",
       "      <th>relation</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>src</td>\n",
       "      <td>aa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>src</td>\n",
       "      <td>aa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>src</td>\n",
       "      <td>aa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>src</td>\n",
       "      <td>aa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>src</td>\n",
       "      <td>aa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2371</th>\n",
       "      <td>UA-3099-PHX-PHL</td>\n",
       "      <td>act_arr_time</td>\n",
       "      <td>5:38_p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2372</th>\n",
       "      <td>AA-4198-ORD-CLE</td>\n",
       "      <td>act_arr_time</td>\n",
       "      <td>12:50_p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2373</th>\n",
       "      <td>CO-45-EWR-MIA</td>\n",
       "      <td>act_arr_time</td>\n",
       "      <td>6:36_p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2374</th>\n",
       "      <td>AA-3809-PHX-LAX</td>\n",
       "      <td>act_arr_time</td>\n",
       "      <td>6:19_a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2375</th>\n",
       "      <td>AA-59-JFK-SFO</td>\n",
       "      <td>act_arr_time</td>\n",
       "      <td>11:12_a.m.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10690 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 head      relation      target\n",
       "0                   1           src          aa\n",
       "1                   2           src          aa\n",
       "2                   3           src          aa\n",
       "3                   4           src          aa\n",
       "4                   5           src          aa\n",
       "...               ...           ...         ...\n",
       "2371  UA-3099-PHX-PHL  act_arr_time   5:38_p.m.\n",
       "2372  AA-4198-ORD-CLE  act_arr_time  12:50_p.m.\n",
       "2373    CO-45-EWR-MIA  act_arr_time   6:36_p.m.\n",
       "2374  AA-3809-PHX-LAX  act_arr_time   6:19_a.m.\n",
       "2375    AA-59-JFK-SFO  act_arr_time  11:12_a.m.\n",
       "\n",
       "[10690 rows x 3 columns]"
      ]
     },
     "execution_count": 1449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1450,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flight_subgraph\n",
    "flight_subgraph = flight_subgraph.sample(frac=1,random_state=42)\n",
    "flight_subgraph.iloc[:10000].to_csv('/home/yanmy/PyTorch-BigGraph-main/data/flight/flight-train.txt',sep='\\t',header=None,index=False)\n",
    "flight_subgraph.iloc[10000:10300].to_csv('/home/yanmy/PyTorch-BigGraph-main/data/flight/flight-valid.txt',sep='\\t',header=None,index=False)\n",
    "flight_subgraph.iloc[10300:].to_csv('/home/yanmy/PyTorch-BigGraph-main/data/flight/flight-test.txt',sep='\\t',header=None,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "act_arr_time      2376\n",
       "act_dep_time      2376\n",
       "src               2376\n",
       "sched_dep_time    2376\n",
       "sched_arr_time    2376\n",
       "flight            2376\n",
       "Name: relation, dtype: int64"
      ]
     },
     "execution_count": 1052,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_subgraph['relation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "torchbiggraph_import_from_tsv \\\n",
    "  --lhs-col=0 --rel-col=1 --rhs-col=2 \\\n",
    "  torchbiggraph/examples/configs/tax_marriage_config_cpu.py \\\n",
    "  data/tax-marriage/tax-marriage-train.txt \\\n",
    "  data/tax-marriage/tax-marriage-valid.txt \\\n",
    "  data/tax-marriage/tax-marriage-test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "torchbiggraph_train \\\n",
    "  torchbiggraph/examples/configs/tax_marriage_config_gpu.py \\\n",
    "  -p edge_paths=data/tax-marriage/tax-marriage-train_partitioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "torchbiggraph_export_to_tsv \\\n",
    "  torchbiggraph/examples/configs/flight_config_cpu.py \\\n",
    "  --entities-output entity_embeddings.tsv \\\n",
    "  --relation-types-output relation_types_parameters.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1082,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "def load_embeddings(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as tsvfile:\n",
    "        reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "        \n",
    "        # Initialize an empty list to store embeddings and a dict for entity to index mapping\n",
    "        embeddings = []\n",
    "        entity_to_index = {}\n",
    "        \n",
    "        for i, line in enumerate(reader):\n",
    "            entity = line[0]\n",
    "            embedding = np.array(line[1:], dtype=np.float32)\n",
    "            \n",
    "            # Add the entity and its embedding to the dict and list\n",
    "            entity_to_index[entity] = i\n",
    "            embeddings.append(embedding)\n",
    "            \n",
    "        # Convert the list of embeddings to a NumPy array\n",
    "        embeddings_array = np.vstack(embeddings)\n",
    "        \n",
    "    return embeddings_array, entity_to_index\n",
    "\n",
    "# Usage\n",
    "file_path = '/home/yanmy/PyTorch-BigGraph-main/data/flight/entity_embeddings.tsv'  # Replace with your actual file path\n",
    "embeddings, entity_to_index = load_embeddings(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1086,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3063, 200)"
      ]
     },
     "execution_count": 1086,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1083,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1505"
      ]
     },
     "execution_count": 1083,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embeddings.shape\n",
    "# entity_to_index\n",
    "entity_to_index['5:49_a.m.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1191,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tuple_id</th>\n",
       "      <th>src</th>\n",
       "      <th>flight</th>\n",
       "      <th>sched_dep_time</th>\n",
       "      <th>act_dep_time</th>\n",
       "      <th>sched_arr_time</th>\n",
       "      <th>act_arr_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:16 a.m.</td>\n",
       "      <td>9:40 a.m.</td>\n",
       "      <td>9:32 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-1733-ORD-PHX</td>\n",
       "      <td>7:45 p.m.</td>\n",
       "      <td>7:58 p.m.</td>\n",
       "      <td>10:30 p.m.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-1640-MIA-MCO</td>\n",
       "      <td>6:30 p.m.</td>\n",
       "      <td></td>\n",
       "      <td>7:25 p.m.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-518-MIA-JFK</td>\n",
       "      <td>6:40 a.m.</td>\n",
       "      <td>6:54 a.m.</td>\n",
       "      <td>9:25 a.m.</td>\n",
       "      <td>9:28 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-3756-ORD-SLC</td>\n",
       "      <td>12:15 p.m.</td>\n",
       "      <td>12:41 p.m.</td>\n",
       "      <td>2:45 p.m.</td>\n",
       "      <td>2:50 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2371</th>\n",
       "      <td>2372</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>UA-3099-PHX-PHL</td>\n",
       "      <td>11:55 a.m.</td>\n",
       "      <td>11:43 a.m.</td>\n",
       "      <td>6:17 p.m.</td>\n",
       "      <td>5:38 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2372</th>\n",
       "      <td>2373</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-4198-ORD-CLE</td>\n",
       "      <td>10:40 a.m.</td>\n",
       "      <td>10:54 a.m.</td>\n",
       "      <td>12:55 p.m.</td>\n",
       "      <td>12:50 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2373</th>\n",
       "      <td>2374</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>CO-45-EWR-MIA</td>\n",
       "      <td>4:00 p.m.</td>\n",
       "      <td>3:58 p.m.</td>\n",
       "      <td>7:05 p.m.</td>\n",
       "      <td>6:36 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2374</th>\n",
       "      <td>2375</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-3809-PHX-LAX</td>\n",
       "      <td>6:00 a.m.</td>\n",
       "      <td>6:10 a.m.</td>\n",
       "      <td>6:40 a.m.</td>\n",
       "      <td>6:19 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2375</th>\n",
       "      <td>2376</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-59-JFK-SFO</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:39 a.m.</td>\n",
       "      <td>10:45 a.m.</td>\n",
       "      <td>11:12 a.m.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2376 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     tuple_id                   src           flight sched_dep_time  \\\n",
       "0           1                    aa  AA-3859-IAH-ORD      7:10 a.m.   \n",
       "1           2                    aa  AA-1733-ORD-PHX      7:45 p.m.   \n",
       "2           3                    aa  AA-1640-MIA-MCO      6:30 p.m.   \n",
       "3           4                    aa   AA-518-MIA-JFK      6:40 a.m.   \n",
       "4           5                    aa  AA-3756-ORD-SLC     12:15 p.m.   \n",
       "...       ...                   ...              ...            ...   \n",
       "2371     2372  world-flight-tracker  UA-3099-PHX-PHL     11:55 a.m.   \n",
       "2372     2373  world-flight-tracker  AA-4198-ORD-CLE     10:40 a.m.   \n",
       "2373     2374  world-flight-tracker    CO-45-EWR-MIA      4:00 p.m.   \n",
       "2374     2375  world-flight-tracker  AA-3809-PHX-LAX      6:00 a.m.   \n",
       "2375     2376  world-flight-tracker    AA-59-JFK-SFO      7:10 a.m.   \n",
       "\n",
       "     act_dep_time sched_arr_time act_arr_time  \n",
       "0       7:16 a.m.      9:40 a.m.    9:32 a.m.  \n",
       "1       7:58 p.m.     10:30 p.m.               \n",
       "2                      7:25 p.m.               \n",
       "3       6:54 a.m.      9:25 a.m.    9:28 a.m.  \n",
       "4      12:41 p.m.      2:45 p.m.    2:50 p.m.  \n",
       "...           ...            ...          ...  \n",
       "2371   11:43 a.m.      6:17 p.m.    5:38 p.m.  \n",
       "2372   10:54 a.m.     12:55 p.m.   12:50 p.m.  \n",
       "2373    3:58 p.m.      7:05 p.m.    6:36 p.m.  \n",
       "2374    6:10 a.m.      6:40 a.m.    6:19 a.m.  \n",
       "2375    7:39 a.m.     10:45 a.m.   11:12 a.m.  \n",
       "\n",
       "[2376 rows x 7 columns]"
      ]
     },
     "execution_count": 1191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_dirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1141,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_set = [c.replace(' ','_') for c in  flight_correct['sched_dep_time'].unique() if c!='empty']\n",
    "candidate_set.extend([str(c) for c in range(1,2377,1) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import h5py\n",
    "import torch\n",
    "from torchbiggraph.operators import ComplexDiagonalDynamicOperator,TranslationDynamicOperator\n",
    "from torchbiggraph.model import DotComparator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1453,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'10:31_p.m.' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/yanmy/raha/raha-master/detector.ipynb 单元格 468\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m relation \u001b[39m=\u001b[39m flight_correct\u001b[39m.\u001b[39mcolumns[i]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m all_possible_value \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(flight_correct\u001b[39m.\u001b[39miloc[:,i]\u001b[39m.\u001b[39munique())\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m output \u001b[39m=\u001b[39m Correction_FD_Graph_Trans(src_set\u001b[39m=\u001b[39;49mcluster_indicator_all,dest_set\u001b[39m=\u001b[39;49mall_possible_value,relation\u001b[39m=\u001b[39;49mrelation,model_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/home/yanmy/PyTorch-BigGraph-main/model/flight\u001b[39;49m\u001b[39m'\u001b[39;49m,data_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/home/yanmy/PyTorch-BigGraph-main/data/flight\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m flight_correct_20 \u001b[39m=\u001b[39m flight_correct\u001b[39m.\u001b[39mcopy()\n",
      "\u001b[1;32m/home/yanmy/raha/raha-master/detector.ipynb 单元格 468\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m comparator \u001b[39m=\u001b[39m DotComparator()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m src_entity_offset \u001b[39m=\u001b[39m [entity_names\u001b[39m.\u001b[39mindex(\u001b[39mstr\u001b[39m(c)) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m src_set]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m dest_entity_offset \u001b[39m=\u001b[39m [entity_names\u001b[39m.\u001b[39mindex(d) \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m dest_set]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m         \u001b[39m# with h5py.File(\"%s/embeddings_all_0.v50.h5\" % model_path, \"r\") as hf:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m         \u001b[39m# src_embedding = torch.from_numpy(hf[\"embeddings\"][src_entity_offset, :])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m         \u001b[39m# dest_embedding = torch.from_numpy(hf[\"embeddings\"][dest_entity_offset, :])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m src_embedding \u001b[39m=\u001b[39m src_embedding_all[src_entity_offset]\n",
      "\u001b[1;32m/home/yanmy/raha/raha-master/detector.ipynb 单元格 468\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m comparator \u001b[39m=\u001b[39m DotComparator()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m src_entity_offset \u001b[39m=\u001b[39m [entity_names\u001b[39m.\u001b[39mindex(\u001b[39mstr\u001b[39m(c)) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m src_set]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m dest_entity_offset \u001b[39m=\u001b[39m [entity_names\u001b[39m.\u001b[39;49mindex(d) \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m dest_set]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m         \u001b[39m# with h5py.File(\"%s/embeddings_all_0.v50.h5\" % model_path, \"r\") as hf:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m         \u001b[39m# src_embedding = torch.from_numpy(hf[\"embeddings\"][src_entity_offset, :])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m         \u001b[39m# dest_embedding = torch.from_numpy(hf[\"embeddings\"][dest_entity_offset, :])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m src_embedding \u001b[39m=\u001b[39m src_embedding_all[src_entity_offset]\n",
      "\u001b[0;31mValueError\u001b[0m: '10:31_p.m.' is not in list"
     ]
    }
   ],
   "source": [
    "def get_max_score_char(data):\n",
    "    # 假设第一列是可转换为浮点数的分数，第二列是字符\n",
    "    data = np.array(data)\n",
    "    scores = data[:, 0].astype(float)  # 转换分数列为浮点数\n",
    "    max_score_index = np.argmax(scores)  # 找到最大分数的索引\n",
    "    return data[max_score_index]  # 返回对应的字符\n",
    "def Correction_FD_Graph_Trans(src_set,dest_set,relation,model_path,data_path,vector_dimension=200):\n",
    "    final_output = []\n",
    "    with open(\"%s/entity_count_all_0.txt\" % data_path, \"rt\") as tf:\n",
    "        entity_count = int(tf.read().strip())\n",
    "    with open(\"%s/entity_names_all_0.json\" % data_path, \"rt\") as tf:\n",
    "        entity_names = json.load(tf)\n",
    "    with open(\"%s/dynamic_rel_names.json\" % data_path, \"rt\") as tf:\n",
    "        rel_type_names = json.load(tf)\n",
    "    # Load count of dynamic relations\n",
    "    with open(\"%s/dynamic_rel_count.txt\" % data_path, \"rt\") as tf:\n",
    "        dynamic_rel_count = int(tf.read().strip())\n",
    "    with h5py.File(\"%s/embeddings_all_0.v200.h5\" % model_path, \"r\") as hf:\n",
    "        src_embedding_all = torch.from_numpy(hf[\"embeddings\"][...])\n",
    "        dest_embedding_all = torch.from_numpy(hf[\"embeddings\"][...])\n",
    "    rel_type_index = rel_type_names.index(relation)\n",
    "    # Load the operator's state dict\n",
    "    with h5py.File(\"%s/model.v200.h5\" % model_path, \"r\") as hf:\n",
    "        operator_state_dict = {\n",
    "            # \"real\": torch.from_numpy(hf[\"model/relations/0/operator/rhs/translations\"][...]),\n",
    "            # \"imag\": torch.from_numpy(hf[\"model/relations/0/operator/rhs/translations\"][...]),\n",
    "            \"translations\": torch.from_numpy(hf[\"model/relations/0/operator/rhs/translations\"][...]),\n",
    "        }\n",
    "    operator = TranslationDynamicOperator(vector_dimension, dynamic_rel_count)\n",
    "    operator.load_state_dict(operator_state_dict)\n",
    "    comparator = DotComparator()\n",
    "    src_entity_offset = [entity_names.index(str(c)) for c in src_set]\n",
    "    dest_entity_offset = [entity_names.index(d) for d in dest_set]\n",
    "            # with h5py.File(\"%s/embeddings_all_0.v50.h5\" % model_path, \"r\") as hf:\n",
    "            # src_embedding = torch.from_numpy(hf[\"embeddings\"][src_entity_offset, :])\n",
    "            # dest_embedding = torch.from_numpy(hf[\"embeddings\"][dest_entity_offset, :])\n",
    "    src_embedding = src_embedding_all[src_entity_offset]\n",
    "    dest_embedding = dest_embedding_all[dest_entity_offset]\n",
    "    m = src_embedding.size(0)  # src_embedding 的实体数量\n",
    "    n = dest_embedding.size(0)  # dest_embedding 的实体数量\n",
    "    src_prepared = comparator.prepare(src_embedding.view(m, 1, vector_dimension))\n",
    "    dest_prepared = comparator.prepare(\n",
    "        operator(\n",
    "            dest_embedding.repeat(m, 1),  # 重复 dest_embedding m 次\n",
    "            torch.tensor([rel_type_index] * m).repeat_interleave(n),\n",
    "        ).view(m, n, vector_dimension)\n",
    "    )\n",
    "\n",
    "    # 执行比较\n",
    "    scores, _, _ = comparator(\n",
    "        src_prepared.expand(m, n, vector_dimension),  # 扩展 src_prepared 以匹配 dest_prepared 的形状\n",
    "        dest_prepared,\n",
    "        torch.empty(m, 0, vector_dimension),  # Left-hand side negatives, not needed\n",
    "        torch.empty(m, 0, vector_dimension),  # Right-hand side negatives, not needed\n",
    "    )\n",
    "    return scores.detach().numpy()\n",
    "i = 6\n",
    "relation = flight_correct.columns[i]\n",
    "all_possible_value = list(flight_correct.iloc[:,i].unique())\n",
    "output = Correction_FD_Graph_Trans(src_set=cluster_indicator_all,dest_set=all_possible_value,relation=relation,model_path='/home/yanmy/PyTorch-BigGraph-main/model/flight',data_path='/home/yanmy/PyTorch-BigGraph-main/data/flight')\n",
    "flight_correct_20 = flight_correct.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1431,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_importance_list = []\n",
    "flight_correct_20 = flight_correct.copy()\n",
    "flight_label_index_select = flight_label_index\n",
    "theta = 0.1\n",
    "for index,row in flight_correct_20.iterrows():\n",
    "    cluster_indicator = row['flight']\n",
    "    cluster_num = len(flight_correct_20[flight_correct_20['flight']==cluster_indicator])\n",
    "    count = 0\n",
    "    for x,y in row.items():\n",
    "        if(y=='empty'): ## Dirty Value\n",
    "            count += 1\n",
    "    if(index in flight_label_index):\n",
    "        flight_importance_list.append(1) ## Most Important\n",
    "    elif(count == 0): ## Secondary Important\n",
    "        flight_importance_list.append(1/cluster_num)\n",
    "    else:\n",
    "        # flight_importance_list.append(1/(theta * cluster_num))\n",
    "        flight_importance_list.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1445,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_index = np.where(np.array(flight_importance_list)!=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1434,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A Voting Mechanism, Process By Cluster\n",
    "# cluster_indicator_all = flight_correct['flight'].unique()\n",
    "# for cluster_indicator in cluster_indicator_all:\n",
    "## Labelling Mechanism\n",
    "def mark_presence(long_list, short_list):\n",
    "    short_set = set(short_list)  # 转换为集合以提高查找效率\n",
    "    return np.array([1 if (item in short_set) and (item!='empty') else 0 for item in long_list])\n",
    "\n",
    "# for f in flight_label_index_select:\n",
    "#     try:\n",
    "#         ground_truth = flight_clean.iloc[f,i].replace(' ','_')\n",
    "#         ground_truth_index = all_possible_value.index(ground_truth)\n",
    "#         update_score = np.zeros((1,len(all_possible_value)))\n",
    "#         update_score[0][ground_truth_index] = 16 ## Ground Truth Value\n",
    "#         output[f] = update_score\n",
    "#     except:\n",
    "#         continue\n",
    "# A Voting Mechanism, Process By Cluster\n",
    "cluster_indicator_all = flight_correct['flight'].unique()\n",
    "for cluster_indicator in cluster_indicator_all:\n",
    "    cluster_index = flight_correct[flight_correct['flight']==cluster_indicator].index\n",
    "    label_overlap = set(list(cluster_index)).intersection(set(flight_label_index_select))\n",
    "    if(len(label_overlap)==0): ## No Ground Truth\n",
    "        apperance_list = flight_correct.iloc[cluster_index,i].unique()\n",
    "        mask = mark_presence(all_possible_value, apperance_list)\n",
    "        # value_list = output[cluster_index] * np.array(flight_importance_list)[cluster_index].reshape((-1,1)) * mask ## message passing mechanism\n",
    "        value_list = output[cluster_index] *  mask ## message passing mechanism\n",
    "        value_list_output_index = np.argmax(value_list.sum(axis=0))\n",
    "        value_list_output = all_possible_value[value_list_output_index]\n",
    "        flight_correct_20.iloc[cluster_index,i] = value_list_output.replace('_',' ')\n",
    "    else:\n",
    "        ground_truth = flight_clean.iloc[list(label_overlap)[0],i]\n",
    "        flight_correct_20.iloc[cluster_index,i] = ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13.308859, 110)"
      ]
     },
     "execution_count": 1405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predict By Indicator\n",
    "def mark_presence(long_list, short_list):\n",
    "    short_set = set(short_list)  # 转换为集合以提高查找效率\n",
    "    return np.array([1 if (item in short_set) and (item!='empty') else 0 for item in long_list])\n",
    "\n",
    "# for f in flight_label_index_select:\n",
    "#     try:\n",
    "#         ground_truth = flight_clean.iloc[f,i].replace(' ','_')\n",
    "#         ground_truth_index = all_possible_value.index(ground_truth)\n",
    "#         update_score = np.zeros((1,len(all_possible_value)))\n",
    "#         update_score[0][ground_truth_index] = 16 ## Ground Truth Value\n",
    "#         output[f] = update_score\n",
    "#     except:\n",
    "#         continue\n",
    "# A Voting Mechanism, Process By Cluster\n",
    "cluster_indicator_all = flight_correct['flight'].unique()\n",
    "for cluster_indicator in cluster_indicator_all:\n",
    "    cluster_index = flight_correct[flight_correct['flight']==cluster_indicator].index\n",
    "    label_overlap = set(list(cluster_index)).intersection(set(flight_label_index_select))\n",
    "    if(len(label_overlap)==0): ## No Ground Truth\n",
    "        apperance_list = flight_correct.iloc[cluster_index,i].unique()\n",
    "        mask = mark_presence(all_possible_value, apperance_list)\n",
    "        # value_list = output[cluster_index] * np.array(flight_importance_list)[cluster_index].reshape((-1,1)) * mask ## message passing mechanism\n",
    "        value_list = output[cluster_index] *  mask ## message passing mechanism\n",
    "        value_list_output_index = np.argmax(value_list.sum(axis=0))\n",
    "        value_list_output = all_possible_value[value_list_output_index]\n",
    "        flight_correct_20.iloc[cluster_index,i] = value_list_output.replace('_',' ')\n",
    "    else:\n",
    "        ground_truth = flight_clean.iloc[list(label_overlap)[0],i]\n",
    "        flight_correct_20.iloc[cluster_index,i] = ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12:15_p.m.    21\n",
       "2:45_p.m.     19\n",
       "2:50_p.m.     15\n",
       "12:42_p.m.    14\n",
       "empty         14\n",
       "12:41_p.m.     8\n",
       "2:52_p.m.      7\n",
       "12:10_p.m.     2\n",
       "2:40_p.m.      2\n",
       "2:35_p.m.      2\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 1438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_subgraph[flight_subgraph['head']=='AA-3756-ORD-SLC']['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tuple_id</th>\n",
       "      <th>src</th>\n",
       "      <th>flight</th>\n",
       "      <th>sched_dep_time</th>\n",
       "      <th>act_dep_time</th>\n",
       "      <th>sched_arr_time</th>\n",
       "      <th>act_arr_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-3756-ORD-SLC</td>\n",
       "      <td>12:15_p.m.</td>\n",
       "      <td>12:41_p.m.</td>\n",
       "      <td>2:45_p.m.</td>\n",
       "      <td>2:40 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-3468-CVG-MIA</td>\n",
       "      <td>7:00_a.m.</td>\n",
       "      <td>7:25_a.m.</td>\n",
       "      <td>9:55_a.m.</td>\n",
       "      <td>9:42 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-466-IAH-MIA</td>\n",
       "      <td>6:00_a.m.</td>\n",
       "      <td>6:08_a.m.</td>\n",
       "      <td>9:20_a.m.</td>\n",
       "      <td>8:50 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td>10:45_a.m.</td>\n",
       "      <td>10:55_a.m.</td>\n",
       "      <td>2:20_p.m.</td>\n",
       "      <td>1:34 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-2957-DFW-CVG</td>\n",
       "      <td>7:55_a.m.</td>\n",
       "      <td>8:04_a.m.</td>\n",
       "      <td>11:05_a.m.</td>\n",
       "      <td>10:58 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2367</th>\n",
       "      <td>2368</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-789-ORD-DEN</td>\n",
       "      <td>1:05_p.m.</td>\n",
       "      <td>1:19_p.m.</td>\n",
       "      <td>2:35_p.m.</td>\n",
       "      <td>2:49 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2369</th>\n",
       "      <td>2370</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-3786-IAH-ORD</td>\n",
       "      <td>4:00_p.m.</td>\n",
       "      <td>4:12_p.m.</td>\n",
       "      <td>6:40_p.m.</td>\n",
       "      <td>6:11 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2373</th>\n",
       "      <td>2374</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>CO-45-EWR-MIA</td>\n",
       "      <td>4:00_p.m.</td>\n",
       "      <td>3:58_p.m.</td>\n",
       "      <td>7:05_p.m.</td>\n",
       "      <td>6:33 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2374</th>\n",
       "      <td>2375</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-3809-PHX-LAX</td>\n",
       "      <td>6:00_a.m.</td>\n",
       "      <td>6:10_a.m.</td>\n",
       "      <td>6:40_a.m.</td>\n",
       "      <td>6:07 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2375</th>\n",
       "      <td>2376</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-59-JFK-SFO</td>\n",
       "      <td>7:10_a.m.</td>\n",
       "      <td>7:39_a.m.</td>\n",
       "      <td>10:45_a.m.</td>\n",
       "      <td>10:49 a.m.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1102 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     tuple_id                   src           flight sched_dep_time  \\\n",
       "4           5                    aa  AA-3756-ORD-SLC     12:15_p.m.   \n",
       "6           7                    aa  AA-3468-CVG-MIA      7:00_a.m.   \n",
       "9          10                    aa   AA-466-IAH-MIA      6:00_a.m.   \n",
       "10         11                    aa  AA-1886-BOS-MIA     10:45_a.m.   \n",
       "11         12                    aa  AA-2957-DFW-CVG      7:55_a.m.   \n",
       "...       ...                   ...              ...            ...   \n",
       "2367     2368  world-flight-tracker   AA-789-ORD-DEN      1:05_p.m.   \n",
       "2369     2370  world-flight-tracker  AA-3786-IAH-ORD      4:00_p.m.   \n",
       "2373     2374  world-flight-tracker    CO-45-EWR-MIA      4:00_p.m.   \n",
       "2374     2375  world-flight-tracker  AA-3809-PHX-LAX      6:00_a.m.   \n",
       "2375     2376  world-flight-tracker    AA-59-JFK-SFO      7:10_a.m.   \n",
       "\n",
       "     act_dep_time sched_arr_time act_arr_time  \n",
       "4      12:41_p.m.      2:45_p.m.    2:40 p.m.  \n",
       "6       7:25_a.m.      9:55_a.m.    9:42 a.m.  \n",
       "9       6:08_a.m.      9:20_a.m.    8:50 a.m.  \n",
       "10     10:55_a.m.      2:20_p.m.    1:34 p.m.  \n",
       "11      8:04_a.m.     11:05_a.m.   10:58 a.m.  \n",
       "...           ...            ...          ...  \n",
       "2367    1:19_p.m.      2:35_p.m.    2:49 p.m.  \n",
       "2369    4:12_p.m.      6:40_p.m.    6:11 p.m.  \n",
       "2373    3:58_p.m.      7:05_p.m.    6:33 p.m.  \n",
       "2374    6:10_a.m.      6:40_a.m.    6:07 a.m.  \n",
       "2375    7:39_a.m.     10:45_a.m.   10:49 a.m.  \n",
       "\n",
       "[1102 rows x 7 columns]"
      ]
     },
     "execution_count": 1452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "flight_correct_20[flight_correct_20[relation]!=flight_clean[relation]]\n",
    "# flight_correct_20[flight_correct_20['flight']=='AA-2050-ORD-MIA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_clean[flight_dirty['flight']=='AA-466-IAH-MIA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_dirty[flight_correct_20['flight']=='AA-466-IAH-MIA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_clean[flight_dirty['flight']=='AA-518-MIA-JFK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_clean[flight_correct_20['sched_dep_time']!=flight_clean['sched_dep_time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0, 400, 400, 200, 200,   0,   0,   0,\n",
       "         0,   0])"
      ]
     },
     "execution_count": 1464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tax_correct_15.iloc[:,:-1]!=tax_clean).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ad2af692584064a6b308f2780dbf32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# def State_Correct(df):\n",
    "df = tax_correct_5.iloc[:20000].copy()\n",
    "col_ind = list(df.columns).index('state')\n",
    "correction = df[df['zip']!='1907'].copy()\n",
    "## attribute Pairs exclude anomaly\n",
    "zip_unique = correction['zip'].unique()\n",
    "for z in tqdm(zip_unique):\n",
    "    most_frequent_all = correction[correction['zip']==z]['state'].value_counts()\n",
    "    if(len(most_frequent_all)>1):\n",
    "        most_frequent = most_frequent_all.idxmax()\n",
    "        indexer = correction[correction['zip']==z].index\n",
    "        correction.loc[indexer,col_ind] = most_frequent\n",
    "# tax_correct_5_0 = State_Correct(tax_correct_5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1523,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Most_Frequent_Zip_State(row):\n",
    "    zip = row['zip']\n",
    "    state = row['state']\n",
    "    if(zip!='1907'):\n",
    "        most_frequent = tax_correct_15[tax_correct_15['zip']==zip]['state'].value_counts().idxmax()\n",
    "        return most_frequent\n",
    "    else:\n",
    "        return state\n",
    "tax_correct_15_0 = tax_correct_15.copy()\n",
    "tax_correct_15_0['state'] = tax_correct_15_0.parallel_apply(Most_Frequent_Zip_State,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1524,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Most_Frequent_City_State_Zip(row):\n",
    "    city = row['city']\n",
    "    state = row['state']\n",
    "    zip = row['zip']\n",
    "    if(zip=='1907'):\n",
    "        select = tax_dirty[(tax_dirty['city']==city) & (tax_dirty['state']==state)]['zip'].value_counts().idxmax()\n",
    "        return select\n",
    "    else:\n",
    "        return zip\n",
    "tax_correct_15_1 = tax_correct_15_0.copy()\n",
    "tax_correct_15_1['zip'] = tax_correct_15_1.parallel_apply(Most_Frequent_City_State_Zip,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['f_name', 'l_name', 'gender', 'area_code', 'phone', 'city', 'state',\n",
       "       'zip', 'marital_status', 'has_child', 'salary', 'rate', 'single_exemp',\n",
       "       'married_exemp', 'child_exemp'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.array(tax_correct_5_1!=tax_clean).sum(axis=0)\n",
    "tax_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1525,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Most_Frequent_marriage(row):\n",
    "    marital_status = row['marital_status']\n",
    "    single_exemp = row['single_exemp']\n",
    "    married_exemp = row['married_exemp']\n",
    "    if not (single_exemp=='0' and married_exemp=='0'):\n",
    "        select = tax_dirty[(tax_dirty['married_exemp']==married_exemp) & (tax_dirty['single_exemp']==single_exemp)]['marital_status'].value_counts().idxmax()\n",
    "        return select\n",
    "    else:\n",
    "        return marital_status\n",
    "tax_correct_15_2 = tax_correct_15_1.copy()\n",
    "tax_correct_15_2['marital_status'] = tax_correct_15_2.parallel_apply(Most_Frequent_marriage,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1526,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Most_Frequent_child(row):\n",
    "    child_exemp = row['child_exemp']\n",
    "    has_child = row['has_child']\n",
    "    if not (child_exemp=='0'):\n",
    "        select = tax_dirty[(tax_dirty['child_exemp']==child_exemp)]['has_child'].value_counts().idxmax()\n",
    "        return select\n",
    "    else:\n",
    "        return has_child\n",
    "tax_correct_15_3 = tax_correct_15_2.copy()\n",
    "tax_correct_15_3['has_child'] = tax_correct_15_3.parallel_apply(Most_Frequent_child,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_dirty_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Most_Frequent_MeasureCode_Stateavg(row):\n",
    "    MeasureCode= row['MeasureCode']\n",
    "    Stateavg = row['Stateavg']\n",
    "    select = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1529,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,  50, 128,   0,   0,   0,   0,   0,\n",
       "         0,   0])"
      ]
     },
     "execution_count": 1529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tax_correct_15_3.iloc[:,:-1]!=tax_clean).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_name</th>\n",
       "      <th>l_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>area_code</th>\n",
       "      <th>phone</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>has_child</th>\n",
       "      <th>salary</th>\n",
       "      <th>rate</th>\n",
       "      <th>single_exemp</th>\n",
       "      <th>married_exemp</th>\n",
       "      <th>child_exemp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pengyuan</td>\n",
       "      <td>Zendler</td>\n",
       "      <td>F</td>\n",
       "      <td>508</td>\n",
       "      <td>744-9007</td>\n",
       "      <td>SWAMPSCOTT</td>\n",
       "      <td>MA</td>\n",
       "      <td>1907</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>90000</td>\n",
       "      <td>5.3</td>\n",
       "      <td>0</td>\n",
       "      <td>7150</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nik</td>\n",
       "      <td>Tacic</td>\n",
       "      <td>M</td>\n",
       "      <td>702</td>\n",
       "      <td>517-7658</td>\n",
       "      <td>LAS VEGAS</td>\n",
       "      <td>NV</td>\n",
       "      <td>89140</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>90000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hovav</td>\n",
       "      <td>Punter</td>\n",
       "      <td>M</td>\n",
       "      <td>501</td>\n",
       "      <td>304-9763</td>\n",
       "      <td>HASTY</td>\n",
       "      <td>AR</td>\n",
       "      <td>72640</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>50000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Xiangning</td>\n",
       "      <td>Vanneste</td>\n",
       "      <td>F</td>\n",
       "      <td>862</td>\n",
       "      <td>651-6469</td>\n",
       "      <td>BRIGANTINE</td>\n",
       "      <td>NJ</td>\n",
       "      <td>8203</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>55000</td>\n",
       "      <td>1.9519792</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Belen</td>\n",
       "      <td>Niccum</td>\n",
       "      <td>F</td>\n",
       "      <td>920</td>\n",
       "      <td>287-1889</td>\n",
       "      <td>FLORENCE</td>\n",
       "      <td>WI</td>\n",
       "      <td>54121</td>\n",
       "      <td>S</td>\n",
       "      <td>Y</td>\n",
       "      <td>85000</td>\n",
       "      <td>5.9232907</td>\n",
       "      <td>700</td>\n",
       "      <td>0</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>Galeno</td>\n",
       "      <td>Abels</td>\n",
       "      <td>F</td>\n",
       "      <td>850</td>\n",
       "      <td>807-1573</td>\n",
       "      <td>MILTON</td>\n",
       "      <td>FL</td>\n",
       "      <td>32570</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>60000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>Ahmadreza</td>\n",
       "      <td>Scherl</td>\n",
       "      <td>F</td>\n",
       "      <td>971</td>\n",
       "      <td>491-3046</td>\n",
       "      <td>SCOTTS MILLS</td>\n",
       "      <td>OR</td>\n",
       "      <td>97375</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>45000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>318</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>Assef</td>\n",
       "      <td>Aba</td>\n",
       "      <td>M</td>\n",
       "      <td>857</td>\n",
       "      <td>147-4697</td>\n",
       "      <td>FRAMINGHAM</td>\n",
       "      <td>MA</td>\n",
       "      <td>1704</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>85000</td>\n",
       "      <td>5.3</td>\n",
       "      <td>3575</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>Neri</td>\n",
       "      <td>Walpole</td>\n",
       "      <td>F</td>\n",
       "      <td>865</td>\n",
       "      <td>592-8528</td>\n",
       "      <td>NASHVILLE</td>\n",
       "      <td>TN</td>\n",
       "      <td>37248</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>15000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>Christoforos</td>\n",
       "      <td>Smagt</td>\n",
       "      <td>M</td>\n",
       "      <td>406</td>\n",
       "      <td>961-2205</td>\n",
       "      <td>GILDFORD</td>\n",
       "      <td>MT</td>\n",
       "      <td>59525</td>\n",
       "      <td>S</td>\n",
       "      <td>Y</td>\n",
       "      <td>75000</td>\n",
       "      <td>6.9</td>\n",
       "      <td>1900</td>\n",
       "      <td>0</td>\n",
       "      <td>1900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              f_name    l_name gender area_code     phone          city state  \\\n",
       "0           Pengyuan   Zendler      F       508  744-9007    SWAMPSCOTT    MA   \n",
       "1                Nik     Tacic      M       702  517-7658     LAS VEGAS    NV   \n",
       "2              Hovav    Punter      M       501  304-9763         HASTY    AR   \n",
       "3          Xiangning  Vanneste      F       862  651-6469    BRIGANTINE    NJ   \n",
       "4              Belen    Niccum      F       920  287-1889      FLORENCE    WI   \n",
       "...              ...       ...    ...       ...       ...           ...   ...   \n",
       "199995        Galeno     Abels      F       850  807-1573        MILTON    FL   \n",
       "199996     Ahmadreza    Scherl      F       971  491-3046  SCOTTS MILLS    OR   \n",
       "199997         Assef       Aba      M       857  147-4697    FRAMINGHAM    MA   \n",
       "199998          Neri   Walpole      F       865  592-8528     NASHVILLE    TN   \n",
       "199999  Christoforos     Smagt      M       406  961-2205      GILDFORD    MT   \n",
       "\n",
       "          zip marital_status has_child salary       rate single_exemp  \\\n",
       "0        1907              M         N  90000        5.3            0   \n",
       "1       89140              M         N  90000        0.0            0   \n",
       "2       72640              S         N  50000        7.0           20   \n",
       "3        8203              M         Y  55000  1.9519792            0   \n",
       "4       54121              S         Y  85000  5.9232907          700   \n",
       "...       ...            ...       ...    ...        ...          ...   \n",
       "199995  32570              S         N  60000        0.0            0   \n",
       "199996  97375              M         N  45000        9.0            0   \n",
       "199997   1704              S         N  85000        5.3         3575   \n",
       "199998  37248              M         N  15000        0.0            0   \n",
       "199999  59525              S         Y  75000        6.9         1900   \n",
       "\n",
       "       married_exemp child_exemp  \n",
       "0               7150           0  \n",
       "1                  0           0  \n",
       "2                  0           0  \n",
       "3               2000        1500  \n",
       "4                  0         400  \n",
       "...              ...         ...  \n",
       "199995             0           0  \n",
       "199996           318           0  \n",
       "199997             0           0  \n",
       "199998             0           0  \n",
       "199999             0        1900  \n",
       "\n",
       "[200000 rows x 15 columns]"
      ]
     },
     "execution_count": 2235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1520,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_correct_5_3.to_csv('datasets/tax/FD/correct_5_FD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_name</th>\n",
       "      <th>l_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>area_code</th>\n",
       "      <th>phone</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>has_child</th>\n",
       "      <th>salary</th>\n",
       "      <th>rate</th>\n",
       "      <th>single_exemp</th>\n",
       "      <th>married_exemp</th>\n",
       "      <th>child_exemp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nik</td>\n",
       "      <td>Tacic</td>\n",
       "      <td>M</td>\n",
       "      <td>702</td>\n",
       "      <td>517-7658</td>\n",
       "      <td>LAS VEGAS</td>\n",
       "      <td>NV</td>\n",
       "      <td>89140</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>90000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Sanpei</td>\n",
       "      <td>Sobieski</td>\n",
       "      <td>F</td>\n",
       "      <td>360</td>\n",
       "      <td>921-9097</td>\n",
       "      <td>MILL CREEK</td>\n",
       "      <td>WA</td>\n",
       "      <td>98082</td>\n",
       "      <td>S</td>\n",
       "      <td>Y</td>\n",
       "      <td>20000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Ilagit</td>\n",
       "      <td>Auinger</td>\n",
       "      <td>F</td>\n",
       "      <td>907</td>\n",
       "      <td>185-6056</td>\n",
       "      <td>ANCHORAGE</td>\n",
       "      <td>AK</td>\n",
       "      <td>99530</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>70000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Avelino</td>\n",
       "      <td>English</td>\n",
       "      <td>M</td>\n",
       "      <td>401</td>\n",
       "      <td>795-3153</td>\n",
       "      <td>WESTERLY</td>\n",
       "      <td>RI</td>\n",
       "      <td>2891</td>\n",
       "      <td>S</td>\n",
       "      <td>Y</td>\n",
       "      <td>70000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Yuyan</td>\n",
       "      <td>Simao</td>\n",
       "      <td>M</td>\n",
       "      <td>603</td>\n",
       "      <td>350-5756</td>\n",
       "      <td>EAST DERRY</td>\n",
       "      <td>NH</td>\n",
       "      <td>3041</td>\n",
       "      <td>S</td>\n",
       "      <td>Y</td>\n",
       "      <td>50000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199967</th>\n",
       "      <td>Tariq</td>\n",
       "      <td>Mercankosk</td>\n",
       "      <td>F</td>\n",
       "      <td>401</td>\n",
       "      <td>735-5238</td>\n",
       "      <td>HOPKINTON</td>\n",
       "      <td>RI</td>\n",
       "      <td>2833</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199971</th>\n",
       "      <td>Jaakko</td>\n",
       "      <td>Gewali</td>\n",
       "      <td>M</td>\n",
       "      <td>605</td>\n",
       "      <td>689-5704</td>\n",
       "      <td>VOLGA</td>\n",
       "      <td>SD</td>\n",
       "      <td>57071</td>\n",
       "      <td>S</td>\n",
       "      <td>Y</td>\n",
       "      <td>100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199974</th>\n",
       "      <td>Rani</td>\n",
       "      <td>Gahleitner</td>\n",
       "      <td>F</td>\n",
       "      <td>720</td>\n",
       "      <td>796-7758</td>\n",
       "      <td>KITTREDGE</td>\n",
       "      <td>CO</td>\n",
       "      <td>80457</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>55000</td>\n",
       "      <td>4.63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199994</th>\n",
       "      <td>Jacob</td>\n",
       "      <td>Kogure</td>\n",
       "      <td>F</td>\n",
       "      <td>754</td>\n",
       "      <td>549-6684</td>\n",
       "      <td>TITUSVILLE</td>\n",
       "      <td>FL</td>\n",
       "      <td>32783</td>\n",
       "      <td>S</td>\n",
       "      <td>Y</td>\n",
       "      <td>80000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>Neri</td>\n",
       "      <td>Walpole</td>\n",
       "      <td>F</td>\n",
       "      <td>865</td>\n",
       "      <td>592-8528</td>\n",
       "      <td>NASHVILLE</td>\n",
       "      <td>TN</td>\n",
       "      <td>37248</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>15000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26769 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         f_name      l_name gender area_code     phone        city state  \\\n",
       "1           Nik       Tacic      M       702  517-7658   LAS VEGAS    NV   \n",
       "21       Sanpei    Sobieski      F       360  921-9097  MILL CREEK    WA   \n",
       "40       Ilagit     Auinger      F       907  185-6056   ANCHORAGE    AK   \n",
       "42      Avelino     English      M       401  795-3153    WESTERLY    RI   \n",
       "45        Yuyan       Simao      M       603  350-5756  EAST DERRY    NH   \n",
       "...         ...         ...    ...       ...       ...         ...   ...   \n",
       "199967    Tariq  Mercankosk      F       401  735-5238   HOPKINTON    RI   \n",
       "199971   Jaakko      Gewali      M       605  689-5704       VOLGA    SD   \n",
       "199974     Rani  Gahleitner      F       720  796-7758   KITTREDGE    CO   \n",
       "199994    Jacob      Kogure      F       754  549-6684  TITUSVILLE    FL   \n",
       "199998     Neri     Walpole      F       865  592-8528   NASHVILLE    TN   \n",
       "\n",
       "          zip marital_status has_child  salary  rate single_exemp  \\\n",
       "1       89140              S         N   90000   0.0            0   \n",
       "21      98082              S         Y   20000   0.0            0   \n",
       "40      99530              S         N   70000   0.0            0   \n",
       "42       2891              S         Y   70000   0.0            0   \n",
       "45       3041              S         Y   50000   0.0            0   \n",
       "...       ...            ...       ...     ...   ...          ...   \n",
       "199967   2833              S         N    5000   0.0            0   \n",
       "199971  57071              S         Y  100000   0.0            0   \n",
       "199974  80457              S         N   55000  4.63            0   \n",
       "199994  32783              S         Y   80000   0.0            0   \n",
       "199998  37248              S         N   15000   0.0            0   \n",
       "\n",
       "       married_exemp child_exemp  \n",
       "1                  0           0  \n",
       "21                 0           0  \n",
       "40                 0           0  \n",
       "42                 0           0  \n",
       "45                 0           0  \n",
       "...              ...         ...  \n",
       "199967             0           0  \n",
       "199971             0           0  \n",
       "199974             0           0  \n",
       "199994             0           0  \n",
       "199998             0           0  \n",
       "\n",
       "[26769 rows x 15 columns]"
      ]
     },
     "execution_count": 1513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_correct_5_2[tax_correct_5_2['marital_status']!=tax_clean['marital_status']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M    13692\n",
       "Name: marital_status, dtype: int64"
      ]
     },
     "execution_count": 1512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_correct_5_1[(tax_correct_5_1['married_exemp']=='6600') & (tax_correct_5_1['single_exemp']=='0')]['marital_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1782,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "# Expanded data to include the new method, with keys being the methods and values being the scores\n",
    "data = {\n",
    "    'Hospital': {'GEIL': [0.37116564417177916, 0.5516304347826088, 0.6361323155216284, 0.759725400457666,0.8141025641025642, 0.8488612836438924, 0.8488612836438924, 0.8488612836438924,0.8795180722891566, 0.8795180722891566, 0.8795180722891566, 0.8795180722891566], 'Baran': [0.1, 0.2, 0.35, 0.4,0.5, 0.5077, 0.55, 0.6,0.65, 0.67, 0.67, 0.67]},\n",
    "    'Flights': {'GEIL': [0.7792714212416625, 0.7933613359286957, 0.7969693867103511, 0.8077237433592153,0.8102637497444286, 0.8141791807130452, 0.8273322422258592, 0.8414484451718494,0.8543073460200533, 0.8679824112894978, 0.9109133681088268, 0.9731667349446949], 'Baran': [0.1, 0.2, 0.4, 0.6,0.65, 0.67, 0.67, 0.67,0.67, 0.67, 0.67, 0.67]},\n",
    "    'Beers': {'GEIL': [0.3149, 0.3149, 0.8288, 0.8288,0.9373, 0.9373, 0.9578, 0.9574,0.9574, 0.9772151898734178, 0.9772151898734178, 0.9772151898734178], 'Baran': [0.15, 0.45, 0.55, 0.68,0.75, 0.8, 0.8, 0.8,0.8, 0.8, 0.8, 0.8]},\n",
    "    'Rayyan': {'GEIL': [0.04, 0.24, 0.35, 0.45,0.75, 0.82, 0.82, 0.82,0.85, 0.867, 0.869, 0.869], 'Baran': [0.04, 0.04, 0.06, 0.08,0.12, 0.135, 0.155, 0.2,0.202, 0.25, 0.28, 0.28]},\n",
    "    'Tax': {'GEIL': [0.2661396574440053, 0.2661396574440053, 0.3394321766561514, 0.4067796610169491,0.700888450148075, 0.9425472077139414, 0.9425472077139414, 0.9425472077139414,0.9425472077139414, 0.9425472077139414, 0.9653679653679654, 0.9653679653679654], 'Baran': [0.19, 0.55, 0.63, 0.67,0.71, 0.73, 0.71, 0.71,0.75, 0.81, 0.81, 0.81]},\n",
    "    # Add similar structure for other datasets...\n",
    "}\n",
    "\n",
    "# Labeled tuples\n",
    "labeled_tuples = [1,2,3,4,5,6,7,8,9, 10, 15, 20]\n",
    "\n",
    "# Colors for each method\n",
    "colors = {\n",
    "    'GEIL': 'green',\n",
    "    'Baran': 'blue',\n",
    "    # Add more colors for additional methods if needed...\n",
    "}\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(2, 3, figsize=(20, 10))  # Adjust the size as needed\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Plot data\n",
    "for i, (dataset, methods) in enumerate(data.items()):\n",
    "    ax = axs[i]\n",
    "    for method, scores in methods.items():\n",
    "        ax.plot(labeled_tuples, scores, 'o-', label=method, color=colors[method])\n",
    "    ax.set_title(f'({chr(97 + i)}) {dataset}')\n",
    "    ax.set_xlabel('Labeled Tuples Count')\n",
    "    ax.set_ylabel('F1 Score')\n",
    "    ax.set_xticks(labeled_tuples)\n",
    "    ax.set_yticks(np.arange(0, 1.01, 0.2))\n",
    "    ax.grid(True)\n",
    "    ax.legend()  # This adds the legend to each subplot\n",
    "\n",
    "# Remove the last subplot (if we have only 7 datasets)\n",
    "fig.delaxes(axs[-1])\n",
    "\n",
    "# Adjust layout and save the figure as SVG\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots_with_legend.pdf', format='pdf')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1783,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'ProviderNumber', 'HospitalName', 'Address1', 'Address2',\n",
       "       'Address3', 'City', 'State', 'ZipCode', 'CountyName', 'PhoneNumber',\n",
       "       'HospitalType', 'HospitalOwner', 'EmergencyService', 'Condition',\n",
       "       'MeasureCode', 'MeasureName', 'Score', 'Sample', 'Stateavg'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1783,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hospital_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1795,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Insert Error for Hospital\n",
    "hospital_dirty_10_error = hospital_dirty.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1801,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_matrix = np.array(hospital_dirty!=hospital_clean).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1814,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def random_select(array, k):\n",
    "\n",
    "  m, n = array.shape\n",
    "\n",
    "  # 找出全为0的列索引\n",
    "  zero_cols = np.argwhere(array.sum(axis=0) == 0).flatten() \n",
    "\n",
    "  # 从所有列中排除零列\n",
    "  valid_cols = np.setdiff1d(np.arange(n), zero_cols)\n",
    "\n",
    "  # 记录除零列外所有0位置 \n",
    "  zeros = np.argwhere(array[:, valid_cols] == 0)\n",
    "\n",
    "  if k > len(zeros):\n",
    "    raise ValueError('k cannot be larger than the number of 0s')\n",
    "\n",
    "  random.shuffle(zeros)\n",
    "  positions = zeros[:k]\n",
    "\n",
    "  # 将位置转换成数组索引形式\n",
    "  positions = [(x[0], valid_cols[x[1]]) for x in positions]\n",
    "\n",
    "  for p in positions:\n",
    "    array[p[0],p[1]] = 1\n",
    "\n",
    "  return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1830,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1491"
      ]
     },
     "execution_count": 1830,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix = np.array(hospital_dirty!=hospital_clean).astype(int)\n",
    "error_rate_origin = input_matrix.sum()\n",
    "output = random_select(input_matrix,int((1000*20) * 0.1 - error_rate_origin))\n",
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1832,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_replace(string):\n",
    "  letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','y','z']\n",
    "  \n",
    "  new_string = list(string)\n",
    "  \n",
    "  # 随机选择一个字母位置\n",
    "  index = random.randrange(len(string))  \n",
    "\n",
    "  # 跳过'x'\n",
    "  while string[index] == 'x':\n",
    "    index = random.randrange(len(string))\n",
    "\n",
    "  # 替换为'x'  \n",
    "  new_string[index] = 'x'\n",
    "\n",
    "  return \"\".join(new_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1840,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "509"
      ]
     },
     "execution_count": 1840,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(hospital_dirty!=hospital_clean).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1865,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1491/1491 [00:00<00:00, 17300.72it/s]\n"
     ]
    }
   ],
   "source": [
    "hospital_dirty_10_error = hospital_dirty.copy()\n",
    "for d in tqdm(output_10):\n",
    "    i = d[0]\n",
    "    j = d[1]\n",
    "    value = hospital_clean.iloc[i,j]\n",
    "    replace = replace_random_char_with_x(value)\n",
    "    hospital_dirty_10_error.iloc[i,j] = replace\n",
    "    if(value==replace):\n",
    "        print(value,replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1866,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 1866,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(hospital_dirty_10_error!=hospital_clean).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1877,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_dirty_10_error.to_csv('datasets/hospital/vary_error_rate/hospital_dirty_10_error.csv')\n",
    "# hospital_dirty_20_error.to_csv('datasets/hospital/vary_error_rate/hospital_dirty_20_error.csv')\n",
    "# hospital_dirty_30_error.to_csv('datasets/hospital/vary_error_rate/hospital_dirty_30_error.csv')\n",
    "# hospital_dirty_40_error.to_csv('datasets/hospital/vary_error_rate/hospital_dirty_40_error.csv')\n",
    "# hospital_dirty_50_error.to_csv('datasets/hospital/vary_error_rate/hospital_dirty_50_error.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1855,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.where(np.array(input_matrix).sum(axis=0)==0)[0]\n",
    "output = [d for d in np.argwhere(input_matrix==0) if d[1] not in [0,4,5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1876,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9491 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9491/9491 [00:00<00:00, 18258.77it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 1876,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def random_select_rows(arr, k):\n",
    "\n",
    "  m, n = arr.shape\n",
    "\n",
    "  # 生成 0 到 m-1 的索引  \n",
    "  indices = list(range(m))\n",
    "\n",
    "  # 随机打乱索引\n",
    "  random.shuffle(indices)\n",
    "\n",
    "  # 返回第一个 k 个索引对应的行\n",
    "  return arr[indices[:k],:]\n",
    "input_matrix = np.array(hospital_dirty!=hospital_clean).astype(int)\n",
    "output_50 = random_select_rows(np.array(output),10000-input_matrix.sum())\n",
    "hospital_dirty_50_error = hospital_dirty.copy()\n",
    "for d in tqdm(output_50):\n",
    "    i = d[0]\n",
    "    j = d[1]\n",
    "    value = hospital_clean.iloc[i,j]\n",
    "    replace = replace_random_char_with_x(value)\n",
    "    hospital_dirty_50_error.iloc[i,j] = replace\n",
    "    if(value==replace):\n",
    "        print(value,replace)\n",
    "np.array(hospital_dirty_50_error!=hospital_clean).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1853,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "a must be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/yanmy/raha/raha-master/detector.ipynb 单元格 533\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z2100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# len(output)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z2100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m output_10 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mchoice(np\u001b[39m.\u001b[39;49marray(output),\u001b[39m2000\u001b[39;49m\u001b[39m-\u001b[39;49minput_matrix\u001b[39m.\u001b[39;49msum(),replace\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32mmtrand.pyx:946\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: a must be 1-dimensional"
     ]
    }
   ],
   "source": [
    "# len(output)\n",
    "# output_10 = np.random.choice(np.array(output),2000-input_matrix.sum(),replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1903,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_dirty_50_error = pd.read_csv('datasets/hospital/vary_error_rate/hospital_dirty_50_error.csv',index_col=0).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1904,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 7/1000 [00:00<00:14, 67.69it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:15<00:00, 63.82it/s]\n"
     ]
    }
   ],
   "source": [
    "### For Different Error_Rate, Add Reference Files For Testing\n",
    "input_matrix = np.array(hospital_dirty!=hospital_clean).astype(int)\n",
    "# input_matrix_select = input_matrix[selected_rows]\n",
    "detector_list_all = []\n",
    "hospital_dirty_vary_error = hospital_dirty_50_error.copy()\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "# for label_tuple in tqdm(selected_rows):\n",
    "for label_tuple in tqdm(range(len(hospital_clean))):\n",
    "    for i in range(len(hospital_clean.columns)):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = hospital_clean.iloc[label_tuple]\n",
    "        dirty_context = hospital_dirty_vary_error.iloc[label_tuple]\n",
    "        clean_cell = hospital_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = hospital_dirty_vary_error.iloc[label_tuple,i]\n",
    "        single_context_clean = 'COL %s VAL %s ' % (hospital_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (hospital_clean.columns[i],dirty_cell)\n",
    "        for c in range(20):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (hospital_clean.columns[c],hospital_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (hospital_dirty_vary_error.columns[c],hospital_dirty_vary_error.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        if(str(dirty_cell)!=str(clean_cell)):\n",
    "            detector_list_all.append([all_context_dirty,single_context_dirty,1])\n",
    "        else:\n",
    "            detector_list_all.append([all_context_dirty,single_context_dirty,0])\n",
    "            # detector_list.append([single_context_dirty,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1905,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10000\n",
       "1    10000\n",
       "Name: 2, dtype: int64"
      ]
     },
     "execution_count": 1905,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(detector_list_all)[2].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1906,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(detector_list_all).to_csv('datasets/hospital/vary_error_rate/hospital_50_detect.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hospital_Training_Data on various error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hospital_dirty_10_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1949,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = hospital_dirty_10_error.copy()\n",
    "temp['index'] = temp['index'].astype(int)-1\n",
    "temp['index'] = temp['index'].astype(str)\n",
    "hospital_dirty_dict = temp.set_index('index').to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1950,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_rows = np.load('datasets/hospital/detector/index.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1957,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = hospital_clean.copy()\n",
    "temp['index'] = temp['index'].astype(int)-1\n",
    "temp['index'] = temp['index'].astype(str)\n",
    "hospital_clean_dict = temp.set_index('index').to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1983,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 56,  57,  58,  79,  94, 106, 128, 130, 143, 153, 156, 160, 161,\n",
       "       165, 175, 179, 182, 184, 192, 195])"
      ]
     },
     "execution_count": 1983,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(detector[:200,3]!=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1995,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ICL_Index_Set(detector,cluster,col):\n",
    "    cluster_index = np.array(cluster) ## list转array\n",
    "    cluster_select = detector[cluster] ## \n",
    "    return cluster_index[np.where(cluster_select[:,col]==0)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2045,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:00<00:00, 73.87it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 326.50it/s]\n"
     ]
    }
   ],
   "source": [
    "## 加入标注数据\n",
    "detector = np.load('datasets/hospital/vary_error_rate/hospital_50_detector.npy')\n",
    "hospital_dirty_50_error = pd.read_csv('datasets/hospital/vary_error_rate/hospital_dirty_50_error.csv',index_col=0).astype(str)\n",
    "hospital_dirty_vary_error = hospital_dirty_50_error.copy()\n",
    "coreset_detect = np.where(detector.sum(axis=1)==0)[0]\n",
    "# coreset_detect = np.random.choice(np.arange(0,1000,1),600)\n",
    "# coreset_detect = np.arange(0,1000,1) + 1\n",
    "header = list(hospital_dirty.columns)\n",
    "noise_col = np.where(detector.sum(axis=0)>0)[0]\n",
    "safe_value = ['empty'] ## 不注入噪声的类型\n",
    "training_list = []\n",
    "for h in tqdm(hospital_cluster): ## 对比学习的采样策略在同cluster内进行\n",
    "    # coreset_subset = [n for n in hospital_cluster[h] if n in coreset_detect]\n",
    "    coreset_subset = [n for n in hospital_cluster[h]]\n",
    "    for coreset_tuple in coreset_subset: ## 取遍coreset所有的tuple，剩下的n-1 tuple是参考对象,coreset_tuple是index\n",
    "        # noise_col_subset = np.random.choice(noise_col,10,replace=False) ## 取5个col作为注入噪声的对象\n",
    "        noise_col_subset = [n for n in noise_col if n in np.where(detector[coreset_tuple]==0)[0]] ## 同时为noise col而且detector认为是正常的value\n",
    "        # noise_col_subset = np.random.choice(noise_col_subset,2,replace=False)\n",
    "        noise_col_subset = np.random.choice(noise_col_subset,2)\n",
    "        # noise_col_subset = [n for n in noise_col_subset if n!=0] ## 排除index\n",
    "        for noise_col_single in noise_col_subset:\n",
    "            col_name = header[noise_col_single] ## 从index转成列名\n",
    "            if(hospital_dirty_dict[str(coreset_tuple)][col_name] not in safe_value and len(coreset_subset)>2):\n",
    "                temp_dict = hospital_dirty.iloc[coreset_tuple,1:].to_dict() ## coreset tuple字典\n",
    "                clean_cell = temp_dict[col_name]\n",
    "                dirty_cell = replace_random_char_with_x(clean_cell) ## 注入噪声\n",
    "                temp_dict[col_name] = dirty_cell\n",
    "                coreset_reference = select_two_different_elements(coreset_subset,coreset_tuple)\n",
    "                template_dict = {}\n",
    "                clean_dict = {}\n",
    "                template_dict[col_name] = ''\n",
    "                clean_dict[col_name] = clean_cell\n",
    "                text_head = 'You are an expert in Cleaning Hospital Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\nTake these rows as reference:\\n\\n' % (col_name, json.dumps(template_dict), json.dumps(temp_dict))\n",
    "                \n",
    "                # ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(hospital_dirty_dict[str(coreset_reference[0])]), json.dumps(hospital_dirty_dict[str(coreset_reference[1])]))\n",
    "                # cluster = [hospital_cluster[h] for h in hospital_cluster if hospital_cluster[h].__contains__(label_tuple)][0] ## 找到label tuple所在的簇\n",
    "                # cluster_coreset = [c for c in cluster if c in coreset_detect] ## 找到簇内coreset元素\n",
    "                cluster_coreset = ICL_Index_Set(detector,coreset_subset,noise_col_single)\n",
    "                if(len(cluster_coreset)>0):\n",
    "                    coreset_reference = np.random.choice(cluster_coreset,1,replace=False) ## 取两个\n",
    "                    # ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]), json.dumps(hospital_dirty_dict[coreset_reference[1]]))\n",
    "                    ICL_text = '%s\\n\\n' % (json.dumps(hospital_dirty_dict[str(coreset_reference[0])]))\n",
    "                else:\n",
    "                    coreset_reference = np.random.choice(selected_rows,1,replace=False) ## 取两个\n",
    "                    # ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]), json.dumps(hospital_dirty_dict[coreset_reference[1]]))\n",
    "                    ICL_text = '%s\\n\\n' % (json.dumps(hospital_clean_dict[str(coreset_reference[0])]))\n",
    "                \n",
    "                training_list.append([text_head,ICL_text,'',json.dumps(clean_dict)])\n",
    "training_list_label = []\n",
    "for label_tuple in tqdm(selected_rows):\n",
    "    for noise_col_single in range(len(header)):\n",
    "        col_name = header[noise_col_single]\n",
    "        template_dict = {}\n",
    "        template_dict[col_name] = ''\n",
    "        clean_dict = {}\n",
    "        temp_dict = hospital_dirty.iloc[label_tuple,1:].to_dict()\n",
    "        clean_cell = hospital_clean.iloc[label_tuple,noise_col_single]\n",
    "        dirty_cell = hospital_dirty.iloc[label_tuple,noise_col_single]\n",
    "        clean_dict[col_name] = clean_cell\n",
    "        if(clean_cell!=dirty_cell):\n",
    "            text_head = 'You are an expert in Cleaning Hospital Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\nTake these rows as reference:\\n\\n' % (col_name, json.dumps(template_dict), json.dumps(temp_dict))\n",
    "            cluster = [hospital_cluster[h] for h in hospital_cluster if hospital_cluster[h].__contains__(label_tuple)][0] ## 找到label tuple所在的簇\n",
    "            # cluster_coreset = [c for c in cluster if c in coreset_detect] ## 找到簇内coreset元素\n",
    "            cluster_coreset = ICL_Index_Set(detector,cluster,noise_col_single)\n",
    "            if(len(cluster_coreset)>0):\n",
    "                coreset_reference = np.random.choice(cluster_coreset,1,replace=False) ## 取两个\n",
    "                # ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]), json.dumps(hospital_dirty_dict[coreset_reference[1]]))\n",
    "                ICL_text = '%s\\n\\n' % (json.dumps(hospital_dirty_dict[str(coreset_reference[0])]))\n",
    "            else:\n",
    "                coreset_reference = np.random.choice(selected_rows,1,replace=False) ## 取两个\n",
    "                # ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]), json.dumps(hospital_dirty_dict[coreset_reference[1]]))\n",
    "                ICL_text = '%s\\n\\n' % (json.dumps(hospital_clean_dict[str(coreset_reference[0])]))\n",
    "            training_list_label.append([text_head,ICL_text,'',json.dumps(clean_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2046,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list_pd = pd.DataFrame(training_list)\n",
    "training_list_label_pd = pd.DataFrame(training_list_label)\n",
    "training_list_pd = pd.concat([training_list_pd,training_list_label_pd]).drop_duplicates()\n",
    "training_list_pd['instruction'] = training_list_pd[0] + training_list_pd[1]\n",
    "training_list_pd['input'] = training_list_pd[2]\n",
    "training_list_pd['output'] = training_list_pd[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2047,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(training_list_pd.iloc[:,-3:].to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/hospital/hospital-train-error-50.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2048,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9823 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9823/9823 [00:02<00:00, 4616.15it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9810"
      ]
     },
     "execution_count": 2048,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 检查所有detector要求detect的元素，组成一个list，查找并附加coreset\n",
    "# for d in detector\n",
    "detector_indice = np.argwhere(detector==1)\n",
    "detector_list_all = []\n",
    "candidate_length = {}\n",
    "right_loc = {}\n",
    "training_list_label_test = []\n",
    "count = 0\n",
    "for d in tqdm(detector_indice):\n",
    "    \n",
    "    label_tuple = d[0] ## 行\n",
    "    i = d[1] ## 列\n",
    "    col_name = hospital_clean.columns[i] ## 列名\n",
    "    all_context_clean = ''\n",
    "    all_context_dirty = ''\n",
    "    # clean_context = hospital_clean.iloc[label_tuple]\n",
    "    dirty_context = hospital_dirty_vary_error.iloc[label_tuple]\n",
    "    clean_cell = hospital_clean.iloc[label_tuple,i]\n",
    "    dirty_cell = hospital_dirty_vary_error.iloc[label_tuple,i]\n",
    "    \n",
    "    template_dict = {}\n",
    "    template_dict[col_name] = ''\n",
    "    temp_dict = hospital_dirty_dict[str(label_tuple)]\n",
    "    \n",
    "    clean_dict = {}\n",
    "    clean_dict[col_name] = clean_cell\n",
    "    # single_context_clean = 'COL %s VAL %s ' % (hospital_clean.columns[i],clean_cell)\n",
    "    # single_context_dirty = 'COL %s VAL %s ' % (hospital_clean.columns[i],dirty_cell)\n",
    "    # if(clean_cell!=dirty_cell):\n",
    "    text_head = 'You are an expert in Cleaning Hospital Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\nTake these rows as reference:\\n\\n' % (col_name, json.dumps(template_dict), json.dumps(temp_dict))\n",
    "    # cluster = [hospital_cluster[h] for h in hospital_cluster if hospital_cluster[h].__contains__(label_tuple)][0] ## 找到label tuple所在的簇\n",
    "    # cluster_coreset = [c for c in cluster if c in coreset_detect] ## 找到簇内coreset元素\n",
    "    # coreset_reference = np.random.choice(cluster_coreset,1,replace=False) ## 取两个\n",
    "    # # ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]), json.dumps(hospital_dirty_dict[coreset_reference[1]]))\n",
    "    # ICL_text = '%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]))\n",
    "    # training_list_label.append([text_head,ICL_text,'',json.dumps(clean_dict)])\n",
    "    cluster = [hospital_cluster[h] for h in hospital_cluster if hospital_cluster[h].__contains__(label_tuple)][0] ## 找到label tuple所在的簇\n",
    "    cluster_coreset = ICL_Index_Set(detector,cluster,i)\n",
    "    if(len(cluster_coreset)>0):\n",
    "        coreset_reference = np.random.choice(cluster_coreset,1,replace=False) ## 取两个\n",
    "        # ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]), json.dumps(hospital_dirty_dict[coreset_reference[1]]))\n",
    "        ICL_text = '%s\\n\\n' % (json.dumps(hospital_dirty_dict[str(coreset_reference[0])]))\n",
    "    else:\n",
    "        coreset_reference = np.random.choice(selected_rows,1,replace=False) ## 取两个\n",
    "        # ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]), json.dumps(hospital_dirty_dict[coreset_reference[1]]))\n",
    "        ICL_text = '%s\\n\\n' % (json.dumps(hospital_clean_dict[str(coreset_reference[0])]))\n",
    "    if(dirty_cell!=clean_cell):\n",
    "        count += 1\n",
    "    training_list_label_test.append([text_head,ICL_text,'',json.dumps(clean_dict)])\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2049,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list_pd = pd.DataFrame(training_list_label_test)\n",
    "# training_list_label_pd = pd.DataFrame(training_list_label)\n",
    "# training_list_pd = pd.concat([training_list_pd,training_list_label_pd]).drop_duplicates()\n",
    "training_list_pd['instruction'] = training_list_pd[0] + training_list_pd[1]\n",
    "training_list_pd['input'] = training_list_pd[2]\n",
    "training_list_pd['output'] = training_list_pd[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2050,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(training_list_pd.iloc[:,-3:].to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/hospital/hospital-train-error-50-test.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 2103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hospital_error = pd.read_csv('datasets/hospital/vary_error_rate/hospital_50.csv').astype(str)\n",
    "np.array(hospital_error!=hospital_clean).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Flight_Noise_Generation_sche_dep(cell):\n",
    "    task = [0,1,2,3,4,5,6]\n",
    "    task_select = np.random.choice(task)\n",
    "    if(task_select==0):\n",
    "        cell_output = cell.replace(' ','').replace('.','')\n",
    "        cell_output = cell_output[:-1] + 'Dec 1'\n",
    "    elif(task_select==1):\n",
    "        cell_output = '11/30 ' + cell\n",
    "    elif(task_select==2):\n",
    "        cell_output = 'Not Available'\n",
    "    elif(task_select==3):\n",
    "        cell_output = cell.replace(' ','').replace('.','')\n",
    "        cell_output = cell_output[:-2] + 'noon'\n",
    "    elif(task_select==4):\n",
    "        cell_output = cell + ' (-00:00)'\n",
    "    elif(task_select==5):\n",
    "        cell_output = '12/02/2011 ' + cell\n",
    "    else:\n",
    "        cell_output = 'Dec 02 ' + cell\n",
    "    return cell_output\n",
    "# def Flight_Noise_Generation_sche_arr(cell):\n",
    "#     task = [0,1,2,3,4,5,6]\n",
    "#     task_select = np.random.choice(task)\n",
    "#     if(task==0):\n",
    "#         cell_output = cell.replace(' ','').replace('.','')\n",
    "#         cell_output = cell_output[:-1] + 'Dec 1'\n",
    "#     elif(task==1):\n",
    "#         cell_output = '12/02/2011 ' + cell\n",
    "#     elif(task==2):\n",
    "#         cell_output = 'Not Available'\n",
    "#     elif(task==3):\n",
    "#         cell_output = cell.replace(' ','').replace('.','')\n",
    "#         cell_output = cell_output[:-2] + 'noon'\n",
    "#     elif(task==4):\n",
    "#         cell_output = cell + ' (-00:00)'\n",
    "#     elif(task==5):\n",
    "#         cell_output = '12/02/2011 ' + cell\n",
    "#     else:\n",
    "#         cell_output = 'Dec 02 ' + cell\n",
    "#     return cell_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2113,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_matrix_flight = np.array(flight_clean!=flight_dirty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2231,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Flight_Error_Rate_50\n",
    "flight_dirty = pd.read_csv('datasets/flights/dirty.csv').fillna('').astype(str)\n",
    "input_matrix_flight = np.array(flight_clean!=flight_dirty)\n",
    "clean_cell_place = np.argwhere(input_matrix_flight==0) ## Clean Cell Place\n",
    "clean_cell_select = [d for d in clean_cell_place if d[1] not in [0,1,2] and d[0] not in flight_label_index and d[0] in inject_index]\n",
    "clean_cell_select_add = [d for d in clean_cell_place if d[1] not in [0,1,2] and d[0] not in flight_label_index and d[0]  not in inject_index]\n",
    "random_index_add = np.random.choice(len(clean_cell_select_add),3280-len(clean_cell_select),replace=False)\n",
    "clean_cell_noise_place = np.array(clean_cell_select_add)[random_index_add] ## 30-50\n",
    "clean_cell_noise_place = np.concatenate([np.array(clean_cell_select),clean_cell_noise_place])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2208,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Flight_Error_Rate_20\n",
    "input_matrix_flight = np.array(flight_clean!=flight_dirty)\n",
    "clean_cell_place = np.argwhere(input_matrix_flight==1) ## Clean Cell Place\n",
    "clean_cell_select = [d for d in clean_cell_place if d[1] not in [0,1,2] and d[0] not in flight_label_index ]\n",
    "random_index = np.random.choice(len(clean_cell_select),1640,replace=False)\n",
    "clean_cell_noise_place = np.array(clean_cell_select)[random_index] ## 30-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1640"
      ]
     },
     "execution_count": 2215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_cell_noise_place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2232,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_dirty = pd.read_csv('datasets/flights/dirty.csv').fillna('').astype(str)\n",
    "flight_dirty_50_error = flight_dirty.copy()\n",
    "for c in clean_cell_noise_place:\n",
    "    i = c[0]\n",
    "    j = c[1]\n",
    "    clean_value = flight_dirty.iloc[i,j]\n",
    "    replace_value = Flight_Noise_Generation_sche_dep(clean_value)\n",
    "    flight_dirty_50_error.iloc[i,j] = replace_value\n",
    "# flight_dirty_10_error = flight_dirty.copy()\n",
    "# for c in clean_cell_noise_place:\n",
    "#     i = c[0]\n",
    "#     j = c[1]\n",
    "#     clean_value = flight_clean.iloc[i,j]\n",
    "#     replace_value = clean_value\n",
    "#     flight_dirty_10_error.iloc[i,j] = replace_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6560"
      ]
     },
     "execution_count": 2227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(flight_dirty_40_error!=flight_clean).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2142,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_dirty_10_error.to_csv('datasets/flights/vary_error_rate/flight_dirty_10_error.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2139,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_dirty_20_error.to_csv('datasets/flights/vary_error_rate/flight_dirty_20_error.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2133,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_dirty.to_csv('datasets/flights/vary_error_rate/flight_dirty_30_error.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2228,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_dirty_40_error.to_csv('datasets/flights/vary_error_rate/40/flight_dirty_40_error.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4920"
      ]
     },
     "execution_count": 2214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(flight_dirty!=flight_clean).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2089,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625\n",
      "1589\n",
      "1603\n",
      "1865\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "hospital_dirty_10_error = pd.read_csv('/home/yanmy/raha/raha-master/datasets/hospital/vary_error_rate/hospital_dirty_10_error.csv',index_col=0).astype(str)\n",
    "hospital_correction = hospital_dirty_10_error.copy()\n",
    "hospital_result = pd.read_csv('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/hospital-train-error-10-test.csv',index_col=0)\n",
    "hospital_detector = np.load('/home/yanmy/raha/raha-master/datasets/hospital/vary_error_rate/hospital_10_detector.npy')\n",
    "import ast\n",
    "for d in np.argwhere(hospital_detector==1):\n",
    "    i = d[0]\n",
    "    j = d[1]\n",
    "    try:\n",
    "        predict = list(ast.literal_eval(hospital_result.iloc[count,-1]).values())[0]\n",
    "        hospital_correction.iloc[i,j] = predict\n",
    "        count += 1\n",
    "    except:\n",
    "        print(count)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2090,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_correction.to_csv('datasets/hospital/vary_error_rate/hospital_error_10_correction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2091,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9102167182662538, 0.882, 0.8958862366683595)"
      ]
     },
     "execution_count": 2091,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hospital_correction = pd.read_csv('/home/yanmy/raha/raha-master/datasets/hospital/correct_result/hospital_correction_20.csv',index_col=0).astype(str)\n",
    "All_Data_Error = 0\n",
    "All_Fixed_Error = 0\n",
    "Correct_Fixed_Error = 0\n",
    "# hospital_correction.iloc[:,3] = hospital_dirty.iloc[:,3]\n",
    "# hospital_correction.iloc[:,14] = hospital_dirty.iloc[:,14]\n",
    "# a = np.where(input_matrix_hospital[hospital_label_index[:9]].sum(axis=0)!=0)\n",
    "# b = np.where(input_matrix_hospital[hospital_label_index[:20]].sum(axis=0)!=0)\n",
    "# for h in [i for i in b[0] if i not in a[0]]:\n",
    "#     hospital_correction.iloc[:,h] = hospital_dirty.iloc[:,h]\n",
    "hospital_dirty_10_error = pd.read_csv('/home/yanmy/raha/raha-master/datasets/hospital/vary_error_rate/hospital_dirty_10_error.csv',index_col=0).astype(str)\n",
    "for i in range(1000):\n",
    "    for j in range(20):\n",
    "        dirty_cell = hospital_dirty_10_error.iloc[i,j]\n",
    "        clean_cell = hospital_clean.iloc[i,j]\n",
    "        correct_cell = hospital_correction.iloc[i,j]\n",
    "        if(correct_cell!=dirty_cell):\n",
    "            All_Fixed_Error += 1\n",
    "        if(clean_cell!=dirty_cell):\n",
    "            All_Data_Error += 1\n",
    "            if(correct_cell==clean_cell):\n",
    "                Correct_Fixed_Error += 1\n",
    "Precision_hospital = Correct_Fixed_Error / All_Fixed_Error\n",
    "Recall_hospital = Correct_Fixed_Error / All_Data_Error\n",
    "F1_hospital = (2 * Precision_hospital * Recall_hospital) / (Precision_hospital + Recall_hospital)\n",
    "Precision_hospital,Recall_hospital,F1_hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 2107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(hospital_clean!=hospital_dirty_50_error).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2106,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_dirty_50_error = pd.read_csv('/home/yanmy/raha/raha-master/datasets/hospital/vary_error_rate/hospital_dirty_50_error.csv',index_col=0).astype(str)\n",
    "hospital_dirty_50_error.to_csv('datasets/hospital/vary_error_rate/hospital_50.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2108,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_dirty_40_error = pd.read_csv('/home/yanmy/raha/raha-master/datasets/hospital/vary_error_rate/hospital_dirty_40_error.csv',index_col=0).astype(str)\n",
    "hospital_dirty_40_error.to_csv('datasets/hospital/vary_error_rate/hospital_40.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2100,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_dirty_30_error = pd.read_csv('/home/yanmy/raha/raha-master/datasets/hospital/vary_error_rate/hospital_dirty_30_error.csv',index_col=0).astype(str)\n",
    "hospital_dirty_30_error.to_csv('datasets/hospital/vary_error_rate/hospital_30.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2101,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_dirty_20_error = pd.read_csv('/home/yanmy/raha/raha-master/datasets/hospital/vary_error_rate/hospital_dirty_20_error.csv',index_col=0).astype(str)\n",
    "hospital_dirty_20_error.to_csv('datasets/hospital/vary_error_rate/hospital_20.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2102,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_dirty_10_error = pd.read_csv('/home/yanmy/raha/raha-master/datasets/hospital/vary_error_rate/hospital_dirty_10_error.csv',index_col=0).astype(str)\n",
    "hospital_dirty_10_error.to_csv('datasets/hospital/vary_error_rate/hospital_10.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2092,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1764, 1938, 2000)"
      ]
     },
     "execution_count": 2092,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Correct_Fixed_Error,All_Fixed_Error,All_Data_Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1764, 1938, 2000) (0.9102167182662538, 0.882, 0.8958862366683595) 10\n",
    "(3620, 3890, 4000) (Correct_Fixed_Error,All_Fixed_Error,All_Data_Error) (0.9305912596401028, 0.905, 0.9176172370088721) 20\n",
    "(5475, 5847, 6000) (0.9363776295536173, 0.9125, 0.9242846290200051) 30\n",
    "(7290, 7750, 8000) (0.9406451612903226, 0.91125, 0.9257142857142856) 40\n",
    "(9207, 9782, 10000) (0.9412185647106931, 0.9207, 0.9308462238398544) 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flight_Vary_Error_Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([   1,    2,    5,   31,   32,   33,   36,   37,   50,   51,\n",
       "            ...\n",
       "            2324, 2325, 2326, 2328, 2339, 2355, 2356, 2357, 2358, 2362],\n",
       "           dtype='int64', length=1326)"
      ]
     },
     "execution_count": 2207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_dirty_30_error = pd.read_csv('datasets/flights/dirty.csv').fillna('').astype(str)\n",
    "flight_temp = flight_dirty_30_error.copy()\n",
    "flight_temp['count'] = flight_temp.apply(Check_Clean_Time_Format,axis=1)\n",
    "inject_index = flight_temp[flight_temp['count']>0].index\n",
    "inject_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8200\n"
     ]
    }
   ],
   "source": [
    "# flight_clean.iloc[C]\n",
    "flight_clean = pd.read_csv('datasets/flights/clean.csv').fillna('').astype(str)\n",
    "def Check_Clean_Time_Format(row):\n",
    "    count = 0\n",
    "    for x,y in row[3:].items():\n",
    "        if not (is_clean_time_format(y)):\n",
    "            count += 1\n",
    "    return count\n",
    "flight_dirty_30_error = pd.read_csv('datasets/flights/dirty.csv').fillna('').astype(str)\n",
    "flight_temp = flight_dirty_50_error.copy()\n",
    "print(np.array(flight_temp!=flight_clean).sum())\n",
    "flight_temp['count'] = flight_temp.apply(Check_Clean_Time_Format,axis=1)\n",
    "flight_dirty_clean = flight_temp.copy()\n",
    "# cluster_select_flight = cluster_select_flight_15\n",
    "cluster_select_flight = cluster_select_flight_20\n",
    "for i in range(100): ## Data Cleaning Clusters\n",
    "    test = flight_temp[flight_temp['flight']==flight_unique[i]]\n",
    "    test_index = test.index\n",
    "    if(i in cluster_select_flight): ## 同cluster内有ground truth，传播结果\n",
    "        clean_cell = flight_clean[flight_clean['flight']==flight_unique[i]].iloc[0:1,-4:] ## clean time, last 4 cells\n",
    "        flight_dirty_clean.iloc[test_index,-5:-1] = clean_cell\n",
    "    else: ## Graph Method to Vote the most common clean files\n",
    "        try:\n",
    "            test_clean = test[test['count']<3]\n",
    "            C = CoresetIndex(test_clean)\n",
    "            clean_cell = flight_temp.iloc[C[0],-5:-1] ## clean time, last 4 cells\n",
    "            flight_dirty_clean.iloc[test_index,-5:-1] = clean_cell\n",
    "        except:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34555492638731594 0.2976829268292683 0.3198375262054507\n"
     ]
    }
   ],
   "source": [
    "flight_clean = pd.read_csv('datasets/flights/clean.csv').fillna('').astype(str)\n",
    "# flight_dirty = pd.read_csv('datasets/flights/dirty.csv').fillna('').astype(str)\n",
    "flight_dirty = flight_dirty_50_error.copy()\n",
    "All_Error = np.array(flight_dirty!=flight_clean).astype(int)\n",
    "All_Data_Error = All_Error.sum()\n",
    "Correct_Fixed_Error = 0\n",
    "All_Fixed_Error = 0\n",
    "for x in range(All_Error.shape[0]):\n",
    "    for y in range(All_Error.shape[1]):\n",
    "        dirty_cell = flight_dirty.iloc[x,y]\n",
    "        fixed_cell = flight_dirty_clean.iloc[x,y]\n",
    "        clean_cell = flight_clean.iloc[x,y]\n",
    "        if(fixed_cell!=dirty_cell):\n",
    "            All_Fixed_Error += 1\n",
    "            if(fixed_cell==clean_cell):\n",
    "                Correct_Fixed_Error += 1\n",
    "Precision = Correct_Fixed_Error / All_Fixed_Error\n",
    "Recall = Correct_Fixed_Error / All_Data_Error\n",
    "F1 = (2*Precision*Recall) / (Precision + Recall)\n",
    "print(Precision,Recall,F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2198,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_dirty_50_error.to_csv('datasets/flights/vary_error_rate/50/flight_dirty_50_error_correction.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8200"
      ]
     },
     "execution_count": 2199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(flight_dirty_50_error!=flight_clean).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 2161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(flight_dirty_clean.iloc[:,:-1]!=flight_clean).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 1 1 10\n",
    "0.9884111009454102 0.988109756097561 0.9882604055496266 20\n",
    "0.96 0.9628048780487805 0.96 30\n",
    "0.7389435989256938 0.6291158536585366 0.6796212433100042 40\n",
    "0.7243723849372385 0.3378048780487805 0.46074517631403855 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generator Parameter Size Effect Research: Hospital"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vicuna-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(496, 508)"
      ]
     },
     "execution_count": 2245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(hospital_detector==1)),len(hospital_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{\"MeasureName\": \"surgery patients who were tak...</td>\n",
       "      <td>{\"MeasureName\": \"surgery patients who were tak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{\"City\": \"birmingham\"}</td>\n",
       "      <td>{\"City\": \"birmingham\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{\"City\": \"birmingham\"}</td>\n",
       "      <td>{\"City\": \"birmingham\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{\"City\": \"sheffield\"}</td>\n",
       "      <td>{\"City\": \"sheffield\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{\"ProviderNumber\": \"10019\"}</td>\n",
       "      <td>{\"ProviderNumber\": \"10019\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>{\"HospitalType\": \"acute care hospitals\"}</td>\n",
       "      <td>{\"HospitalType\": \"acute care hospitals\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>{\"City\": \"oneonta\"}</td>\n",
       "      <td>{\"City\": \"enterprise\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>{\"Address1\": \"150 gilbreath drive\"}</td>\n",
       "      <td>{\"Address1\": \"150 gilbreath drive\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>{\"HospitalName\": \"st vincents blount\"}</td>\n",
       "      <td>{\"HospitalName\": \"st vincents blount\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>{\"MeasureName\": \"surgery patients who were tak...</td>\n",
       "      <td>{\"MeasureName\": \"surgery patients who were tre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>508 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 label  \\\n",
       "0    {\"MeasureName\": \"surgery patients who were tak...   \n",
       "1                               {\"City\": \"birmingham\"}   \n",
       "2                               {\"City\": \"birmingham\"}   \n",
       "3                                {\"City\": \"sheffield\"}   \n",
       "4                          {\"ProviderNumber\": \"10019\"}   \n",
       "..                                                 ...   \n",
       "503           {\"HospitalType\": \"acute care hospitals\"}   \n",
       "504                                {\"City\": \"oneonta\"}   \n",
       "505                {\"Address1\": \"150 gilbreath drive\"}   \n",
       "506             {\"HospitalName\": \"st vincents blount\"}   \n",
       "507  {\"MeasureName\": \"surgery patients who were tak...   \n",
       "\n",
       "                                               predict  \n",
       "0    {\"MeasureName\": \"surgery patients who were tak...  \n",
       "1                               {\"City\": \"birmingham\"}  \n",
       "2                               {\"City\": \"birmingham\"}  \n",
       "3                                {\"City\": \"sheffield\"}  \n",
       "4                          {\"ProviderNumber\": \"10019\"}  \n",
       "..                                                 ...  \n",
       "503           {\"HospitalType\": \"acute care hospitals\"}  \n",
       "504                             {\"City\": \"enterprise\"}  \n",
       "505                {\"Address1\": \"150 gilbreath drive\"}  \n",
       "506             {\"HospitalName\": \"st vincents blount\"}  \n",
       "507  {\"MeasureName\": \"surgery patients who were tre...  \n",
       "\n",
       "[508 rows x 2 columns]"
      ]
     },
     "execution_count": 2258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_json('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/bloom/hospital-test/generated_predictions.jsonl',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "508 508\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "# hospital_dirty = pd.read_csv('/home/yanmy/raha/raha-master/datasets/hospital/vary_error_rate/hospital_dirty_10_error.csv',index_col=0).astype(str)\n",
    "hospital_correction = hospital_dirty.copy()\n",
    "# hospital_result = pd.read_csv('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/vicuna_7b_hospital-test.csv',index_col=0) ## Vicuna-7B\n",
    "hospital_result = pd.read_csv('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/vicuna-13b-hospital-test.csv',index_col=0) ## Vicuna-13B\n",
    "# hospital_result = pd.read_json('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/bloom/hospital-test/generated_predictions.jsonl',lines=True) ## BLOOM-560B\n",
    "# hospital_result = pd.read_json('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/hospital-test/generated_predictions.jsonl',lines=True) ## ChatGLM3\n",
    "hospital_detector = np.load('/home/yanmy/raha/raha-master/datasets/hospital/detector/detector_5.npy').reshape((-1,20))\n",
    "import ast\n",
    "for d in np.argwhere(hospital_detector==1):\n",
    "    i = d[0]\n",
    "    j = d[1]\n",
    "    try:\n",
    "        predict = list(ast.literal_eval(hospital_result.iloc[count,-1]).values())[0]\n",
    "        hospital_correction.iloc[i,j] = predict\n",
    "        count += 1\n",
    "    except:\n",
    "        print(count)\n",
    "        count += 1\n",
    "print(count,len(hospital_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2737,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9823 9823 9823\n"
     ]
    }
   ],
   "source": [
    "## T5 Baseline for Beers\n",
    "count = 0\n",
    "hospital_result = pd.read_csv('/home/yanmy/raha/raha-master/datasets/hospital/t5/test_50_correction.csv',index_col=0).fillna('')\n",
    "hospital_detector = np.load('/home/yanmy/raha/raha-master/datasets/hospital/vary_error_rate/hospital_50_detector.npy')\n",
    "hospital_correction = hospital_dirty.copy()\n",
    "for d in np.argwhere(hospital_detector==1):\n",
    "    i = d[0] \n",
    "    j = d[1]\n",
    "    \n",
    "    try:\n",
    "        # predict = list(ast.literal_eval(beer_result.iloc[count,-1]).values())[0]\n",
    "        predict = hospital_result.iloc[count,-1]\n",
    "        hospital_correction.iloc[i,j] = predict\n",
    "        count += 1\n",
    "    except:\n",
    "        print(count)\n",
    "        count += 1\n",
    "print(count,len(np.argwhere(hospital_detector==1)),len(hospital_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2738,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.02627800114876508, 0.35952848722986247, 0.04897631473303895)"
      ]
     },
     "execution_count": 2738,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hospital_correction = pd.read_csv('/home/yanmy/raha/raha-master/datasets/hospital/correct_result/hospital_correction_20.csv',index_col=0).astype(str)\n",
    "All_Data_Error = 0\n",
    "All_Fixed_Error = 0\n",
    "Correct_Fixed_Error = 0\n",
    "# hospital_correction.iloc[:,3] = hospital_dirty.iloc[:,3]\n",
    "# hospital_correction.iloc[:,14] = hospital_dirty.iloc[:,14]\n",
    "# a = np.where(input_matrix_hospital[hospital_label_index[:9]].sum(axis=0)!=0)\n",
    "# b = np.where(input_matrix_hospital[hospital_label_index[:20]].sum(axis=0)!=0)\n",
    "# for h in [i for i in b[0] if i not in a[0]]:\n",
    "#     hospital_correction.iloc[:,h] = hospital_dirty.iloc[:,h]\n",
    "# hospital_dirty_10_error = pd.read_csv('/home/yanmy/raha/raha-master/datasets/hospital/vary_error_rate/hospital_dirty_10_error.csv',index_col=0).astype(str)\n",
    "for i in range(1000):\n",
    "    for j in range(20):\n",
    "        dirty_cell = hospital_dirty.iloc[i,j]\n",
    "        clean_cell = hospital_clean.iloc[i,j]\n",
    "        correct_cell = hospital_correction.iloc[i,j]\n",
    "        if(correct_cell!=dirty_cell):\n",
    "            All_Fixed_Error += 1\n",
    "        if(clean_cell!=dirty_cell):\n",
    "            All_Data_Error += 1\n",
    "            if(correct_cell==clean_cell):\n",
    "                Correct_Fixed_Error += 1\n",
    "Precision_hospital = Correct_Fixed_Error / All_Fixed_Error\n",
    "Recall_hospital = Correct_Fixed_Error / All_Data_Error\n",
    "F1_hospital = (2 * Precision_hospital * Recall_hospital) / (Precision_hospital + Recall_hospital)\n",
    "Precision_hospital,Recall_hospital,F1_hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.14211695040710584, 0.37721021611001965, 0.2064516129032258)\n",
    "(0.07027225901398086, 0.37524557956778, 0.11837620080570188)\n",
    "(0.04485935984481086, 0.36345776031434185, 0.0798618605655083)\n",
    "(0.03348334833483348, 0.3654223968565815, 0.06134564643799473)\n",
    "(0.02627800114876508, 0.35952848722986247, 0.04897631473303895)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hospital_Vicuna-7B:(0.9105367793240556, 0.899803536345776, 0.9051383399209485)\n",
    "Hospital_Bloom-560B:(0.7924528301886793, 0.7426326129666012, 0.7667342799188641)\n",
    "Hospital_ChatGLM3-6B:(0.9027777777777778, 0.8939096267190569, 0.8983218163869695)\n",
    "Hospital_Vicuna-13B:(0.9447731755424064, 0.9410609037328095, 0.9429133858267718)\n",
    "Hospital_T5:(0.5409836065573771, 0.3889980353634578, 0.4525714285714286)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rayyan-Generator-Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1117 1117 1117\n"
     ]
    }
   ],
   "source": [
    "## Try to test recall on Rayyan Dataset\n",
    "import json\n",
    "rayyan_clean = pd.read_csv('datasets/rayyan/clean.csv').fillna('')\n",
    "rayyan_dirty = pd.read_csv('datasets/rayyan/dirty.csv').fillna('')\n",
    "# rayyan_result = pd.read_csv('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/vicuna_7b_rayyan-test-20.csv',index_col=0) ## Vicuna-7B\n",
    "# rayyan_result = pd.read_json('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/bloom/rayyan-test-20/generated_predictions.jsonl',lines=True) ## Bloom-560M\n",
    "# rayyan_result = pd.read_json('//home/yanmy/LLaMA-Efficient-Tuning-main/inference/rayyan-test-20/generated_predictions.jsonl',lines=True) ## ChatGLM-3\n",
    "rayyan_result = pd.read_csv('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/vicuna-13b-rayyan-test-ablation.csv',index_col=0) ## Ablation Study for Rayyan_Generation\n",
    "def Str2Int(row):\n",
    "    for index in range(11):\n",
    "        temp = row[index]\n",
    "        try:\n",
    "            row[index] = str(int(temp))\n",
    "        except:\n",
    "            continue\n",
    "    return row\n",
    "rayyan_clean = rayyan_clean.apply(Str2Int,axis=1)\n",
    "rayyan_dirty = rayyan_dirty.apply(Str2Int,axis=1)\n",
    "count = 0\n",
    "valid_count = 0\n",
    "rayyan_correction = rayyan_dirty.copy()\n",
    "import ast\n",
    "for d in np.argwhere(rayyan_detector==1):\n",
    "    i = d[0]\n",
    "    j = d[1] + 1 ## Ignore Index\n",
    "    try:\n",
    "        predict = list(ast.literal_eval(rayyan_result.iloc[count,-1]).values())[0]\n",
    "        rayyan_correction.iloc[i,j] = predict\n",
    "        valid_count += 1\n",
    "    except:\n",
    "        predict = rayyan_result.iloc[count,-1]\n",
    "        rayyan_correction.iloc[i,j] = predict\n",
    "    count += 1\n",
    "print(count,len(rayyan_result),len(np.argwhere(rayyan_detector==1)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1117 1117 1117\n"
     ]
    }
   ],
   "source": [
    "## T5 Baseline for Beers\n",
    "count = 0\n",
    "rayyan_result = pd.read_csv('/home/yanmy/raha/raha-master/datasets/rayyan/t5/t5_correction.csv',index_col=0).fillna('')\n",
    "rayyan_correction = rayyan_dirty.copy()\n",
    "for d in np.argwhere(rayyan_detector==1):\n",
    "    i = d[0] \n",
    "    j = d[1] + 1 ## skip id\n",
    "    \n",
    "    try:\n",
    "        # predict = list(ast.literal_eval(beer_result.iloc[count,-1]).values())[0]\n",
    "        predict = rayyan_result.iloc[count,-1]\n",
    "        rayyan_correction.iloc[i,j] = predict\n",
    "        count += 1\n",
    "    except:\n",
    "        print(count)\n",
    "        count += 1\n",
    "print(count,len(np.argwhere(rayyan_detector==1)),len(rayyan_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.13339301700984782, 0.1571729957805907, 0.14430992736077483)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "rayyan_clean = pd.read_csv('datasets/rayyan/clean.csv').fillna('')\n",
    "rayyan_dirty = pd.read_csv('datasets/rayyan/dirty.csv').fillna('')\n",
    "def Str2Int(row):\n",
    "    for index in range(11):\n",
    "        temp = row[index]\n",
    "        try:\n",
    "            row[index] = str(int(temp))\n",
    "        except:\n",
    "            continue\n",
    "    return row\n",
    "rayyan_clean = rayyan_clean.apply(Str2Int,axis=1)\n",
    "rayyan_dirty = rayyan_dirty.apply(Str2Int,axis=1)\n",
    "All_Data_Error = 0\n",
    "All_Fixed_Error = 0\n",
    "Correct_Fixed_Error = 0\n",
    "for i in range(1000):\n",
    "    for j in range(11):\n",
    "        dirty_cell = rayyan_dirty.iloc[i,j]\n",
    "        clean_cell = rayyan_clean.iloc[i,j]\n",
    "        correct_cell = rayyan_correction.iloc[i,j]\n",
    "        if(correct_cell!=dirty_cell):\n",
    "            All_Fixed_Error += 1\n",
    "        if(clean_cell!=dirty_cell):\n",
    "            All_Data_Error += 1\n",
    "            if(correct_cell==clean_cell):\n",
    "                Correct_Fixed_Error += 1\n",
    "Precision_hospital = Correct_Fixed_Error / All_Fixed_Error\n",
    "Recall_hospital = Correct_Fixed_Error / All_Data_Error\n",
    "F1_hospital = (2 * Precision_hospital * Recall_hospital) / (Precision_hospital + Recall_hospital)\n",
    "Precision_hospital,Recall_hospital,F1_hospital"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rayyan_Vicuna-7B:(0.6332720588235294, 0.7267932489451476, 0.6768172888015718)\n",
    "Rayyan_Bloom-560M:(0.5892672858617131, 0.6023206751054853, 0.5957224830464266)\n",
    "ChatGLM-6B:(0.6975023126734505, 0.7953586497890295, 0.7432232626909808)\n",
    "Rayyan_t5 (0.5532516493873704, 0.619198312236287, 0.5843703334992533)\n",
    "Rayyan_Ablation_Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beers-Generator Parameter Size Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector_beer = np.load('datasets/beers/detector/multi-view/detection_cell_5.npy').reshape((-1,9))\n",
    "## Only the labelled attributes is dirty\n",
    "# detector_beer.sum(axis=0)\n",
    "detector_beer[:,0] = 0\n",
    "detector_beer[:,1] = 0\n",
    "detector_beer[:,4] = 0\n",
    "detector_beer[:,5] = 0\n",
    "detector_beer[:,6] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3364 3364 3364\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "# beer_result = pd.read_csv('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/vicuna_7b_beer-test-20.csv',index_col=0) ## Vicuna-7B\n",
    "# beer_result = pd.read_csv('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/beer-test-20.csv',index_col=0) ## Vicuna-13B\n",
    "# beer_result = pd.read_json('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/bloom/beer-test-20/generated_predictions.jsonl',lines=True) ## Bloom-560M\n",
    "# beer_result = pd.read_json('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/beer-test-20/generated_predictions.jsonl',lines=True) ## Bloom-560M\n",
    "beer_result = pd.read_csv('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/vicuna-13b-beer-test-ablation.csv',index_col=0) ## generation-ablation\n",
    "beer_correction = beer_dirty.copy()\n",
    "for d in np.argwhere(detector_beer==1):\n",
    "    i = d[0] \n",
    "    j = d[1] + 2\n",
    "    \n",
    "    try:\n",
    "        predict = list(ast.literal_eval(beer_result.iloc[count,-1]).values())[0]\n",
    "        beer_correction.iloc[i,j] = predict\n",
    "        count += 1\n",
    "    except:\n",
    "        # print(count)\n",
    "        count += 1\n",
    "print(count,len(np.argwhere(detector_beer==1)),len(beer_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2579,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26510 3364 26510\n"
     ]
    }
   ],
   "source": [
    "## T5 Baseline for Beers\n",
    "count = 0\n",
    "beer_result = pd.read_csv('/home/yanmy/raha/raha-master/datasets/beers/t5/t5_correction_all.csv',index_col=0).fillna('')\n",
    "beer_correction = beer_dirty.copy()\n",
    "for d in np.argwhere(np.zeros(beer_clean.shape)==0):\n",
    "    i = d[0] \n",
    "    j = d[1]\n",
    "    \n",
    "    try:\n",
    "        # predict = list(ast.literal_eval(beer_result.iloc[count,-1]).values())[0]\n",
    "        predict = beer_result.iloc[count,-1]\n",
    "        beer_correction.iloc[i,j] = predict\n",
    "        count += 1\n",
    "    except:\n",
    "        print(count)\n",
    "        count += 1\n",
    "print(count,len(np.argwhere(detector_beer==1)),len(beer_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5689655172413793, 0.5701519213583557, 0.5695581014729951)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_Data_Error = 0\n",
    "All_Fixed_Error = 0\n",
    "Correct_Fixed_Error = 0\n",
    "for i in range(len(beer_clean)):\n",
    "    for j in range(11):\n",
    "        dirty_cell = beer_dirty.iloc[i,j]\n",
    "        clean_cell = beer_clean.iloc[i,j]\n",
    "        correct_cell = beer_correction.iloc[i,j]\n",
    "        if(correct_cell!=dirty_cell):\n",
    "            All_Fixed_Error += 1\n",
    "        if(clean_cell!=dirty_cell):\n",
    "            All_Data_Error += 1\n",
    "            if(correct_cell==clean_cell):\n",
    "                Correct_Fixed_Error += 1\n",
    "Precision_hospital = Correct_Fixed_Error / All_Fixed_Error\n",
    "Recall_hospital = Correct_Fixed_Error / All_Data_Error\n",
    "F1_hospital = (2 * Precision_hospital * Recall_hospital) / (Precision_hospital + Recall_hospital)\n",
    "Precision_hospital,Recall_hospital,F1_hospital"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beer_Vicuna-7B:(0.8127229488703924, 0.8144176347929699, 0.8135694093140902)\n",
    "Beer_Bloom-560M:(0.92496765847348, 0.8519511468573131, 0.8869592184834859)\n",
    "ChatGLM-6B: (0.9628418549346016, 0.9648495680667263, 0.9638446659723255)\n",
    "Beer_t5: (0.7261744966442953, 0.966934763181412, 0.8294365657339978)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2357,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "def imdb_year_detection(cell):\n",
    "    \"\"\"\n",
    "    Check if the given cell is dirty or not for the startYear column.\n",
    "\n",
    "    :param cell: A string representing a cell from the startYear column.\n",
    "    :return: Boolean indicating whether the cell is dirty.\n",
    "    \"\"\"\n",
    "    # Regular expression for a clean cell: exactly four digits\n",
    "    clean_pattern = r'^\\d{4}$'\n",
    "\n",
    "    # Return True (dirty) if cell does not match the clean pattern\n",
    "    return not re.match(clean_pattern, cell)\n",
    "def imdb_year_generation(clean_cell):\n",
    "    \"\"\"\n",
    "    Generate a dirty cell from a clean one. The clean cell is assumed to be\n",
    "    a four-digit year. The dirty cell will be the last two digits of the year.\n",
    "\n",
    "    :param clean_cell: A string representing a clean cell (four-digit year).\n",
    "    :return: A string representing a dirty cell (last two digits of the year).\n",
    "    \"\"\"\n",
    "    # Extract the last two digits of the year\n",
    "    dirty_cell = clean_cell[-2:]\n",
    "\n",
    "    return dirty_cell\n",
    "def imdb_runtime_detection(cell):\n",
    "    pattern = r'^\\d+$'\n",
    "    return not re.match(pattern, cell)\n",
    "\n",
    "def imdb_runtime_generation(clean_cell):\n",
    "    minutes = int(clean_cell)\n",
    "\n",
    "    # Convert minutes to hours and add a random level of decimal precision\n",
    "    hours = minutes / 60\n",
    "    precision = random.choice([1, 2, 3])  # Random precision level\n",
    "    formatted_hours = round(hours, precision)\n",
    "\n",
    "    # Format the dirty cell\n",
    "    dirty_cell = f\"{formatted_hours} h\"\n",
    "\n",
    "    return dirty_cell\n",
    "def imdb_director_detection(cell):\n",
    "    # Regular expression to detect if the cell contains special characters or combining diacritical marks\n",
    "    pattern = re.compile(r'[�\\u0300-\\u036F]')\n",
    "    return bool(pattern.search(cell))\n",
    "def imdb_director_generation(cell):\n",
    "    special_chars = ['�', '\\u0301', '\\u0300', '\\u0302', '\\u0303', '\\u0304']\n",
    "    \n",
    "    # Randomly choose a special character\n",
    "    char = random.choice(special_chars)\n",
    "    \n",
    "    # Randomly choose a position to insert the special character\n",
    "    position = random.randint(0, len(cell))\n",
    "    \n",
    "    # Insert the special character at the chosen position\n",
    "    dirty_cell = cell[:position] + char + cell[position:]\n",
    "    \n",
    "    return dirty_cell\n",
    "def imdb_title_detection(cell):\n",
    "    return cell.__contains__('x')\n",
    "def imdb_title_generation(input_string):\n",
    "    if not input_string:\n",
    "        return input_string\n",
    "\n",
    "    # 随机选择要替换的字符\n",
    "    char_to_replace = random.choice(input_string)\n",
    "\n",
    "    # 使用字符串的 replace 方法替换所有相同字符为 'x'\n",
    "    result_string = input_string.replace(char_to_replace, 'x')\n",
    "\n",
    "    return result_string\n",
    "def Imdb_Row_Detect(x,cell):\n",
    "    if(x=='titleType'):\n",
    "        return imdb_title_detection(cell)\n",
    "    elif(x=='title'):\n",
    "        return imdb_title_detection(cell)\n",
    "    elif(x=='startYear'):\n",
    "        return imdb_year_detection(cell)\n",
    "    elif(x=='runtimeMinutes'):\n",
    "        return imdb_runtime_detection(cell)\n",
    "    elif(x=='director'):\n",
    "        return imdb_director_detection(cell)\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def Imdb_Row_Generate(x,cell):\n",
    "    if(x=='titleType'):\n",
    "        return imdb_title_generation(cell)\n",
    "    elif(x=='title'):\n",
    "        return imdb_title_generation(cell)\n",
    "    elif(x=='startYear'):\n",
    "        return imdb_year_generation(cell)\n",
    "    elif(x=='runtimeMinutes'):\n",
    "        return imdb_runtime_generation(cell)\n",
    "    elif(x=='director'):\n",
    "        return imdb_director_generation(cell)\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    48,     52,     67, ..., 999956, 999987, 999994])"
      ]
     },
     "execution_count": 2371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_error_index = np.where(input_matrix_imdb.sum(axis=1)!=0)[0]\n",
    "imdb_error_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2372,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('datasets/imdb/imdb_error_index.npy',np.array(imdb_error_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e73884e8cc6b42529845e042896d7f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### For Different Error_Rate, Add Reference Files For Testing\n",
    "imdb_dirty = pd.read_csv('datasets/imdb/dirty.csv').fillna('')\n",
    "imdb_dirty = imdb_dirty.parallel_apply(Str2Int,axis=1)\n",
    "input_matrix_imdb = np.array(imdb_dirty!=imdb_clean).astype(int)\n",
    "# input_matrix_select = input_matrix[selected_rows]\n",
    "detector_list_all = []\n",
    "imdb_dirty_vary_error = imdb_dirty.copy()\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "# for label_tuple in tqdm(imdb_label_index):\n",
    "for label_tuple in tqdm(range(len(imdb_clean))):\n",
    "# for label_tuple in tqdm(imdb_error_index):\n",
    "    for i in range(len(imdb_clean.columns)):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = imdb_clean.iloc[label_tuple].copy()\n",
    "        dirty_context = imdb_dirty_vary_error.iloc[label_tuple].copy()\n",
    "        clean_cell = imdb_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = imdb_dirty_vary_error.iloc[label_tuple,i]\n",
    "        # single_context_clean = 'COL %s VAL %s ' % (imdb_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (imdb_clean.columns[i],dirty_cell)\n",
    "        for c in range(6):\n",
    "            # all_context_clean += 'COL %s VAL %s ' % (imdb_clean.columns[c],imdb_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (imdb_dirty_vary_error.columns[c],imdb_dirty_vary_error.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        if(str(dirty_cell)!=str(clean_cell)):\n",
    "            detector_list_all.append([all_context_dirty,single_context_dirty,1])\n",
    "            # detector_list_all.append([all_context_clean,single_context_clean,0])\n",
    "        else:\n",
    "            detector_list_all.append([all_context_dirty,single_context_dirty,0])\n",
    "            # detector_list.append([single_context_dirty,1])\n",
    "        # if(dirty_cell!=clean_cell):\n",
    "        #     detector_list_tax.append([all_context_dirty,single_context_dirty,1])\n",
    "        #     detector_list_tax.append([all_context_clean,single_context_clean,0])\n",
    "        # else:\n",
    "        #     detector_list_tax.append([all_context_dirty,single_context_dirty,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2460,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_detection_test = pd.DataFrame(detector_list_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2461,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_detection_test.to_csv('datasets/imdb/detector/test_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2411,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_detection_test.to_csv('datasets/imdb/detector/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a20e3ffaefa24664baefcedd19258225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Data Augmentation\n",
    "imdb_dirty = pd.read_csv('datasets/imdb/dirty.csv').fillna('')\n",
    "imdb_dirty = imdb_dirty.parallel_apply(Str2Int,axis=1)\n",
    "input_matrix_imdb = np.array(imdb_dirty!=imdb_clean).astype(int)\n",
    "# input_matrix_select = input_matrix[selected_rows]\n",
    "detector_list_imdb = []\n",
    "imdb_dirty_vary_error = imdb_dirty.copy()\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "# for label_tuple in tqdm(selected_rows):\n",
    "# for label_tuple in tqdm(range(len(imdb_clean))):\n",
    "for label_tuple in tqdm(imdb_error_index):\n",
    "    for i in range(len(imdb_dirty_vary_error.columns)-1): ## Exclude genres\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        clean_cell = imdb_dirty_vary_error.iloc[label_tuple,i]\n",
    "        if(not Imdb_Row_Detect(imdb_dirty_vary_error.columns[i],clean_cell)): ## Clean Value\n",
    "            all_context_clean = ''\n",
    "            all_context_dirty = ''\n",
    "            clean_context = imdb_dirty.iloc[label_tuple].copy()\n",
    "            dirty_context = imdb_dirty.iloc[label_tuple].copy()\n",
    "            # clean_cell = imdb_dirty_vary_error.iloc[label_tuple,i]\n",
    "            dirty_cell = Imdb_Row_Generate(imdb_dirty_vary_error.columns[i],clean_cell)\n",
    "            dirty_context[imdb_dirty_vary_error.columns[i]] = dirty_cell ## Inject Noise On Dirty_Cell\n",
    "            single_context_clean = 'COL %s VAL %s ' % (imdb_dirty_vary_error.columns[i],clean_cell)\n",
    "            single_context_dirty = 'COL %s VAL %s ' % (imdb_dirty_vary_error.columns[i],dirty_cell)\n",
    "            for c in range(6):\n",
    "                all_context_clean += 'COL %s VAL %s ' % (imdb_dirty_vary_error.columns[c],clean_context[c])\n",
    "                all_context_dirty += 'COL %s VAL %s ' % (imdb_dirty_vary_error.columns[c],dirty_context[c])\n",
    "            # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "            # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "            # detector_list.append([single_context_clean,0])        \n",
    "            # detector_list.append([all_context_clean,0])\n",
    "            # if(str(dirty_cell)!=str(clean_cell)):\n",
    "            #     detector_list_all.append([all_context_dirty,single_context_dirty,1])\n",
    "            # else:\n",
    "            #     detector_list_all.append([all_context_dirty,single_context_dirty,0])\n",
    "                # detector_list.append([single_context_dirty,1])\n",
    "            if(dirty_cell!=clean_cell):\n",
    "                detector_list_imdb.append([all_context_dirty,single_context_dirty,1])\n",
    "                detector_list_imdb.append([all_context_clean,single_context_clean,0])\n",
    "            else:\n",
    "                detector_list_imdb.append([all_context_dirty,single_context_dirty,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2409,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_detection_test_aug = pd.DataFrame(detector_list_imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2412,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_detection_test_aug.to_csv('datasets/imdb/detector/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raha's performance on imdb:\n",
    "Precision = 0.28\n",
    "Recall = 0.37\n",
    "F1 = 0.32\n",
    "Baran's performance on imdb:\n",
    "Precision = 0.19\n",
    "Recall = 0.08\n",
    "F1 = 0.12\n",
    "运行时间：3818.2799582481384 秒"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0467520c17b4957bab973673adb5134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# def ICL_Beer(row):\n",
    "training_list_label = []\n",
    "for index in tqdm(imdb_error_index):\n",
    "    for i in range(5):\n",
    "        clean_cell = imdb_dirty.iloc[index,i]\n",
    "        if(not Imdb_Row_Detect(imdb_dirty.columns[i],clean_cell)): ## Clean Value\n",
    "            dirty_cell = Imdb_Row_Generate(imdb_dirty_vary_error.columns[i],clean_cell)\n",
    "            \n",
    "            col_name = imdb_dirty.columns[i]\n",
    "            template_dict = {}\n",
    "            template_dict[col_name] = ''\n",
    "            temp_dict = imdb_dirty.iloc[index].to_dict()\n",
    "            temp_dict[col_name] = dirty_cell\n",
    "            clean_dict = {}\n",
    "            clean_dict[col_name] = clean_cell\n",
    "            coreset_reference = np.random.choice([c for c in imdb_label_index if c!=index],3,replace=False)\n",
    "            if(dirty_cell!=clean_cell):\n",
    "                text_head = 'You are an expert in cleaning IMDB Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\nTake these rows as reference:\\n\\n' % (col_name, json.dumps(template_dict), json.dumps(temp_dict))\n",
    "                dict_0 = imdb_clean.iloc[coreset_reference[0]].to_dict()\n",
    "                dict_1 = imdb_clean.iloc[coreset_reference[1]].to_dict()\n",
    "                dict_2 = imdb_clean.iloc[coreset_reference[2]].to_dict()\n",
    "                ICL_text = '%s\\n\\n%s\\n\\n%s\\n\\n' % (json.dumps(dict_0),json.dumps(dict_1),json.dumps(dict_2))\n",
    "                training_list_label.append([text_head,ICL_text,'',json.dumps(clean_dict)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Imdb_Row_Detect('title','WrestleMania XV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2427,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list_pd = pd.DataFrame(training_list_label).sample(n=3500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2428,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list_pd['instruction'] = training_list_pd[0] + training_list_pd[1]\n",
    "training_list_pd['input'] = training_list_pd[2]\n",
    "training_list_pd['output'] = training_list_pd[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2429,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(training_list_pd.iloc[:,-3:].to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/imdb/imdb-train.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2435,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Construct Testing Inference Set\n",
    "training_list_label = []\n",
    "imdb_detector = np.load('datasets/imdb/detector/detector.npy')\n",
    "count = 0\n",
    "for d in np.argwhere(imdb_detector==1):\n",
    "    index = imdb_error_index[d[0]]\n",
    "    i = d[1]\n",
    "    dirty_cell = imdb_dirty.iloc[index,i]\n",
    "    clean_cell = imdb_clean.iloc[index,i]\n",
    "    col_name = imdb_dirty.columns[i]\n",
    "    template_dict = {}\n",
    "    template_dict[col_name] = ''\n",
    "    temp_dict = imdb_dirty.iloc[index].to_dict()\n",
    "    temp_dict[col_name] = dirty_cell\n",
    "    clean_dict = {}\n",
    "    clean_dict[col_name] = clean_cell\n",
    "    coreset_reference = np.random.choice([c for c in imdb_label_index if c!=index],3,replace=False)\n",
    "    # if(dirty_cell!=clean_cell):\n",
    "    text_head = 'You are an expert in cleaning IMDB Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\nTake these rows as reference:\\n\\n' % (col_name, json.dumps(template_dict), json.dumps(temp_dict))\n",
    "    dict_0 = imdb_clean.iloc[coreset_reference[0]].to_dict()\n",
    "    dict_1 = imdb_clean.iloc[coreset_reference[1]].to_dict()\n",
    "    dict_2 = imdb_clean.iloc[coreset_reference[2]].to_dict()\n",
    "    ICL_text = '%s\\n\\n%s\\n\\n%s\\n\\n' % (json.dumps(dict_0),json.dumps(dict_1),json.dumps(dict_2))\n",
    "    training_list_label.append([text_head,ICL_text,'',json.dumps(clean_dict)])\n",
    "    if(dirty_cell==clean_cell):\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1036"
      ]
     },
     "execution_count": 2437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list_pd = pd.DataFrame(training_list_label)\n",
    "training_list_pd['instruction'] = training_list_pd[0] + training_list_pd[1]\n",
    "training_list_pd['input'] = training_list_pd[2]\n",
    "training_list_pd['output'] = training_list_pd[3]\n",
    "training_list_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2439,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(training_list_pd.iloc[:,-3:].to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/imdb/imdb-test.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2440,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(training_list_pd.iloc[:15000,-3:].to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/imdb/imdb-test-0.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
    "json.dump(training_list_pd.iloc[15000:30000,-3:].to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/imdb/imdb-test-1.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
    "json.dump(training_list_pd.iloc[30000:45000,-3:].to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/imdb/imdb-test-2.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
    "json.dump(training_list_pd.iloc[45000:,-3:].to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/imdb/imdb-test-3.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_detector = np.load('datasets/imdb/detector/detector.npy')\n",
    "imdb_error_index = np.load('datasets/imdb/imdb_error_index.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are an expert in cleaning IMDB Dataset. Gi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"startYear\": \"1999\"}</td>\n",
       "      <td>{\"startYear\": \"1999\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are an expert in cleaning IMDB Dataset. Gi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"director\": \"Kevin Dunn\"}</td>\n",
       "      <td>{\"director\": \"Kevin Dunn\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are an expert in cleaning IMDB Dataset. Gi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"title\": \"WWE TLC: Tables, Ladders &amp; Chairs\"}</td>\n",
       "      <td>{\"title\": \"WWE TLC: Tables, Ladders &amp; Chairs\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are an expert in cleaning IMDB Dataset. Gi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"title\": \"WWE Royal Rumble\"}</td>\n",
       "      <td>{\"title\": \"WWE Royal Rumble\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are an expert in cleaning IMDB Dataset. Gi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"title\": \"WrestleMania X\"}</td>\n",
       "      <td>{\"title\": \"WrestleMania X\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58494</th>\n",
       "      <td>You are an expert in cleaning IMDB Dataset. Gi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"runtimeMinutes\": \"90\"}</td>\n",
       "      <td>{\"runtimeMinutes\": \"90\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58495</th>\n",
       "      <td>You are an expert in cleaning IMDB Dataset. Gi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"titleType\": \"movie\"}</td>\n",
       "      <td>{\"titleType\": \"movie\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58496</th>\n",
       "      <td>You are an expert in cleaning IMDB Dataset. Gi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"titleType\": \"movie\"}</td>\n",
       "      <td>{\"titleType\": \"movie\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58497</th>\n",
       "      <td>You are an expert in cleaning IMDB Dataset. Gi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"director\": \"Stijn Coninx\"}</td>\n",
       "      <td>{\"director\": \"Stijn Coninx\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58498</th>\n",
       "      <td>You are an expert in cleaning IMDB Dataset. Gi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"title\": \"Chi Chi Em Em\"}</td>\n",
       "      <td>{\"title\": \"Chi Chi Em Em\"}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58499 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             instruction  input  \\\n",
       "0      You are an expert in cleaning IMDB Dataset. Gi...    NaN   \n",
       "1      You are an expert in cleaning IMDB Dataset. Gi...    NaN   \n",
       "2      You are an expert in cleaning IMDB Dataset. Gi...    NaN   \n",
       "3      You are an expert in cleaning IMDB Dataset. Gi...    NaN   \n",
       "4      You are an expert in cleaning IMDB Dataset. Gi...    NaN   \n",
       "...                                                  ...    ...   \n",
       "58494  You are an expert in cleaning IMDB Dataset. Gi...    NaN   \n",
       "58495  You are an expert in cleaning IMDB Dataset. Gi...    NaN   \n",
       "58496  You are an expert in cleaning IMDB Dataset. Gi...    NaN   \n",
       "58497  You are an expert in cleaning IMDB Dataset. Gi...    NaN   \n",
       "58498  You are an expert in cleaning IMDB Dataset. Gi...    NaN   \n",
       "\n",
       "                                               output  \\\n",
       "0                               {\"startYear\": \"1999\"}   \n",
       "1                          {\"director\": \"Kevin Dunn\"}   \n",
       "2      {\"title\": \"WWE TLC: Tables, Ladders & Chairs\"}   \n",
       "3                       {\"title\": \"WWE Royal Rumble\"}   \n",
       "4                         {\"title\": \"WrestleMania X\"}   \n",
       "...                                               ...   \n",
       "58494                        {\"runtimeMinutes\": \"90\"}   \n",
       "58495                          {\"titleType\": \"movie\"}   \n",
       "58496                          {\"titleType\": \"movie\"}   \n",
       "58497                    {\"director\": \"Stijn Coninx\"}   \n",
       "58498                      {\"title\": \"Chi Chi Em Em\"}   \n",
       "\n",
       "                                              predict  \n",
       "0                               {\"startYear\": \"1999\"}  \n",
       "1                          {\"director\": \"Kevin Dunn\"}  \n",
       "2      {\"title\": \"WWE TLC: Tables, Ladders & Chairs\"}  \n",
       "3                       {\"title\": \"WWE Royal Rumble\"}  \n",
       "4                         {\"title\": \"WrestleMania X\"}  \n",
       "...                                               ...  \n",
       "58494                        {\"runtimeMinutes\": \"90\"}  \n",
       "58495                          {\"titleType\": \"movie\"}  \n",
       "58496                          {\"titleType\": \"movie\"}  \n",
       "58497                    {\"director\": \"Stijn Coninx\"}  \n",
       "58498                      {\"title\": \"Chi Chi Em Em\"}  \n",
       "\n",
       "[58499 rows x 4 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_0 = pd.read_csv('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/vicuna-13b-imdb-test-0.csv',index_col=0)\n",
    "result_1 = pd.read_csv('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/vicuna-13b-imdb-test-1.csv',index_col=0)\n",
    "result_2 = pd.read_csv('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/vicuna-13b-imdb-test-2.csv',index_col=0)\n",
    "result_3 = pd.read_csv('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/vicuna-13b-imdb-test-3.csv',index_col=0)\n",
    "imdb_result = pd.concat([result_0,result_1,result_2,result_3]).reset_index(drop=True)\n",
    "imdb_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58499 58499 58499\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "# beer_result = pd.read_csv('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/vicuna_7b_beer-test-20.csv',index_col=0) ## Vicuna-7B\n",
    "# beer_result = pd.read_csv('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/beer-test-20.csv',index_col=0) ## Vicuna-13B\n",
    "# beer_result = pd.read_json('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/bloom/beer-test-20/generated_predictions.jsonl',lines=True) ## Bloom-560M\n",
    "# beer_result = pd.read_json('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/beer-test-20/generated_predictions.jsonl',lines=True) ## Bloom-560M\n",
    "imdb_correction = imdb_dirty.copy()\n",
    "# imdb_result = pd.read_csv('/home/yanmy/raha/raha-master/datasets/imdb/t5/t5_correction.csv',index_col=0)\n",
    "for d in np.argwhere(imdb_detector==1):\n",
    "    i = imdb_error_index[d[0]]\n",
    "    j = d[1]\n",
    "    \n",
    "    try:\n",
    "        predict = list(ast.literal_eval(imdb_result.iloc[count,-1]).values())[0]\n",
    "        # predict = imdb_result.iloc[count,-1]\n",
    "        imdb_correction.iloc[i,j] = predict\n",
    "        count += 1\n",
    "    except:\n",
    "        # print(count)\n",
    "        count += 1\n",
    "print(count,len(np.argwhere(imdb_detector==1)),len(imdb_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All_Data_Error\n",
    "# All_Fixed_Error\n",
    "Correct_Fixed_Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titleType</th>\n",
       "      <th>title</th>\n",
       "      <th>startYear</th>\n",
       "      <th>runtimeMinutes</th>\n",
       "      <th>director</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tvSpecial</td>\n",
       "      <td>Survivor Series</td>\n",
       "      <td>2008</td>\n",
       "      <td>180</td>\n",
       "      <td>Kevin Dunn</td>\n",
       "      <td>Sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tvSpecial</td>\n",
       "      <td>Summerslam</td>\n",
       "      <td>1998</td>\n",
       "      <td>166</td>\n",
       "      <td>Kevin Dunn</td>\n",
       "      <td>Sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tvSpecial</td>\n",
       "      <td>WrestleMania</td>\n",
       "      <td>2018</td>\n",
       "      <td>314</td>\n",
       "      <td>Kevin Dunn</td>\n",
       "      <td>Sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tvSpecial</td>\n",
       "      <td>Summerslam</td>\n",
       "      <td>1999</td>\n",
       "      <td>155</td>\n",
       "      <td>Kevin Dunn</td>\n",
       "      <td>Sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tvSpecial</td>\n",
       "      <td>WrestleMania X-Seven</td>\n",
       "      <td>2001</td>\n",
       "      <td>225</td>\n",
       "      <td>Kevin Dunn</td>\n",
       "      <td>Sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>movie</td>\n",
       "      <td>Making Maya</td>\n",
       "      <td>2003</td>\n",
       "      <td>85</td>\n",
       "      <td>Rolla Selbak</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>movie</td>\n",
       "      <td>Colors</td>\n",
       "      <td>1988</td>\n",
       "      <td>120</td>\n",
       "      <td>Dennis Hopper</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>movie</td>\n",
       "      <td>Legacy</td>\n",
       "      <td>2019</td>\n",
       "      <td>105</td>\n",
       "      <td>Dorian Boguta</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>movie</td>\n",
       "      <td>Heaven &amp; Earth</td>\n",
       "      <td>1993</td>\n",
       "      <td>140</td>\n",
       "      <td>Oliver Stone</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>movie</td>\n",
       "      <td>Chosen</td>\n",
       "      <td>2016</td>\n",
       "      <td>105</td>\n",
       "      <td>Jasmin Dizdar</td>\n",
       "      <td>War</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        titleType                 title startYear runtimeMinutes  \\\n",
       "0       tvSpecial       Survivor Series      2008            180   \n",
       "1       tvSpecial            Summerslam      1998            166   \n",
       "2       tvSpecial          WrestleMania      2018            314   \n",
       "3       tvSpecial            Summerslam      1999            155   \n",
       "4       tvSpecial  WrestleMania X-Seven      2001            225   \n",
       "...           ...                   ...       ...            ...   \n",
       "999995      movie           Making Maya      2003             85   \n",
       "999996      movie                Colors      1988            120   \n",
       "999997      movie                Legacy      2019            105   \n",
       "999998      movie        Heaven & Earth      1993            140   \n",
       "999999      movie                Chosen      2016            105   \n",
       "\n",
       "             director  genres  \n",
       "0          Kevin Dunn   Sport  \n",
       "1          Kevin Dunn   Sport  \n",
       "2          Kevin Dunn   Sport  \n",
       "3          Kevin Dunn   Sport  \n",
       "4          Kevin Dunn   Sport  \n",
       "...               ...     ...  \n",
       "999995   Rolla Selbak   Drama  \n",
       "999996  Dennis Hopper   Drama  \n",
       "999997  Dorian Boguta   Drama  \n",
       "999998   Oliver Stone  Action  \n",
       "999999  Jasmin Dizdar     War  \n",
       "\n",
       "[1000000 rows x 6 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56240/56240 [00:14<00:00, 3764.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7992240041386446, 0.8064134463139213, 0.8028026294137516)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_Data_Error = 0\n",
    "All_Fixed_Error = 0\n",
    "Correct_Fixed_Error = 0\n",
    "# for i in tqdm(range(len(imdb_clean))):\n",
    "for i in tqdm(imdb_error_index):\n",
    "    for j in range(6):\n",
    "        dirty_cell = imdb_dirty.iloc[i,j]\n",
    "        clean_cell = imdb_clean.iloc[i,j]\n",
    "        correct_cell = imdb_correction.iloc[i,j]\n",
    "        if(correct_cell!=dirty_cell):\n",
    "            All_Fixed_Error += 1\n",
    "        if(clean_cell!=dirty_cell):\n",
    "            All_Data_Error += 1\n",
    "            if(correct_cell==clean_cell):\n",
    "                Correct_Fixed_Error += 1\n",
    "Precision_hospital = Correct_Fixed_Error / All_Fixed_Error\n",
    "Recall_hospital = Correct_Fixed_Error / All_Data_Error\n",
    "F1_hospital = (2 * Precision_hospital * Recall_hospital) / (Precision_hospital + Recall_hospital)\n",
    "Precision_hospital,Recall_hospital,F1_hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_correction.to_csv('GEIL_Data/imdb/correction/result/correction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDB-T5 (0.4541498644430775, 0.346841125398013, 0.39330742063413765)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2596,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2472 2472 2472\n"
     ]
    }
   ],
   "source": [
    "## T5 on tax datasets\n",
    "count = 0\n",
    "# beer_result = pd.read_csv('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/vicuna_7b_beer-test-20.csv',index_col=0) ## Vicuna-7B\n",
    "# beer_result = pd.read_csv('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/beer-test-20.csv',index_col=0) ## Vicuna-13B\n",
    "# beer_result = pd.read_json('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/bloom/beer-test-20/generated_predictions.jsonl',lines=True) ## Bloom-560M\n",
    "# beer_result = pd.read_json('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/beer-test-20/generated_predictions.jsonl',lines=True) ## Bloom-560M\n",
    "tax_correction = tax_dirty.copy()\n",
    "tax_result = pd.read_csv('/home/yanmy/raha/raha-master/datasets/tax/t5/t5_correction.csv',index_col=0)\n",
    "for d in np.argwhere(tax_detector.reshape((-1,15))==1):\n",
    "    i = tax_error[d[0]]\n",
    "    j = d[1]\n",
    "    \n",
    "    try:\n",
    "        # predict = list(ast.literal_eval(imdb_result.iloc[count,-1]).values())[0]\n",
    "        predict = tax_result.iloc[count,-1]\n",
    "        tax_correction.iloc[i,j] = predict\n",
    "        count += 1\n",
    "    except:\n",
    "        # print(count)\n",
    "        count += 1\n",
    "print(count,len(np.argwhere(tax_detector.reshape((-1,15))==1)),len(tax_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2597,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ac07be09c7411db61b671609e819c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2929 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.7251751133086115, 0.5933917734322319, 0.6526979417763767)"
      ]
     },
     "execution_count": 2597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_Data_Error = 0\n",
    "All_Fixed_Error = 0\n",
    "Correct_Fixed_Error = 0\n",
    "# for i in tqdm(range(len(imdb_clean))):\n",
    "for i in tqdm(tax_error):\n",
    "    for j in range(15):\n",
    "        dirty_cell = tax_dirty.iloc[i,j]\n",
    "        clean_cell = tax_clean.iloc[i,j]\n",
    "        correct_cell = tax_correction.iloc[i,j]\n",
    "        if(correct_cell!=dirty_cell):\n",
    "            All_Fixed_Error += 1\n",
    "        if(clean_cell!=dirty_cell):\n",
    "            All_Data_Error += 1\n",
    "            if(correct_cell==clean_cell):\n",
    "                Correct_Fixed_Error += 1\n",
    "Precision_hospital = Correct_Fixed_Error / All_Fixed_Error\n",
    "Recall_hospital = Correct_Fixed_Error / All_Data_Error\n",
    "F1_hospital = (2 * Precision_hospital * Recall_hospital) / (Precision_hospital + Recall_hospital)\n",
    "Precision_hospital,Recall_hospital,F1_hospital"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tax_T5: (0.7251751133086115, 0.5933917734322319, 0.6526979417763767)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flight_T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2744,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16632 16632 16632\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94684d990b7141548846ac015ad889da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.3899716513111269, 0.2684146341463415, 0.3179716844842531)"
      ]
     },
     "execution_count": 2744,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## T5 on tax datasets\n",
    "count = 0\n",
    "# beer_result = pd.read_csv('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/vicuna_7b_beer-test-20.csv',index_col=0) ## Vicuna-7B\n",
    "# beer_result = pd.read_csv('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/beer-test-20.csv',index_col=0) ## Vicuna-13B\n",
    "# beer_result = pd.read_json('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/bloom/beer-test-20/generated_predictions.jsonl',lines=True) ## Bloom-560M\n",
    "# beer_result = pd.read_json('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/beer-test-20/generated_predictions.jsonl',lines=True) ## Bloom-560M\n",
    "flight_correction = flight_dirty.copy()\n",
    "flight_result = pd.read_csv('/home/yanmy/raha/raha-master/datasets/flights/t5/test_50_correction.csv',index_col=0)\n",
    "for d in np.argwhere(np.zeros(flight_clean.shape)==0):\n",
    "    i = d[0]\n",
    "    j = d[1]\n",
    "    \n",
    "    try:\n",
    "        # predict = list(ast.literal_eval(imdb_result.iloc[count,-1]).values())[0]\n",
    "        predict = flight_result.iloc[count,-1]\n",
    "        flight_correction.iloc[i,j] = predict\n",
    "        count += 1\n",
    "    except:\n",
    "        # print(count)\n",
    "        count += 1\n",
    "print(count,len(np.argwhere(np.zeros(flight_clean.shape)==0)),len(flight_result))\n",
    "All_Data_Error = 0\n",
    "All_Fixed_Error = 0\n",
    "Correct_Fixed_Error = 0\n",
    "# for i in tqdm(range(len(imdb_clean))):\n",
    "for i in tqdm(range(len(flight_clean))):\n",
    "    for j in range(7):\n",
    "        dirty_cell = flight_dirty.iloc[i,j]\n",
    "        clean_cell = flight_clean.iloc[i,j]\n",
    "        correct_cell = flight_correction.iloc[i,j]\n",
    "        if(correct_cell!=dirty_cell):\n",
    "            All_Fixed_Error += 1\n",
    "        if(clean_cell!=dirty_cell):\n",
    "            All_Data_Error += 1\n",
    "            if(correct_cell==clean_cell):\n",
    "                Correct_Fixed_Error += 1\n",
    "Precision_hospital = Correct_Fixed_Error / All_Fixed_Error\n",
    "Recall_hospital = Correct_Fixed_Error / All_Data_Error\n",
    "F1_hospital = (2 * Precision_hospital * Recall_hospital) / (Precision_hospital + Recall_hospital)\n",
    "Precision_hospital,Recall_hospital,F1_hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2740,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1880dcbb51f24efc9c90e72b6c1d32a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.7468832640725349, 0.7232926829268292, 0.7348987051607707)"
      ]
     },
     "execution_count": 2740,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_Data_Error = 0\n",
    "All_Fixed_Error = 0\n",
    "Correct_Fixed_Error = 0\n",
    "# for i in tqdm(range(len(imdb_clean))):\n",
    "for i in tqdm(range(len(flight_clean))):\n",
    "    for j in range(7):\n",
    "        dirty_cell = flight_dirty.iloc[i,j]\n",
    "        clean_cell = flight_clean.iloc[i,j]\n",
    "        correct_cell = flight_correction.iloc[i,j]\n",
    "        if(correct_cell!=dirty_cell):\n",
    "            All_Fixed_Error += 1\n",
    "        if(clean_cell!=dirty_cell):\n",
    "            All_Data_Error += 1\n",
    "            if(correct_cell==clean_cell):\n",
    "                Correct_Fixed_Error += 1\n",
    "Precision_hospital = Correct_Fixed_Error / All_Fixed_Error\n",
    "Recall_hospital = Correct_Fixed_Error / All_Data_Error\n",
    "F1_hospital = (2 * Precision_hospital * Recall_hospital) / (Precision_hospital + Recall_hospital)\n",
    "Precision_hospital,Recall_hospital,F1_hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.7468832640725349, 0.7232926829268292, 0.7348987051607707)\n",
    "(0.6534463894967177, 0.5826829268292683, 0.6160391954615783)\n",
    "(0.5353489771359807, 0.43402439024390244, 0.4793911637931034)\n",
    "(0.4312357846853677, 0.3468292682926829, 0.3844542075025346)\n",
    "(0.3899716513111269, 0.2684146341463415, 0.3179716844842531)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9826521814105881 0.9914916569519601 0.9870521292535271\n"
     ]
    }
   ],
   "source": [
    "predict = np.array(imdb_dirty!=imdb_correction).flatten().astype(int)\n",
    "gt = np.array(imdb_dirty!=imdb_clean).flatten().astype(int)\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "print(precision_score(y_true=gt,y_pred=predict),recall_score(y_true=gt,y_pred=predict),f1_score(y_true=gt,y_pred=predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e9ff5da8b1248cfa68a58d206cbe2ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## IMDB Baseline for T5\n",
    "### For Different Error_Rate, Add Reference Files For Testing\n",
    "# imdb_dirty = pd.read_csv('datasets/imdb/dirty.csv').fillna('')\n",
    "# imdb_dirty = imdb_dirty.parallel_apply(Str2Int,axis=1)\n",
    "# input_matrix_imdb = np.array(imdb_dirty!=imdb_clean).astype(int)\n",
    "# input_matrix_select = input_matrix[selected_rows]\n",
    "detector_list_all = []\n",
    "imdb_dirty_vary_error = imdb_dirty.copy()\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "for label_tuple in tqdm(imdb_label_index):\n",
    "# for label_tuple in tqdm(range(len(imdb_clean))):\n",
    "# for label_tuple in tqdm(imdb_error_index):\n",
    "# for d in np.argwhere(imdb_detector==1):\n",
    "    # label_tuple = imdb_error_index[d[0]]\n",
    "    # i = d[1]\n",
    "    for i in range(len(imdb_clean.columns)):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = imdb_clean.iloc[label_tuple].copy()\n",
    "        dirty_context = imdb_dirty_vary_error.iloc[label_tuple].copy()\n",
    "        clean_cell = imdb_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = imdb_dirty_vary_error.iloc[label_tuple,i]\n",
    "        # single_context_clean = 'COL %s VAL %s ' % (imdb_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (imdb_clean.columns[i],dirty_cell)\n",
    "        for c in range(6):\n",
    "            # all_context_clean += 'COL %s VAL %s ' % (imdb_clean.columns[c],imdb_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (imdb_dirty_vary_error.columns[c],imdb_dirty_vary_error.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        if(str(dirty_cell)!=str(clean_cell)):\n",
    "        #     detector_list_all.append([all_context_dirty,single_context_dirty,1])\n",
    "        #     # detector_list_all.append([all_context_clean,single_context_clean,0])\n",
    "        # else:\n",
    "        #     detector_list_all.append([all_context_dirty,single_context_dirty,0])\n",
    "            detector_list_all.append([all_context_dirty,single_context_dirty,clean_cell])\n",
    "            # detector_list.append([single_context_dirty,1])\n",
    "        # if(dirty_cell!=clean_cell):\n",
    "        #     detector_list_tax.append([all_context_dirty,single_context_dirty,1])\n",
    "        #     detector_list_tax.append([all_context_clean,single_context_clean,0])\n",
    "        # else:\n",
    "        #     detector_list_tax.append([all_context_dirty,single_context_dirty,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2474,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_all).to_csv('datasets/imdb/detector/generation_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39f1760e719648d3bdbe7d3c5edcde51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Hospital Baseline for T5\n",
    "### For Different Error_Rate, Add Reference Files For Testing\n",
    "# imdb_dirty = pd.read_csv('datasets/imdb/dirty.csv').fillna('')\n",
    "# imdb_dirty = imdb_dirty.parallel_apply(Str2Int,axis=1)\n",
    "# input_matrix_imdb = np.array(imdb_dirty!=imdb_clean).astype(int)\n",
    "# input_matrix_select = input_matrix[selected_rows]\n",
    "detector_list_all = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "for label_tuple in tqdm(hospital_label_index):\n",
    "# for label_tuple in tqdm(range(len(imdb_clean))):\n",
    "# for label_tuple in tqdm(imdb_error_index):\n",
    "# for d in np.argwhere(imdb_detector==1):\n",
    "    # label_tuple = imdb_error_index[d[0]]\n",
    "    # i = d[1]\n",
    "    for i in range(len(hospital_clean.columns)):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = hospital_clean.iloc[label_tuple].copy()\n",
    "        dirty_context = hospital_dirty.iloc[label_tuple].copy()\n",
    "        clean_cell = hospital_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = hospital_dirty.iloc[label_tuple,i]\n",
    "        # single_context_clean = 'COL %s VAL %s ' % (imdb_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (hospital_clean.columns[i],dirty_cell)\n",
    "        for c in range(len(hospital_clean.columns)):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (hospital_clean.columns[c],hospital_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (hospital_dirty.columns[c],hospital_dirty.iloc[label_tuple,c])\n",
    "        detector_list_all.append([all_context_dirty,single_context_dirty,clean_cell])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2487,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hospital Baseline for T5:Testing Inference with hospital_label_index\n",
    "detector_list_all = []\n",
    "for d in np.argwhere(hospital_detector==1):\n",
    "    label_tuple = d[0]\n",
    "    i = d[1]\n",
    "    # for i in range(len(hospital_clean.columns)):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "    all_context_clean = ''\n",
    "    all_context_dirty = ''\n",
    "    clean_context = hospital_clean.iloc[label_tuple].copy()\n",
    "    dirty_context = hospital_dirty.iloc[label_tuple].copy()\n",
    "    clean_cell = hospital_clean.iloc[label_tuple,i]\n",
    "    dirty_cell = hospital_dirty.iloc[label_tuple,i]\n",
    "    # single_context_clean = 'COL %s VAL %s ' % (imdb_clean.columns[i],clean_cell)\n",
    "    single_context_dirty = 'COL %s VAL %s ' % (hospital_clean.columns[i],dirty_cell)\n",
    "    for c in range(len(hospital_clean.columns)):\n",
    "        all_context_clean += 'COL %s VAL %s ' % (hospital_clean.columns[c],hospital_clean.iloc[label_tuple,c])\n",
    "        all_context_dirty += 'COL %s VAL %s ' % (hospital_dirty.columns[c],hospital_dirty.iloc[label_tuple,c])\n",
    "    detector_list_all.append([all_context_dirty,single_context_dirty,clean_cell])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2714,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hospital Baseline for T5:Testing Inference with hospital_label_index Vary-Error-Rate\n",
    "hospital_detector_vary = np.load('/home/yanmy/raha/raha-master/datasets/hospital/vary_error_rate/hospital_50_detector.npy')\n",
    "hospital_dirty_vary = hospital_dirty_50_error.copy()\n",
    "detector_list_all = []\n",
    "for d in np.argwhere(hospital_detector_vary==1):\n",
    "    label_tuple = d[0]\n",
    "    i = d[1]\n",
    "    # for i in range(len(hospital_clean.columns)):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "    all_context_clean = ''\n",
    "    all_context_dirty = ''\n",
    "    clean_context = hospital_clean.iloc[label_tuple].copy()\n",
    "    dirty_context = hospital_dirty_vary.iloc[label_tuple].copy()\n",
    "    clean_cell = hospital_clean.iloc[label_tuple,i]\n",
    "    dirty_cell = hospital_dirty_vary.iloc[label_tuple,i]\n",
    "    # single_context_clean = 'COL %s VAL %s ' % (imdb_clean.columns[i],clean_cell)\n",
    "    single_context_dirty = 'COL %s VAL %s ' % (hospital_clean.columns[i],dirty_cell)\n",
    "    for c in range(len(hospital_clean.columns)):\n",
    "        all_context_clean += 'COL %s VAL %s ' % (hospital_clean.columns[c],hospital_clean.iloc[label_tuple,c])\n",
    "        all_context_dirty += 'COL %s VAL %s ' % (hospital_dirty_vary.columns[c],hospital_dirty_vary.iloc[label_tuple,c])\n",
    "    detector_list_all.append([all_context_dirty,single_context_dirty,clean_cell])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2715,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_all).to_csv('datasets/hospital/t5/test_50.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2483,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_all).to_csv('datasets/hospital/t5/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93bb28beb66148daa294faf6ee507176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Beer Baselines for T5\n",
    "detector_list_all = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "for label_tuple in tqdm(beer_label_index):\n",
    "    for i in range(len(beer_clean.columns)):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = beer_clean.iloc[label_tuple].copy()\n",
    "        dirty_context = beer_dirty.iloc[label_tuple].copy()\n",
    "        clean_cell = beer_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = beer_dirty.iloc[label_tuple,i]\n",
    "        # single_context_clean = 'COL %s VAL %s ' % (imdb_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (beer_clean.columns[i],dirty_cell)\n",
    "        for c in range(len(beer_clean.columns)):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (beer_clean.columns[c],beer_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (beer_dirty.columns[c],beer_dirty.iloc[label_tuple,c])\n",
    "        detector_list_all.append([all_context_dirty,single_context_dirty,clean_cell])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2493,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_all).to_csv('datasets/beers/t5/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2498,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hospital Baseline for T5:Testing Inference with hospital_label_index\n",
    "## Ablation for Generation\n",
    "detector_list_all = []\n",
    "for d in np.argwhere(detector_beer==1):\n",
    "    label_tuple = d[0]\n",
    "    i = d[1] + 2 ## skip index and id\n",
    "    # for i in range(len(hospital_clean.columns)):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "    all_context_clean = ''\n",
    "    all_context_dirty = ''\n",
    "    clean_context = beer_clean.iloc[label_tuple].copy()\n",
    "    dirty_context = beer_dirty.iloc[label_tuple].copy()\n",
    "    clean_cell = beer_clean.iloc[label_tuple,i]\n",
    "    dirty_cell = beer_dirty.iloc[label_tuple,i]\n",
    "    # single_context_clean = 'COL %s VAL %s ' % (imdb_clean.columns[i],clean_cell)\n",
    "    single_context_dirty = 'COL %s VAL %s ' % (beer_clean.columns[i],dirty_cell)\n",
    "    for c in range(len(beer_clean.columns)):\n",
    "        all_context_clean += 'COL %s VAL %s ' % (beer_clean.columns[c],beer_clean.iloc[label_tuple,c])\n",
    "        all_context_dirty += 'COL %s VAL %s ' % (beer_dirty.columns[c],beer_dirty.iloc[label_tuple,c])\n",
    "    detector_list_all.append([all_context_dirty,single_context_dirty,clean_cell])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2537,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hospital Baseline for T5:Testing Inference with hospital_label_index \n",
    "## Beers_Test_All\n",
    "detector_list_all = []\n",
    "# for d in np.argwhere(detector_beer==1):\n",
    "for label_tuple in range(len(beer_clean)):\n",
    "    for i in range(len(beer_clean.columns)):\n",
    "    # label_tuple = d[0]\n",
    "    # i = d[1] + 2 ## skip index and id\n",
    "    # for i in range(len(hospital_clean.columns)):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = beer_clean.iloc[label_tuple].copy()\n",
    "        dirty_context = beer_dirty.iloc[label_tuple].copy()\n",
    "        clean_cell = beer_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = beer_dirty.iloc[label_tuple,i]\n",
    "        # single_context_clean = 'COL %s VAL %s ' % (imdb_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (beer_clean.columns[i],dirty_cell)\n",
    "        for c in range(len(beer_clean.columns)):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (beer_clean.columns[c],beer_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (beer_dirty.columns[c],beer_dirty.iloc[label_tuple,c])\n",
    "        detector_list_all.append([all_context_dirty,single_context_dirty,clean_cell])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2539,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_all).to_csv('datasets/beers/t5/test_all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tax_Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b6d932f4074ca8b46d8b6f3647f863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Beer Baselines for T5\n",
    "detector_list_all = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "for label_tuple in tqdm(tax_label_index):\n",
    "    for i in range(len(tax_clean.columns)):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = tax_clean.iloc[label_tuple].copy()\n",
    "        dirty_context = tax_dirty.iloc[label_tuple].copy()\n",
    "        clean_cell = tax_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = tax_dirty.iloc[label_tuple,i]\n",
    "        # single_context_clean = 'COL %s VAL %s ' % (imdb_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (tax_clean.columns[i],dirty_cell)\n",
    "        for c in range(len(tax_clean.columns)):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (tax_clean.columns[c],tax_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (tax_dirty.columns[c],tax_dirty.iloc[label_tuple,c])\n",
    "        detector_list_all.append([all_context_dirty,single_context_dirty,clean_cell])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2503,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_all).to_csv('datasets/tax/t5/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 2508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_detector = np.load('/home/yanmy/raha/raha-master/datasets/tax/detector/detection_cell.npy')\n",
    "tax_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2532,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hospital Baseline for T5:Testing Inference with hospital_label_index\n",
    "detector_list_all = []\n",
    "for d in np.argwhere(tax_detector.reshape((-1,15))==1):\n",
    "    label_tuple = tax_error[d[0]]\n",
    "    i = d[1]\n",
    "    # for i in range(len(hospital_clean.columns)):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "    all_context_clean = ''\n",
    "    all_context_dirty = ''\n",
    "    clean_context = tax_clean.iloc[label_tuple].copy()\n",
    "    dirty_context = tax_dirty.iloc[label_tuple].copy()\n",
    "    clean_cell = tax_clean.iloc[label_tuple,i]\n",
    "    dirty_cell = tax_dirty.iloc[label_tuple,i]\n",
    "    # single_context_clean = 'COL %s VAL %s ' % (imdb_clean.columns[i],clean_cell)\n",
    "    single_context_dirty = 'COL %s VAL %s ' % (tax_clean.columns[i],dirty_cell)\n",
    "    for c in range(len(tax_clean.columns)):\n",
    "        all_context_clean += 'COL %s VAL %s ' % (tax_clean.columns[c],tax_clean.iloc[label_tuple,c])\n",
    "        all_context_dirty += 'COL %s VAL %s ' % (tax_dirty.columns[c],tax_dirty.iloc[label_tuple,c])\n",
    "    detector_list_all.append([all_context_dirty,single_context_dirty,clean_cell])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2534,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_all).to_csv('datasets/tax/t5/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rayyan_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2542,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41981e1d2ac646c1b60bc6541f60b0a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Beer Baselines for T5\n",
    "detector_list_all = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "for label_tuple in tqdm(rayyan_label_index):\n",
    "    for i in range(len(rayyan_clean.columns)):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = rayyan_clean.iloc[label_tuple].copy()\n",
    "        dirty_context = rayyan_dirty.iloc[label_tuple].copy()\n",
    "        clean_cell = rayyan_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = rayyan_dirty.iloc[label_tuple,i]\n",
    "        # single_context_clean = 'COL %s VAL %s ' % (imdb_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (rayyan_clean.columns[i],dirty_cell)\n",
    "        for c in range(len(rayyan_clean.columns)):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (rayyan_clean.columns[c],rayyan_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (rayyan_dirty.columns[c],rayyan_dirty.iloc[label_tuple,c])\n",
    "        detector_list_all.append([all_context_dirty,single_context_dirty,clean_cell])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2544,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_all).to_csv('datasets/rayyan/t5/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2550,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hospital Baseline for T5:Testing Inference with hospital_label_index\n",
    "detector_list_all = []\n",
    "for d in np.argwhere(rayyan_detector==1):\n",
    "    label_tuple = d[0]\n",
    "    i = d[1] + 1 ## Ignore ID\n",
    "    # for i in range(len(hospital_clean.columns)):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "    all_context_clean = ''\n",
    "    all_context_dirty = ''\n",
    "    clean_context = rayyan_clean.iloc[label_tuple].copy()\n",
    "    dirty_context = rayyan_dirty.iloc[label_tuple].copy()\n",
    "    clean_cell = rayyan_clean.iloc[label_tuple,i]\n",
    "    dirty_cell = rayyan_dirty.iloc[label_tuple,i]\n",
    "    # single_context_clean = 'COL %s VAL %s ' % (imdb_clean.columns[i],clean_cell)\n",
    "    single_context_dirty = 'COL %s VAL %s ' % (rayyan_clean.columns[i],dirty_cell)\n",
    "    for c in range(len(rayyan_clean.columns)):\n",
    "        all_context_clean += 'COL %s VAL %s ' % (rayyan_clean.columns[c],rayyan_clean.iloc[label_tuple,c])\n",
    "        all_context_dirty += 'COL %s VAL %s ' % (rayyan_dirty.columns[c],rayyan_dirty.iloc[label_tuple,c])\n",
    "    detector_list_all.append([all_context_dirty,single_context_dirty,clean_cell])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2552,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_all).to_csv('datasets/rayyan/t5/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flight_T5_Baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2554,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "789df79cf0094143882663197bf66084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "detector_list_all = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "for label_tuple in tqdm(flight_label_index):\n",
    "    for i in range(len(flight_clean.columns)):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = flight_clean.iloc[label_tuple].copy()\n",
    "        dirty_context = flight_dirty.iloc[label_tuple].copy()\n",
    "        clean_cell = flight_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = flight_dirty.iloc[label_tuple,i]\n",
    "        # single_context_clean = 'COL %s VAL %s ' % (imdb_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (flight_clean.columns[i],dirty_cell)\n",
    "        for c in range(len(flight_clean.columns)):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (flight_clean.columns[c],flight_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (flight_dirty.columns[c],flight_dirty.iloc[label_tuple,c])\n",
    "        detector_list_all.append([all_context_dirty,single_context_dirty,clean_cell])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2726,
   "metadata": {},
   "outputs": [],
   "source": [
    "## T5 for Flight vary error rate\n",
    "flight_dirty_vary = flight_dirty_50_error.copy()\n",
    "detector_list_all = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "for label_tuple in range(len(flight_clean)):\n",
    "    for i in range(len(flight_clean.columns)):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = flight_clean.iloc[label_tuple].copy()\n",
    "        dirty_context = flight_dirty_vary.iloc[label_tuple].copy()\n",
    "        clean_cell = flight_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = flight_dirty_vary.iloc[label_tuple,i]\n",
    "        # single_context_clean = 'COL %s VAL %s ' % (imdb_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (flight_clean.columns[i],dirty_cell)\n",
    "        for c in range(len(flight_clean.columns)):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (flight_clean.columns[c],flight_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (flight_dirty_vary.columns[c],flight_dirty_vary.iloc[label_tuple,c])\n",
    "        detector_list_all.append([all_context_dirty,single_context_dirty,clean_cell])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2727,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_all).to_csv('/home/yanmy/raha/raha-master/datasets/flights/t5/test_50.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2664,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c609894eecfb4494bf234e8f32a71df6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Flights_Rotom_Baseline\n",
    "detector_list_all = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "# for label_tuple in tqdm(flight_label_index):\n",
    "for label_tuple in tqdm(range(len(flight_clean))):\n",
    "    for i in range(len(flight_clean.columns)):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = flight_clean.iloc[label_tuple].copy()\n",
    "        dirty_context = flight_dirty.iloc[label_tuple].copy()\n",
    "        clean_cell = flight_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = flight_dirty.iloc[label_tuple,i]\n",
    "        # single_context_clean = 'COL %s VAL %s ' % (imdb_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (flight_clean.columns[i],dirty_cell)\n",
    "        for c in range(len(flight_clean.columns)):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (flight_clean.columns[c],flight_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (flight_dirty.columns[c],flight_dirty.iloc[label_tuple,c])\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            detector_list_all.append([all_context_dirty,single_context_dirty,1])\n",
    "        else:\n",
    "            detector_list_all.append([all_context_dirty,single_context_dirty,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2665,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COL tuple_id VAL 1 COL src VAL aa COL flight V...</td>\n",
       "      <td>COL tuple_id VAL 1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COL tuple_id VAL 1 COL src VAL aa COL flight V...</td>\n",
       "      <td>COL src VAL aa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COL tuple_id VAL 1 COL src VAL aa COL flight V...</td>\n",
       "      <td>COL flight VAL AA-3859-IAH-ORD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COL tuple_id VAL 1 COL src VAL aa COL flight V...</td>\n",
       "      <td>COL sched_dep_time VAL 7:10 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COL tuple_id VAL 1 COL src VAL aa COL flight V...</td>\n",
       "      <td>COL act_dep_time VAL 7:16aDec 1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16627</th>\n",
       "      <td>COL tuple_id VAL 2376 COL src VAL world-flight...</td>\n",
       "      <td>COL flight VAL AA-59-JFK-SFO</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16628</th>\n",
       "      <td>COL tuple_id VAL 2376 COL src VAL world-flight...</td>\n",
       "      <td>COL sched_dep_time VAL 7:10 a.m. (-00:00)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16629</th>\n",
       "      <td>COL tuple_id VAL 2376 COL src VAL world-flight...</td>\n",
       "      <td>COL act_dep_time VAL Not Available</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16630</th>\n",
       "      <td>COL tuple_id VAL 2376 COL src VAL world-flight...</td>\n",
       "      <td>COL sched_arr_time VAL 10:45 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16631</th>\n",
       "      <td>COL tuple_id VAL 2376 COL src VAL world-flight...</td>\n",
       "      <td>COL act_arr_time VAL 12/02/2011 11:12 a.m.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16632 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0  \\\n",
       "0      COL tuple_id VAL 1 COL src VAL aa COL flight V...   \n",
       "1      COL tuple_id VAL 1 COL src VAL aa COL flight V...   \n",
       "2      COL tuple_id VAL 1 COL src VAL aa COL flight V...   \n",
       "3      COL tuple_id VAL 1 COL src VAL aa COL flight V...   \n",
       "4      COL tuple_id VAL 1 COL src VAL aa COL flight V...   \n",
       "...                                                  ...   \n",
       "16627  COL tuple_id VAL 2376 COL src VAL world-flight...   \n",
       "16628  COL tuple_id VAL 2376 COL src VAL world-flight...   \n",
       "16629  COL tuple_id VAL 2376 COL src VAL world-flight...   \n",
       "16630  COL tuple_id VAL 2376 COL src VAL world-flight...   \n",
       "16631  COL tuple_id VAL 2376 COL src VAL world-flight...   \n",
       "\n",
       "                                                 1  2  \n",
       "0                              COL tuple_id VAL 1   0  \n",
       "1                                  COL src VAL aa   0  \n",
       "2                  COL flight VAL AA-3859-IAH-ORD   0  \n",
       "3                COL sched_dep_time VAL 7:10 a.m.   0  \n",
       "4                 COL act_dep_time VAL 7:16aDec 1   1  \n",
       "...                                            ... ..  \n",
       "16627                COL flight VAL AA-59-JFK-SFO   0  \n",
       "16628   COL sched_dep_time VAL 7:10 a.m. (-00:00)   1  \n",
       "16629          COL act_dep_time VAL Not Available   1  \n",
       "16630           COL sched_arr_time VAL 10:45 a.m.   0  \n",
       "16631  COL act_arr_time VAL 12/02/2011 11:12 a.m.   1  \n",
       "\n",
       "[16632 rows x 3 columns]"
      ]
     },
     "execution_count": 2665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(detector_list_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2666,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_all).to_csv('/home/yanmy/raha/raha-master/datasets/flights/detector/multi-view/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2556,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_all).to_csv('datasets/flights/t5/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2557,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hospital Baseline for T5:Testing Inference with hospital_label_index \n",
    "## Beers_Test_All\n",
    "detector_list_all = []\n",
    "# for d in np.argwhere(detector_beer==1):\n",
    "for label_tuple in range(len(flight_clean)):\n",
    "    for i in range(len(flight_clean.columns)):\n",
    "    # label_tuple = d[0]\n",
    "    # i = d[1] + 2 ## skip index and id\n",
    "    # for i in range(len(hospital_clean.columns)):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = flight_clean.iloc[label_tuple].copy()\n",
    "        dirty_context = flight_dirty.iloc[label_tuple].copy()\n",
    "        clean_cell = flight_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = flight_dirty.iloc[label_tuple,i]\n",
    "        # single_context_clean = 'COL %s VAL %s ' % (imdb_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (flight_clean.columns[i],dirty_cell)\n",
    "        for c in range(len(flight_clean.columns)):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (flight_clean.columns[c],flight_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (flight_dirty.columns[c],flight_dirty.iloc[label_tuple,c])\n",
    "        detector_list_all.append([all_context_dirty,single_context_dirty,clean_cell])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2686,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Roberta-Baseline-Flights\n",
    "detector_list_all = []\n",
    "# for d in np.argwhere(detector_beer==1):\n",
    "for label_tuple in flight_label_index:\n",
    "# for label_tuple in range(len(flight_clean)):\n",
    "    for i in range(len(flight_clean.columns)):\n",
    "    # label_tuple = d[0]\n",
    "    # i = d[1] + 2 ## skip index and id\n",
    "    # for i in range(len(hospital_clean.columns)):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = flight_clean.iloc[label_tuple].copy()\n",
    "        dirty_context = flight_dirty.iloc[label_tuple].copy()\n",
    "        clean_cell = flight_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = flight_dirty.iloc[label_tuple,i]\n",
    "        single_context_clean = 'COL %s VAL %s ' % (flight_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (flight_clean.columns[i],dirty_cell)\n",
    "        for c in range(len(flight_clean.columns)):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (flight_clean.columns[c],flight_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (flight_dirty.columns[c],flight_dirty.iloc[label_tuple,c])\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            detector_list_all.append([all_context_dirty,single_context_clean,1])\n",
    "            detector_list_all.append([all_context_clean,single_context_dirty,0])\n",
    "        else:\n",
    "            detector_list_all.append([all_context_dirty,single_context_dirty,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2689,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>COL tuple_id VAL 2 COL src VAL aa COL flight V...</td>\n",
       "      <td>COL tuple_id VAL 2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>COL tuple_id VAL 2 COL src VAL aa COL flight V...</td>\n",
       "      <td>COL src VAL aa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>COL tuple_id VAL 2 COL src VAL aa COL flight V...</td>\n",
       "      <td>COL flight VAL AA-1733-ORD-PHX</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>COL tuple_id VAL 2 COL src VAL aa COL flight V...</td>\n",
       "      <td>COL sched_dep_time VAL 7:45 p.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>COL tuple_id VAL 2 COL src VAL aa COL flight V...</td>\n",
       "      <td>COL act_dep_time VAL 7:58 p.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>135</td>\n",
       "      <td>COL tuple_id VAL 144 COL src VAL helloflight C...</td>\n",
       "      <td>COL flight VAL UA-3099-PHX-PHL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>136</td>\n",
       "      <td>COL tuple_id VAL 144 COL src VAL helloflight C...</td>\n",
       "      <td>COL sched_dep_time VAL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>137</td>\n",
       "      <td>COL tuple_id VAL 144 COL src VAL helloflight C...</td>\n",
       "      <td>COL act_dep_time VAL 11:55 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>138</td>\n",
       "      <td>COL tuple_id VAL 144 COL src VAL helloflight C...</td>\n",
       "      <td>COL sched_arr_time VAL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>139</td>\n",
       "      <td>COL tuple_id VAL 144 COL src VAL helloflight C...</td>\n",
       "      <td>COL act_arr_time VAL 5:34 p.m.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                                  0  \\\n",
       "0             0  COL tuple_id VAL 2 COL src VAL aa COL flight V...   \n",
       "1             1  COL tuple_id VAL 2 COL src VAL aa COL flight V...   \n",
       "2             2  COL tuple_id VAL 2 COL src VAL aa COL flight V...   \n",
       "3             3  COL tuple_id VAL 2 COL src VAL aa COL flight V...   \n",
       "4             4  COL tuple_id VAL 2 COL src VAL aa COL flight V...   \n",
       "..          ...                                                ...   \n",
       "135         135  COL tuple_id VAL 144 COL src VAL helloflight C...   \n",
       "136         136  COL tuple_id VAL 144 COL src VAL helloflight C...   \n",
       "137         137  COL tuple_id VAL 144 COL src VAL helloflight C...   \n",
       "138         138  COL tuple_id VAL 144 COL src VAL helloflight C...   \n",
       "139         139  COL tuple_id VAL 144 COL src VAL helloflight C...   \n",
       "\n",
       "                                     1  2  \n",
       "0                  COL tuple_id VAL 2   0  \n",
       "1                      COL src VAL aa   0  \n",
       "2      COL flight VAL AA-1733-ORD-PHX   0  \n",
       "3    COL sched_dep_time VAL 7:45 p.m.   0  \n",
       "4      COL act_dep_time VAL 7:58 p.m.   0  \n",
       "..                                 ... ..  \n",
       "135    COL flight VAL UA-3099-PHX-PHL   0  \n",
       "136           COL sched_dep_time VAL    1  \n",
       "137   COL act_dep_time VAL 11:55 a.m.   0  \n",
       "138           COL sched_arr_time VAL    1  \n",
       "139    COL act_arr_time VAL 5:34 p.m.   1  \n",
       "\n",
       "[140 rows x 4 columns]"
      ]
     },
     "execution_count": 2689,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('datasets/flights/detector/multi-view/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2688,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COL tuple_id VAL 2 COL src VAL aa COL flight V...</td>\n",
       "      <td>COL tuple_id VAL 2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COL tuple_id VAL 2 COL src VAL aa COL flight V...</td>\n",
       "      <td>COL src VAL aa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COL tuple_id VAL 2 COL src VAL aa COL flight V...</td>\n",
       "      <td>COL flight VAL AA-1733-ORD-PHX</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COL tuple_id VAL 2 COL src VAL aa COL flight V...</td>\n",
       "      <td>COL sched_dep_time VAL 7:45 p.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COL tuple_id VAL 2 COL src VAL aa COL flight V...</td>\n",
       "      <td>COL act_dep_time VAL 7:58 p.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>COL tuple_id VAL 144 COL src VAL helloflight C...</td>\n",
       "      <td>COL act_dep_time VAL 11:55 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>COL tuple_id VAL 144 COL src VAL helloflight C...</td>\n",
       "      <td>COL sched_arr_time VAL 6:17 p.m.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>COL tuple_id VAL 144 COL src VAL helloflight C...</td>\n",
       "      <td>COL sched_arr_time VAL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>COL tuple_id VAL 144 COL src VAL helloflight C...</td>\n",
       "      <td>COL act_arr_time VAL 5:38 p.m.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>COL tuple_id VAL 144 COL src VAL helloflight C...</td>\n",
       "      <td>COL act_arr_time VAL 5:34 p.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>194 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0  \\\n",
       "0    COL tuple_id VAL 2 COL src VAL aa COL flight V...   \n",
       "1    COL tuple_id VAL 2 COL src VAL aa COL flight V...   \n",
       "2    COL tuple_id VAL 2 COL src VAL aa COL flight V...   \n",
       "3    COL tuple_id VAL 2 COL src VAL aa COL flight V...   \n",
       "4    COL tuple_id VAL 2 COL src VAL aa COL flight V...   \n",
       "..                                                 ...   \n",
       "189  COL tuple_id VAL 144 COL src VAL helloflight C...   \n",
       "190  COL tuple_id VAL 144 COL src VAL helloflight C...   \n",
       "191  COL tuple_id VAL 144 COL src VAL helloflight C...   \n",
       "192  COL tuple_id VAL 144 COL src VAL helloflight C...   \n",
       "193  COL tuple_id VAL 144 COL src VAL helloflight C...   \n",
       "\n",
       "                                     1  2  \n",
       "0                  COL tuple_id VAL 2   0  \n",
       "1                      COL src VAL aa   0  \n",
       "2      COL flight VAL AA-1733-ORD-PHX   0  \n",
       "3    COL sched_dep_time VAL 7:45 p.m.   0  \n",
       "4      COL act_dep_time VAL 7:58 p.m.   0  \n",
       "..                                 ... ..  \n",
       "189   COL act_dep_time VAL 11:55 a.m.   0  \n",
       "190  COL sched_arr_time VAL 6:17 p.m.   1  \n",
       "191           COL sched_arr_time VAL    0  \n",
       "192    COL act_arr_time VAL 5:38 p.m.   1  \n",
       "193    COL act_arr_time VAL 5:34 p.m.   0  \n",
       "\n",
       "[194 rows x 3 columns]"
      ]
     },
     "execution_count": 2688,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(detector_list_all).to_csv('datasets/flights/detector/multi-view/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2561,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_all).to_csv('datasets/flights/t5/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB Dataset for T5 baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2562,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce74a6dee3e4817b96c02ba2c057668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Beer Baselines for T5\n",
    "detector_list_all = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "for label_tuple in tqdm(imdb_label_index):\n",
    "    for i in range(len(imdb_clean.columns)):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = imdb_clean.iloc[label_tuple].copy()\n",
    "        dirty_context = imdb_dirty.iloc[label_tuple].copy()\n",
    "        clean_cell = imdb_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = imdb_dirty.iloc[label_tuple,i]\n",
    "        # single_context_clean = 'COL %s VAL %s ' % (imdb_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (imdb_clean.columns[i],dirty_cell)\n",
    "        for c in range(len(imdb_clean.columns)):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (imdb_clean.columns[c],imdb_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (imdb_dirty.columns[c],imdb_dirty.iloc[label_tuple,c])\n",
    "        detector_list_all.append([all_context_dirty,single_context_dirty,clean_cell])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2564,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_all).to_csv('datasets/imdb/t5/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2569,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((56240, 6), (56240,))"
      ]
     },
     "execution_count": 2569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "imdb_detector.shape,imdb_error_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2573,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('datasets/imdb/detector/detector_final.npy',np.array(imdb_detector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2571,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hospital Baseline for T5:Testing Inference with hospital_label_index\n",
    "detector_list_all = []\n",
    "for d in np.argwhere(imdb_detector==1):\n",
    "    label_tuple = imdb_error_index[d[0]]\n",
    "    i = d[1]\n",
    "    # for i in range(len(hospital_clean.columns)):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "    all_context_clean = ''\n",
    "    all_context_dirty = ''\n",
    "    clean_context = imdb_clean.iloc[label_tuple].copy()\n",
    "    dirty_context = imdb_dirty.iloc[label_tuple].copy()\n",
    "    clean_cell = imdb_clean.iloc[label_tuple,i]\n",
    "    dirty_cell = imdb_dirty.iloc[label_tuple,i]\n",
    "    # single_context_clean = 'COL %s VAL %s ' % (imdb_clean.columns[i],clean_cell)\n",
    "    single_context_dirty = 'COL %s VAL %s ' % (imdb_clean.columns[i],dirty_cell)\n",
    "    for c in range(len(imdb_clean.columns)):\n",
    "        all_context_clean += 'COL %s VAL %s ' % (imdb_clean.columns[c],imdb_clean.iloc[label_tuple,c])\n",
    "        all_context_dirty += 'COL %s VAL %s ' % (imdb_dirty.columns[c],imdb_dirty.iloc[label_tuple,c])\n",
    "    detector_list_all.append([all_context_dirty,single_context_dirty,clean_cell])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2574,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_all).to_csv('datasets/imdb/t5/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2589,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_correction.to_csv('datasets/imdb/t5/correct.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2590,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_correction.to_csv('datasets/hospital/t5/correct.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2591,
   "metadata": {},
   "outputs": [],
   "source": [
    "rayyan_correction.to_csv('datasets/rayyan/t5/correct.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2593,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_correction.to_csv('datasets/beers/t5/correct.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2604,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotom = pd.read_csv('/home/yanmy/rotom/data/cleaning/beers/100_10000/0/train.txt',sep='\\t',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_train_rotom = pd.read_csv('/home/yanmy/raha/raha-master/datasets/hospital/detector/train.csv',index_col=0)\n",
    "\n",
    "hospital_train_rotom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2610,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_train_rotom.iloc[:,1:].drop_duplicates().to_csv('/home/yanmy/rotom/data/cleaning/hospital/train.txt',header=None,index=False,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_aug = pd.read_json('/home/yanmy/rotom/data/cleaning/hospital/train.txt.augment.jsonl',lines=True)\n",
    "hospital_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2625,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2118/1895701257.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hospital_train_aug['index'] = hospital_train_aug.reset_index(drop=True).index\n",
      "pandas bar: 100%|██████████| 271/271 [00:00<00:00, 576.09it/s]\n"
     ]
    }
   ],
   "source": [
    "hospital_aug_index = hospital_train_rotom.iloc[:,1:].drop_duplicates().index\n",
    "hospital_train_aug = hospital_train_rotom.iloc[hospital_aug_index]\n",
    "hospital_train_aug['index'] = hospital_train_aug.reset_index(drop=True).index\n",
    "def Augmented_sample(row):\n",
    "    context = row[0]\n",
    "    content = row[1]\n",
    "    label = row[2]\n",
    "    index = row['index']\n",
    "    row['sid'] = index\n",
    "    row['original'] = content\n",
    "    row['augment'] = [context]\n",
    "    row['label'] = label\n",
    "    return row[-4:]\n",
    "hospital_train_aug_output = hospital_train_aug.progress_apply(Augmented_sample,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2628,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_train_aug_output['sid'] = hospital_train_aug_output['sid'] + 1\n",
    "hospital_train_aug_output.to_json('/home/yanmy/rotom/data/cleaning/hospital/train.txt.augment.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2630,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_train_rotom = pd.read_csv('/home/yanmy/raha/raha-master/datasets/beers/detector/train.csv',index_col=0)\n",
    "beer_train_rotom.iloc[:,1:].drop_duplicates().to_csv('/home/yanmy/rotom/data/cleaning/beers/train.txt',header=None,index=False,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_aug_index = beer_train_rotom.iloc[:,1:].drop_duplicates().index\n",
    "beer_train_aug = beer_train_rotom.iloc[hospital_aug_index]\n",
    "beer_train_aug['index'] = beer_train_aug.reset_index(drop=True).index\n",
    "def Augmented_sample(row):\n",
    "    context = row[0]\n",
    "    content = row[1]\n",
    "    label = row[2]\n",
    "    index = row['index']\n",
    "    row['sid'] = index\n",
    "    row['original'] = content\n",
    "    row['augment'] = [context]\n",
    "    row['label'] = label\n",
    "    return row[-4:]\n",
    "beer_train_aug_output = beer_train_aug.progress_apply(Augmented_sample,axis=1)\n",
    "beer_train_aug_output.to_json('/home/yanmy/rotom/data/cleaning/beers/train.txt.augment.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2639,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_valid_rotom = pd.read_csv('/home/yanmy/raha/raha-master/datasets/beers/detector/multi-view/train_aug.csv',index_col=0)\n",
    "# hospital_valid_rotom\n",
    "unlabelled = hospital_valid_rotom.iloc[:,1:].drop_duplicates()\n",
    "unlabelled.iloc[:,-1] = 0\n",
    "unlabelled.to_csv('/home/yanmy/rotom/data/cleaning/beers/unlabeled.txt',header=None,index=False,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2637,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_test_rotom = pd.read_csv('/home/yanmy/raha/raha-master/datasets/beers/detector/multi-view/test.csv',index_col=0)\n",
    "# hospital_valid_rotom\n",
    "test = hospital_test_rotom\n",
    "# test\n",
    "# unlabelled.iloc[:,-1] = 0\n",
    "test.iloc[:,1:].to_csv('/home/yanmy/rotom/data/cleaning/beers/test.txt',header=None,index=False,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2612,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_valid_rotom = pd.read_csv('/home/yanmy/raha/raha-master/datasets/hospital/detector/multi-view/train_20.csv',index_col=0)\n",
    "# hospital_valid_rotom\n",
    "unlabelled = hospital_valid_rotom.iloc[:,1:].drop_duplicates()\n",
    "unlabelled.iloc[:,-1] = 0\n",
    "unlabelled.to_csv('/home/yanmy/rotom/data/cleaning/hospital/unlabeled.txt',header=None,index=False,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2618,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_test_rotom = pd.read_csv('/home/yanmy/raha/raha-master/datasets/hospital/detector/multi-view/test.csv',index_col=0)\n",
    "# hospital_valid_rotom\n",
    "test = hospital_test_rotom.iloc[20000:40000,1:]\n",
    "\n",
    "# unlabelled.iloc[:,-1] = 0\n",
    "test.to_csv('/home/yanmy/rotom/data/cleaning/hospital/test.txt',header=None,index=False,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# CUDA_VISIBLE_DEVICES=0 python train_any.py \\\n",
    "#   --task cleaning_hospital \\\n",
    "#   --size 300 \\\n",
    "#   --logdir results_cleaning/ \\\n",
    "#   --finetuning \\\n",
    "#   --batch_size 32 \\\n",
    "#   --lr 3e-5 \\\n",
    "#   --n_epochs 20 \\\n",
    "#   --max_len 128 \\\n",
    "#   --fp16 \\\n",
    "#   --lm /home/yanmy/roberta \\\n",
    "#   --da auto_filter_weight \\\n",
    "#   --balance \\\n",
    "#   --run_id 0\n",
    "CUDA_VISIBLE_DEVICES=0 python train_any.py \\\n",
    "  --task cleaning_flights \\\n",
    "  --size 300 \\\n",
    "  --logdir results_cleaning_flights/ \\\n",
    "  --finetuning \\\n",
    "  --batch_size 32 \\\n",
    "  --lr 3e-5 \\\n",
    "  --n_epochs 20 \\\n",
    "  --max_len 128 \\\n",
    "  --fp16 \\\n",
    "  --lm roberta \\\n",
    "  --da auto_filter_weight \\\n",
    "  --balance \\\n",
    "  --run_id 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============cleaning_hospital==================\n",
    "accuracy=0.997\n",
    "precision=0.954\n",
    "recall=0.941\n",
    "f1=0.948\n",
    "=============cleaning_beers==================\n",
    "accuracy=0.978\n",
    "precision=0.913\n",
    "recall=0.950\n",
    "f1=0.931\n",
    "=============cleaning_rayyan==================\n",
    "accuracy=0.750\n",
    "precision=0.260\n",
    "recall=0.884\n",
    "f1=0.402\n",
    "=============cleaning_tax==================\n",
    "accuracy=0.973\n",
    "precision=1.000\n",
    "recall=0.595\n",
    "f1=0.746\n",
    "=============cleaning_flights==================\n",
    "accuracy=0.443\n",
    "precision=0.466\n",
    "recall=0.893\n",
    "f1=0.612\n",
    "=============cleaning_imdb==================\n",
    "accuracy=0.170\n",
    "precision=0.170\n",
    "recall=1.000\n",
    "f1=0.291"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2667,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2118/2065729242.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  beer_train_aug['index'] = beer_train_aug.reset_index(drop=True).index\n",
      "pandas bar:   0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 90/90 [00:00<00:00, 591.36it/s]\n"
     ]
    }
   ],
   "source": [
    "beer_train_rotom = pd.read_csv('/home/yanmy/raha/raha-master/datasets/flights/detector/multi-view/train.csv',index_col=0)\n",
    "\n",
    "hospital_test_rotom = pd.read_csv('/home/yanmy/raha/raha-master/datasets/flights/detector/multi-view/test.csv',index_col=0)\n",
    "hospital_valid_rotom = pd.read_csv('/home/yanmy/raha/raha-master/datasets/flights/detector/multi-view/test.csv',index_col=0)\n",
    "\n",
    "\n",
    "unlabelled = hospital_valid_rotom.iloc[:,1:].drop_duplicates()\n",
    "unlabelled.iloc[:,-1] = 0\n",
    "\n",
    "test = hospital_test_rotom\n",
    "\n",
    "hospital_aug_index = beer_train_rotom.iloc[:,1:].drop_duplicates().index\n",
    "beer_train_aug = beer_train_rotom.iloc[hospital_aug_index]\n",
    "beer_train_aug['index'] = beer_train_aug.reset_index(drop=True).index\n",
    "def Augmented_sample(row):\n",
    "    context = row[0]\n",
    "    content = row[1]\n",
    "    label = row[2]\n",
    "    index = row['index']\n",
    "    row['sid'] = index\n",
    "    row['original'] = content\n",
    "    row['augment'] = [context]\n",
    "    row['label'] = label\n",
    "    return row[-4:]\n",
    "beer_train_aug_output = beer_train_aug.progress_apply(Augmented_sample,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2670,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_train_rotom.iloc[:,1:].drop_duplicates().to_csv('/home/yanmy/rotom/data/cleaning/flights/train.txt',header=None,index=False,sep='\\t')\n",
    "test.iloc[:,1:].to_csv('/home/yanmy/rotom/data/cleaning/flights/test.txt',header=None,index=False,sep='\\t')\n",
    "beer_train_aug_output.to_json('/home/yanmy/rotom/data/cleaning/flights/train.txt.augment.jsonl', orient='records', lines=True)\n",
    "unlabelled.to_csv('/home/yanmy/rotom/data/cleaning/flights/unlabeled.txt',header=None,index=False,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2680,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = pd.read_csv('/home/yanmy/raha/BClean-main/baseline/Garf-master-main/data/hospital/hospital_clean.csv',dtype=str).fillna('')\n",
    "dirty = pd.read_csv('/home/yanmy/raha/BClean-main/baseline/Garf-master-main/data/hospital/hospital_dirty.csv',dtype=str).fillna('')\n",
    "repair = pd.read_csv('/home/yanmy/raha/BClean-main/baseline/Garf-master-main/data/hospital/hospital_repair.csv',dtype=str).fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2683,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   2,   5,  14,  31,  32,  33,  36,  37,  56,  86,  88,  96,\n",
       "       107, 113, 122, 123, 127, 129, 143])"
      ]
     },
     "execution_count": 2683,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(flight_label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2681,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e20ab87977b14527a09451af786f993a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.3889695210449927, 0.589010989010989, 0.46853146853146854)"
      ]
     },
     "execution_count": 2681,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_Data_Error = 0\n",
    "All_Fixed_Error = 0\n",
    "Correct_Fixed_Error = 0\n",
    "# clean = pd.read_csv('/home/yanmy/raha/BClean-main/baseline/Garf-master-main/data/beers/beers_clean.csv')\n",
    "# for i in tqdm(range(len(imdb_clean))):\n",
    "for i in tqdm(range(len(clean))):\n",
    "    for j in range(len(clean.columns)):\n",
    "        dirty_cell = dirty.iloc[i,j]\n",
    "        clean_cell = clean.iloc[i,j]\n",
    "        correct_cell = repair.iloc[i,j]\n",
    "        if(correct_cell!=dirty_cell):\n",
    "            All_Fixed_Error += 1\n",
    "        if(clean_cell!=dirty_cell):\n",
    "            All_Data_Error += 1\n",
    "            if(correct_cell==clean_cell):\n",
    "                Correct_Fixed_Error += 1\n",
    "Precision_hospital = Correct_Fixed_Error / All_Fixed_Error\n",
    "Recall_hospital = Correct_Fixed_Error / All_Data_Error\n",
    "F1_hospital = (2 * Precision_hospital * Recall_hospital) / (Precision_hospital + Recall_hospital)\n",
    "Precision_hospital,Recall_hospital,F1_hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2703,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f86cdefe2c4b2e91cfbd5289028631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.40893470790378006, 0.023443656422379826, 0.04434507173467486)"
      ]
     },
     "execution_count": 2703,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_Data_Error = 0\n",
    "All_Fixed_Error = 0\n",
    "Correct_Fixed_Error = 0\n",
    "clean = pd.read_csv('/home/yanmy/raha/BClean-main/baseline/Garf-master-main/data/beers/beers_clean.csv')\n",
    "dirty = pd.read_csv('/home/yanmy/raha/BClean-main/baseline/Garf-master-main/data/beers/beers_dirty.csv')\n",
    "repair = pd.read_csv('/home/yanmy/raha/BClean-main/baseline/Garf-master-main/data/beers/beers_repair.csv')\n",
    "for i in beer_label_index:\n",
    "    repair.iloc[i] = clean.iloc[i]\n",
    "# for i in tqdm(range(len(imdb_clean))):\n",
    "for i in tqdm(range(len(clean))):\n",
    "    for j in range(len(clean.columns)-1):\n",
    "        dirty_cell = dirty.iloc[i,j]\n",
    "        clean_cell = clean.iloc[i,j]\n",
    "        correct_cell = repair.iloc[i,j]\n",
    "        if(correct_cell!=dirty_cell):\n",
    "            All_Fixed_Error += 1\n",
    "        if(clean_cell!=dirty_cell):\n",
    "            All_Data_Error += 1\n",
    "            if(correct_cell==clean_cell):\n",
    "                Correct_Fixed_Error += 1\n",
    "Precision_hospital = Correct_Fixed_Error / All_Fixed_Error\n",
    "Recall_hospital = Correct_Fixed_Error / All_Data_Error\n",
    "F1_hospital = (2 * Precision_hospital * Recall_hospital) / (Precision_hospital + Recall_hospital)\n",
    "Precision_hospital,Recall_hospital,F1_hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2699,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2410, 2410,    0,    0,  127,  129, 2410])"
      ]
     },
     "execution_count": 2699,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(clean!=dirty).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([532, 157, 213, 228, 238, 256, 332, 386, 411, 493, 572, 698, 956,\n",
       "        22,  24,  42,  56,  57,  93,  94])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hospital_train = pd.read_csv('/home/yanmy/raha/raha-master/datasets/hospital/detector/multi-view/train_20.csv',index_col=0)\n",
    "# hospital_train\n",
    "hospital_label_index = np.load('/home/yanmy/raha/raha-master/datasets/hospital/detector/index.npy')\n",
    "hospital_label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2410, 11)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# detector_beer.shape\n",
    "beer_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 121, 2370,  230,  386, 1137, 1059,  183,  918,  309, 1287, 2204,\n",
       "       1735, 1288, 1430, 2135, 2393, 1905, 1691, 1088, 1670])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer_label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "training_list = []\n",
    "# for label_tuple in beer_label_index:\n",
    "for label_tuple in range(len(beer_clean)):\n",
    "    clean_text = beer_clean.iloc[label_tuple].to_dict()\n",
    "    dirty_text = beer_dirty.iloc[label_tuple].to_dict()\n",
    "    for j in range(11):\n",
    "        col_name = beer_clean.columns[j]\n",
    "        clean_cell = beer_clean.iloc[label_tuple,j]\n",
    "        dirty_cell = beer_dirty.iloc[label_tuple,j]\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            text_head = 'You are an expert in Cleaning Beers Dataset. Given the dirty row Entity 1, you are required to detect the values of %s in Entity 1 is correct or not.\\n\\nReturn yes or no.\\n\\nEntity 1:\\n\\n%s' % (col_name, json.dumps(dirty_text))\n",
    "            result = 'no'\n",
    "            # training_list.append([text_head,'',result])\n",
    "            # text_head = 'You are an expert in Cleaning Beers Dataset. Given the dirty row Entity 1, you are required to detect the values of %s in Entity 1 is correct or not.\\n\\nReturn yes or no.\\n\\nEntity 1:\\n\\n%s' % (col_name, json.dumps(clean_text))\n",
    "            # result = 'yes'\n",
    "            # training_list.append([text_head,'',result])\n",
    "        else:\n",
    "            text_head = 'You are an expert in Cleaning Beers Dataset. Given the dirty row Entity 1, you are required to detect the values of %s in Entity 1 is correct or not.\\n\\nReturn yes or no.\\n\\nEntity 1:\\n\\n%s' % (col_name, json.dumps(dirty_text))\n",
    "            result = 'yes'\n",
    "            training_list.append([text_head,'',result])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in Cleaning Beers Dataset. Given the dirty row Entity 1, you are required to detect the values of abv in Entity 1 is correct or not.\n",
      "\n",
      "Return yes or no.\n",
      "\n",
      "Entity 1:\n",
      "\n",
      "{\"index\": \"1\", \"id\": \"1436\", \"beer-name\": \"Pub Beer\", \"style\": \"American Pale Lager\", \"ounces\": \"12.0 oz\", \"abv\": \"0.05\", \"ibu\": \"\", \"brewery_id\": \"408\", \"brewery-name\": \"10 Barrel Brewing Company\", \"city\": \"Bend\", \"state\": \"OR\"}\n"
     ]
    }
   ],
   "source": [
    "print(training_list_pd.iloc[4,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list_pd = pd.DataFrame(training_list)\n",
    "training_list_pd.columns = ['instruction','input','output']\n",
    "json.dump(training_list_pd.to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/beer/beer-test-detection.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ablation Study, to decide whether a LLM classifier is inferior with LM Classifier or not\n",
    "## Hospital\n",
    "\n",
    "text_head = 'You are an expert in Cleaning Hospital Dataset. Given the dirty row Entity 1, you are required to judge the values of %s in Entity 1 is clean or not.\\n\\nEntity 1:\\n\\n%s\\n\\n' % (col_name, json.dumps(template_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ablation Study for Generation and Correction\n",
    "rayyan_test = pd.read_json('/home/yanmy/LLaMA-Efficient-Tuning-main/data/rayyan/rayyan-train-20.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar:  38%|███▊      | 1432/3812 [00:00<00:00, 14311.76it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas bar: 100%|██████████| 3812/3812 [00:00<00:00, 14303.74it/s]\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "# rayyan_test.iloc[0,0].split('\\n\\n')[3]\n",
    "# ast.literal_eval(rayyan_test.iloc[0,0].split('\\n\\n')[5])\n",
    "def Extract_Row(row):\n",
    "    template = row[0].split('\\n\\n')[3]\n",
    "    content = row[0].split('\\n\\n')[5]\n",
    "    template = list(ast.literal_eval(template).keys())[0]\n",
    "    content = ast.literal_eval(content)\n",
    "    content[template] = ''\n",
    "    return '\\n\\n'.join(row[0].split('\\n\\n')[:5]) + '\\n\\n' + json.dumps(content)\n",
    "rayyan_test_revise = rayyan_test.copy()\n",
    "rayyan_test_revise['instruction'] = rayyan_test.progress_apply(Extract_Row,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(rayyan_test_revise.to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/rayyan/rayyan-train-ablation.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evalation\n",
    "hospital_ablation = pd.read_csv('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/vicuna-13b-beer-test-ablation.csv',index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23148</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23149</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23150</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23151</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23152</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23153 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             instruction  input output predict\n",
       "0      You are an expert in Cleaning Beers Dataset. G...    NaN    yes     yes\n",
       "1      You are an expert in Cleaning Beers Dataset. G...    NaN    yes     yes\n",
       "2      You are an expert in Cleaning Beers Dataset. G...    NaN    yes     yes\n",
       "3      You are an expert in Cleaning Beers Dataset. G...    NaN    yes     yes\n",
       "4      You are an expert in Cleaning Beers Dataset. G...    NaN    yes     yes\n",
       "...                                                  ...    ...    ...     ...\n",
       "23148  You are an expert in Cleaning Beers Dataset. G...    NaN    yes     yes\n",
       "23149  You are an expert in Cleaning Beers Dataset. G...    NaN    yes     yes\n",
       "23150  You are an expert in Cleaning Beers Dataset. G...    NaN    yes     yes\n",
       "23151  You are an expert in Cleaning Beers Dataset. G...    NaN    yes     yes\n",
       "23152  You are an expert in Cleaning Beers Dataset. G...    NaN    yes     yes\n",
       "\n",
       "[23153 rows x 4 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_detect = pd.read_csv('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/vicuna-13b-beer-test-detection.csv',index_col=0)\n",
    "result_detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "yes    23153\n",
       "Name: output, dtype: int64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_detect['output'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "yes    426\n",
       "Name: output, dtype: int64"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_detect[result_detect['output']!=result_detect['predict']]['output'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.14747474747474748, 0.1437007874015748, 0.14556331006979062)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "73/495,73/508,(2*(73/495)*(73/508))/(73/495+73/508)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- hospital_ablation -->\n",
    "(0.14747474747474748, 0.1437007874015748, 0.14556331006979062)\n",
    "<!-- Beers_ablation -->\n",
    "(0.5689655172413793, 0.5701519213583557, 0.5695581014729951)\n",
    "<!-- Rayyan_ablation -->\n",
    "(0.13339301700984782, 0.1571729957805907, 0.14430992736077483)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
