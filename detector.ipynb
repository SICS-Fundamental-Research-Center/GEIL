{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 32 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "## import necessary package\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "# from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "## Config\n",
    "tqdm.pandas(desc='pandas bar')\n",
    "from pandarallel import pandarallel\n",
    "import argparse\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=False)\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import cross_val_predict, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import ExtraTreesClassifier,AdaBoostClassifier\n",
    "import ast\n",
    "import re\n",
    "# from cleanlab.filter import find_label_issues\n",
    "# from cleanlab.classification import CleanLearning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_clean = pd.read_csv('datasets/hospital/clean.csv').astype(str)\n",
    "hospital_dirty = pd.read_csv('datasets/hospital/dirty.csv').astype(str)\n",
    "hospital_query = pd.read_csv('datasets/hospital/dirty_query.csv')\n",
    "hospital_dirty.columns = hospital_clean.columns\n",
    "hospital_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "509"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix = np.array(hospital_clean!=hospital_dirty)\n",
    "input_matrix.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所选行的索引: [532, 157, 213, 228, 238, 256, 332, 386, 411, 493, 572, 698, 956, 22, 24, 42, 56, 57, 93, 94]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def find_max_coverage(matrix):\n",
    "    # 转置矩阵以便按列计算列和\n",
    "    transposed_matrix = np.transpose(matrix)\n",
    "    \n",
    "    # 初始化一个列表，用于记录每列的和以及列的索引\n",
    "    column_sums = [(sum(column), index) for index, column in enumerate(transposed_matrix)]\n",
    "    \n",
    "    # 按列和降序排序\n",
    "    column_sums.sort(reverse=True)\n",
    "    \n",
    "    selected_rows = []\n",
    "    selected_columns = set()\n",
    "    \n",
    "    for _, column_index in column_sums:\n",
    "        # 如果所选列已经包含了这一列，跳过\n",
    "        if column_index in selected_columns:\n",
    "            continue\n",
    "        \n",
    "        # 找到可以添加的行\n",
    "        best_row = None\n",
    "        best_row_sum = -1\n",
    "        \n",
    "        for row_index, row in enumerate(matrix):\n",
    "            if row_index in selected_rows:\n",
    "                continue\n",
    "            \n",
    "            # 计算将此行添加到已选行中后的行之和\n",
    "            new_row_sum = sum(row)\n",
    "            \n",
    "            if new_row_sum > best_row_sum:\n",
    "                best_row_sum = new_row_sum\n",
    "                best_row = row_index\n",
    "        \n",
    "        # 如果找到了合适的行，添加它\n",
    "        if best_row is not None:\n",
    "            selected_rows.append(best_row)\n",
    "            selected_columns.update([column_index])\n",
    "            \n",
    "            # 如果已经选择的行数超过20，停止\n",
    "            if len(selected_rows) >= 20:\n",
    "                break\n",
    "    \n",
    "    return selected_rows\n",
    "\n",
    "# 示例用法\n",
    "if __name__ == \"__main__\":\n",
    "    matrix = np.random.randint(2, size=(1000, 20))  # 随机生成一个1000x20的二进制矩阵\n",
    "    selected_rows = find_max_coverage(input_matrix)\n",
    "    print(\"所选行的索引:\", selected_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix[selected_rows].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 4, 0, 0, 3, 4, 3, 2, 5, 5, 7, 3, 4, 0, 4, 4, 1, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix_select.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 找到选中的20个tuple\n",
    "input_matrix_select = input_matrix[selected_rows]\n",
    "detector_list = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "for label_tuple in selected_rows:\n",
    "    for i in range(len(hospital_clean.columns)):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = hospital_clean.iloc[label_tuple]\n",
    "        dirty_context = hospital_dirty.iloc[label_tuple]\n",
    "        clean_cell = hospital_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = hospital_dirty.iloc[label_tuple,i]\n",
    "        single_context_clean = 'COL %s VAL %s ' % (hospital_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (hospital_clean.columns[i],dirty_cell)\n",
    "        for c in range(20):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (hospital_clean.columns[c],hospital_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (hospital_dirty.columns[c],hospital_dirty.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            detector_list.append([all_context_dirty,single_context_dirty,1])\n",
    "            detector_list.append([all_context_clean,single_context_clean,0])\n",
    "            detector_list.append([all_context_dirty,single_context_clean,0])\n",
    "            # detector_list.append([single_context_dirty,1])\n",
    "        else:\n",
    "            detector_list.append([all_context_dirty,single_context_dirty,0])\n",
    "            detector_list.append([all_context_clean,single_context_clean,0])\n",
    "            detector_list.append([all_context_dirty,single_context_clean,0])\n",
    "        # else:\n",
    "        #     detector_list.append([all_context_dirty,1])\n",
    "        #     detector_list.append([single_context_dirty,1])\n",
    "            \n",
    "                # break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19039,\n",
       " 3555,\n",
       " 9924,\n",
       " 825,\n",
       " 2150,\n",
       " 2652,\n",
       " 2923,\n",
       " 3338,\n",
       " 3534,\n",
       " 3630,\n",
       " 5400,\n",
       " 9234,\n",
       " 10347,\n",
       " 11305,\n",
       " 12145,\n",
       " 14408,\n",
       " 15975,\n",
       " 16157,\n",
       " 16326,\n",
       " 17044]"
      ]
     },
     "execution_count": 1383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:15<00:00, 65.06it/s]\n"
     ]
    }
   ],
   "source": [
    "input_matrix_select = input_matrix[selected_rows]\n",
    "detector_list_all = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "# for label_tuple in tqdm(selected_rows):\n",
    "for label_tuple in tqdm(range(len(hospital_clean))):\n",
    "    for i in range(len(hospital_clean.columns)):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = hospital_clean.iloc[label_tuple]\n",
    "        dirty_context = hospital_dirty.iloc[label_tuple]\n",
    "        clean_cell = hospital_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = hospital_dirty.iloc[label_tuple,i]\n",
    "        single_context_clean = 'COL %s VAL %s ' % (hospital_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (hospital_clean.columns[i],dirty_cell)\n",
    "        for c in range(20):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (hospital_clean.columns[c],hospital_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (hospital_dirty.columns[c],hospital_dirty.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            detector_list_all.append([all_context_dirty,single_context_dirty,1])\n",
    "        else:\n",
    "            detector_list_all.append([all_context_dirty,single_context_dirty,0])\n",
    "            # detector_list.append([single_context_dirty,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COL index VAL 533 COL ProviderNumber VAL 10023...</td>\n",
       "      <td>COL index VAL 533</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COL index VAL 533 COL ProviderNumber VAL 10023...</td>\n",
       "      <td>COL index VAL 533</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COL index VAL 533 COL ProviderNumber VAL 10023...</td>\n",
       "      <td>COL ProviderNumber VAL 10023</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COL index VAL 533 COL ProviderNumber VAL 10023...</td>\n",
       "      <td>COL ProviderNumber VAL 10023</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COL index VAL 533 COL ProviderNumber VAL 10023...</td>\n",
       "      <td>COL HospitalName VAL baptist medical cexter so...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1192</th>\n",
       "      <td>COL index VAL 95 COL ProviderNumber VAL 10007 ...</td>\n",
       "      <td>COL Score VAL 60%</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>COL index VAL 95 COL ProviderNumber VAL 10007 ...</td>\n",
       "      <td>COL Sample VAL 5 patients</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>COL index VAL 95 COL ProviderNumber VAL 10007 ...</td>\n",
       "      <td>COL Sample VAL 5 patients</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>COL index VAL 95 COL ProviderNumber VAL 10007 ...</td>\n",
       "      <td>COL Stateavg VAL al_ami-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>COL index VAL 95 COL ProviderNumber VAL 10007 ...</td>\n",
       "      <td>COL Stateavg VAL al_ami-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>854 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0  \\\n",
       "0     COL index VAL 533 COL ProviderNumber VAL 10023...   \n",
       "1     COL index VAL 533 COL ProviderNumber VAL 10023...   \n",
       "3     COL index VAL 533 COL ProviderNumber VAL 10023...   \n",
       "4     COL index VAL 533 COL ProviderNumber VAL 10023...   \n",
       "6     COL index VAL 533 COL ProviderNumber VAL 10023...   \n",
       "...                                                 ...   \n",
       "1192  COL index VAL 95 COL ProviderNumber VAL 10007 ...   \n",
       "1194  COL index VAL 95 COL ProviderNumber VAL 10007 ...   \n",
       "1195  COL index VAL 95 COL ProviderNumber VAL 10007 ...   \n",
       "1197  COL index VAL 95 COL ProviderNumber VAL 10007 ...   \n",
       "1198  COL index VAL 95 COL ProviderNumber VAL 10007 ...   \n",
       "\n",
       "                                                      1  2  \n",
       "0                                    COL index VAL 533   0  \n",
       "1                                    COL index VAL 533   0  \n",
       "3                         COL ProviderNumber VAL 10023   0  \n",
       "4                         COL ProviderNumber VAL 10023   0  \n",
       "6     COL HospitalName VAL baptist medical cexter so...  1  \n",
       "...                                                 ... ..  \n",
       "1192                                 COL Score VAL 60%   0  \n",
       "1194                         COL Sample VAL 5 patients   0  \n",
       "1195                         COL Sample VAL 5 patients   0  \n",
       "1197                         COL Stateavg VAL al_ami-1   0  \n",
       "1198                         COL Stateavg VAL al_ami-1   0  \n",
       "\n",
       "[854 rows x 3 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(detector_list).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list).drop_duplicates().to_csv('datasets/hospital/detector/test_cell.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(detector_list)[1].value_counts()\n",
    "# pd.DataFrame(detector_list).to_csv('datasets/hospital/detector/train.csv')\n",
    "dirty_cell_indice = np.argwhere(input_matrix==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dirty_cell_indice:\n",
    "    clean_cell = hospital_clean.iloc[d[0],d[1]]\n",
    "    dirty_cell = hospital_dirty.iloc[d[0],d[1]]\n",
    "    dirty_value = hospital_dirty.iloc[:,d[1]].unique()\n",
    "    if(clean_cell not in dirty_value):\n",
    "        print(d)\n",
    "        print(clean_cell)\n",
    "        print(dirty_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = np.load('datasets/hospital/detector/detection.npy')\n",
    "detector = detector.reshape((1000,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_cell_indice = np.argwhere(detector!=input_matrix)\n",
    "for d in false_cell_indice:\n",
    "    clean_cell = hospital_clean.iloc[d[0],d[1]]\n",
    "    dirty_cell = hospital_dirty.iloc[d[0],d[1]]\n",
    "    # dirty_value = hospital_dirty.iloc[:,d[1]].unique()\n",
    "    # if(clean_cell not in dirty_value):\n",
    "    print(d,input_matrix[d[0],d[1]],detector[d[0],d[1]])\n",
    "    print(clean_cell)\n",
    "    print(dirty_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util\n",
    "def generate_similarity_dict(similarity_matrix):\n",
    "    similarity_dict = {}\n",
    "    num_elements = similarity_matrix.shape[0]\n",
    "\n",
    "    for i in range(num_elements):\n",
    "        # Get similarity scores for element i with all other elements\n",
    "        similarity_scores = similarity_matrix[i, :]\n",
    "        \n",
    "        # Get indices of top 50 similar elements (excluding self)\n",
    "        top_indices = np.argsort(similarity_scores)[::-1]\n",
    "        \n",
    "        # Add the list of top 50 similar elements to the dictionary\n",
    "        similarity_dict[i] = top_indices.tolist()\n",
    "\n",
    "    return similarity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/yanmy/anaconda3/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda112_nocublaslt.so\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.2/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.0\n",
      "CUDA SETUP: Detected CUDA version 112\n",
      "CUDA SETUP: Loading binary /home/yanmy/anaconda3/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda112_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanmy/anaconda3/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /home/yanmy/anaconda3 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/yanmy/anaconda3/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-15 00:03:32,958] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer,util,LoggingHandler, losses, InputExample\n",
    "model = SentenceTransformer('/home/yanmy/sentence_transformer_model/bge-large-en-1.5/').to('cuda:3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util\n",
    "sim_dict = {}\n",
    "value_dict = {}\n",
    "for i in range(20):\n",
    "    value_list = list(hospital_dirty.iloc[:,i].unique())\n",
    "    value_dict[i] = value_list\n",
    "    embedding_A = model.encode(value_list,show_progress_bar=True)\n",
    "    sim_A_B = util.cos_sim(embedding_A, embedding_A).numpy()\n",
    "    sim_A_B_dict = generate_similarity_dict(sim_A_B) \n",
    "    sim_dict[i] = sim_A_B_dict   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['36067',\n",
       " '3x0x7',\n",
       " '35653',\n",
       " '35150',\n",
       " 'x5150',\n",
       " '99508',\n",
       " '99559',\n",
       " '36360',\n",
       " 'x6x60',\n",
       " '35960']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_and_sort_strings(input_list, target_element):\n",
    "    # 如果目标元素长度小于等于5，则无需相似度限制\n",
    "    if len(target_element) <= 5:\n",
    "        return sorted([s for s in input_list if len(s) == len(target_element)], reverse=True)\n",
    "\n",
    "    # 计算目标元素字母的频率\n",
    "    target_element_letter_frequency = {}\n",
    "    for letter in target_element:\n",
    "        if letter in target_element_letter_frequency:\n",
    "            target_element_letter_frequency[letter] += 1\n",
    "        else:\n",
    "            target_element_letter_frequency[letter] = 1\n",
    "\n",
    "    def is_similar(string):\n",
    "        # 计算当前字符串字母的频率\n",
    "        string_letter_frequency = {}\n",
    "        for letter in string:\n",
    "            if letter in string_letter_frequency:\n",
    "                string_letter_frequency[letter] += 1\n",
    "            else:\n",
    "                string_letter_frequency[letter] = 1\n",
    "\n",
    "        # 计算相同字母的比例\n",
    "        common_letters = set(target_element_letter_frequency.keys()) & set(string_letter_frequency.keys())\n",
    "        common_letter_count = sum(min(target_element_letter_frequency[letter], string_letter_frequency[letter]) for letter in common_letters)\n",
    "        total_letter_count = len(target_element)  # 假设目标元素的长度与字符串长度相同\n",
    "\n",
    "        similarity_ratio = common_letter_count / total_letter_count\n",
    "        return similarity_ratio > 0.8\n",
    "\n",
    "    # 找到与目标元素长度相同且相似度超过80%的字符串\n",
    "    matching_strings = [s for s in input_list if len(s) == len(target_element) and is_similar(s)]\n",
    "\n",
    "    # 根据字母顺序进行倒序排序\n",
    "    sorted_strings = sorted(matching_strings, reverse=True)\n",
    "\n",
    "    return sorted_strings\n",
    "def find_elements_around_element(input_list, target_element):\n",
    "    # 确保目标元素在列表中\n",
    "    if target_element not in input_list:\n",
    "        return []\n",
    "\n",
    "    index = input_list.index(target_element)\n",
    "    start_index = max(0, index - 5)\n",
    "    end_index = min(len(input_list), index + 6)\n",
    "\n",
    "    # 使用切片获取目标元素前五位到后五位的元素，不包括目标元素本身\n",
    "    elements_around = input_list[start_index:end_index]\n",
    "    elements_around.remove(target_element)  # 移除目标元素自身\n",
    "\n",
    "    return elements_around\n",
    "\n",
    "\n",
    "find_elements_around_element(value_dict[8],'3x1x0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_cell_indice = np.argwhere(detector!=input_matrix)\n",
    "for d in false_cell_indice:\n",
    "    clean_cell = hospital_clean.iloc[d[0],d[1]]\n",
    "    dirty_cell = hospital_dirty.iloc[d[0],d[1]]\n",
    "    # dirty_value = hospital_dirty.iloc[:,d[1]].unique()\n",
    "    # if(clean_cell not in dirty_value):\n",
    "    print(d,input_matrix[d[0],d[1]],detector[d[0],d[1]])\n",
    "    print(clean_cell)\n",
    "    print(dirty_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 496/496 [00:00<00:00, 2309.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# for d in detector\n",
    "detector_indice = np.argwhere(detector==1)\n",
    "detector_list_all = []\n",
    "candidate_length = {}\n",
    "right_loc = {}\n",
    "for d in tqdm(detector_indice):\n",
    "    \n",
    "    label_tuple = d[0] ## 行\n",
    "    i = d[1] ## 列\n",
    "    all_context_clean = ''\n",
    "    all_context_dirty = ''\n",
    "    # clean_context = hospital_clean.iloc[label_tuple]\n",
    "    dirty_context = hospital_dirty.iloc[label_tuple]\n",
    "    clean_cell = hospital_clean.iloc[label_tuple,i]\n",
    "    dirty_cell = hospital_dirty.iloc[label_tuple,i]\n",
    "    # single_context_clean = 'COL %s VAL %s ' % (hospital_clean.columns[i],clean_cell)\n",
    "    # single_context_dirty = 'COL %s VAL %s ' % (hospital_clean.columns[i],dirty_cell)\n",
    "    for c in range(20):\n",
    "        # all_context_clean += 'COL %s VAL %s ' % (hospital_clean.columns[c],hospital_clean.iloc[label_tuple,c])\n",
    "        all_context_dirty += 'COL %s VAL %s ' % (hospital_dirty.columns[c],hospital_dirty.iloc[label_tuple,c])\n",
    "    # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "    # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "    # detector_list.append([single_context_clean,0])        \n",
    "    # detector_list.append([all_context_clean,0])\n",
    "    # if(dirty_cell!=clean_cell):\n",
    "    #     detector_list_all.append([all_context_dirty,single_context_dirty,1])\n",
    "    # else:\n",
    "    #     detector_list_all.append([all_context_dirty,single_context_dirty,0])\n",
    "    candidate_length[str([d[0],d[1]])] = len(find_elements_around_element(value_dict[i],dirty_cell))\n",
    "    for candidate in find_elements_around_element(value_dict[i],dirty_cell):\n",
    "        if(candidate==clean_cell):\n",
    "            single_context_dirty = 'COL %s VAL %s ' % (hospital_clean.columns[i],candidate)\n",
    "            detector_list_all.append([all_context_dirty,single_context_dirty,0])\n",
    "        else:\n",
    "            single_context_dirty = 'COL %s VAL %s ' % (hospital_clean.columns[i],candidate)\n",
    "            detector_list_all.append([all_context_dirty,single_context_dirty,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['birmingham', 'birmingxam', 'sheffield', 'sheffxeld', 'dothan', 'boaz'],\n",
       " 'birminghxm')"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_elements_around_element(value_dict[6],hospital_dirty.iloc[3,6]),hospital_dirty.iloc[3,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_all).to_csv('datasets/hospital/detector/cleaning.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0],\n",
       "       [  0,   1],\n",
       "       [  0,   2],\n",
       "       ...,\n",
       "       [999,  17],\n",
       "       [999,  18],\n",
       "       [999,  19]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector_indice_norm = np.argwhere(detector==0)\n",
    "detector_indice_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(detector.sum(axis=0)>0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1xx19', 'x0005', '1000x', 'x00xx', 'x00x5', '1xx15', '1xx16',\n",
       "       '100x8', '100x6', 'x0x08', '1xx24', 'x0027', 'x0029', '1xx29',\n",
       "       '1xx32', '100x4', '100x5', '1xx35', '1xx36', '1003x', '1xx39',\n",
       "       '100x9', '1xx44', '1xx45', 'x0045', '1xx47', '1004x'], dtype=object)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hospital_dirty.iloc[np.where(detector[:,1]==1)[0],1].unique()\n",
    "# np.where(detector[:,1]==1)[0]\n",
    "detector_list_all = []\n",
    "noise_col = np.where(detector.sum(axis=0)>0)[0]\n",
    "for d in tqdm(detector_indice_norm):\n",
    "    i = d[1]\n",
    "    label_tuple = d[0] ## 行\n",
    "    if(i in noise_col):\n",
    "        detection_candidate = hospital_dirty.iloc[np.where(detector[:,i]==1)[0],i].unique() ## 可能有错误的dirty data的集合\n",
    "        \n",
    "        ## 列\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        \n",
    "        # clean_context = hospital_clean.iloc[label_tuple]\n",
    "        dirty_context = hospital_dirty.iloc[label_tuple]\n",
    "        # clean_cell = hospital_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = hospital_dirty.iloc[label_tuple,i]\n",
    "        if(find_elements_around_element(value_dict[i],dirty_cell)):\n",
    "            \n",
    "        # single_context_clean = 'COL %s VAL %s ' % (hospital_clean.columns[i],clean_cell)\n",
    "        # single_context_dirty = 'COL %s VAL %s ' % (hospital_clean.columns[i],dirty_cell)\n",
    "        for c in range(20):\n",
    "            # all_context_clean += 'COL %s VAL %s ' % (hospital_clean.columns[c],hospital_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (hospital_dirty.columns[c],hospital_dirty.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        # if(dirty_cell!=clean_cell):\n",
    "        #     detector_list_all.append([all_context_dirty,single_context_dirty,1])\n",
    "        # else:\n",
    "        #     detector_list_all.append([all_context_dirty,single_context_dirty,0])\n",
    "        candidate_length[str([d[0],d[1]])] = len(find_elements_around_element(value_dict[i],dirty_cell))\n",
    "        for candidate in find_elements_around_element(value_dict[i],dirty_cell):\n",
    "            if(candidate==clean_cell):\n",
    "                single_context_dirty = 'COL %s VAL %s ' % (hospital_clean.columns[i],candidate)\n",
    "                detector_list_all.append([all_context_dirty,single_context_dirty,0])\n",
    "            else:\n",
    "                single_context_dirty = 'COL %s VAL %s ' % (hospital_clean.columns[i],candidate)\n",
    "                detector_list_all.append([all_context_dirty,single_context_dirty,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = pd.read_csv('datasets/movies_1/clean.csv').astype(str)\n",
    "dirty = pd.read_csv('datasets/movies_1/dirty.csv').astype(str)\n",
    "dirty.columns = clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 190,   17,  190,  190,   43,  129,    0,    0,    0,    0,  187,\n",
       "        187, 7158,    0,  180,    0,  188])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean.iloc[58,-1],dirty.iloc[58,-1]\n",
    "np.where(np.array(clean!=dirty)[:,0]==1)\n",
    "np.array(clean!=dirty).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def replace_random_char_with_x(input_string):\n",
    "    if not input_string:\n",
    "        return input_string\n",
    "\n",
    "    # 随机选择要替换的字符\n",
    "    char_to_replace = random.choice(input_string)\n",
    "\n",
    "    # 使用字符串的 replace 方法替换所有相同字符为 'x'\n",
    "    result_string = input_string.replace(char_to_replace, 'x')\n",
    "\n",
    "    return result_string\n",
    "def create_provider_index_dict(dataframe):\n",
    "    # 确保输入的 dataframe 包含 ProviderNumber 列\n",
    "    if 'ProviderNumber' not in dataframe.columns:\n",
    "        return None\n",
    "\n",
    "    # 创建一个空字典来存储结果\n",
    "    provider_index_dict = {}\n",
    "\n",
    "    # 遍历 DataFrame 的行\n",
    "    for index, row in dataframe.iterrows():\n",
    "        provider_number = row['ProviderNumber']\n",
    "\n",
    "        # 如果 ProviderNumber 已经在字典中，将索引追加到对应的列表\n",
    "        if provider_number in provider_index_dict:\n",
    "            provider_index_dict[provider_number].append(index)\n",
    "        else:\n",
    "            # 否则，创建一个新的键值对\n",
    "            provider_index_dict[provider_number] = [index]\n",
    "\n",
    "    return provider_index_dict\n",
    "# 测试函数\n",
    "# input_str = \"hello, world!\"\n",
    "# result = replace_random_char_with_x(input_str)\n",
    "# print(f\"Input: {input_str}\")\n",
    "# print(f\"Output: {result}\")\n",
    "def select_two_different_elements(input_list, given_element):\n",
    "    # 将输入列表转换为 NumPy 数组\n",
    "    np_array = np.array(input_list)\n",
    "\n",
    "    # 创建一个掩码，标记与给定元素不同的元素\n",
    "    mask = np_array != given_element\n",
    "\n",
    "    # 获取所有与给定元素不同的元素的索引\n",
    "    valid_indices = np.where(mask)[0]\n",
    "\n",
    "    # 检查是否有足够的元素用于选择\n",
    "    if len(valid_indices) < 2:\n",
    "        return None\n",
    "\n",
    "    # 随机选择两个不同的索引\n",
    "    selected_indices = np.random.choice(valid_indices, 2, replace=False)\n",
    "\n",
    "    # 根据索引获取对应的元素\n",
    "    selected_elements = np_array[selected_indices]\n",
    "\n",
    "    return selected_elements\n",
    "hospital_cluster = create_provider_index_dict(hospital_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hospital_dirty.iloc[hospital_cluster['10018']].iloc[:,-4].\n",
    "coreset_detect = np.where(detector.sum(axis=1)==0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_dirty['index'] = hospital_dirty['index'].astype(int)\n",
    "hospital_clean['index'] = hospital_clean['index'].astype(int)\n",
    "hospital_dirty_dict = hospital_dirty.set_index('index').to_dict('index')\n",
    "hospital_clean_dict = hospital_clean.set_index('index').to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ProviderNumber</th>\n",
       "      <th>HospitalName</th>\n",
       "      <th>Address1</th>\n",
       "      <th>Address2</th>\n",
       "      <th>Address3</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>ZipCode</th>\n",
       "      <th>CountyName</th>\n",
       "      <th>...</th>\n",
       "      <th>HospitalType</th>\n",
       "      <th>HospitalOwner</th>\n",
       "      <th>EmergencyService</th>\n",
       "      <th>Condition</th>\n",
       "      <th>MeasureCode</th>\n",
       "      <th>MeasureName</th>\n",
       "      <th>Score</th>\n",
       "      <th>Sample</th>\n",
       "      <th>Stateavg</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10018</td>\n",
       "      <td>callahan eye foundation hospital</td>\n",
       "      <td>1720 university blvd</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>birmingham</td>\n",
       "      <td>al</td>\n",
       "      <td>35233</td>\n",
       "      <td>jefferson</td>\n",
       "      <td>...</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>yes</td>\n",
       "      <td>surgical infection prevention</td>\n",
       "      <td>scip-card-2</td>\n",
       "      <td>surgery patients who were taking heart drugs c...</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>al_scip-card-2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10018</td>\n",
       "      <td>callahan eye foundation hospital</td>\n",
       "      <td>1720 university blvd</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>birmingham</td>\n",
       "      <td>al</td>\n",
       "      <td>35233</td>\n",
       "      <td>jefferson</td>\n",
       "      <td>...</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>yes</td>\n",
       "      <td>surgical infection prevention</td>\n",
       "      <td>scip-inf-1</td>\n",
       "      <td>surgery patients who were given an antibiotic ...</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>al_scip-inf-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10018</td>\n",
       "      <td>callahan eye foundation hospital</td>\n",
       "      <td>1720 university blvd</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>birmingham</td>\n",
       "      <td>al</td>\n",
       "      <td>35233</td>\n",
       "      <td>jefferson</td>\n",
       "      <td>...</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>yes</td>\n",
       "      <td>surgical infection prevention</td>\n",
       "      <td>scip-inf-2</td>\n",
       "      <td>surgery patients who were given the  right kin...</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>al_scip-inf-2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>10018</td>\n",
       "      <td>callahan eye foundation hospital</td>\n",
       "      <td>1720 university blvd</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>birminghxm</td>\n",
       "      <td>al</td>\n",
       "      <td>35233</td>\n",
       "      <td>jefferson</td>\n",
       "      <td>...</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>yes</td>\n",
       "      <td>surgical infection prevention</td>\n",
       "      <td>scip-inf-3</td>\n",
       "      <td>surgery patients whose preventive antibiotics ...</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>al_scip-inf-3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>10018</td>\n",
       "      <td>callahan eye foundation hospital</td>\n",
       "      <td>1720 university blvd</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>birmingham</td>\n",
       "      <td>al</td>\n",
       "      <td>35233</td>\n",
       "      <td>jefferson</td>\n",
       "      <td>...</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>yes</td>\n",
       "      <td>surgical infection prevention</td>\n",
       "      <td>scip-inf-4</td>\n",
       "      <td>all heart surgery patients whose blood sugar (...</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>al_scip-inf-4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>996</td>\n",
       "      <td>10050</td>\n",
       "      <td>st vincents blount</td>\n",
       "      <td>150 gilbreath drive</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>oneonta</td>\n",
       "      <td>al</td>\n",
       "      <td>35121</td>\n",
       "      <td>blount</td>\n",
       "      <td>...</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>yes</td>\n",
       "      <td>pneumonia</td>\n",
       "      <td>pn-6</td>\n",
       "      <td>pneumonia patients given the most appropriate ...</td>\n",
       "      <td>77%</td>\n",
       "      <td>74 patients</td>\n",
       "      <td>al_pn-6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>997</td>\n",
       "      <td>10050</td>\n",
       "      <td>st vincents blount</td>\n",
       "      <td>150 gilbreath drive</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>oneonta</td>\n",
       "      <td>al</td>\n",
       "      <td>35121</td>\n",
       "      <td>blount</td>\n",
       "      <td>...</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>yes</td>\n",
       "      <td>pneumonia</td>\n",
       "      <td>pn-7</td>\n",
       "      <td>pneumonia patients assessed and given influenz...</td>\n",
       "      <td>64%</td>\n",
       "      <td>55 patients</td>\n",
       "      <td>al_pn-7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>998</td>\n",
       "      <td>10050</td>\n",
       "      <td>st vincents blount</td>\n",
       "      <td>150 gilbreath drive</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>oneonta</td>\n",
       "      <td>al</td>\n",
       "      <td>35121</td>\n",
       "      <td>blount</td>\n",
       "      <td>...</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>yes</td>\n",
       "      <td>surgical infection prevention</td>\n",
       "      <td>scip-card-2</td>\n",
       "      <td>surgery pxtients who were txking hexrt drugs c...</td>\n",
       "      <td>25%</td>\n",
       "      <td>8 patients</td>\n",
       "      <td>al_scip-card-2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>999</td>\n",
       "      <td>10050</td>\n",
       "      <td>st vincents blount</td>\n",
       "      <td>150 gilbreath drive</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>oneonta</td>\n",
       "      <td>al</td>\n",
       "      <td>35121</td>\n",
       "      <td>blount</td>\n",
       "      <td>...</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>yes</td>\n",
       "      <td>surgical infection prevention</td>\n",
       "      <td>scip-inf-1</td>\n",
       "      <td>surgery patients who were given an antibiotic ...</td>\n",
       "      <td>64%</td>\n",
       "      <td>28 patients</td>\n",
       "      <td>al_scip-inf-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1000</td>\n",
       "      <td>10050</td>\n",
       "      <td>st vincents blount</td>\n",
       "      <td>150 gilbreath drive</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>oneonta</td>\n",
       "      <td>al</td>\n",
       "      <td>35121</td>\n",
       "      <td>blount</td>\n",
       "      <td>...</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>yes</td>\n",
       "      <td>surgical infection prevention</td>\n",
       "      <td>scip-inf-2</td>\n",
       "      <td>surgery patients who were given the  right kin...</td>\n",
       "      <td>88%</td>\n",
       "      <td>25 patients</td>\n",
       "      <td>al_scip-inf-2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index ProviderNumber                      HospitalName  \\\n",
       "0       1          10018  callahan eye foundation hospital   \n",
       "1       2          10018  callahan eye foundation hospital   \n",
       "2       3          10018  callahan eye foundation hospital   \n",
       "3       4          10018  callahan eye foundation hospital   \n",
       "4       5          10018  callahan eye foundation hospital   \n",
       "..    ...            ...                               ...   \n",
       "995   996          10050                st vincents blount   \n",
       "996   997          10050                st vincents blount   \n",
       "997   998          10050                st vincents blount   \n",
       "998   999          10050                st vincents blount   \n",
       "999  1000          10050                st vincents blount   \n",
       "\n",
       "                 Address1 Address2 Address3        City State ZipCode  \\\n",
       "0    1720 university blvd    empty    empty  birmingham    al   35233   \n",
       "1    1720 university blvd    empty    empty  birmingham    al   35233   \n",
       "2    1720 university blvd    empty    empty  birmingham    al   35233   \n",
       "3    1720 university blvd    empty    empty  birminghxm    al   35233   \n",
       "4    1720 university blvd    empty    empty  birmingham    al   35233   \n",
       "..                    ...      ...      ...         ...   ...     ...   \n",
       "995   150 gilbreath drive    empty    empty     oneonta    al   35121   \n",
       "996   150 gilbreath drive    empty    empty     oneonta    al   35121   \n",
       "997   150 gilbreath drive    empty    empty     oneonta    al   35121   \n",
       "998   150 gilbreath drive    empty    empty     oneonta    al   35121   \n",
       "999   150 gilbreath drive    empty    empty     oneonta    al   35121   \n",
       "\n",
       "    CountyName  ...          HospitalType                   HospitalOwner  \\\n",
       "0    jefferson  ...  acute care hospitals  voluntary non-profit - private   \n",
       "1    jefferson  ...  acute care hospitals  voluntary non-profit - private   \n",
       "2    jefferson  ...  acute care hospitals  voluntary non-profit - private   \n",
       "3    jefferson  ...  acute care hospitals  voluntary non-profit - private   \n",
       "4    jefferson  ...  acute care hospitals  voluntary non-profit - private   \n",
       "..         ...  ...                   ...                             ...   \n",
       "995     blount  ...  acute care hospitals  voluntary non-profit - private   \n",
       "996     blount  ...  acute care hospitals  voluntary non-profit - private   \n",
       "997     blount  ...  acute care hospitals  voluntary non-profit - private   \n",
       "998     blount  ...  acute care hospitals  voluntary non-profit - private   \n",
       "999     blount  ...  acute care hospitals  voluntary non-profit - private   \n",
       "\n",
       "    EmergencyService                      Condition  MeasureCode  \\\n",
       "0                yes  surgical infection prevention  scip-card-2   \n",
       "1                yes  surgical infection prevention   scip-inf-1   \n",
       "2                yes  surgical infection prevention   scip-inf-2   \n",
       "3                yes  surgical infection prevention   scip-inf-3   \n",
       "4                yes  surgical infection prevention   scip-inf-4   \n",
       "..               ...                            ...          ...   \n",
       "995              yes                      pneumonia         pn-6   \n",
       "996              yes                      pneumonia         pn-7   \n",
       "997              yes  surgical infection prevention  scip-card-2   \n",
       "998              yes  surgical infection prevention   scip-inf-1   \n",
       "999              yes  surgical infection prevention   scip-inf-2   \n",
       "\n",
       "                                           MeasureName  Score       Sample  \\\n",
       "0    surgery patients who were taking heart drugs c...  empty        empty   \n",
       "1    surgery patients who were given an antibiotic ...  empty        empty   \n",
       "2    surgery patients who were given the  right kin...  empty        empty   \n",
       "3    surgery patients whose preventive antibiotics ...  empty        empty   \n",
       "4    all heart surgery patients whose blood sugar (...  empty        empty   \n",
       "..                                                 ...    ...          ...   \n",
       "995  pneumonia patients given the most appropriate ...    77%  74 patients   \n",
       "996  pneumonia patients assessed and given influenz...    64%  55 patients   \n",
       "997  surgery pxtients who were txking hexrt drugs c...    25%   8 patients   \n",
       "998  surgery patients who were given an antibiotic ...    64%  28 patients   \n",
       "999  surgery patients who were given the  right kin...    88%  25 patients   \n",
       "\n",
       "           Stateavg count  \n",
       "0    al_scip-card-2     1  \n",
       "1     al_scip-inf-1     0  \n",
       "2     al_scip-inf-2     0  \n",
       "3     al_scip-inf-3     1  \n",
       "4     al_scip-inf-4     0  \n",
       "..              ...   ...  \n",
       "995         al_pn-6     0  \n",
       "996         al_pn-7     0  \n",
       "997  al_scip-card-2     1  \n",
       "998   al_scip-inf-1     0  \n",
       "999   al_scip-inf-2     0  \n",
       "\n",
       "[1000 rows x 21 columns]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hospital_dirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_dirty['index'] = hospital_dirty['index'].astype(int)\n",
    "hospital_clean['index'] = hospital_clean['index'].astype(int)\n",
    "hospital_dirty_dict = hospital_dirty.iloc[:,:-1].set_index('index').to_dict('index')\n",
    "hospital_clean_dict = hospital_clean.set_index('index').to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('datasets/hospital/detector/hospital_cluster.npy',hospital_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 构造cleaning数据集\n",
    "import json\n",
    "noise_col = np.where(detector.sum(axis=0)>0)[0]\n",
    "header = list(hospital_dirty.columns)\n",
    "safe_value = ['empty'] ## 不注入噪声的类型\n",
    "training_list = []\n",
    "for h in tqdm(hospital_cluster): ## 对比学习的采样策略在同cluster内进行\n",
    "    coreset_subset = [n for n in hospital_cluster[h] if n in coreset_detect]\n",
    "    for coreset_tuple in coreset_subset: ## 取遍coreset所有的tuple，剩下的n-1 tuple是参考对象,coreset_tuple是index\n",
    "        noise_col_subset = np.random.choice(noise_col,5,replace=False) ## 取5个col作为注入噪声的对象\n",
    "        for noise_col_single in noise_col_subset:\n",
    "            col_name = header[noise_col_single] ## 从index转成列名\n",
    "            if(hospital_dirty_dict[coreset_tuple][col_name] not in safe_value and len(coreset_subset)>2):\n",
    "                temp_dict = hospital_dirty.iloc[coreset_tuple,1:].to_dict() ## coreset tuple字典\n",
    "                clean_cell = temp_dict[col_name]\n",
    "                dirty_cell = replace_random_char_with_x(clean_cell) ## 注入噪声\n",
    "                temp_dict[col_name] = dirty_cell\n",
    "                coreset_reference = select_two_different_elements(coreset_subset,coreset_tuple)\n",
    "                template_dict = {}\n",
    "                clean_dict = {}\n",
    "                template_dict[col_name] = ''\n",
    "                clean_dict[col_name] = clean_cell\n",
    "                text_head = 'You are an expert in Cleaning Hospital Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\nTake these rows as reference:\\n\\n' % (col_name, json.dumps(template_dict), json.dumps(temp_dict))\n",
    "                \n",
    "                # ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]), json.dumps(hospital_dirty_dict[coreset_reference[1]]))\n",
    "                \n",
    "                ICL_text = '%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]))\n",
    "                \n",
    "                training_list.append([text_head,ICL_text,'',json.dumps(clean_dict)])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b618e1f284914fd8a2176ef1fa125d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 加入标注数据\n",
    "# header = list(hospital_dirty.columns)\n",
    "# safe_value = ['empty'] ## 不注入噪声的类型\n",
    "# training_list = []\n",
    "# for h in tqdm(hospital_cluster): ## 对比学习的采样策略在同cluster内进行\n",
    "#     coreset_subset = [n for n in hospital_cluster[h] if n in coreset_detect]\n",
    "#     for coreset_tuple in coreset_subset: ## 取遍coreset所有的tuple，剩下的n-1 tuple是参考对象,coreset_tuple是index\n",
    "#         noise_col_subset = np.random.choice(noise_col,5,replace=False) ## 取5个col作为注入噪声的对象\n",
    "#         for noise_col_single in noise_col_subset:\n",
    "#             col_name = header[noise_col_single] ## 从index转成列名\n",
    "#             if(hospital_dirty_dict[coreset_tuple][col_name] not in safe_value and len(coreset_subset)>2):\n",
    "#                 temp_dict = hospital_dirty.iloc[coreset_tuple,1:].to_dict() ## coreset tuple字典\n",
    "#                 clean_cell = temp_dict[col_name]\n",
    "#                 dirty_cell = replace_random_char_with_x(clean_cell) ## 注入噪声\n",
    "#                 temp_dict[col_name] = dirty_cell\n",
    "#                 coreset_reference = select_two_different_elements(coreset_subset,coreset_tuple)\n",
    "#                 template_dict = {}\n",
    "#                 clean_dict = {}\n",
    "#                 template_dict[col_name] = ''\n",
    "#                 clean_dict[col_name] = clean_cell\n",
    "#                 text_head = 'You are an expert in Cleaning Hospital Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\nTake these rows as reference:\\n\\n' % (col_name, json.dumps(template_dict), json.dumps(temp_dict))\n",
    "                \n",
    "#                 ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]), json.dumps(hospital_dirty_dict[coreset_reference[1]]))\n",
    "                \n",
    "#                 training_list.append([text_head,ICL_text,'',json.dumps(clean_dict)])\n",
    "training_list_label = []\n",
    "for label_tuple in tqdm(selected_rows):\n",
    "    for noise_col_single in range(len(header)):\n",
    "        col_name = header[noise_col_single]\n",
    "        template_dict = {}\n",
    "        template_dict[col_name] = ''\n",
    "        clean_dict = {}\n",
    "        temp_dict = hospital_dirty.iloc[label_tuple,1:].to_dict()\n",
    "        clean_cell = hospital_clean.iloc[label_tuple,noise_col_single]\n",
    "        dirty_cell = hospital_dirty.iloc[label_tuple,noise_col_single]\n",
    "        clean_dict[col_name] = clean_cell\n",
    "        if(clean_cell!=dirty_cell):\n",
    "            text_head = 'You are an expert in Cleaning Hospital Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\nTake these rows as reference:\\n\\n' % (col_name, json.dumps(template_dict), json.dumps(temp_dict))\n",
    "            cluster = [hospital_cluster[h] for h in hospital_cluster if hospital_cluster[h].__contains__(label_tuple)][0] ## 找到label tuple所在的簇\n",
    "            cluster_coreset = [c for c in cluster if c in coreset_detect] ## 找到簇内coreset元素\n",
    "            coreset_reference = np.random.choice(cluster_coreset,2,replace=False) ## 取两个\n",
    "            # ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]), json.dumps(hospital_dirty_dict[coreset_reference[1]]))\n",
    "            ICL_text = '%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]))\n",
    "            training_list_label.append([text_head,ICL_text,'',json.dumps(clean_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list_label_pd = pd.DataFrame(training_list_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given element: 2\n",
      "Resulting list: [1, 4]\n"
     ]
    }
   ],
   "source": [
    "def get_list_for_given_element(result_dict, given_element):\n",
    "    if given_element in result_dict:\n",
    "        return result_dict[given_element]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# 示例用法\n",
    "result_dict = {1: [0, 2, 6], 2: [1, 4], 3: [3], 4: [5]}\n",
    "given_element = 2\n",
    "result_list = get_list_for_given_element(result_dict, given_element)\n",
    "print(f\"Given element: {given_element}\")\n",
    "print(f\"Resulting list: {result_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list_pd = pd.DataFrame(training_list).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...</td>\n",
       "      <td></td>\n",
       "      <td>{\"MeasureName\": \"surgery patients who were giv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ZipCode\": \"35233\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...</td>\n",
       "      <td></td>\n",
       "      <td>{\"Stateavg\": \"al_scip-inf-1\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...</td>\n",
       "      <td></td>\n",
       "      <td>{\"MeasureCode\": \"scip-inf-1\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...</td>\n",
       "      <td></td>\n",
       "      <td>{\"City\": \"birmingham\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10005\", \"HospitalName\": \"m...</td>\n",
       "      <td></td>\n",
       "      <td>{\"PhoneNumber\": \"2565938310\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10006\", \"HospitalName\": \"e...</td>\n",
       "      <td></td>\n",
       "      <td>{\"City\": \"florence\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10006\", \"HospitalName\": \"e...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ZipCode\": \"35631\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10007\", \"HospitalName\": \"m...</td>\n",
       "      <td></td>\n",
       "      <td>{\"Address1\": \"702 n main st\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10007\", \"HospitalName\": \"m...</td>\n",
       "      <td></td>\n",
       "      <td>{\"MeasureName\": \"heart attack patients given a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3003 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0  \\\n",
       "0   You are an expert in Cleaning Hospital Dataset...   \n",
       "1   You are an expert in Cleaning Hospital Dataset...   \n",
       "2   You are an expert in Cleaning Hospital Dataset...   \n",
       "3   You are an expert in Cleaning Hospital Dataset...   \n",
       "4   You are an expert in Cleaning Hospital Dataset...   \n",
       "..                                                ...   \n",
       "49  You are an expert in Cleaning Hospital Dataset...   \n",
       "50  You are an expert in Cleaning Hospital Dataset...   \n",
       "51  You are an expert in Cleaning Hospital Dataset...   \n",
       "52  You are an expert in Cleaning Hospital Dataset...   \n",
       "53  You are an expert in Cleaning Hospital Dataset...   \n",
       "\n",
       "                                                    1 2   \\\n",
       "0   {\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...      \n",
       "1   {\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...      \n",
       "2   {\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...      \n",
       "3   {\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...      \n",
       "4   {\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...      \n",
       "..                                                ... ..   \n",
       "49  {\"ProviderNumber\": \"10005\", \"HospitalName\": \"m...      \n",
       "50  {\"ProviderNumber\": \"10006\", \"HospitalName\": \"e...      \n",
       "51  {\"ProviderNumber\": \"10006\", \"HospitalName\": \"e...      \n",
       "52  {\"ProviderNumber\": \"10007\", \"HospitalName\": \"m...      \n",
       "53  {\"ProviderNumber\": \"10007\", \"HospitalName\": \"m...      \n",
       "\n",
       "                                                    3  \n",
       "0   {\"MeasureName\": \"surgery patients who were giv...  \n",
       "1                                {\"ZipCode\": \"35233\"}  \n",
       "2                       {\"Stateavg\": \"al_scip-inf-1\"}  \n",
       "3                       {\"MeasureCode\": \"scip-inf-1\"}  \n",
       "4                              {\"City\": \"birmingham\"}  \n",
       "..                                                ...  \n",
       "49                      {\"PhoneNumber\": \"2565938310\"}  \n",
       "50                               {\"City\": \"florence\"}  \n",
       "51                               {\"ZipCode\": \"35631\"}  \n",
       "52                      {\"Address1\": \"702 n main st\"}  \n",
       "53  {\"MeasureName\": \"heart attack patients given a...  \n",
       "\n",
       "[3003 rows x 4 columns]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_list_pd = pd.concat([training_list_pd,training_list_label_pd]).drop_duplicates()\n",
    "training_list_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list_pd['instruction'] = training_list_pd[0] + training_list_pd[1]\n",
    "training_list_pd['input'] = training_list_pd[2]\n",
    "training_list_pd['output'] = training_list_pd[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(training_list_pd.iloc[:,-3:].to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/hospital/hospital-train.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COL index VAL 1 COL ProviderNumber VAL 10018 C...</td>\n",
       "      <td>COL MeasureName VAL surgery patients who were ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COL index VAL 1 COL ProviderNumber VAL 10018 C...</td>\n",
       "      <td>COL MeasureName VAL surgery patients who were ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COL index VAL 1 COL ProviderNumber VAL 10018 C...</td>\n",
       "      <td>COL MeasureName VAL surgery patients whose pre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COL index VAL 1 COL ProviderNumber VAL 10018 C...</td>\n",
       "      <td>COL MeasureName VAL all heart surgery patients...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COL index VAL 1 COL ProviderNumber VAL 10018 C...</td>\n",
       "      <td>COL MeasureName VAL surgery patients needing h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4413</th>\n",
       "      <td>COL index VAL 998 COL ProviderNumber VAL 10050...</td>\n",
       "      <td>COL MeasureName VAL heart attack patxents gxve...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4414</th>\n",
       "      <td>COL index VAL 998 COL ProviderNumber VAL 10050...</td>\n",
       "      <td>COL MeasureName VAL pneumonix pxtients given i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4415</th>\n",
       "      <td>COL index VAL 998 COL ProviderNumber VAL 10050...</td>\n",
       "      <td>COL MeasureName VAL pneumonia patients assesse...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4416</th>\n",
       "      <td>COL index VAL 998 COL ProviderNumber VAL 10050...</td>\n",
       "      <td>COL MeasureName VAL childrenxwhoxreceivedxreli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4417</th>\n",
       "      <td>COL index VAL 998 COL ProviderNumber VAL 10050...</td>\n",
       "      <td>COL MeasureName VAL hearx failure paxienxs giv...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4418 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0  \\\n",
       "0     COL index VAL 1 COL ProviderNumber VAL 10018 C...   \n",
       "1     COL index VAL 1 COL ProviderNumber VAL 10018 C...   \n",
       "2     COL index VAL 1 COL ProviderNumber VAL 10018 C...   \n",
       "3     COL index VAL 1 COL ProviderNumber VAL 10018 C...   \n",
       "4     COL index VAL 1 COL ProviderNumber VAL 10018 C...   \n",
       "...                                                 ...   \n",
       "4413  COL index VAL 998 COL ProviderNumber VAL 10050...   \n",
       "4414  COL index VAL 998 COL ProviderNumber VAL 10050...   \n",
       "4415  COL index VAL 998 COL ProviderNumber VAL 10050...   \n",
       "4416  COL index VAL 998 COL ProviderNumber VAL 10050...   \n",
       "4417  COL index VAL 998 COL ProviderNumber VAL 10050...   \n",
       "\n",
       "                                                      1  2  \n",
       "0     COL MeasureName VAL surgery patients who were ...  1  \n",
       "1     COL MeasureName VAL surgery patients who were ...  1  \n",
       "2     COL MeasureName VAL surgery patients whose pre...  1  \n",
       "3     COL MeasureName VAL all heart surgery patients...  1  \n",
       "4     COL MeasureName VAL surgery patients needing h...  1  \n",
       "...                                                 ... ..  \n",
       "4413  COL MeasureName VAL heart attack patxents gxve...  1  \n",
       "4414  COL MeasureName VAL pneumonix pxtients given i...  1  \n",
       "4415  COL MeasureName VAL pneumonia patients assesse...  1  \n",
       "4416  COL MeasureName VAL childrenxwhoxreceivedxreli...  1  \n",
       "4417  COL MeasureName VAL hearx failure paxienxs giv...  1  \n",
       "\n",
       "[4418 rows x 3 columns]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('/home/yanmy/raha/raha-master/datasets/hospital/detector/cleaning.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "md5: 2f30ebc845e2848f39a4fbe90608fe8c\n",
      "sha1: 22165873741237686c1a6303d0c7927314a68006\n",
      "sha256: d4e9ef7cf3e0acbe6cce2c60ec9f603d62d67e0e1535e3c8e18407ba443066ec\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "\n",
    "def encrypt(fpath: str, algorithm: str) -> str:\n",
    "    with open(fpath, 'rb') as f:\n",
    "        return hashlib.new(algorithm, f.read()).hexdigest()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for algorithm in ('md5', 'sha1', 'sha256'):\n",
    "        hexdigest = encrypt('/home/yanmy/LLaMA-Efficient-Tuning-main/data/rayyan/rayyan-train-20.json', algorithm)\n",
    "        print(f'{algorithm}: {hexdigest}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "95ee149094747afbe2ee7c72447cf0526a5c8406"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd99d910efa94e10b77d8e9e047fb059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Lengths: 1329\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "prompt_opt = pd.read_json('//home/yanmy/LLaMA-Efficient-Tuning-main/data/rayyan/rayyan-train-20.json')\n",
    "# 加载 RoBERTa tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('/home/yanmy/roberta-base/')\n",
    "\n",
    "# 给定的列表\n",
    "# text_list = [\n",
    "#     \"This is the first sentence.\",\n",
    "#     \"Here is another sentence.\",\n",
    "#     \"Yet another example sentence.\"\n",
    "# ]\n",
    "\n",
    "# 统计每个元素的 token 长度\n",
    "token_lengths = [len(tokenizer.encode(text, add_special_tokens=True)) for text in tqdm(prompt_opt['instruction'].to_list())]\n",
    "\n",
    "print(\"Token Lengths:\", max(token_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 18, 182, 464, 724, 730, 541, 250, 114,  27,   5]),\n",
       " array([584., 597., 610., 623., 636., 649., 662., 675., 688., 701., 714.]))"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(token_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1036,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tuple_id</th>\n",
       "      <th>src</th>\n",
       "      <th>flight</th>\n",
       "      <th>sched_dep_time</th>\n",
       "      <th>act_dep_time</th>\n",
       "      <th>sched_arr_time</th>\n",
       "      <th>act_arr_time</th>\n",
       "      <th>count</th>\n",
       "      <th>text</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:16 a.m.</td>\n",
       "      <td>9:40 a.m.</td>\n",
       "      <td>9:32 a.m.</td>\n",
       "      <td>0</td>\n",
       "      <td>COL src VAL aa COL flight VAL AA-3859-IAH-ORD ...</td>\n",
       "      <td>You are an expert in data cleaning. Based on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-1733-ORD-PHX</td>\n",
       "      <td>7:45 p.m.</td>\n",
       "      <td>7:58 p.m.</td>\n",
       "      <td>10:30 p.m.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>COL src VAL aa COL flight VAL AA-1733-ORD-PHX ...</td>\n",
       "      <td>You are an expert in data cleaning. Based on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-1640-MIA-MCO</td>\n",
       "      <td>6:30 p.m.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7:25 p.m.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>COL src VAL aa COL flight VAL AA-1640-MIA-MCO ...</td>\n",
       "      <td>You are an expert in data cleaning. Based on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-518-MIA-JFK</td>\n",
       "      <td>6:40 a.m.</td>\n",
       "      <td>6:54 a.m.</td>\n",
       "      <td>9:25 a.m.</td>\n",
       "      <td>9:28 a.m.</td>\n",
       "      <td>0</td>\n",
       "      <td>COL src VAL aa COL flight VAL AA-518-MIA-JFK C...</td>\n",
       "      <td>You are an expert in data cleaning. Based on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-3756-ORD-SLC</td>\n",
       "      <td>12:15 p.m.</td>\n",
       "      <td>12:41 p.m.</td>\n",
       "      <td>2:45 p.m.</td>\n",
       "      <td>2:50 p.m.</td>\n",
       "      <td>0</td>\n",
       "      <td>COL src VAL aa COL flight VAL AA-3756-ORD-SLC ...</td>\n",
       "      <td>You are an expert in data cleaning. Based on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2371</th>\n",
       "      <td>2372</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>UA-3099-PHX-PHL</td>\n",
       "      <td>11:55 a.m.</td>\n",
       "      <td>11:43 a.m.</td>\n",
       "      <td>6:17 p.m.</td>\n",
       "      <td>5:38 p.m.</td>\n",
       "      <td>1</td>\n",
       "      <td>COL src VAL world-flight-tracker COL flight VA...</td>\n",
       "      <td>You are an expert in data cleaning. Based on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2372</th>\n",
       "      <td>2373</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-4198-ORD-CLE</td>\n",
       "      <td>10:40 a.m.</td>\n",
       "      <td>10:54 a.m.</td>\n",
       "      <td>12:55 p.m.</td>\n",
       "      <td>12:50 p.m.</td>\n",
       "      <td>0</td>\n",
       "      <td>COL src VAL world-flight-tracker COL flight VA...</td>\n",
       "      <td>You are an expert in data cleaning. Based on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2373</th>\n",
       "      <td>2374</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>CO-45-EWR-MIA</td>\n",
       "      <td>4:00 p.m.</td>\n",
       "      <td>3:58 p.m.</td>\n",
       "      <td>7:05 p.m.</td>\n",
       "      <td>6:36 p.m.</td>\n",
       "      <td>0</td>\n",
       "      <td>COL src VAL world-flight-tracker COL flight VA...</td>\n",
       "      <td>You are an expert in data cleaning. Based on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2374</th>\n",
       "      <td>2375</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-3809-PHX-LAX</td>\n",
       "      <td>6:00 a.m.</td>\n",
       "      <td>6:10 a.m.</td>\n",
       "      <td>6:40 a.m.</td>\n",
       "      <td>6:19 a.m.</td>\n",
       "      <td>0</td>\n",
       "      <td>COL src VAL world-flight-tracker COL flight VA...</td>\n",
       "      <td>You are an expert in data cleaning. Based on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2375</th>\n",
       "      <td>2376</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-59-JFK-SFO</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:39 a.m.</td>\n",
       "      <td>10:45 a.m.</td>\n",
       "      <td>11:12 a.m.</td>\n",
       "      <td>0</td>\n",
       "      <td>COL src VAL world-flight-tracker COL flight VA...</td>\n",
       "      <td>You are an expert in data cleaning. Based on t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2376 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tuple_id                   src           flight sched_dep_time  \\\n",
       "0            1                    aa  AA-3859-IAH-ORD      7:10 a.m.   \n",
       "1            2                    aa  AA-1733-ORD-PHX      7:45 p.m.   \n",
       "2            3                    aa  AA-1640-MIA-MCO      6:30 p.m.   \n",
       "3            4                    aa   AA-518-MIA-JFK      6:40 a.m.   \n",
       "4            5                    aa  AA-3756-ORD-SLC     12:15 p.m.   \n",
       "...        ...                   ...              ...            ...   \n",
       "2371      2372  world-flight-tracker  UA-3099-PHX-PHL     11:55 a.m.   \n",
       "2372      2373  world-flight-tracker  AA-4198-ORD-CLE     10:40 a.m.   \n",
       "2373      2374  world-flight-tracker    CO-45-EWR-MIA      4:00 p.m.   \n",
       "2374      2375  world-flight-tracker  AA-3809-PHX-LAX      6:00 a.m.   \n",
       "2375      2376  world-flight-tracker    AA-59-JFK-SFO      7:10 a.m.   \n",
       "\n",
       "     act_dep_time sched_arr_time act_arr_time  count  \\\n",
       "0       7:16 a.m.      9:40 a.m.    9:32 a.m.      0   \n",
       "1       7:58 p.m.     10:30 p.m.          NaN      1   \n",
       "2             NaN      7:25 p.m.          NaN      2   \n",
       "3       6:54 a.m.      9:25 a.m.    9:28 a.m.      0   \n",
       "4      12:41 p.m.      2:45 p.m.    2:50 p.m.      0   \n",
       "...           ...            ...          ...    ...   \n",
       "2371   11:43 a.m.      6:17 p.m.    5:38 p.m.      1   \n",
       "2372   10:54 a.m.     12:55 p.m.   12:50 p.m.      0   \n",
       "2373    3:58 p.m.      7:05 p.m.    6:36 p.m.      0   \n",
       "2374    6:10 a.m.      6:40 a.m.    6:19 a.m.      0   \n",
       "2375    7:39 a.m.     10:45 a.m.   11:12 a.m.      0   \n",
       "\n",
       "                                                   text  \\\n",
       "0     COL src VAL aa COL flight VAL AA-3859-IAH-ORD ...   \n",
       "1     COL src VAL aa COL flight VAL AA-1733-ORD-PHX ...   \n",
       "2     COL src VAL aa COL flight VAL AA-1640-MIA-MCO ...   \n",
       "3     COL src VAL aa COL flight VAL AA-518-MIA-JFK C...   \n",
       "4     COL src VAL aa COL flight VAL AA-3756-ORD-SLC ...   \n",
       "...                                                 ...   \n",
       "2371  COL src VAL world-flight-tracker COL flight VA...   \n",
       "2372  COL src VAL world-flight-tracker COL flight VA...   \n",
       "2373  COL src VAL world-flight-tracker COL flight VA...   \n",
       "2374  COL src VAL world-flight-tracker COL flight VA...   \n",
       "2375  COL src VAL world-flight-tracker COL flight VA...   \n",
       "\n",
       "                                                  query  \n",
       "0     You are an expert in data cleaning. Based on t...  \n",
       "1     You are an expert in data cleaning. Based on t...  \n",
       "2     You are an expert in data cleaning. Based on t...  \n",
       "3     You are an expert in data cleaning. Based on t...  \n",
       "4     You are an expert in data cleaning. Based on t...  \n",
       "...                                                 ...  \n",
       "2371  You are an expert in data cleaning. Based on t...  \n",
       "2372  You are an expert in data cleaning. Based on t...  \n",
       "2373  You are an expert in data cleaning. Based on t...  \n",
       "2374  You are an expert in data cleaning. Based on t...  \n",
       "2375  You are an expert in data cleaning. Based on t...  \n",
       "\n",
       "[2376 rows x 10 columns]"
      ]
     },
     "execution_count": 1036,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Flight Detector Test\n",
    "flight_clean = pd.read_csv('datasets/flights/clean.csv').fillna('').astype(str)\n",
    "flight_dirty = pd.read_csv('datasets/flights/dirty.csv').fillna('').astype(str)\n",
    "flight_query = pd.read_csv('datasets/flights/dirty_query.csv',index_col=0)\n",
    "flight_dirty.columns = flight_clean.columns\n",
    "flight_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_Error = np.array(flight_dirty!=flight_clean).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_Data_Error = All_Error.sum()\n",
    "Correct_Fixed_Error = 0\n",
    "All_Fixed_Error = 0\n",
    "for x in range(All_Error.shape[0]):\n",
    "    for y in range(All_Error.shape[1]):\n",
    "        dirty_cell = flight_dirty.iloc[x,y]\n",
    "        fixed_cell = flight_dirty_clean.iloc[x,y]\n",
    "        clean_cell = flight_clean.iloc[x,y]\n",
    "        if(fixed_cell!=dirty_cell):\n",
    "            All_Fixed_Error += 1\n",
    "            if(fixed_cell==clean_cell):\n",
    "                Correct_Fixed_Error += 1\n",
    "Precision = Correct_Fixed_Error / All_Fixed_Error\n",
    "Recall = Correct_Fixed_Error / All_Data_Error\n",
    "F1 = (2*Precision*Recall) / (Precision + Recall)\n",
    "print(Precision,Recall,F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4729, 4897, 4920)"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Correct_Fixed_Error,All_Fixed_Error,All_Data_Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9656932816009802 0.9611788617886179 0.9634307833350311\n"
     ]
    }
   ],
   "source": [
    "Precision = Correct_Fixed_Error / All_Fixed_Error\n",
    "Recall = Correct_Fixed_Error / All_Data_Error\n",
    "F1 = (2*Precision*Recall) / (Precision + Recall)\n",
    "print(Precision,Recall,F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.9656932816009802 0.9611788617886179 0.9634307833350311 22 labels(P/R/F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flight_clean['flight'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1040,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flight_dirty['sched_dep_time'].unique()\n",
    "import re\n",
    "\n",
    "def is_clean_time_format(time_str):\n",
    "    # Define a regular expression pattern for a clean time format (HH:MM a.m./p.m.)\n",
    "    time_pattern = r'^\\d{1,2}:\\d{2} (a\\.m\\.|p\\.m\\.)$'\n",
    "    \n",
    "    # Use regex to check if the time string matches the expected pattern\n",
    "    if re.match(time_pattern, time_str):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_clean_time_format('11:55 a.m.')\n",
    "def Check_Clean_Time_Format(row):\n",
    "    count = 0\n",
    "    for x,y in row[3:].items():\n",
    "        if not (is_clean_time_format(y)):\n",
    "            count += 1\n",
    "    return count\n",
    "flight_dirty['count'] = flight_dirty.apply(Check_Clean_Time_Format,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1367,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flight_dirty[flight_dirty['count']==0]\n",
    "flight_unique = list(flight_dirty['flight'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 6\n",
    "flight_cluster_dict = {}\n",
    "for i in range(100):\n",
    "    temp_dict = {}\n",
    "    i_0 = flight_clean[flight_clean['flight']==flight_unique[i]].iloc[0,3]\n",
    "    i_1 = flight_clean[flight_clean['flight']==flight_unique[i]].iloc[0,4]\n",
    "    i_2 = flight_clean[flight_clean['flight']==flight_unique[i]].iloc[0,5]\n",
    "    i_3 = flight_clean[flight_clean['flight']==flight_unique[i]].iloc[0,6]\n",
    "    temp_dict['all'] = len(flight_dirty[(flight_dirty['flight']==flight_unique[i]) & (flight_dirty['count']==0)])\n",
    "    # print(len(flight_dirty[(flight_dirty['flight']==flight_unique[i]) & (flight_dirty['count']==0)]))\n",
    "    # flight_dirty[(flight_dirty['flight']==flight_unique[i]) & (flight_dirty['sched_dep_time']==i_0)& (flight_dirty['act_dep_time']==i_1)& (flight_dirty['sched_arr_time']==i_2)& (flight_dirty['act_arr_time']==i_3)]\n",
    "    temp_dict['clean'] = len(flight_dirty[(flight_dirty['flight']==flight_unique[i]) & (flight_dirty['sched_dep_time']==i_0)& (flight_dirty['act_dep_time']==i_1)& (flight_dirty['sched_arr_time']==i_2)& (flight_dirty['act_arr_time']==i_3)])\n",
    "    flight_cluster_dict[i] = temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tuple_id</th>\n",
       "      <th>src</th>\n",
       "      <th>flight</th>\n",
       "      <th>sched_dep_time</th>\n",
       "      <th>act_dep_time</th>\n",
       "      <th>sched_arr_time</th>\n",
       "      <th>act_arr_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td>10:45 a.m.</td>\n",
       "      <td>10:55 a.m.</td>\n",
       "      <td>2:20 p.m.</td>\n",
       "      <td>1:40 p.m.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tuple_id src           flight sched_dep_time act_dep_time sched_arr_time  \\\n",
       "10       11  aa  AA-1886-BOS-MIA     10:45 a.m.   10:55 a.m.      2:20 p.m.   \n",
       "\n",
       "   act_arr_time  \n",
       "10    1:40 p.m.  "
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_clean[flight_clean['flight']==flight_unique[i]].iloc[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tuple_id</th>\n",
       "      <th>src</th>\n",
       "      <th>flight</th>\n",
       "      <th>sched_dep_time</th>\n",
       "      <th>act_dep_time</th>\n",
       "      <th>sched_arr_time</th>\n",
       "      <th>act_arr_time</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td>10:45 a.m.</td>\n",
       "      <td>10:55 a.m.</td>\n",
       "      <td>2:20 p.m.</td>\n",
       "      <td>1:40 p.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>71</td>\n",
       "      <td>helloflight</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td></td>\n",
       "      <td>10:54 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>1:36 p.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>171</td>\n",
       "      <td>boston</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td></td>\n",
       "      <td>10:55 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>1:34 p.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>275</td>\n",
       "      <td>weather</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td>10:45 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>2:20 p.m.</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>358</td>\n",
       "      <td>airtravelcenter</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td></td>\n",
       "      <td>10:54 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>1:36 p.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>458</td>\n",
       "      <td>flightview</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td></td>\n",
       "      <td>10:55 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>1:40 p.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>650</td>\n",
       "      <td>panynj</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td></td>\n",
       "      <td>10:55 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>1:40 p.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>769</td>\n",
       "      <td>flightexplorer</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td></td>\n",
       "      <td>10:55 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>1:36 p.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>832</td>\n",
       "      <td>flights</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td></td>\n",
       "      <td>10:55 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>1:40 p.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>1012</td>\n",
       "      <td>travelocity</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td>10:45 a.m.</td>\n",
       "      <td>10:42 a.m.</td>\n",
       "      <td>2:20 p.m.</td>\n",
       "      <td>1:40 p.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>1091</td>\n",
       "      <td>foxbusiness</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td></td>\n",
       "      <td>10:55 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>1:40 p.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>1183</td>\n",
       "      <td>usatoday</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td>10:45 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>2:20 p.m.</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>1244</td>\n",
       "      <td>myrateplan</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td></td>\n",
       "      <td>10:54 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>1:36 p.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>1344</td>\n",
       "      <td>orbitz</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td>10:45aDec 1</td>\n",
       "      <td>10:42aDec 1</td>\n",
       "      <td>2:20 p.m.</td>\n",
       "      <td>1:40 p.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507</th>\n",
       "      <td>1508</td>\n",
       "      <td>flytecomm</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td></td>\n",
       "      <td>10:54 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>1:36 p.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1778</th>\n",
       "      <td>1779</td>\n",
       "      <td>flylouisville</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td></td>\n",
       "      <td>10:55 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>1:40 p.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>1971</td>\n",
       "      <td>allegiantair</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td></td>\n",
       "      <td>10:55 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>1:40 p.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2070</th>\n",
       "      <td>2071</td>\n",
       "      <td>businesstravellogue</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td></td>\n",
       "      <td>10:55 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>1:40 p.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2175</th>\n",
       "      <td>2176</td>\n",
       "      <td>gofox</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td></td>\n",
       "      <td>10:55 a.m.</td>\n",
       "      <td></td>\n",
       "      <td>1:40 p.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tuple_id                  src           flight sched_dep_time  \\\n",
       "10         11                   aa  AA-1886-BOS-MIA     10:45 a.m.   \n",
       "70         71          helloflight  AA-1886-BOS-MIA                  \n",
       "170       171               boston  AA-1886-BOS-MIA                  \n",
       "274       275              weather  AA-1886-BOS-MIA     10:45 a.m.   \n",
       "357       358      airtravelcenter  AA-1886-BOS-MIA                  \n",
       "457       458           flightview  AA-1886-BOS-MIA                  \n",
       "649       650               panynj  AA-1886-BOS-MIA                  \n",
       "768       769       flightexplorer  AA-1886-BOS-MIA                  \n",
       "831       832              flights  AA-1886-BOS-MIA                  \n",
       "1011     1012          travelocity  AA-1886-BOS-MIA     10:45 a.m.   \n",
       "1090     1091          foxbusiness  AA-1886-BOS-MIA                  \n",
       "1182     1183             usatoday  AA-1886-BOS-MIA     10:45 a.m.   \n",
       "1243     1244           myrateplan  AA-1886-BOS-MIA                  \n",
       "1343     1344               orbitz  AA-1886-BOS-MIA    10:45aDec 1   \n",
       "1507     1508            flytecomm  AA-1886-BOS-MIA                  \n",
       "1778     1779        flylouisville  AA-1886-BOS-MIA                  \n",
       "1970     1971         allegiantair  AA-1886-BOS-MIA                  \n",
       "2070     2071  businesstravellogue  AA-1886-BOS-MIA                  \n",
       "2175     2176                gofox  AA-1886-BOS-MIA                  \n",
       "\n",
       "     act_dep_time sched_arr_time act_arr_time  count  \n",
       "10     10:55 a.m.      2:20 p.m.    1:40 p.m.      0  \n",
       "70     10:54 a.m.                   1:36 p.m.      2  \n",
       "170    10:55 a.m.                   1:34 p.m.      2  \n",
       "274                    2:20 p.m.                   2  \n",
       "357    10:54 a.m.                   1:36 p.m.      2  \n",
       "457    10:55 a.m.                   1:40 p.m.      2  \n",
       "649    10:55 a.m.                   1:40 p.m.      2  \n",
       "768    10:55 a.m.                   1:36 p.m.      2  \n",
       "831    10:55 a.m.                   1:40 p.m.      2  \n",
       "1011   10:42 a.m.      2:20 p.m.    1:40 p.m.      0  \n",
       "1090   10:55 a.m.                   1:40 p.m.      2  \n",
       "1182                   2:20 p.m.                   2  \n",
       "1243   10:54 a.m.                   1:36 p.m.      2  \n",
       "1343  10:42aDec 1      2:20 p.m.    1:40 p.m.      2  \n",
       "1507   10:54 a.m.                   1:36 p.m.      2  \n",
       "1778   10:55 a.m.                   1:40 p.m.      2  \n",
       "1970   10:55 a.m.                   1:40 p.m.      2  \n",
       "2070   10:55 a.m.                   1:40 p.m.      2  \n",
       "2175   10:55 a.m.                   1:40 p.m.      2  "
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 10\n",
    "i_0 = flight_clean[flight_clean['flight']==flight_unique[i]].iloc[0,3]\n",
    "i_1 = flight_clean[flight_clean['flight']==flight_unique[i]].iloc[0,4]\n",
    "i_2 = flight_clean[flight_clean['flight']==flight_unique[i]].iloc[0,5]\n",
    "i_3 = flight_clean[flight_clean['flight']==flight_unique[i]].iloc[0,6]\n",
    "temp_dict['all'] = len(flight_dirty[(flight_dirty['flight']==flight_unique[i]) & (flight_dirty['count']==0)])\n",
    "# print(len(flight_dirty[(flight_dirty['flight']==flight_unique[i]) & (flight_dirty['count']==0)]))\n",
    "# flight_dirty[(flight_dirty['flight']==flight_unique[i]) & (flight_dirty['sched_dep_time']==i_0)& (flight_dirty['act_dep_time']==i_1)& (flight_dirty['sched_arr_time']==i_2)& (flight_dirty['act_arr_time']==i_3)]\n",
    "temp_dict['clean'] = len(flight_dirty[(flight_dirty['flight']==flight_unique[i]) & (flight_dirty['sched_dep_time']==i_0)& (flight_dirty['act_dep_time']==i_1)& (flight_dirty['sched_arr_time']==i_2)& (flight_dirty['act_arr_time']==i_3)])\n",
    "flight_cluster_dict[i] = temp_dict\n",
    "flight_dirty[(flight_dirty['flight']==flight_unique[i]) & (flight_dirty['count']==0)]\n",
    "flight_dirty[(flight_dirty['flight']==flight_unique[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([1, 2, 5, 31, 32, 33, 36, 37, 85, 89, 95], dtype='int64')"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_cluster_pd[flight_cluster_pd['clean']==0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_select_flight = [1,2,5,14,31,32,33,36,37,53,58,64,72,74,76,82,85,89,90,93,95,98]\n",
    "len(cluster_select_flight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n",
      "2 6\n",
      "5 3\n",
      "14 13\n",
      "31 1\n",
      "32 2\n",
      "33 4\n",
      "36 2\n",
      "37 2\n",
      "53 14\n",
      "58 17\n",
      "64 5\n",
      "72 16\n",
      "74 4\n",
      "76 4\n",
      "82 3\n",
      "85 3\n",
      "89 2\n",
      "90 3\n",
      "93 4\n",
      "95 0\n",
      "98 16\n"
     ]
    }
   ],
   "source": [
    "# flight_cluster_dict\n",
    "for c in cluster_select_flight:\n",
    "    print(c,flight_cluster_dict[c]['all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([32, 1, 37, 36, 89, 5, 85, 90, 82, 93, 74, 76, 33, 64, 2, 14, 53,\n",
       "            72, 98, 58],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(flight_cluster_dict).T.iloc[cluster_select_flight].sort_values('all').index[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_select_flight_core = [1, 2, 5, 31, 32, 33, 36, 37, 85, 89, 95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_select_flight_5 = [14, 53, 72, 98, 58]\n",
    "cluster_select_flight_10 = [74, 76, 33, 64, 2, 14, 53, 72, 98, 58]\n",
    "cluster_select_flight_15 = [5, 85, 90, 82, 93, 74, 76, 33, 64, 2, 14, 53, 72, 98, 58]\n",
    "cluster_select_flight_20 = [32, 1, 37, 36, 89, 5, 85, 90, 82, 93, 74, 76, 33, 64, 2, 14, 53,\n",
    "            72, 98, 58]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tuple_id</th>\n",
       "      <th>src</th>\n",
       "      <th>flight</th>\n",
       "      <th>sched_dep_time</th>\n",
       "      <th>act_dep_time</th>\n",
       "      <th>sched_arr_time</th>\n",
       "      <th>act_arr_time</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1665</th>\n",
       "      <td>1666</td>\n",
       "      <td>wunderground</td>\n",
       "      <td>AA-1733-ORD-PHX</td>\n",
       "      <td>7:45 p.m.</td>\n",
       "      <td>7:59 p.m.</td>\n",
       "      <td>10:15 p.m.</td>\n",
       "      <td>10:28 p.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2259</th>\n",
       "      <td>2260</td>\n",
       "      <td>flightaware</td>\n",
       "      <td>AA-1733-ORD-PHX</td>\n",
       "      <td>7:45 p.m.</td>\n",
       "      <td>7:59 p.m.</td>\n",
       "      <td>10:15 p.m.</td>\n",
       "      <td>10:28 p.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tuple_id           src           flight sched_dep_time act_dep_time  \\\n",
       "1665     1666  wunderground  AA-1733-ORD-PHX      7:45 p.m.    7:59 p.m.   \n",
       "2259     2260   flightaware  AA-1733-ORD-PHX      7:45 p.m.    7:59 p.m.   \n",
       "\n",
       "     sched_arr_time act_arr_time  count  \n",
       "1665     10:15 p.m.   10:28 p.m.      0  \n",
       "2259     10:15 p.m.   10:28 p.m.      0  "
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_dirty[(flight_dirty['flight']==flight_unique[i]) & (flight_dirty['count']==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flight_dirty[flight_dirty['flight']==flight_unique[i]]#\n",
    "test = flight_dirty[flight_dirty['flight']==flight_unique[44]]\n",
    "# test[test['act_dep_time'].str.contains('7:59 p.m.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1365,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 创建示例DataFrame\n",
    "# data = {\n",
    "#     'A': [1, 2, 3, 4, 5],\n",
    "#     'B': [2, 2, 3, 9, 2],\n",
    "#     'C': [3, 3, 13, 14, 3],\n",
    "#     'D': [4, 4, 18, 19, 4],\n",
    "#     'E': [5, 5, 8, 9, 5]\n",
    "# }\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # 选择后四列\n",
    "def CoresetIndex(df):\n",
    "    df_tail = df.iloc[:, -4:]\n",
    "\n",
    "    # 计算后四列的哈希值\n",
    "    hash_values = df_tail.apply(tuple, axis=1).apply(hash)\n",
    "\n",
    "    # 找到最常出现的哈希值\n",
    "    most_common_hash = hash_values.mode().values[0]\n",
    "\n",
    "    # 找到对应的行索引\n",
    "    indices = hash_values[hash_values == most_common_hash].index\n",
    "    return indices\n",
    "\n",
    "# 输出索引\n",
    "# print(\"行数最多的行索引:\", indices[0])\n",
    "# C = CoresetIndex(test_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_dirty_clean = flight_dirty\n",
    "# flight_dirty_clean.iloc[:3,-5:-1] = flight_clean[flight_clean['flight']==flight_unique[i]].iloc[0:1,-4:]\n",
    "# flight_dirty_clean.iloc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cluster_select_flight' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/yanmy/raha/raha-master/detector.ipynb 单元格 74\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y562sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m cluster_select_flight\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cluster_select_flight' is not defined"
     ]
    }
   ],
   "source": [
    "# cluster_select_flight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tuple_id</th>\n",
       "      <th>src</th>\n",
       "      <th>flight</th>\n",
       "      <th>sched_dep_time</th>\n",
       "      <th>act_dep_time</th>\n",
       "      <th>sched_arr_time</th>\n",
       "      <th>act_arr_time</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [tuple_id, src, flight, sched_dep_time, act_dep_time, sched_arr_time, act_arr_time, count]\n",
       "Index: []"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14, 53, 72, 98, 58]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_select_flight_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1368,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/yanmy/raha/raha-master/detector.ipynb 单元格 74\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y133sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m             count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y133sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m count\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y133sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m flight_dirty[\u001b[39m'\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m flight_dirty\u001b[39m.\u001b[39;49mapply(Check_Clean_Time_Format,axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y133sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m flight_dirty_clean \u001b[39m=\u001b[39m flight_dirty\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y133sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# cluster_select_flight = cluster_select_flight_15\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:9568\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9557\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[1;32m   9559\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[1;32m   9560\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   9561\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9566\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[1;32m   9567\u001b[0m )\n\u001b[0;32m-> 9568\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py:764\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[1;32m    762\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[0;32m--> 764\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py:891\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 891\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[1;32m    893\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[1;32m    894\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py:907\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    905\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[1;32m    906\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 907\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[1;32m    908\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    909\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    910\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    911\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32m/home/yanmy/raha/raha-master/detector.ipynb 单元格 74\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y133sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y133sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m x,y \u001b[39min\u001b[39;00m row[\u001b[39m3\u001b[39m:]\u001b[39m.\u001b[39mitems():\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y133sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_clean_time_format(y)):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y133sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y133sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mreturn\u001b[39;00m count\n",
      "\u001b[1;32m/home/yanmy/raha/raha-master/detector.ipynb 单元格 74\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y133sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m time_pattern \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m^\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md\u001b[39m\u001b[39m{\u001b[39m\u001b[39m1,2}:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md\u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m (a\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.m\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.|p\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.m\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.)$\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y133sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Use regex to check if the time string matches the expected pattern\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y133sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mif\u001b[39;00m re\u001b[39m.\u001b[39;49mmatch(time_pattern, time_str):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y133sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y133sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/re.py:191\u001b[0m, in \u001b[0;36mmatch\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmatch\u001b[39m(pattern, string, flags\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m    189\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Try to apply the pattern at the start of the string, returning\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[39m    a Match object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m     \u001b[39mreturn\u001b[39;00m _compile(pattern, flags)\u001b[39m.\u001b[39;49mmatch(string)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# flight_clean.iloc[C]\n",
    "def Check_Clean_Time_Format(row):\n",
    "    count = 0\n",
    "    for x,y in row[3:].items():\n",
    "        if not (is_clean_time_format(y)):\n",
    "            count += 1\n",
    "    return count\n",
    "flight_dirty['count'] = flight_dirty.apply(Check_Clean_Time_Format,axis=1)\n",
    "flight_dirty_clean = flight_dirty.copy()\n",
    "# cluster_select_flight = cluster_select_flight_15\n",
    "cluster_select_flight = []\n",
    "for i in range(100): ## Data Cleaning Clusters\n",
    "    test = flight_dirty[flight_dirty['flight']==flight_unique[i]]\n",
    "    test_index = test.index\n",
    "    if(i in cluster_select_flight): ## 同cluster内有ground truth，传播结果\n",
    "        clean_cell = flight_clean[flight_clean['flight']==flight_unique[i]].iloc[0:1,-4:] ## clean time, last 4 cells\n",
    "        flight_dirty_clean.iloc[test_index,-5:-1] = clean_cell\n",
    "    else: ## Graph Method to Vote the most common clean files\n",
    "        try:\n",
    "            test_clean = test[test['count']==0]\n",
    "            C = CoresetIndex(test_clean)\n",
    "            clean_cell = flight_dirty.iloc[C[0],-5:-1] ## clean time, last 4 cells\n",
    "            flight_dirty_clean.iloc[test_index,-5:-1] = clean_cell\n",
    "        except:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7869430051813472 0.7717479674796748 0.7792714212416625\n"
     ]
    }
   ],
   "source": [
    "flight_clean = pd.read_csv('datasets/flights/clean.csv').fillna('').astype(str)\n",
    "flight_dirty = pd.read_csv('datasets/flights/dirty.csv').fillna('').astype(str)\n",
    "All_Data_Error = All_Error.sum()\n",
    "Correct_Fixed_Error = 0\n",
    "All_Fixed_Error = 0\n",
    "for x in range(All_Error.shape[0]):\n",
    "    for y in range(All_Error.shape[1]):\n",
    "        dirty_cell = flight_dirty.iloc[x,y]\n",
    "        fixed_cell = flight_dirty_clean.iloc[x,y]\n",
    "        clean_cell = flight_clean.iloc[x,y]\n",
    "        if(fixed_cell!=dirty_cell):\n",
    "            All_Fixed_Error += 1\n",
    "            if(fixed_cell==clean_cell):\n",
    "                Correct_Fixed_Error += 1\n",
    "Precision = Correct_Fixed_Error / All_Fixed_Error\n",
    "Recall = Correct_Fixed_Error / All_Data_Error\n",
    "F1 = (2*Precision*Recall) / (Precision + Recall)\n",
    "print(Precision,Recall,F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.9722682119205298 0.9548780487804878 0.9634946677604593 20\n",
    "0.9081527347781218 0.8943089430894309 0.9011776753712237 15\n",
    "0.8646585516814524 0.8518292682926829 0.8581959660079861 10\n",
    "0.8095532221535927 0.7991869918699187 0.8043367086018206 5\n",
    "0.7869430051813472 0.7717479674796748 0.7792714212416625 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tuple_id</th>\n",
       "      <th>src</th>\n",
       "      <th>flight</th>\n",
       "      <th>sched_dep_time</th>\n",
       "      <th>act_dep_time</th>\n",
       "      <th>sched_arr_time</th>\n",
       "      <th>act_arr_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:16 a.m.</td>\n",
       "      <td>9:40 a.m.</td>\n",
       "      <td>9:22 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-1733-ORD-PHX</td>\n",
       "      <td>7:45 p.m.</td>\n",
       "      <td>7:58 p.m.</td>\n",
       "      <td>10:30 p.m.</td>\n",
       "      <td>10:30 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-1640-MIA-MCO</td>\n",
       "      <td>6:30 p.m.</td>\n",
       "      <td>6:30 p.m.</td>\n",
       "      <td>7:25 p.m.</td>\n",
       "      <td>7:25 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-518-MIA-JFK</td>\n",
       "      <td>6:40 a.m.</td>\n",
       "      <td>6:54 a.m.</td>\n",
       "      <td>9:25 a.m.</td>\n",
       "      <td>9:28 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-3756-ORD-SLC</td>\n",
       "      <td>12:15 p.m.</td>\n",
       "      <td>12:41 p.m.</td>\n",
       "      <td>2:45 p.m.</td>\n",
       "      <td>2:50 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2371</th>\n",
       "      <td>2372</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>UA-3099-PHX-PHL</td>\n",
       "      <td>11:55 a.m.</td>\n",
       "      <td>11:55 a.m.</td>\n",
       "      <td>6:17 p.m.</td>\n",
       "      <td>5:38 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2372</th>\n",
       "      <td>2373</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-4198-ORD-CLE</td>\n",
       "      <td>10:40 a.m.</td>\n",
       "      <td>10:55 a.m.</td>\n",
       "      <td>12:55 p.m.</td>\n",
       "      <td>12:48 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2373</th>\n",
       "      <td>2374</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>CO-45-EWR-MIA</td>\n",
       "      <td>4:00 p.m.</td>\n",
       "      <td>3:58 p.m.</td>\n",
       "      <td>7:05 p.m.</td>\n",
       "      <td>6:36 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2374</th>\n",
       "      <td>2375</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-3809-PHX-LAX</td>\n",
       "      <td>6:00 a.m.</td>\n",
       "      <td>6:10 a.m.</td>\n",
       "      <td>6:40 a.m.</td>\n",
       "      <td>6:19 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2375</th>\n",
       "      <td>2376</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-59-JFK-SFO</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:39 a.m.</td>\n",
       "      <td>10:45 a.m.</td>\n",
       "      <td>11:12 a.m.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2376 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     tuple_id                   src           flight sched_dep_time  \\\n",
       "0           1                    aa  AA-3859-IAH-ORD      7:10 a.m.   \n",
       "1           2                    aa  AA-1733-ORD-PHX      7:45 p.m.   \n",
       "2           3                    aa  AA-1640-MIA-MCO      6:30 p.m.   \n",
       "3           4                    aa   AA-518-MIA-JFK      6:40 a.m.   \n",
       "4           5                    aa  AA-3756-ORD-SLC     12:15 p.m.   \n",
       "...       ...                   ...              ...            ...   \n",
       "2371     2372  world-flight-tracker  UA-3099-PHX-PHL     11:55 a.m.   \n",
       "2372     2373  world-flight-tracker  AA-4198-ORD-CLE     10:40 a.m.   \n",
       "2373     2374  world-flight-tracker    CO-45-EWR-MIA      4:00 p.m.   \n",
       "2374     2375  world-flight-tracker  AA-3809-PHX-LAX      6:00 a.m.   \n",
       "2375     2376  world-flight-tracker    AA-59-JFK-SFO      7:10 a.m.   \n",
       "\n",
       "     act_dep_time sched_arr_time act_arr_time  \n",
       "0       7:16 a.m.      9:40 a.m.    9:22 a.m.  \n",
       "1       7:58 p.m.     10:30 p.m.   10:30 p.m.  \n",
       "2       6:30 p.m.      7:25 p.m.    7:25 p.m.  \n",
       "3       6:54 a.m.      9:25 a.m.    9:28 a.m.  \n",
       "4      12:41 p.m.      2:45 p.m.    2:50 p.m.  \n",
       "...           ...            ...          ...  \n",
       "2371   11:55 a.m.      6:17 p.m.    5:38 p.m.  \n",
       "2372   10:55 a.m.     12:55 p.m.   12:48 p.m.  \n",
       "2373    3:58 p.m.      7:05 p.m.    6:36 p.m.  \n",
       "2374    6:10 a.m.      6:40 a.m.    6:19 a.m.  \n",
       "2375    7:39 a.m.     10:45 a.m.   11:12 a.m.  \n",
       "\n",
       "[2376 rows x 7 columns]"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_dirty_clean = flight_dirty_clean.iloc[:,:-1]\n",
    "flight_dirty_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tuple_id</th>\n",
       "      <th>src</th>\n",
       "      <th>flight</th>\n",
       "      <th>sched_dep_time</th>\n",
       "      <th>act_dep_time</th>\n",
       "      <th>sched_arr_time</th>\n",
       "      <th>act_arr_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:16 a.m.</td>\n",
       "      <td>9:40 a.m.</td>\n",
       "      <td>9:22 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-1733-ORD-PHX</td>\n",
       "      <td>7:45 p.m.</td>\n",
       "      <td>7:58 p.m.</td>\n",
       "      <td>10:30 p.m.</td>\n",
       "      <td>10:30 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-1640-MIA-MCO</td>\n",
       "      <td>6:30 p.m.</td>\n",
       "      <td>6:30 p.m.</td>\n",
       "      <td>7:25 p.m.</td>\n",
       "      <td>7:25 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-518-MIA-JFK</td>\n",
       "      <td>6:40 a.m.</td>\n",
       "      <td>6:54 a.m.</td>\n",
       "      <td>9:25 a.m.</td>\n",
       "      <td>9:28 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-3756-ORD-SLC</td>\n",
       "      <td>12:15 p.m.</td>\n",
       "      <td>12:41 p.m.</td>\n",
       "      <td>2:45 p.m.</td>\n",
       "      <td>2:50 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2371</th>\n",
       "      <td>2372</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>UA-3099-PHX-PHL</td>\n",
       "      <td>11:55 a.m.</td>\n",
       "      <td>11:55 a.m.</td>\n",
       "      <td>6:17 p.m.</td>\n",
       "      <td>5:38 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2372</th>\n",
       "      <td>2373</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-4198-ORD-CLE</td>\n",
       "      <td>10:40 a.m.</td>\n",
       "      <td>10:55 a.m.</td>\n",
       "      <td>12:55 p.m.</td>\n",
       "      <td>12:48 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2373</th>\n",
       "      <td>2374</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>CO-45-EWR-MIA</td>\n",
       "      <td>4:00 p.m.</td>\n",
       "      <td>3:58 p.m.</td>\n",
       "      <td>7:05 p.m.</td>\n",
       "      <td>6:36 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2374</th>\n",
       "      <td>2375</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-3809-PHX-LAX</td>\n",
       "      <td>6:00 a.m.</td>\n",
       "      <td>6:10 a.m.</td>\n",
       "      <td>6:40 a.m.</td>\n",
       "      <td>6:19 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2375</th>\n",
       "      <td>2376</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-59-JFK-SFO</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:39 a.m.</td>\n",
       "      <td>10:45 a.m.</td>\n",
       "      <td>11:12 a.m.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2376 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     tuple_id                   src           flight sched_dep_time  \\\n",
       "0           1                    aa  AA-3859-IAH-ORD      7:10 a.m.   \n",
       "1           2                    aa  AA-1733-ORD-PHX      7:45 p.m.   \n",
       "2           3                    aa  AA-1640-MIA-MCO      6:30 p.m.   \n",
       "3           4                    aa   AA-518-MIA-JFK      6:40 a.m.   \n",
       "4           5                    aa  AA-3756-ORD-SLC     12:15 p.m.   \n",
       "...       ...                   ...              ...            ...   \n",
       "2371     2372  world-flight-tracker  UA-3099-PHX-PHL     11:55 a.m.   \n",
       "2372     2373  world-flight-tracker  AA-4198-ORD-CLE     10:40 a.m.   \n",
       "2373     2374  world-flight-tracker    CO-45-EWR-MIA      4:00 p.m.   \n",
       "2374     2375  world-flight-tracker  AA-3809-PHX-LAX      6:00 a.m.   \n",
       "2375     2376  world-flight-tracker    AA-59-JFK-SFO      7:10 a.m.   \n",
       "\n",
       "     act_dep_time sched_arr_time act_arr_time  \n",
       "0       7:16 a.m.      9:40 a.m.    9:22 a.m.  \n",
       "1       7:58 p.m.     10:30 p.m.   10:30 p.m.  \n",
       "2       6:30 p.m.      7:25 p.m.    7:25 p.m.  \n",
       "3       6:54 a.m.      9:25 a.m.    9:28 a.m.  \n",
       "4      12:41 p.m.      2:45 p.m.    2:50 p.m.  \n",
       "...           ...            ...          ...  \n",
       "2371   11:55 a.m.      6:17 p.m.    5:38 p.m.  \n",
       "2372   10:55 a.m.     12:55 p.m.   12:48 p.m.  \n",
       "2373    3:58 p.m.      7:05 p.m.    6:36 p.m.  \n",
       "2374    6:10 a.m.      6:40 a.m.    6:19 a.m.  \n",
       "2375    7:39 a.m.     10:45 a.m.   11:12 a.m.  \n",
       "\n",
       "[2376 rows x 7 columns]"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_dirty_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(flight_dirty.iloc[:,:-1]!=flight_clean).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1, 2, 5, 31, 32, 33, 36, 37, 85, 89, 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1376,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_label_index = []\n",
    "for i in cluster_select_flight:\n",
    "    ind = flight_clean[flight_clean['flight']==flight_unique[i]].index\n",
    "    flight_label_index.append(ind[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 5,\n",
       " 14,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 36,\n",
       " 37,\n",
       " 56,\n",
       " 86,\n",
       " 88,\n",
       " 96,\n",
       " 107,\n",
       " 113,\n",
       " 122,\n",
       " 123,\n",
       " 127,\n",
       " 129,\n",
       " 143]"
      ]
     },
     "execution_count": 1377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "cluster_select_flight = [1,2,5,14,31,32,33,36,37,53,72,74,76,82,85,89,90,93,95,98]\n",
    "# cluster_select_flight_core = [1, 2, 5, 31, 32, 33, 36, 37, 85, 89, 95]\n",
    "\n",
    "# cluster_select_flight_add = np.random.choice(100,9,replace=False)\n",
    "# cluster_select_flight = np.concatenate([cluster_select_flight_core,cluster_select_flight_add])\n",
    "print(len(cluster_select_flight))\n",
    "# cluster_select_flight = [1,2,5,14,31,32,33,36,37,53,58,64,72,74,82,85,89,90,93,95,98]\n",
    "flight_clean = pd.read_csv('datasets/flights/clean.csv').fillna('').astype(str)\n",
    "flight_dirty = pd.read_csv('datasets/flights/dirty.csv').fillna('').astype(str)\n",
    "flight_query = pd.read_csv('datasets/flights/dirty_query.csv',index_col=0)\n",
    "flight_dirty.columns = flight_clean.columns\n",
    "def Check_Clean_Time_Format(row):\n",
    "    count = 0\n",
    "    for x,y in row[3:].items():\n",
    "        if not (is_clean_time_format(y)):\n",
    "            count += 1\n",
    "    return count\n",
    "flight_dirty['count'] = flight_dirty.apply(Check_Clean_Time_Format,axis=1)\n",
    "flight_dirty_clean = flight_dirty.copy()\n",
    "for i in range(100): ## Data Cleaning Clusters\n",
    "    test = flight_dirty[flight_dirty['flight']==flight_unique[i]]\n",
    "    test_index = test.index\n",
    "    if(i in cluster_select_flight): ## 同cluster内有ground truth，传播结果\n",
    "        clean_cell = flight_clean[flight_clean['flight']==flight_unique[i]].iloc[0:1,-4:] ## clean time, last 4 cells\n",
    "        flight_dirty_clean.iloc[test_index,-5:-1] = clean_cell\n",
    "    else: ## Graph Method to Vote the most common clean files\n",
    "        test_clean = test[test['count']==0]\n",
    "        C = CoresetIndex(test_clean)\n",
    "        clean_cell = flight_dirty.iloc[C[0],-5:-1] ## clean time, last 4 cells\n",
    "        flight_dirty_clean.iloc[test_index,-5:-1] = clean_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1372,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_Error = np.array(flight_clean!=flight_dirty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tuple_id</th>\n",
       "      <th>src</th>\n",
       "      <th>flight</th>\n",
       "      <th>sched_dep_time</th>\n",
       "      <th>act_dep_time</th>\n",
       "      <th>sched_arr_time</th>\n",
       "      <th>act_arr_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>81</td>\n",
       "      <td>helloflight</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>181</td>\n",
       "      <td>boston</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>276</td>\n",
       "      <td>weather</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>368</td>\n",
       "      <td>airtravelcenter</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>468</td>\n",
       "      <td>flightview</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>567</td>\n",
       "      <td>flightstats</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>660</td>\n",
       "      <td>panynj</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>749</td>\n",
       "      <td>flightexplorer</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>842</td>\n",
       "      <td>flights</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>923</td>\n",
       "      <td>flightarrival</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>1014</td>\n",
       "      <td>travelocity</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>1101</td>\n",
       "      <td>foxbusiness</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>1184</td>\n",
       "      <td>usatoday</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>1254</td>\n",
       "      <td>myrateplan</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353</th>\n",
       "      <td>1354</td>\n",
       "      <td>orbitz</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>1518</td>\n",
       "      <td>flytecomm</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>1603</td>\n",
       "      <td>mytripandmore</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1677</th>\n",
       "      <td>1678</td>\n",
       "      <td>wunderground</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1788</th>\n",
       "      <td>1789</td>\n",
       "      <td>flylouisville</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1887</th>\n",
       "      <td>1888</td>\n",
       "      <td>quicktrip</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>1981</td>\n",
       "      <td>allegiantair</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2080</th>\n",
       "      <td>2081</td>\n",
       "      <td>businesstravellogue</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2185</th>\n",
       "      <td>2186</td>\n",
       "      <td>gofox</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2271</th>\n",
       "      <td>2272</td>\n",
       "      <td>flightaware</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2337</th>\n",
       "      <td>2338</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:18 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:56 a.m.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tuple_id                   src           flight sched_dep_time  \\\n",
       "12         13                    aa  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "80         81           helloflight  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "180       181                boston  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "275       276               weather  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "367       368       airtravelcenter  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "467       468            flightview  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "566       567           flightstats  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "659       660                panynj  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "748       749        flightexplorer  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "841       842               flights  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "922       923         flightarrival  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1013     1014           travelocity  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1100     1101           foxbusiness  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1183     1184              usatoday  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1253     1254            myrateplan  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1353     1354                orbitz  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1517     1518             flytecomm  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1602     1603         mytripandmore  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1677     1678          wunderground  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1788     1789         flylouisville  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1887     1888             quicktrip  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1980     1981          allegiantair  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "2080     2081   businesstravellogue  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "2185     2186                 gofox  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "2271     2272           flightaware  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "2337     2338  world-flight-tracker  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "\n",
       "     act_dep_time sched_arr_time act_arr_time  \n",
       "12     10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "80     10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "180    10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "275    10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "367    10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "467    10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "566    10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "659    10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "748    10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "841    10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "922    10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "1013   10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "1100   10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "1183   10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "1253   10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "1353   10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "1517   10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "1602   10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "1677   10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "1788   10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "1887   10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "1980   10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "2080   10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "2185   10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "2271   10:18 a.m.     12:10 p.m.   11:56 a.m.  \n",
       "2337   10:18 a.m.     12:10 p.m.   11:56 a.m.  "
      ]
     },
     "execution_count": 1375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_clean[flight_dirty_clean[relation]!=flight_clean[relation]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tuple_id</th>\n",
       "      <th>src</th>\n",
       "      <th>flight</th>\n",
       "      <th>sched_dep_time</th>\n",
       "      <th>act_dep_time</th>\n",
       "      <th>sched_arr_time</th>\n",
       "      <th>act_arr_time</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>81</td>\n",
       "      <td>helloflight</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>181</td>\n",
       "      <td>boston</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>276</td>\n",
       "      <td>weather</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>368</td>\n",
       "      <td>airtravelcenter</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>468</td>\n",
       "      <td>flightview</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>567</td>\n",
       "      <td>flightstats</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>660</td>\n",
       "      <td>panynj</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>749</td>\n",
       "      <td>flightexplorer</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>842</td>\n",
       "      <td>flights</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>923</td>\n",
       "      <td>flightarrival</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>1014</td>\n",
       "      <td>travelocity</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>1101</td>\n",
       "      <td>foxbusiness</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>1184</td>\n",
       "      <td>usatoday</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>1254</td>\n",
       "      <td>myrateplan</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353</th>\n",
       "      <td>1354</td>\n",
       "      <td>orbitz</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>1518</td>\n",
       "      <td>flytecomm</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>1603</td>\n",
       "      <td>mytripandmore</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1677</th>\n",
       "      <td>1678</td>\n",
       "      <td>wunderground</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1788</th>\n",
       "      <td>1789</td>\n",
       "      <td>flylouisville</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1887</th>\n",
       "      <td>1888</td>\n",
       "      <td>quicktrip</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>1981</td>\n",
       "      <td>allegiantair</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2080</th>\n",
       "      <td>2081</td>\n",
       "      <td>businesstravellogue</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2185</th>\n",
       "      <td>2186</td>\n",
       "      <td>gofox</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2271</th>\n",
       "      <td>2272</td>\n",
       "      <td>flightaware</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2337</th>\n",
       "      <td>2338</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-1664-MIA-ATL</td>\n",
       "      <td>10:15 a.m.</td>\n",
       "      <td>10:19 a.m.</td>\n",
       "      <td>12:10 p.m.</td>\n",
       "      <td>11:50 a.m.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tuple_id                   src           flight sched_dep_time  \\\n",
       "12         13                    aa  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "80         81           helloflight  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "180       181                boston  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "275       276               weather  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "367       368       airtravelcenter  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "467       468            flightview  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "566       567           flightstats  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "659       660                panynj  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "748       749        flightexplorer  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "841       842               flights  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "922       923         flightarrival  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1013     1014           travelocity  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1100     1101           foxbusiness  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1183     1184              usatoday  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1253     1254            myrateplan  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1353     1354                orbitz  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1517     1518             flytecomm  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1602     1603         mytripandmore  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1677     1678          wunderground  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1788     1789         flylouisville  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1887     1888             quicktrip  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "1980     1981          allegiantair  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "2080     2081   businesstravellogue  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "2185     2186                 gofox  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "2271     2272           flightaware  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "2337     2338  world-flight-tracker  AA-1664-MIA-ATL     10:15 a.m.   \n",
       "\n",
       "     act_dep_time sched_arr_time act_arr_time  count  \n",
       "12     10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "80     10:19 a.m.     12:10 p.m.   11:50 a.m.      2  \n",
       "180    10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "275    10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "367    10:19 a.m.     12:10 p.m.   11:50 a.m.      2  \n",
       "467    10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "566    10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "659    10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "748    10:19 a.m.     12:10 p.m.   11:50 a.m.      2  \n",
       "841    10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "922    10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "1013   10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "1100   10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "1183   10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "1253   10:19 a.m.     12:10 p.m.   11:50 a.m.      2  \n",
       "1353   10:19 a.m.     12:10 p.m.   11:50 a.m.      3  \n",
       "1517   10:19 a.m.     12:10 p.m.   11:50 a.m.      2  \n",
       "1602   10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "1677   10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "1788   10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "1887   10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "1980   10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "2080   10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "2185   10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "2271   10:19 a.m.     12:10 p.m.   11:50 a.m.      0  \n",
       "2337   10:19 a.m.     12:10 p.m.   11:50 a.m.      0  "
      ]
     },
     "execution_count": 1374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_dirty_clean[flight_dirty_clean[relation]!=flight_clean[relation]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9832345123696585 0.9774390243902439 0.980328203037407\n"
     ]
    }
   ],
   "source": [
    "flight_clean = pd.read_csv('datasets/flights/clean.csv').fillna('').astype(str)\n",
    "flight_dirty = pd.read_csv('datasets/flights/dirty.csv').fillna('').astype(str)\n",
    "flight_query = pd.read_csv('datasets/flights/dirty_query.csv',index_col=0)\n",
    "flight_dirty.columns = flight_clean.columns\n",
    "All_Data_Error = All_Error.sum()\n",
    "Correct_Fixed_Error = 0\n",
    "All_Fixed_Error = 0\n",
    "for x in range(All_Error.shape[0]):\n",
    "    for y in range(All_Error.shape[1]):\n",
    "        dirty_cell = flight_dirty.iloc[x,y]\n",
    "        fixed_cell = flight_dirty_clean.iloc[x,y]\n",
    "        clean_cell = flight_clean.iloc[x,y]\n",
    "        if(fixed_cell!=dirty_cell):\n",
    "            All_Fixed_Error += 1\n",
    "            if(fixed_cell==clean_cell):\n",
    "                Correct_Fixed_Error += 1\n",
    "Precision = Correct_Fixed_Error / All_Fixed_Error\n",
    "Recall = Correct_Fixed_Error / All_Data_Error\n",
    "F1 = (2*Precision*Recall) / (Precision + Recall)\n",
    "print(Precision,Recall,F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Beer Dataset\n",
    "beer_clean = pd.read_csv('datasets/beers/clean.csv').fillna('')\n",
    "beer_dirty = pd.read_csv('datasets/beers/dirty.csv').fillna('')\n",
    "beer_dirty.columns = beer_clean.columns\n",
    "def try_convert_to_int(row):\n",
    "    for x,y in row.items():\n",
    "        if(x in ['ounces','ibu']):\n",
    "            try:\n",
    "                row[x] = int(y)\n",
    "            except:\n",
    "                row[x] = y\n",
    "    return row\n",
    "beer_clean = beer_clean.apply(try_convert_to_int,axis=1).astype(str)\n",
    "beer_dirty = beer_dirty.apply(try_convert_to_int,axis=1).astype(str)\n",
    "\n",
    "# beer_query = pd.read_csv('datasets/beers/dirty_query.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3357"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(beer_clean!=beer_dirty).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All_Data_Error: 3357"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[4,5,6,9,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = -1\n",
    "beer_label = beer_dirty[(beer_clean.iloc[:,9]!=beer_dirty.iloc[:,9]) & (beer_clean.iloc[:,5]!=beer_dirty.iloc[:,5])].sample(n=20)\n",
    "beer_label_index = beer_label.index\n",
    "# len(beer_dirty['brewery-name'].unique())\n",
    "# beer_dirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_matrix_beer = np.array(beer_clean!=beer_dirty)\n",
    "selected_rows_beer = beer_label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2410, 11)"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Detector Training\n",
    "input_matrix_select_beer = input_matrix_beer[selected_rows_beer]\n",
    "detector_list_beer = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "for label_tuple in selected_rows_beer:\n",
    "    for i in range(1,len(beer_clean.columns),1):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = beer_clean.iloc[label_tuple]\n",
    "        dirty_context = beer_dirty.iloc[label_tuple]\n",
    "        clean_cell = beer_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = beer_dirty.iloc[label_tuple,i]\n",
    "        single_context_clean = 'COL %s VAL %s ' % (beer_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (beer_clean.columns[i],dirty_cell)\n",
    "        for c in range(1,11,1):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (beer_clean.columns[c],beer_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (beer_dirty.columns[c],beer_dirty.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            detector_list_beer.append([all_context_dirty,single_context_dirty,1])\n",
    "            detector_list_beer.append([all_context_clean,single_context_clean,0])\n",
    "            detector_list_beer.append([all_context_dirty,single_context_clean,0])\n",
    "            # detector_list.append([single_context_dirty,1])\n",
    "        else:\n",
    "            detector_list_beer.append([all_context_dirty,single_context_dirty,0])\n",
    "            detector_list_beer.append([all_context_clean,single_context_clean,0])\n",
    "            detector_list_beer.append([all_context_dirty,single_context_clean,0])\n",
    "        # else:\n",
    "        #     detector_list.append([all_context_dirty,1])\n",
    "        #     detector_list.append([single_context_dirty,1])\n",
    "            \n",
    "                # break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa2e2bba197046278bd6bd40d822fd5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Detector Inference\n",
    "input_matrix_select_beer = input_matrix_beer[selected_rows_beer]\n",
    "detector_list_beer = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "# for label_tuple in tqdm(selected_rows):\n",
    "for label_tuple in tqdm(range(len(beer_clean))):\n",
    "    for i in range(1,len(beer_clean.columns),1):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = beer_clean.iloc[label_tuple]\n",
    "        dirty_context = beer_dirty.iloc[label_tuple]\n",
    "        clean_cell = beer_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = beer_dirty.iloc[label_tuple,i]\n",
    "        single_context_clean = 'COL %s VAL %s ' % (beer_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (beer_clean.columns[i],dirty_cell)\n",
    "        for c in range(1,11,1):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (beer_clean.columns[c],beer_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (beer_dirty.columns[c],beer_dirty.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            detector_list_beer.append([all_context_dirty,single_context_dirty,1])\n",
    "        else:\n",
    "            detector_list_beer.append([all_context_dirty,single_context_dirty,0])\n",
    "            # detector_list.append([single_context_dirty,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COL id VAL 1436 COL beer-name VAL Pub Beer COL...</td>\n",
       "      <td>COL id VAL 1436</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COL id VAL 1436 COL beer-name VAL Pub Beer COL...</td>\n",
       "      <td>COL beer-name VAL Pub Beer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COL id VAL 1436 COL beer-name VAL Pub Beer COL...</td>\n",
       "      <td>COL style VAL American Pale Lager</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COL id VAL 1436 COL beer-name VAL Pub Beer COL...</td>\n",
       "      <td>COL ounces VAL 12.0 oz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COL id VAL 1436 COL beer-name VAL Pub Beer COL...</td>\n",
       "      <td>COL abv VAL 0.05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24095</th>\n",
       "      <td>COL id VAL 84 COL beer-name VAL Rail Yard Ale ...</td>\n",
       "      <td>COL ibu VAL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24096</th>\n",
       "      <td>COL id VAL 84 COL beer-name VAL Rail Yard Ale ...</td>\n",
       "      <td>COL brewery_id VAL 424</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24097</th>\n",
       "      <td>COL id VAL 84 COL beer-name VAL Rail Yard Ale ...</td>\n",
       "      <td>COL brewery-name VAL Wynkoop Brewing Company</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24098</th>\n",
       "      <td>COL id VAL 84 COL beer-name VAL Rail Yard Ale ...</td>\n",
       "      <td>COL city VAL Denver</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24099</th>\n",
       "      <td>COL id VAL 84 COL beer-name VAL Rail Yard Ale ...</td>\n",
       "      <td>COL state VAL CO</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0  \\\n",
       "0      COL id VAL 1436 COL beer-name VAL Pub Beer COL...   \n",
       "1      COL id VAL 1436 COL beer-name VAL Pub Beer COL...   \n",
       "2      COL id VAL 1436 COL beer-name VAL Pub Beer COL...   \n",
       "3      COL id VAL 1436 COL beer-name VAL Pub Beer COL...   \n",
       "4      COL id VAL 1436 COL beer-name VAL Pub Beer COL...   \n",
       "...                                                  ...   \n",
       "24095  COL id VAL 84 COL beer-name VAL Rail Yard Ale ...   \n",
       "24096  COL id VAL 84 COL beer-name VAL Rail Yard Ale ...   \n",
       "24097  COL id VAL 84 COL beer-name VAL Rail Yard Ale ...   \n",
       "24098  COL id VAL 84 COL beer-name VAL Rail Yard Ale ...   \n",
       "24099  COL id VAL 84 COL beer-name VAL Rail Yard Ale ...   \n",
       "\n",
       "                                                   1  2  \n",
       "0                                   COL id VAL 1436   0  \n",
       "1                        COL beer-name VAL Pub Beer   0  \n",
       "2                 COL style VAL American Pale Lager   0  \n",
       "3                            COL ounces VAL 12.0 oz   1  \n",
       "4                                  COL abv VAL 0.05   0  \n",
       "...                                              ... ..  \n",
       "24095                                  COL ibu VAL    0  \n",
       "24096                        COL brewery_id VAL 424   0  \n",
       "24097  COL brewery-name VAL Wynkoop Brewing Company   0  \n",
       "24098                           COL city VAL Denver   0  \n",
       "24099                              COL state VAL CO   0  \n",
       "\n",
       "[24100 rows x 3 columns]"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector_list_beer_pd = pd.DataFrame(detector_list_beer)\n",
    "detector_list_beer_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector_list_beer_pd.to_csv('datasets/beers/detector/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_detector = np.load('datasets/beers/detector/detection.npy')\n",
    "beer_detector = beer_detector.reshape((-1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,    1,    2, ..., 2405, 2406, 2409]),)"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer_detector_coreset = beer_detector.sum(axis=1)\n",
    "np.where(beer_detector_coreset==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ICL_Beer(row):\n",
    "training_list_label = []\n",
    "for index in beer_label_index:\n",
    "    for i in range(1,11,1):\n",
    "        dirty_cell = beer_dirty.iloc[index,i]\n",
    "        clean_cell = beer_clean.iloc[index,i]\n",
    "        col_name = beer_clean.columns[i]\n",
    "        template_dict = {}\n",
    "        template_dict[col_name] = ''\n",
    "        temp_dict = beer_dirty.iloc[index,1:].to_dict()\n",
    "        clean_dict = {}\n",
    "        clean_dict[col_name] = clean_cell\n",
    "        coreset_reference = np.random.choice([c for c in beer_label_index if c!=index],3,replace=False)\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            text_head = 'You are an expert in cleaning Beers Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\nTake these rows as reference:\\n\\n' % (col_name, json.dumps(template_dict), json.dumps(temp_dict))\n",
    "            dict_0 = beer_clean.iloc[coreset_reference[0],1:].to_dict()\n",
    "            dict_1 = beer_clean.iloc[coreset_reference[1],1:].to_dict()\n",
    "            dict_2 = beer_clean.iloc[coreset_reference[2],1:].to_dict()\n",
    "            ICL_text = '%s\\n\\n%s\\n\\n%s\\n\\n' % (json.dumps(dict_0),json.dumps(dict_1),json.dumps(dict_2))\n",
    "            training_list_label.append([text_head,ICL_text,'',json.dumps(clean_dict)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in cleaning Beers Dataset. Given the dirty row Entity 1, you are required to correct the values of ounces in Entity 1.\n",
      "\n",
      "Return in json format.\n",
      "\n",
      "Output Format Example:\n",
      "\n",
      "{\"ounces\": \"\"}\n",
      "\n",
      "Entity 1:\n",
      "\n",
      "{\"id\": \"409\", \"beer-name\": \"Oaky's Oatmeal Stout\", \"style\": \"Oatmeal Stout\", \"ounces\": \"16.0 oz.\", \"abv\": \"0.047%\", \"ibu\": \"\", \"brewery_id\": \"542\", \"brewery-name\": \"Angry Minnow Brewing Company\", \"city\": \"Hayward WI\", \"state\": \"\"}\n",
      "\n",
      "Take these rows as reference:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(pd.DataFrame(training_list_label).iloc[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list_pd = pd.DataFrame(training_list_label)\n",
    "training_list_pd['instruction'] = training_list_pd[0] + training_list_pd[1]\n",
    "training_list_pd['input'] = training_list_pd[2]\n",
    "training_list_pd['output'] = training_list_pd[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"id\": \"1851\", \"beer-name\": \"Cougar\", \"style\":...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"16\"}</td>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"16\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"id\": \"1082\", \"beer-name\": \"RecreationAle\", \"...</td>\n",
       "      <td></td>\n",
       "      <td>{\"abv\": \"0.047\"}</td>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"abv\": \"0.047\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"id\": \"1082\", \"beer-name\": \"RecreationAle\", \"...</td>\n",
       "      <td></td>\n",
       "      <td>{\"city\": \"Hayward\"}</td>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"city\": \"Hayward\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"id\": \"736\", \"beer-name\": \"Ornery Amber Lager...</td>\n",
       "      <td></td>\n",
       "      <td>{\"state\": \"WI\"}</td>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"state\": \"WI\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"id\": \"1463\", \"beer-name\": \"Hideout Helles\", ...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"id\": \"938\", \"beer-name\": \"Gangway IPA\", \"sty...</td>\n",
       "      <td></td>\n",
       "      <td>{\"state\": \"OR\"}</td>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"state\": \"OR\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"id\": \"1254\", \"beer-name\": \"JP's Ould Sod Iri...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"id\": \"2619\", \"beer-name\": \"Insert Hop Refere...</td>\n",
       "      <td></td>\n",
       "      <td>{\"abv\": \"0.069\"}</td>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"abv\": \"0.069\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"id\": \"2105\", \"beer-name\": \"Even Keel\", \"styl...</td>\n",
       "      <td></td>\n",
       "      <td>{\"city\": \"Dillon\"}</td>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"city\": \"Dillon\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"id\": \"1851\", \"beer-name\": \"Cougar\", \"style\":...</td>\n",
       "      <td></td>\n",
       "      <td>{\"state\": \"CO\"}</td>\n",
       "      <td>You are an expert in cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"state\": \"CO\"}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0  \\\n",
       "0   You are an expert in cleaning Beers Dataset. G...   \n",
       "1   You are an expert in cleaning Beers Dataset. G...   \n",
       "2   You are an expert in cleaning Beers Dataset. G...   \n",
       "3   You are an expert in cleaning Beers Dataset. G...   \n",
       "4   You are an expert in cleaning Beers Dataset. G...   \n",
       "..                                                ...   \n",
       "75  You are an expert in cleaning Beers Dataset. G...   \n",
       "76  You are an expert in cleaning Beers Dataset. G...   \n",
       "77  You are an expert in cleaning Beers Dataset. G...   \n",
       "78  You are an expert in cleaning Beers Dataset. G...   \n",
       "79  You are an expert in cleaning Beers Dataset. G...   \n",
       "\n",
       "                                                    1 2                     3  \\\n",
       "0   {\"id\": \"1851\", \"beer-name\": \"Cougar\", \"style\":...        {\"ounces\": \"16\"}   \n",
       "1   {\"id\": \"1082\", \"beer-name\": \"RecreationAle\", \"...        {\"abv\": \"0.047\"}   \n",
       "2   {\"id\": \"1082\", \"beer-name\": \"RecreationAle\", \"...     {\"city\": \"Hayward\"}   \n",
       "3   {\"id\": \"736\", \"beer-name\": \"Ornery Amber Lager...         {\"state\": \"WI\"}   \n",
       "4   {\"id\": \"1463\", \"beer-name\": \"Hideout Helles\", ...        {\"ounces\": \"12\"}   \n",
       "..                                                ... ..                  ...   \n",
       "75  {\"id\": \"938\", \"beer-name\": \"Gangway IPA\", \"sty...         {\"state\": \"OR\"}   \n",
       "76  {\"id\": \"1254\", \"beer-name\": \"JP's Ould Sod Iri...        {\"ounces\": \"12\"}   \n",
       "77  {\"id\": \"2619\", \"beer-name\": \"Insert Hop Refere...        {\"abv\": \"0.069\"}   \n",
       "78  {\"id\": \"2105\", \"beer-name\": \"Even Keel\", \"styl...      {\"city\": \"Dillon\"}   \n",
       "79  {\"id\": \"1851\", \"beer-name\": \"Cougar\", \"style\":...         {\"state\": \"CO\"}   \n",
       "\n",
       "                                          instruction input  \\\n",
       "0   You are an expert in cleaning Beers Dataset. G...         \n",
       "1   You are an expert in cleaning Beers Dataset. G...         \n",
       "2   You are an expert in cleaning Beers Dataset. G...         \n",
       "3   You are an expert in cleaning Beers Dataset. G...         \n",
       "4   You are an expert in cleaning Beers Dataset. G...         \n",
       "..                                                ...   ...   \n",
       "75  You are an expert in cleaning Beers Dataset. G...         \n",
       "76  You are an expert in cleaning Beers Dataset. G...         \n",
       "77  You are an expert in cleaning Beers Dataset. G...         \n",
       "78  You are an expert in cleaning Beers Dataset. G...         \n",
       "79  You are an expert in cleaning Beers Dataset. G...         \n",
       "\n",
       "                 output  \n",
       "0      {\"ounces\": \"16\"}  \n",
       "1      {\"abv\": \"0.047\"}  \n",
       "2   {\"city\": \"Hayward\"}  \n",
       "3       {\"state\": \"WI\"}  \n",
       "4      {\"ounces\": \"12\"}  \n",
       "..                  ...  \n",
       "75      {\"state\": \"OR\"}  \n",
       "76     {\"ounces\": \"12\"}  \n",
       "77     {\"abv\": \"0.069\"}  \n",
       "78   {\"city\": \"Dillon\"}  \n",
       "79      {\"state\": \"CO\"}  \n",
       "\n",
       "[80 rows x 7 columns]"
      ]
     },
     "execution_count": 626,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_list_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_list_pd\n",
    "json.dump(training_list_pd.iloc[:,-3:].to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/beer/beer-train.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Data on Hospital\n",
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 16])"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector_indice[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc39f8d9e3c4e879b5678ce3557249e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/496 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 检查所有detector要求detect的元素，组成一个list，查找并附加coreset\n",
    "# for d in detector\n",
    "detector_indice = np.argwhere(detector==1)\n",
    "detector_list_all = []\n",
    "candidate_length = {}\n",
    "right_loc = {}\n",
    "training_list_label = []\n",
    "for d in tqdm(detector_indice):\n",
    "    \n",
    "    label_tuple = d[0] ## 行\n",
    "    i = d[1] ## 列\n",
    "    col_name = hospital_clean.columns[i] ## 列名\n",
    "    all_context_clean = ''\n",
    "    all_context_dirty = ''\n",
    "    # clean_context = hospital_clean.iloc[label_tuple]\n",
    "    dirty_context = hospital_dirty.iloc[label_tuple]\n",
    "    clean_cell = hospital_clean.iloc[label_tuple,i]\n",
    "    dirty_cell = hospital_dirty.iloc[label_tuple,i]\n",
    "    \n",
    "    template_dict = {}\n",
    "    template_dict[col_name] = ''\n",
    "    temp_dict = hospital_dirty_dict[label_tuple + 1]\n",
    "    \n",
    "    clean_dict = {}\n",
    "    clean_dict[col_name] = clean_cell\n",
    "    # single_context_clean = 'COL %s VAL %s ' % (hospital_clean.columns[i],clean_cell)\n",
    "    # single_context_dirty = 'COL %s VAL %s ' % (hospital_clean.columns[i],dirty_cell)\n",
    "    # if(clean_cell!=dirty_cell):\n",
    "    text_head = 'You are an expert in Cleaning Hospital Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\nTake these rows as reference:\\n\\n' % (col_name, json.dumps(template_dict), json.dumps(temp_dict))\n",
    "    cluster = [hospital_cluster[h] for h in hospital_cluster if hospital_cluster[h].__contains__(label_tuple)][0] ## 找到label tuple所在的簇\n",
    "    cluster_coreset = [c for c in cluster if c in coreset_detect] ## 找到簇内coreset元素\n",
    "    coreset_reference = np.random.choice(cluster_coreset,1,replace=False) ## 取两个\n",
    "    # ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]), json.dumps(hospital_dirty_dict[coreset_reference[1]]))\n",
    "    ICL_text = '%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]))\n",
    "    training_list_label.append([text_head,ICL_text,'',json.dumps(clean_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pd.DataFrame(training_list_label).iloc[0,3])\n",
    "training_list_pd = pd.DataFrame(training_list_label)\n",
    "training_list_pd['instruction'] = training_list_pd[0] + training_list_pd[1]\n",
    "training_list_pd['input'] = training_list_pd[2]\n",
    "training_list_pd['output'] = training_list_pd[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(training_list_pd.iloc[:,-3:].to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/hospital/hospital-test.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "496"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training_list_pd\n",
    "len(detector_indice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'ProviderNumber', 'HospitalName', 'Address1', 'Address2',\n",
       "       'Address3', 'City', 'State', 'ZipCode', 'CountyName', 'PhoneNumber',\n",
       "       'HospitalType', 'HospitalOwner', 'EmergencyService', 'Condition',\n",
       "       'MeasureCode', 'MeasureName', 'Score', 'Sample', 'Stateavg'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hospital_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      scip-card-2\n",
       "1       scip-inf-1\n",
       "2       scip-inf-2\n",
       "3       scip-inf-3\n",
       "4       scip-inf-4\n",
       "          ...     \n",
       "995           pn-6\n",
       "996           pn-7\n",
       "997    scip-card-2\n",
       "998     scip-inf-1\n",
       "999     scip-inf-2\n",
       "Name: MeasureCode, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hospital_clean['MeasureCode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"MeasureName\": \"surgery patients who were tak...</td>\n",
       "      <td>{\"MeasureName\": \"surgery patients who were tak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"City\": \"birmingham\"}</td>\n",
       "      <td>{\"City\": \"birmingham\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"City\": \"birmingham\"}</td>\n",
       "      <td>{\"City\": \"birmingham\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"City\": \"sheffield\"}</td>\n",
       "      <td>{\"City\": \"sheffield\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ProviderNumber\": \"10019\"}</td>\n",
       "      <td>{\"ProviderNumber\": \"10019\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"HospitalType\": \"acute care hospitals\"}</td>\n",
       "      <td>{\"HospitalType\": \"acute care hospitals\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"City\": \"oneonta\"}</td>\n",
       "      <td>{\"City\": \"oneonta\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"Address1\": \"150 gilbreath drive\"}</td>\n",
       "      <td>{\"Address1\": \"150 gilbreath drive\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"HospitalName\": \"st vincents blount\"}</td>\n",
       "      <td>{\"HospitalName\": \"st vincent's birmingham\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"MeasureName\": \"surgery patients who were tak...</td>\n",
       "      <td>{\"MeasureName\": \"surgery patients who were tak...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>496 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           instruction  input  \\\n",
       "0    You are an expert in Cleaning Hospital Dataset...    NaN   \n",
       "1    You are an expert in Cleaning Hospital Dataset...    NaN   \n",
       "2    You are an expert in Cleaning Hospital Dataset...    NaN   \n",
       "3    You are an expert in Cleaning Hospital Dataset...    NaN   \n",
       "4    You are an expert in Cleaning Hospital Dataset...    NaN   \n",
       "..                                                 ...    ...   \n",
       "491  You are an expert in Cleaning Hospital Dataset...    NaN   \n",
       "492  You are an expert in Cleaning Hospital Dataset...    NaN   \n",
       "493  You are an expert in Cleaning Hospital Dataset...    NaN   \n",
       "494  You are an expert in Cleaning Hospital Dataset...    NaN   \n",
       "495  You are an expert in Cleaning Hospital Dataset...    NaN   \n",
       "\n",
       "                                                output  \\\n",
       "0    {\"MeasureName\": \"surgery patients who were tak...   \n",
       "1                               {\"City\": \"birmingham\"}   \n",
       "2                               {\"City\": \"birmingham\"}   \n",
       "3                                {\"City\": \"sheffield\"}   \n",
       "4                          {\"ProviderNumber\": \"10019\"}   \n",
       "..                                                 ...   \n",
       "491           {\"HospitalType\": \"acute care hospitals\"}   \n",
       "492                                {\"City\": \"oneonta\"}   \n",
       "493                {\"Address1\": \"150 gilbreath drive\"}   \n",
       "494             {\"HospitalName\": \"st vincents blount\"}   \n",
       "495  {\"MeasureName\": \"surgery patients who were tak...   \n",
       "\n",
       "                                               predict  \n",
       "0    {\"MeasureName\": \"surgery patients who were tak...  \n",
       "1                               {\"City\": \"birmingham\"}  \n",
       "2                               {\"City\": \"birmingham\"}  \n",
       "3                                {\"City\": \"sheffield\"}  \n",
       "4                          {\"ProviderNumber\": \"10019\"}  \n",
       "..                                                 ...  \n",
       "491           {\"HospitalType\": \"acute care hospitals\"}  \n",
       "492                                {\"City\": \"oneonta\"}  \n",
       "493                {\"Address1\": \"150 gilbreath drive\"}  \n",
       "494        {\"HospitalName\": \"st vincent's birmingham\"}  \n",
       "495  {\"MeasureName\": \"surgery patients who were tak...  \n",
       "\n",
       "[496 rows x 4 columns]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hospital_result = pd.read_csv('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/hospital-test.csv',index_col=0)\n",
    "hospital_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hospital_detector = np.load('/home/yanmy/raha/raha-master/datasets/hospital/detector/detection.npy').reshape((1000,-1))\n",
    "hospital_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(hospital_detector==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "hospital_correction = hospital_dirty.iloc[:,:-1]\n",
    "import ast\n",
    "for d in np.argwhere(hospital_detector==1):\n",
    "    i = d[0]\n",
    "    j = d[1]\n",
    "    predict = list(ast.literal_eval(hospital_result.iloc[count,-1]).values())[0]\n",
    "    hospital_correction.iloc[i,j] = predict\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_correction.to_csv('/home/yanmy/raha/raha-master/datasets/hospital/correct_result/hospital_correction_20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8993839835728953, 0.8605108055009824, 0.8795180722891566)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_Data_Error = 0\n",
    "All_Fixed_Error = 0\n",
    "Correct_Fixed_Error = 0\n",
    "for i in range(1000):\n",
    "    for j in range(20):\n",
    "        dirty_cell = hospital_dirty.iloc[i,j]\n",
    "        clean_cell = hospital_clean.iloc[i,j]\n",
    "        correct_cell = hospital_correction.iloc[i,j]\n",
    "        if(correct_cell!=dirty_cell):\n",
    "            All_Fixed_Error += 1\n",
    "        if(clean_cell!=dirty_cell):\n",
    "            All_Data_Error += 1\n",
    "            if(correct_cell==clean_cell):\n",
    "                Correct_Fixed_Error += 1\n",
    "Precision_hospital = Correct_Fixed_Error / All_Fixed_Error\n",
    "Recall_hospital = Correct_Fixed_Error / All_Data_Error\n",
    "F1_hospital = (2 * Precision_hospital * Recall_hospital) / (Precision_hospital + Recall_hospital)\n",
    "Precision_hospital,Recall_hospital,F1_hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1551,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"abv\": \"0.09\"}</td>\n",
       "      <td>{\"abv\": \"0.09\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3352</th>\n",
       "      <td>3352</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3353</th>\n",
       "      <td>3353</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"abv\": \"0.055\"}</td>\n",
       "      <td>{\"abv\": \"0.055\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3354</th>\n",
       "      <td>3354</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3355</th>\n",
       "      <td>3355</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"abv\": \"0.055\"}</td>\n",
       "      <td>{\"abv\": \"0.055\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3356</th>\n",
       "      <td>3356</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3357 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                        instruction  input  \\\n",
       "0              0  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "1              1  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "2              2  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "3              3  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "4              4  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "...          ...                                                ...    ...   \n",
       "3352        3352  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "3353        3353  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "3354        3354  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "3355        3355  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "3356        3356  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "\n",
       "                output           predict  \n",
       "0     {\"ounces\": \"12\"}  {\"ounces\": \"12\"}  \n",
       "1     {\"ounces\": \"12\"}  {\"ounces\": \"12\"}  \n",
       "2     {\"ounces\": \"12\"}  {\"ounces\": \"12\"}  \n",
       "3     {\"ounces\": \"12\"}  {\"ounces\": \"12\"}  \n",
       "4      {\"abv\": \"0.09\"}   {\"abv\": \"0.09\"}  \n",
       "...                ...               ...  \n",
       "3352  {\"ounces\": \"12\"}  {\"ounces\": \"12\"}  \n",
       "3353  {\"abv\": \"0.055\"}  {\"abv\": \"0.055\"}  \n",
       "3354  {\"ounces\": \"12\"}  {\"ounces\": \"12\"}  \n",
       "3355  {\"abv\": \"0.055\"}  {\"abv\": \"0.055\"}  \n",
       "3356  {\"ounces\": \"12\"}  {\"ounces\": \"12\"}  \n",
       "\n",
       "[3357 rows x 5 columns]"
      ]
     },
     "execution_count": 1551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer_result = pd.read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"Stateavg\": \"al_scip-vte-2\"}</td>\n",
       "      <td>{\"Stateavg\": \"al_scipxvtex2\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"Stateavg\": \"al_scip-vte-1\"}</td>\n",
       "      <td>{\"Stateavg\": \"al_scipxvtex1\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"Stateavg\": \"al_scip-inf-2\"}</td>\n",
       "      <td>{\"Stateavg\": \"al_scipxinfx2\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"Stateavg\": \"al_scip-vte-1\"}</td>\n",
       "      <td>{\"Stateavg\": \"al_sci-vte-1\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"Stateavg\": \"al_pn-6\"}</td>\n",
       "      <td>{\"Stateavg\": \"al_pnx6\"}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           instruction  input  \\\n",
       "169  You are an expert in Cleaning Hospital Dataset...    NaN   \n",
       "293  You are an expert in Cleaning Hospital Dataset...    NaN   \n",
       "391  You are an expert in Cleaning Hospital Dataset...    NaN   \n",
       "394  You are an expert in Cleaning Hospital Dataset...    NaN   \n",
       "426  You are an expert in Cleaning Hospital Dataset...    NaN   \n",
       "\n",
       "                            output                        predict  \n",
       "169  {\"Stateavg\": \"al_scip-vte-2\"}  {\"Stateavg\": \"al_scipxvtex2\"}  \n",
       "293  {\"Stateavg\": \"al_scip-vte-1\"}  {\"Stateavg\": \"al_scipxvtex1\"}  \n",
       "391  {\"Stateavg\": \"al_scip-inf-2\"}  {\"Stateavg\": \"al_scipxinfx2\"}  \n",
       "394  {\"Stateavg\": \"al_scip-vte-1\"}   {\"Stateavg\": \"al_sci-vte-1\"}  \n",
       "426        {\"Stateavg\": \"al_pn-6\"}        {\"Stateavg\": \"al_pnx6\"}  "
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(hospital_result[hospital_result['output']!=hospital_result['predict'])\n",
    "hospital_result[(hospital_result['output'].str.contains('Stateavg')) & (hospital_result['output']!=hospital_result['predict'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "509"
      ]
     },
     "execution_count": 632,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum(input_matrix)\n",
    "input_matrix.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8850806451612904, 0.862475442043222, 0.87363184079602)"
      ]
     },
     "execution_count": 634,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Precision_hospital = (496-57) / 496\n",
    "Recall_hospital = (496-57) / input_matrix.sum()\n",
    "F1_hospital = (2 * Precision_hospital * Recall_hospital) / (Precision_hospital + Recall_hospital)\n",
    "Precision_hospital,Recall_hospital,F1_hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.8850806451612904, 0.862475442043222, 0.87363184079602) Error_Correction_HOSPITAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"MeasureName\": \"surgery patients who were taking heart blockers before coming to the hospital who were kept on the beta blockers during the period just before and after their surgery\"}'"
      ]
     },
     "execution_count": 600,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(detector_indice)\n",
    "hospital_result.iloc[495,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on Beers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    3],\n",
       "       [   1,    3],\n",
       "       [   2,    3],\n",
       "       ...,\n",
       "       [2408,    3],\n",
       "       [2408,    4],\n",
       "       [2409,    3]])"
      ]
     },
     "execution_count": 610,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer_detector_dirty = np.argwhere(beer_detector==1)\n",
    "beer_detector_dirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3342"
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(beer_detector_dirty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ICL_Beer(row):\n",
    "training_list_label = []\n",
    "for d in beer_detector_dirty:\n",
    "    index = d[0]\n",
    "    i = d[1] + 1 ## 重要，我们在detector中忽略了index列\n",
    "    dirty_cell = beer_dirty.iloc[index,i]\n",
    "    clean_cell = beer_clean.iloc[index,i]\n",
    "    col_name = beer_clean.columns[i]\n",
    "    template_dict = {}\n",
    "    template_dict[col_name] = ''\n",
    "    temp_dict = beer_dirty.iloc[index,1:].to_dict()\n",
    "    clean_dict = {}\n",
    "    clean_dict[col_name] = clean_cell\n",
    "    coreset_reference = np.random.choice([c for c in beer_label_index if c!=index],3,replace=False)\n",
    "    # if(dirty_cell!=clean_cell): ## 在LLM Inference中忽略\n",
    "    text_head = 'You are an expert in cleaning Beers Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\nTake these rows as reference:\\n\\n' % (col_name, json.dumps(template_dict), json.dumps(temp_dict))\n",
    "    dict_0 = beer_clean.iloc[coreset_reference[0],1:].to_dict()\n",
    "    dict_1 = beer_clean.iloc[coreset_reference[1],1:].to_dict()\n",
    "    dict_2 = beer_clean.iloc[coreset_reference[2],1:].to_dict()\n",
    "    ICL_text = '%s\\n\\n%s\\n\\n%s\\n\\n' % (json.dumps(dict_0),json.dumps(dict_1),json.dumps(dict_2))\n",
    "    training_list_label.append([text_head,ICL_text,'',json.dumps(clean_dict)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(training_list_label)\n",
    "training_list_pd = pd.DataFrame(training_list_label)\n",
    "training_list_pd['instruction'] = training_list_pd[0] + training_list_pd[1]\n",
    "training_list_pd['input'] = training_list_pd[2]\n",
    "training_list_pd['output'] = training_list_pd[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(training_list_pd.iloc[:,-3:].to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/beer/beer-test.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1552,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"abv\": \"0.09\"}</td>\n",
       "      <td>{\"abv\": \"0.09\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3359</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3360</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"abv\": \"0.055\"}</td>\n",
       "      <td>{\"abv\": \"0.055\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3361</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3362</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"abv\": \"0.055\"}</td>\n",
       "      <td>{\"abv\": \"0.055\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3363</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3364 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            instruction  input  \\\n",
       "0     You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "1     You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "2     You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "3     You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "4     You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "...                                                 ...    ...   \n",
       "3359  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "3360  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "3361  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "3362  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "3363  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "\n",
       "                output           predict  \n",
       "0     {\"ounces\": \"12\"}  {\"ounces\": \"12\"}  \n",
       "1     {\"ounces\": \"12\"}  {\"ounces\": \"12\"}  \n",
       "2     {\"ounces\": \"12\"}  {\"ounces\": \"12\"}  \n",
       "3     {\"ounces\": \"12\"}  {\"ounces\": \"12\"}  \n",
       "4      {\"abv\": \"0.09\"}   {\"abv\": \"0.09\"}  \n",
       "...                ...               ...  \n",
       "3359  {\"ounces\": \"12\"}  {\"ounces\": \"12\"}  \n",
       "3360  {\"abv\": \"0.055\"}  {\"abv\": \"0.055\"}  \n",
       "3361  {\"ounces\": \"12\"}  {\"ounces\": \"12\"}  \n",
       "3362  {\"abv\": \"0.055\"}  {\"abv\": \"0.055\"}  \n",
       "3363  {\"ounces\": \"12\"}  {\"ounces\": \"12\"}  \n",
       "\n",
       "[3364 rows x 4 columns]"
      ]
     },
     "execution_count": 1552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer_result = pd.read_csv('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/beer-test-20.csv',index_col=0)\n",
    "beer_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1553,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ast.literal_eval()\n",
    "import ast\n",
    "beer_result['item'] = ''\n",
    "beer_result['gt'] = ''\n",
    "for index,row in beer_result.iterrows():\n",
    "    temp_dict = ast.literal_eval(row[3])\n",
    "    gt_dict = ast.literal_eval(row[2])\n",
    "    beer_result.iloc[index,-2] = list(temp_dict.values())[0]\n",
    "    beer_result.iloc[index,-1] = list(gt_dict.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1563,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>predict</th>\n",
       "      <th>item</th>\n",
       "      <th>gt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"abv\": \"0.09\"}</td>\n",
       "      <td>{\"abv\": \"0.09\"}</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3359</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3360</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"abv\": \"0.055\"}</td>\n",
       "      <td>{\"abv\": \"0.055\"}</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3361</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3362</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"abv\": \"0.055\"}</td>\n",
       "      <td>{\"abv\": \"0.055\"}</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3363</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3364 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            instruction  input  \\\n",
       "0     You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "1     You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "2     You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "3     You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "4     You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "...                                                 ...    ...   \n",
       "3359  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "3360  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "3361  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "3362  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "3363  You are an expert in Cleaning Beers Dataset. G...    NaN   \n",
       "\n",
       "                output           predict   item     gt  \n",
       "0     {\"ounces\": \"12\"}  {\"ounces\": \"12\"}     12     12  \n",
       "1     {\"ounces\": \"12\"}  {\"ounces\": \"12\"}     12     12  \n",
       "2     {\"ounces\": \"12\"}  {\"ounces\": \"12\"}     12     12  \n",
       "3     {\"ounces\": \"12\"}  {\"ounces\": \"12\"}     12     12  \n",
       "4      {\"abv\": \"0.09\"}   {\"abv\": \"0.09\"}   0.09   0.09  \n",
       "...                ...               ...    ...    ...  \n",
       "3359  {\"ounces\": \"12\"}  {\"ounces\": \"12\"}     12     12  \n",
       "3360  {\"abv\": \"0.055\"}  {\"abv\": \"0.055\"}  0.055  0.055  \n",
       "3361  {\"ounces\": \"12\"}  {\"ounces\": \"12\"}     12     12  \n",
       "3362  {\"abv\": \"0.055\"}  {\"abv\": \"0.055\"}  0.055  0.055  \n",
       "3363  {\"ounces\": \"12\"}  {\"ounces\": \"12\"}     12     12  \n",
       "\n",
       "[3364 rows x 6 columns]"
      ]
     },
     "execution_count": 1563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1566,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_beer.shape\n",
    "count = 0\n",
    "beer_correct_20 = beer_dirty.copy()\n",
    "for d in np.argwhere(detector_beer==1):\n",
    "    i = d[0] \n",
    "    j = d[1] + 2\n",
    "    predict = beer_result.iloc[count,-2]\n",
    "    beer_correct_20.iloc[i,j] = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1562,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>beer-name</th>\n",
       "      <th>style</th>\n",
       "      <th>ounces</th>\n",
       "      <th>abv</th>\n",
       "      <th>ibu</th>\n",
       "      <th>brewery_id</th>\n",
       "      <th>brewery-name</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1436</td>\n",
       "      <td>Pub Beer</td>\n",
       "      <td>American Pale Lager</td>\n",
       "      <td>12</td>\n",
       "      <td>0.05</td>\n",
       "      <td></td>\n",
       "      <td>408</td>\n",
       "      <td>10 Barrel Brewing Company</td>\n",
       "      <td>Bend</td>\n",
       "      <td>OR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2265</td>\n",
       "      <td>Devil's Cup</td>\n",
       "      <td>American Pale Ale (APA)</td>\n",
       "      <td>12</td>\n",
       "      <td>0.066</td>\n",
       "      <td></td>\n",
       "      <td>177</td>\n",
       "      <td>18th Street Brewery</td>\n",
       "      <td>Gary</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2264</td>\n",
       "      <td>Rise of the Phoenix</td>\n",
       "      <td>American IPA</td>\n",
       "      <td>12</td>\n",
       "      <td>0.071</td>\n",
       "      <td></td>\n",
       "      <td>177</td>\n",
       "      <td>18th Street Brewery</td>\n",
       "      <td>Gary</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2263</td>\n",
       "      <td>Sinister</td>\n",
       "      <td>American Double / Imperial IPA</td>\n",
       "      <td>12</td>\n",
       "      <td>0.09</td>\n",
       "      <td></td>\n",
       "      <td>177</td>\n",
       "      <td>18th Street Brewery</td>\n",
       "      <td>Gary</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2262</td>\n",
       "      <td>Sex and Candy</td>\n",
       "      <td>American IPA</td>\n",
       "      <td>12</td>\n",
       "      <td>0.075</td>\n",
       "      <td></td>\n",
       "      <td>177</td>\n",
       "      <td>18th Street Brewery</td>\n",
       "      <td>Gary</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2405</th>\n",
       "      <td>2406</td>\n",
       "      <td>928</td>\n",
       "      <td>Belgorado</td>\n",
       "      <td>Belgian IPA</td>\n",
       "      <td>12</td>\n",
       "      <td>0.067</td>\n",
       "      <td>45</td>\n",
       "      <td>424</td>\n",
       "      <td>Wynkoop Brewing Company</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2406</th>\n",
       "      <td>2407</td>\n",
       "      <td>807</td>\n",
       "      <td>Rail Yard Ale</td>\n",
       "      <td>American Amber / Red Ale</td>\n",
       "      <td>12</td>\n",
       "      <td>0.052</td>\n",
       "      <td></td>\n",
       "      <td>424</td>\n",
       "      <td>Wynkoop Brewing Company</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2407</th>\n",
       "      <td>2408</td>\n",
       "      <td>620</td>\n",
       "      <td>B3K Black Lager</td>\n",
       "      <td>Schwarzbier</td>\n",
       "      <td>12</td>\n",
       "      <td>0.055</td>\n",
       "      <td></td>\n",
       "      <td>424</td>\n",
       "      <td>Wynkoop Brewing Company</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2408</th>\n",
       "      <td>2409</td>\n",
       "      <td>145</td>\n",
       "      <td>Silverback Pale Ale</td>\n",
       "      <td>American Pale Ale (APA)</td>\n",
       "      <td>12</td>\n",
       "      <td>0.055</td>\n",
       "      <td>40</td>\n",
       "      <td>424</td>\n",
       "      <td>Wynkoop Brewing Company</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2409</th>\n",
       "      <td>2410</td>\n",
       "      <td>84</td>\n",
       "      <td>Rail Yard Ale (2009)</td>\n",
       "      <td>American Amber / Red Ale</td>\n",
       "      <td>12</td>\n",
       "      <td>0.052</td>\n",
       "      <td></td>\n",
       "      <td>424</td>\n",
       "      <td>Wynkoop Brewing Company</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2410 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index    id             beer-name                           style ounces  \\\n",
       "0        1  1436              Pub Beer             American Pale Lager     12   \n",
       "1        2  2265           Devil's Cup         American Pale Ale (APA)     12   \n",
       "2        3  2264   Rise of the Phoenix                    American IPA     12   \n",
       "3        4  2263              Sinister  American Double / Imperial IPA     12   \n",
       "4        5  2262         Sex and Candy                    American IPA     12   \n",
       "...    ...   ...                   ...                             ...    ...   \n",
       "2405  2406   928             Belgorado                     Belgian IPA     12   \n",
       "2406  2407   807         Rail Yard Ale        American Amber / Red Ale     12   \n",
       "2407  2408   620       B3K Black Lager                     Schwarzbier     12   \n",
       "2408  2409   145   Silverback Pale Ale         American Pale Ale (APA)     12   \n",
       "2409  2410    84  Rail Yard Ale (2009)        American Amber / Red Ale     12   \n",
       "\n",
       "        abv ibu brewery_id               brewery-name    city state  \n",
       "0      0.05            408  10 Barrel Brewing Company    Bend    OR  \n",
       "1     0.066            177        18th Street Brewery    Gary    IN  \n",
       "2     0.071            177        18th Street Brewery    Gary    IN  \n",
       "3      0.09            177        18th Street Brewery    Gary    IN  \n",
       "4     0.075            177        18th Street Brewery    Gary    IN  \n",
       "...     ...  ..        ...                        ...     ...   ...  \n",
       "2405  0.067  45        424    Wynkoop Brewing Company  Denver    CO  \n",
       "2406  0.052            424    Wynkoop Brewing Company  Denver    CO  \n",
       "2407  0.055            424    Wynkoop Brewing Company  Denver    CO  \n",
       "2408  0.055  40        424    Wynkoop Brewing Company  Denver    CO  \n",
       "2409  0.052            424    Wynkoop Brewing Company  Denver    CO  \n",
       "\n",
       "[2410 rows x 11 columns]"
      ]
     },
     "execution_count": 1562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.09'"
      ]
     },
     "execution_count": 724,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# beer_result\n",
    "# float(0.055)<1\n",
    "def TypeTransfer(content):\n",
    "    # print(content)\n",
    "    try:\n",
    "        content_test = float(content)\n",
    "        if(content_test<1):\n",
    "            # print('case 0')\n",
    "            return str(content)\n",
    "        else:\n",
    "            # print('case 1')\n",
    "            return str(int(content_test))\n",
    "    except:\n",
    "        try:\n",
    "            content_test = int(content)\n",
    "            return str(content_test)\n",
    "        except:\n",
    "            print(content)\n",
    "            return content\n",
    "a = TypeTransfer(beer_result.iloc[4,-2])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_result['item_output'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TypeTransfer(beer_result.iloc[0,-2])\n",
    "# # beer_result.iloc[0,-2]\n",
    "# beer_result['item_output']  = beer_result['item'].apply(TypeTransfer,axis=1)\n",
    "for index,row in beer_result.iterrows():\n",
    "    output = TypeTransfer(beer_result.iloc[index,-3])\n",
    "    beer_result.iloc[index,-1] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 3310)"
      ]
     },
     "execution_count": 732,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# beer_result[beer_result['gt']!=beer_result['item_output']].iloc[100:150,-2:]\n",
    "len(beer_result[beer_result['gt']!=beer_result['item_output']].iloc[:,-2:]),len(beer_result[beer_result['gt']==beer_result['item_output']].iloc[:,-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3357"
      ]
     },
     "execution_count": 733,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Correct_Fixed_Error_beer = 3310\n",
    "All_Fixed_Error_beer = 3342\n",
    "All_Data_Error_beer = input_matrix_beer.sum()\n",
    "All_Data_Error_beer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.990424895272292, 0.9859994042299672, 0.9882071951037469)"
      ]
     },
     "execution_count": 735,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Precision = Correct_Fixed_Error_beer / All_Fixed_Error_beer\n",
    "Recall = Correct_Fixed_Error_beer / All_Data_Error_beer\n",
    "F1_beer = (2 * Precision * Recall) / (Precision + Recall)\n",
    "Precision,Recall,F1_beer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.990424895272292, 0.9859994042299672, 0.9882071951037469) Beers P/R/F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_dirty.iloc[beer_label_index,4].to_list(),beer_clean.iloc[beer_label_index,4].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_cell(cell):\n",
    "    if re.search(r'^\\d+(\\.?\\d+)? oz\\.?$', cell.value):\n",
    "        cleaned_value = float(re.search(r'^\\d+(\\.?\\d+)?', cell).group(0))\n",
    "        return cleaned_value\n",
    "    else:\n",
    "        return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/yanmy/raha/raha-master/detector.ipynb 单元格 139\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y255sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m clean_cell(\u001b[39m'\u001b[39;49m\u001b[39m12.0 oz\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/home/yanmy/raha/raha-master/detector.ipynb 单元格 139\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y255sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclean_cell\u001b[39m(cell):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y255sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mif\u001b[39;00m re\u001b[39m.\u001b[39msearch(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m^\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+(\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.?\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+)? oz\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.?$\u001b[39m\u001b[39m'\u001b[39m, cell\u001b[39m.\u001b[39;49mvalue):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y255sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m         cleaned_value \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(re\u001b[39m.\u001b[39msearch(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m^\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+(\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.?\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+)?\u001b[39m\u001b[39m'\u001b[39m, cell)\u001b[39m.\u001b[39mgroup(\u001b[39m0\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y255sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m cleaned_value\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "clean_cell('12.0 oz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/yanmy/raha/raha-master/detector.ipynb 单元格 130\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y330sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m dirty_cells \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39m16.0 oz.\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y330sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m'\u001b[39m\u001b[39m12.0 oz\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y330sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m'\u001b[39m\u001b[39m12.0 oz.\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y330sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m'\u001b[39m\u001b[39m16.0 oz.\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y330sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m'\u001b[39m\u001b[39m12.0 oz.\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y330sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m clean_cells \u001b[39m=\u001b[39m [clean_cell(cell) \u001b[39mfor\u001b[39;00m cell \u001b[39min\u001b[39;00m dirty_cells]\n",
      "\u001b[1;32m/home/yanmy/raha/raha-master/detector.ipynb 单元格 130\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y330sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m dirty_cells \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39m16.0 oz.\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y330sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m'\u001b[39m\u001b[39m12.0 oz\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y330sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m'\u001b[39m\u001b[39m12.0 oz.\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y330sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m'\u001b[39m\u001b[39m16.0 oz.\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y330sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m'\u001b[39m\u001b[39m12.0 oz.\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y330sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m clean_cells \u001b[39m=\u001b[39m [clean_cell(cell) \u001b[39mfor\u001b[39;00m cell \u001b[39min\u001b[39;00m dirty_cells]\n",
      "\u001b[1;32m/home/yanmy/raha/raha-master/detector.ipynb 单元格 130\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y330sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclean_cell\u001b[39m(cell):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y330sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mif\u001b[39;00m re\u001b[39m.\u001b[39msearch(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m^\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+(\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.?\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+)? oz\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.?$\u001b[39m\u001b[39m'\u001b[39m, cell\u001b[39m.\u001b[39;49mvalue):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y330sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m         cleaned_value \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(re\u001b[39m.\u001b[39msearch(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m^\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+(\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.?\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+)?\u001b[39m\u001b[39m'\u001b[39m, cell)\u001b[39m.\u001b[39mgroup(\u001b[39m0\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y330sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m cleaned_value\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "dirty_cells = ['16.0 oz.',\n",
    "'12.0 oz',\n",
    "'12.0 oz.',\n",
    "'16.0 oz.',\n",
    "'12.0 oz',\n",
    "'12.0 oz',\n",
    "'12.0 ounce',\n",
    "'12.0 ounce',\n",
    "'12.0 oz',\n",
    "'12.0 oz.',\n",
    "'12.0 oz.',\n",
    "'12.0 oz.',\n",
    "'12.0 oz.',\n",
    "'16.0 oz.',\n",
    "'12.0 oz',\n",
    "'12.0 ounce',\n",
    "'12.0 oz. Alumi-Tek',\n",
    "'12.0 oz.',\n",
    "'16.0 oz.',\n",
    "'12.0 oz.']\n",
    "\n",
    "clean_cells = [clean_cell(cell) for cell in dirty_cells]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['0.047%',\n",
       "  '0.068%',\n",
       "  '0.06%',\n",
       "  '0.057999999999999996%',\n",
       "  '0.06%',\n",
       "  '0.057999999999999996%',\n",
       "  '0.038%',\n",
       "  '0.042%',\n",
       "  '0.08%',\n",
       "  '0.065%',\n",
       "  '0.055%',\n",
       "  '0.048%',\n",
       "  '0.065%',\n",
       "  '0.053%',\n",
       "  '0.047%',\n",
       "  '0.065%',\n",
       "  '0.057999999999999996%',\n",
       "  '0.062%',\n",
       "  '0.057999999999999996%',\n",
       "  '0.069%'],\n",
       " ['0.047',\n",
       "  '0.068',\n",
       "  '0.06',\n",
       "  '0.058',\n",
       "  '0.06',\n",
       "  '0.058',\n",
       "  '0.038',\n",
       "  '0.042',\n",
       "  '0.08',\n",
       "  '0.065',\n",
       "  '0.055',\n",
       "  '0.048',\n",
       "  '0.065',\n",
       "  '0.053',\n",
       "  '0.047',\n",
       "  '0.065',\n",
       "  '0.058',\n",
       "  '0.062',\n",
       "  '0.058',\n",
       "  '0.069'])"
      ]
     },
     "execution_count": 717,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer_dirty.iloc[beer_label_index,5].to_list(),beer_clean.iloc[beer_label_index,5].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0.05', '0.066', '0.071', '0.09%', '0.075', '0.077', '0.045%',\n",
       "       '0.065', '0.055', '0.086', '0.072', '0.073', '0.069%', '0.085',\n",
       "       '0.061%', '0.06', '0.082', '0.099', '0.079', '0.044', '0.049',\n",
       "       '0.049%', '0.07', '0.07%', '0.097', '0.079%', '0.068%', '0.083',\n",
       "       '0.059', '0.035', '0.055%', '0.09', '0.069', '0.046%',\n",
       "       '0.052000000000000005%', '0.054', '0.084', '0.038', '0.042',\n",
       "       '0.045', '0.08', '0.125%', '0.04', '0.076', '0.051%', '0.065%',\n",
       "       '0.053', '0.052', '0.057', '0.043', '0.062',\n",
       "       '0.059000000000000004%', '0.055999999999999994%', '0.048%',\n",
       "       '0.056', '0.057999999999999996%', '0.057%', '0.058',\n",
       "       '0.054000000000000006%', '0.047%', '0.05%', '0.068', '0.085%',\n",
       "       '0.047', '0.06%', '0.092', '0.032', '0.048',\n",
       "       '0.08199999999999999%', '0.062%', '0.051', '0.08%', '0.064',\n",
       "       '0.063', '0.067', '0.061', '0.044000000000000004%', '0.038%',\n",
       "       '0.046', '0.067%', '0.088', '0.078%', '0.081', '0.095%', '0.041',\n",
       "       '0.099%', '0.098', '0.078', '0.073%', '', '0.096', '0.042%',\n",
       "       '0.064%', '0.066%', '0.040999999999999995%', '0.093', '0.095',\n",
       "       '0.092%', '0.053%', '0.043%', '0.039', '0.089',\n",
       "       '0.07200000000000001%', '0.075%', '0.063%', '0.074', '0.04%',\n",
       "       '0.027000000000000003%', '0.071%', '0.07400000000000001%',\n",
       "       '0.076%', '0.094', '0.087', '0.083%', '0.039%',\n",
       "       '0.09300000000000001%', '0.09699999999999999%', '0.037', '0.086%',\n",
       "       '0.034', '0.087%', '0.035%', '0.091%', '0.08900000000000001%',\n",
       "       '0.077%', '0.1', '0.096%', '0.12', '0.001%', '0.128%',\n",
       "       '0.10400000000000001%', '0.028'], dtype=object)"
      ]
     },
     "execution_count": 719,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer_dirty.iloc[:,5].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input ['16.0 oz.', '12.0 oz', '12.0 oz.', '12.0 ounce',\n",
    "        '12.0 oz. Alumi-Tek'] are some dirty cells from table Beers-ounce, and the input ['16',\n",
    "'12',\n",
    "'12',\n",
    "'12',\n",
    "'12'] are corresponding clean ones, please write a function to correct dirty cell to clean ones, and correct the following cells: ['12.0 oz', '12.0 oz.', '12.0 ounce', '12.0 OZ.',\n",
    "'12.0 oz. Alumi-Tek', '8.4 ounce', '16.0 ounce', '16.0 oz.',\n",
    "'16.0 oz. Alumi-Tek', '16.0 oz', '16.0 OZ.', '12.0 oz. Silo Can',\n",
    "'16.0 oz. Silo Can', '24.0 oz.', '24.0 oz', '19.2 oz.', '19.2 oz',\n",
    "'24.0 ounce', '24.0 oz. Alumi-Tek', '32.0 oz. Alumi-Tek',\n",
    "'32.0 OZ.', '32.0 oz.', '19.2 oz. Alumi-Tek', '19.2 OZ.',\n",
    "'16.9 OZ.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 12.0 oz\n",
      "Corrected: 12\n",
      "\n",
      "Original: 12.0 oz.\n",
      "Corrected: 12\n",
      "\n",
      "Original: 12.0 ounce\n",
      "Corrected: 12\n",
      "\n",
      "Original: 12.0 OZ.\n",
      "Corrected: 12\n",
      "\n",
      "Original: 12.0 oz. Alumi-Tek\n",
      "Corrected: 12\n",
      "\n",
      "Original: 8.4 ounce\n",
      "Corrected: 8.4\n",
      "\n",
      "Original: 16.0 ounce\n",
      "Corrected: 16\n",
      "\n",
      "Original: 16.0 oz.\n",
      "Corrected: 16\n",
      "\n",
      "Original: 16.0 oz. Alumi-Tek\n",
      "Corrected: 16\n",
      "\n",
      "Original: 16.0 oz\n",
      "Corrected: 16\n",
      "\n",
      "Original: 16.0 OZ.\n",
      "Corrected: 16\n",
      "\n",
      "Original: 12.0 oz. Silo Can\n",
      "Corrected: 12\n",
      "\n",
      "Original: 16.0 oz. Silo Can\n",
      "Corrected: 16\n",
      "\n",
      "Original: 24.0 oz.\n",
      "Corrected: 24\n",
      "\n",
      "Original: 24.0 oz\n",
      "Corrected: 24\n",
      "\n",
      "Original: 19.2 oz.\n",
      "Corrected: 19.2\n",
      "\n",
      "Original: 19.2 oz\n",
      "Corrected: 19.2\n",
      "\n",
      "Original: 24.0 ounce\n",
      "Corrected: 24\n",
      "\n",
      "Original: 24.0 oz. Alumi-Tek\n",
      "Corrected: 24\n",
      "\n",
      "Original: 32.0 oz. Alumi-Tek\n",
      "Corrected: 32\n",
      "\n",
      "Original: 32.0 OZ.\n",
      "Corrected: 32\n",
      "\n",
      "Original: 32.0 oz.\n",
      "Corrected: 32\n",
      "\n",
      "Original: 19.2 oz. Alumi-Tek\n",
      "Corrected: 19.2\n",
      "\n",
      "Original: 19.2 OZ.\n",
      "Corrected: 19.2\n",
      "\n",
      "Original: 16.9 OZ.\n",
      "Corrected: 16.9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def correct_dirty_cell(cell):\n",
    "    # Regular expression to extract the numeric value from the cell\n",
    "    match = re.search(r'(\\d+(\\.\\d+)?)', cell)\n",
    "    if match:\n",
    "        value = match.group(1)\n",
    "        # Convert to integer string if the value ends with \".0\"\n",
    "        if value.endswith('.0'):\n",
    "            return str(int(float(value)))\n",
    "        return value\n",
    "    return cell  # If no match is found, return the original cell\n",
    "\n",
    "# Test\n",
    "cells_to_correct = ['12.0 oz', '12.0 oz.', '12.0 ounce', '12.0 OZ.',\n",
    "'12.0 oz. Alumi-Tek', '8.4 ounce', '16.0 ounce', '16.0 oz.',\n",
    "'16.0 oz. Alumi-Tek', '16.0 oz', '16.0 OZ.', '12.0 oz. Silo Can',\n",
    "'16.0 oz. Silo Can', '24.0 oz.', '24.0 oz', '19.2 oz.', '19.2 oz',\n",
    "'24.0 ounce', '24.0 oz. Alumi-Tek', '32.0 oz. Alumi-Tek',\n",
    "'32.0 OZ.', '32.0 oz.', '19.2 oz. Alumi-Tek', '19.2 OZ.',\n",
    "'16.9 OZ.']\n",
    "\n",
    "for cell in cells_to_correct:\n",
    "    print(f\"Original: {cell}\")\n",
    "    print(f\"Corrected: {correct_dirty_cell(cell)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_cells(cell):\n",
    "    # Remove any extraneous characters (e.g. periods, spaces)\n",
    "    cell = cell.strip()\n",
    "\n",
    "    # Convert to lowercase\n",
    "    cell = cell.lower()\n",
    "\n",
    "    # Remove any units that are not 'oz' or 'ounce'\n",
    "    cell = re.sub(r'(?<!\\boz\\b|\\bone\\b)(\\d+(\\.\\d+)?)', r'\\1', cell)\n",
    "\n",
    "    # Convert to integer\n",
    "    cell = int(cell)\n",
    "\n",
    "    return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[correct_cells(c) for c in ['12.0 oz', '12.0 oz.', '12.0 ounce', '12.0 OZ.',\n",
    "'12.0 oz. Alumi-Tek', '8.4 ounce', '16.0 ounce', '16.0 oz.',\n",
    "'16.0 oz. Alumi-Tek', '16.0 oz', '16.0 OZ.', '12.0 oz. Silo Can',\n",
    "'16.0 oz. Silo Can', '24.0 oz.', '24.0 oz', '19.2 oz.', '19.2 oz',\n",
    "'24.0 ounce', '24.0 oz. Alumi-Tek', '32.0 oz. Alumi-Tek',\n",
    "'32.0 OZ.', '32.0 oz.', '19.2 oz. Alumi-Tek', '19.2 OZ.',\n",
    "'16.9 OZ.']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rayyan\n",
    "rayyan_clean = pd.read_csv('datasets/rayyan/clean.csv').fillna('')\n",
    "rayyan_dirty = pd.read_csv('datasets/rayyan/dirty.csv').fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "rayyan_clean = pd.read_csv('datasets/rayyan/clean.csv').fillna('')\n",
    "rayyan_dirty = pd.read_csv('datasets/rayyan/dirty.csv').fillna('')\n",
    "def Str2Int(row):\n",
    "    for index in range(11):\n",
    "        temp = row[index]\n",
    "        try:\n",
    "            row[index] = str(int(temp))\n",
    "        except:\n",
    "            continue\n",
    "    return row\n",
    "rayyan_clean = rayyan_clean.apply(Str2Int,axis=1)\n",
    "rayyan_dirty = rayyan_dirty.apply(Str2Int,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                                 659297\n",
       "article_title           How assistive technology use by individuals wi...\n",
       "article_language                                                      eng\n",
       "journal_title           American Journal Of Physical Medicine & Rehabi...\n",
       "jounral_abbreviation                                Am J Phys Med Rehabil\n",
       "journal_issn                                                    1537-7385\n",
       "article_jvolumn                                                        91\n",
       "article_jissue                                                         11\n",
       "article_jcreated_at                                               1/12/11\n",
       "article_pagination                                                 984-98\n",
       "author_list             {\"James Lenker\",\"W Ben Mortenson\",\"Frank DeRuy...\n",
       "Name: 52, dtype: object"
      ]
     },
     "execution_count": 798,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_clean.iloc[52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 946,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix_rayyan = np.array(rayyan_dirty!=rayyan_clean).astype(int)\n",
    "input_matrix_rayyan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "948"
      ]
     },
     "execution_count": 947,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix_rayyan.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1191,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_matrix_tax = np.array(tax_clean!=tax_dirty).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所选行的索引: [19039, 3555, 4677, 4682, 4684, 4694, 5952, 9924, 10273, 10304, 10347, 12263, 17614, 17657, 17730]\n"
     ]
    }
   ],
   "source": [
    "### From here, we find the label index\n",
    "import numpy as np\n",
    "\n",
    "def find_max_coverage(matrix):\n",
    "    # 转置矩阵以便按列计算列和\n",
    "    transposed_matrix = np.transpose(matrix)\n",
    "    \n",
    "    # 初始化一个列表，用于记录每列的和以及列的索引\n",
    "    column_sums = [(sum(column), index) for index, column in enumerate(transposed_matrix)]\n",
    "    \n",
    "    # 按列和降序排序\n",
    "    column_sums.sort(reverse=True)\n",
    "    \n",
    "    selected_rows = []\n",
    "    selected_columns = set()\n",
    "    \n",
    "    for _, column_index in column_sums:\n",
    "        # 如果所选列已经包含了这一列，跳过\n",
    "        if column_index in selected_columns:\n",
    "            continue\n",
    "        \n",
    "        # 找到可以添加的行\n",
    "        best_row = None\n",
    "        best_row_sum = -1\n",
    "        \n",
    "        for row_index, row in enumerate(matrix):\n",
    "            if row_index in selected_rows:\n",
    "                continue\n",
    "            \n",
    "            # 计算将此行添加到已选行中后的行之和\n",
    "            new_row_sum = sum(row)\n",
    "            \n",
    "            if new_row_sum > best_row_sum:\n",
    "                best_row_sum = new_row_sum\n",
    "                best_row = row_index\n",
    "        \n",
    "        # 如果找到了合适的行，添加它\n",
    "        if best_row is not None:\n",
    "            selected_rows.append(best_row)\n",
    "            selected_columns.update([column_index])\n",
    "            \n",
    "            # 如果已经选择的行数超过20，停止\n",
    "            if len(selected_rows) >= 20:\n",
    "                break\n",
    "    \n",
    "    return selected_rows\n",
    "\n",
    "# 示例用法\n",
    "if __name__ == \"__main__\":\n",
    "    # matrix = np.random.randint(2, size=(1000, 20))  # 随机生成一个1000x20的二进制矩阵\n",
    "    selected_rows_tax = find_max_coverage(input_matrix_tax)\n",
    "    print(\"所选行的索引:\", selected_rows_tax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([357, 130, 532, 212, 694, 924, 918])"
      ]
     },
     "execution_count": 778,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(selected_rows_rayyan)\n",
    "np.random.choice(np.where((pd.Series(input_matrix_rayyan.sum(axis=1))==4)==1)[0],7,replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[158, 455] 6\n",
    "[85, 118, 322, 384, 392, 615, 656, 796, 862, 975, 979] 5\n",
    "[357, 130, 532, 212, 694, 924, 918] 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 801,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_label_index = [158, 455,85, 118, 322, 384, 392, 615, 656, 796, 862, 975, 979, 357, 130, 532, 212, 694, 924, 918]\n",
    "len(rayyan_label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [],
   "source": [
    "rayyan_dirty.to_csv('datasets/rayyan/dirty_process.csv')\n",
    "rayyan_clean.to_csv('datasets/rayyan/clean_process.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [],
   "source": [
    "rayyan_dirty_process = rayyan_dirty\n",
    "rayyan_clean_process = rayyan_clean\n",
    "rayyan_dirty_process['index'] = rayyan_dirty_process.index\n",
    "rayyan_clean_process['index'] = rayyan_clean_process.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rayyan_dirty['index'] = rayyan_dirty['index'].astype(int)\n",
    "# hospital_clean['index'] = hospital_clean['index'].astype(int)\n",
    "rayyan_dirty_dict = rayyan_dirty_process.set_index('index').to_dict('index')\n",
    "rayyan_clean_dict = rayyan_clean_process.set_index('index').to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rayyan Detector Training\n",
    "\n",
    "input_matrix_select_rayyan = input_matrix_rayyan[rayyan_label_index]\n",
    "detector_list_rayyan = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "for label_tuple in rayyan_label_index:\n",
    "    for i in range(1,len(rayyan_clean.columns),1):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = rayyan_clean.iloc[label_tuple]\n",
    "        dirty_context = rayyan_dirty.iloc[label_tuple]\n",
    "        clean_cell = rayyan_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = rayyan_dirty.iloc[label_tuple,i]\n",
    "        single_context_clean = 'COL %s VAL %s ' % (rayyan_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (rayyan_clean.columns[i],dirty_cell)\n",
    "        for c in range(1,11,1):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (rayyan_clean.columns[c],rayyan_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (rayyan_dirty.columns[c],rayyan_dirty.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            detector_list_rayyan.append([all_context_dirty,single_context_dirty,1])\n",
    "            detector_list_rayyan.append([all_context_clean,single_context_clean,0])\n",
    "            detector_list_rayyan.append([all_context_dirty,single_context_clean,0])\n",
    "            # detector_list.append([single_context_dirty,1])\n",
    "        else:\n",
    "            detector_list_rayyan.append([all_context_dirty,single_context_dirty,0])\n",
    "            detector_list_rayyan.append([all_context_clean,single_context_clean,0])\n",
    "            detector_list_rayyan.append([all_context_dirty,single_context_clean,0])\n",
    "        # else:\n",
    "        #     detector_list.append([all_context_dirty,1])\n",
    "        #     detector_list.append([single_context_dirty,1])\n",
    "            \n",
    "                # break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_rayyan).to_csv('datasets/rayyan/detector/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 963,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd98757b5da344f39959f45412686422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_matrix_select_rayyan = input_matrix_rayyan[rayyan_label_index]\n",
    "detector_list_rayyan = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "# for label_tuple in tqdm(selected_rows):\n",
    "for label_tuple in tqdm(range(len(rayyan_clean))):\n",
    "    for i in range(1,len(rayyan_clean.columns)-1,1):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = rayyan_clean.iloc[label_tuple]\n",
    "        dirty_context = rayyan_dirty.iloc[label_tuple]\n",
    "        clean_cell = rayyan_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = rayyan_dirty.iloc[label_tuple,i]\n",
    "        single_context_clean = 'COL %s VAL %s ' % (rayyan_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (rayyan_clean.columns[i],dirty_cell)\n",
    "        for c in range(1,11,1):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (rayyan_clean.columns[c],rayyan_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (rayyan_dirty.columns[c],rayyan_dirty.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            detector_list_rayyan.append([all_context_dirty,single_context_dirty,1])\n",
    "        else:\n",
    "            detector_list_rayyan.append([all_context_dirty,single_context_dirty,0])\n",
    "            # detector_list.append([single_context_dirty,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 962,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "948"
      ]
     },
     "execution_count": 962,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(detector_list_rayyan)[2].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_rayyan).to_csv('datasets/rayyan/detector/test_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [],
   "source": [
    "rayyan_label_dirty = rayyan_dirty.iloc[rayyan_label_index]\n",
    "rayyan_label_clean = rayyan_clean.iloc[rayyan_label_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Paliativn�_ Schanzova osteotomie p��i nereponibiln�_ luxaci ky��eln�_ho kloubu u pacient�� s d��tskou mozkovou obrnou v adolescentn�_m v��u',\n",
       " 'Nowoczesne biomateria��y jako opatrunki hemostatyczne w chirurgii oszczedzajacej miazsz nerki--model zwierzecy. Doniesienie wstepne.',\n",
       " 'G̩riatrie et Psychologie Neuropsychiatrie du Vieillissement',\n",
       " 'Actas espa̱olas de psiquiatr�_a',\n",
       " 'Cirug�_a Pedi��trica: Organo Oficial De La Sociedad Espa̱ola De Cirug�_a Pedi��trica']"
      ]
     },
     "execution_count": 836,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_label_dirty[rayyan_label_dirty['journal_title']!=rayyan_label_clean['journal_title']]['journal_title'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Paliativn_ Schanzova osteotomie p_i nereponibiln_ luxaci kyeln_ho kloubu u pacient s d_tskou mozkovou obrnou v adolescentn_m v_u',\n",
       " 'Nowoczesne biomateria_y jako opatrunki hemostatyczne w chirurgii oszczedzajacej miazsz nerki--model zwierzecy. Doniesienie wstepne.',\n",
       " 'Griatrie et Psychologie Neuropsychiatrie du Vieillissement',\n",
       " 'Actas espaolas de psiquiatr_a',\n",
       " 'Cirug_a Peditrica: Organo Oficial De La Sociedad Espaola De Cirug_a Peditrica']"
      ]
     },
     "execution_count": 837,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_label_clean[rayyan_label_dirty['journal_title']!=rayyan_label_clean['journal_title']]['journal_title'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input ['Paliativn�_ Schanzova osteotomie p��i nereponibiln�_ luxaci ky��eln�_ho kloubu u pacient�� s d��tskou mozkovou obrnou v adolescentn�_m v��u',\n",
    " 'Nowoczesne biomateria��y jako opatrunki hemostatyczne w chirurgii oszczedzajacej miazsz nerki--model zwierzecy. Doniesienie wstepne.',\n",
    " 'G̩riatrie et Psychologie Neuropsychiatrie du Vieillissement',\n",
    " 'Actas espa̱olas de psiquiatr�_a',\n",
    " 'Cirug�_a Pedi��trica: Organo Oficial De La Sociedad Espa̱ola De Cirug�_a Pedi��trica'] are some dirty cells from table rayyan column journal_title, and the input ['Paliativn_ Schanzova osteotomie p_i nereponibiln_ luxaci kyeln_ho kloubu u pacient s d_tskou mozkovou obrnou v adolescentn_m v_u',\n",
    " 'Nowoczesne biomateria_y jako opatrunki hemostatyczne w chirurgii oszczedzajacej miazsz nerki--model zwierzecy. Doniesienie wstepne.',\n",
    " 'Griatrie et Psychologie Neuropsychiatrie du Vieillissement',\n",
    " 'Actas espaolas de psiquiatr_a',\n",
    " 'Cirug_a Peditrica: Organo Oficial De La Sociedad Espaola De Cirug_a Peditrica'] are corresponding corrected ones, please write a general function to detect whether a given cell is dirty or not, and correct the following dirty cells: ['12.0 oz', '12.0 oz.', '12.0 ounce', '12.0 OZ.',\n",
    "'12.0 oz. Alumi-Tek', '8.4 ounce', '16.0 ounce', '16.0 oz.',\n",
    "'16.0 oz. Alumi-Tek', '16.0 oz', '16.0 OZ.', '12.0 oz. Silo Can',\n",
    "'16.0 oz. Silo Can', '24.0 oz.', '24.0 oz', '19.2 oz.', '19.2 oz',\n",
    "'24.0 ounce', '24.0 oz. Alumi-Tek', '32.0 oz. Alumi-Tek',\n",
    "'32.0 OZ.', '32.0 oz.', '19.2 oz. Alumi-Tek', '19.2 OZ.',\n",
    "'16.9 OZ.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 905,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "def is_dirty_cell(cell):\n",
    "    # cell = row['journal_title']\n",
    "    # 设置脏字符黑名单，这些字符不在黑名单中被认为是干净的\n",
    "    dirty_chars = set([\"�\", \"̩\", \"̨\", \"̦\", \"̬\", \"̯\", \"̺\", \"̖\", \"̗\", \"̪\", \"̟\", \"̥\", \"̟\", \"̝\", \"̞\", \"̙\", \"̜\", \"̲\", \"̯\", \"̼\", \"̩\"])\n",
    "    \n",
    "    # 检查单元格中是否包含脏字符\n",
    "    for char in cell:\n",
    "        if char in dirty_chars:\n",
    "            return True\n",
    "    return False\n",
    "is_dirty_cell(\"Paliativn�_ Schanzova osteotomie p��i nereponibiln�_ luxaci ky��eln�_ho kloubu u pacient�� s d��tskou mozkovou obrnou v adolescentn�_m v��u\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,  14,   0,   9,   0,  12,  22,  53, 722,  32,  84])"
      ]
     },
     "execution_count": 883,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix_rayyan.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1000,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['52',\n",
       " '28',\n",
       " '0',\n",
       " '904',\n",
       " '24',\n",
       " '3',\n",
       " '88',\n",
       " '64',\n",
       " '8',\n",
       " '15',\n",
       " '90',\n",
       " '34',\n",
       " '34',\n",
       " '27',\n",
       " '42',\n",
       " '71',\n",
       " '0',\n",
       " '10',\n",
       " '71',\n",
       " '21',\n",
       " '70',\n",
       " '76',\n",
       " '91',\n",
       " '13',\n",
       " '90',\n",
       " '71',\n",
       " '18',\n",
       " '7',\n",
       " '0',\n",
       " '22']"
      ]
     },
     "execution_count": 1000,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_dirty.iloc[30:60,6].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1051,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The input \\n\\n['51-5', '70-6', '93-6', '3111-9', '91-8', '72-7', '71-7', '3541-4']\\n\\nare some dirty cells from table rayyan column article_pagination, and the input \\n\\n['May-51', 'Jun-70', 'Jun-93', '11-Sep', 'Aug-91', 'Jul-72', 'Jul-71', 'Apr-41']\\n\\n are corrected clean cells, and ['256-62', '1937-1958', '323-330', '196-203', '167-72', '3017-28', '373-6', '1182-7', '80-96', '603-10', '248-54', '277-80'] are examples of corresponding corrected clean cells. please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\""
      ]
     },
     "execution_count": 1051,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_name = rayyan_label_clean.columns[9]\n",
    "clean_list = rayyan_label_clean[rayyan_label_dirty[col_name]!=rayyan_label_clean[col_name]][col_name].to_list()\n",
    "dirty_list = rayyan_label_dirty[rayyan_label_dirty[col_name]!=rayyan_label_clean[col_name]][col_name].to_list()\n",
    "clean_list_origin = rayyan_label_clean[rayyan_label_dirty[col_name]==rayyan_label_clean[col_name]][col_name].to_list()\n",
    "# clean_list = rayyan_label_clean[col_name].to_list()\n",
    "# dirty_list = rayyan_label_dirty[col_name].to_list()\n",
    "# clean_list\n",
    "# detector_inference = \"The input \\n\\n%s\\n\\nare some dirty cells from table rayyan column %s, and the input \\n\\n%s\\n\\n are corresponding clean ones, please write a general function to detect whether a given cell is dirty or not\" % (dirty_list,col_name,clean_list)\n",
    "detector_inference = \"The input \\n\\n%s\\n\\nare some dirty cells from table rayyan column %s, and the input \\n\\n%s\\n\\n are corrected clean cells, and %s are examples of corresponding corrected clean cells. please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (dirty_list,col_name,clean_list,clean_list_origin)\n",
    "# print(detector_inference)\n",
    "detector_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1068,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1068,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_dirty_cell('1937-1958')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1077,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'05-51'"
      ]
     },
     "execution_count": 1077,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = '51-5'\n",
    "clean_dirty_cell(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1070,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Cells:\n",
      "Cell 1: May-51\n",
      "Cell 2: Jun-70\n",
      "Cell 3: Jun-93\n",
      "Cell 4: 11-Sep\n",
      "Cell 5: Aug-91\n",
      "Cell 6: Jul-72\n",
      "Cell 7: Jul-71\n",
      "Cell 8: Apr-41\n",
      "Cell 9: 1937-1958\n",
      "Cell 10: 05-51\n",
      "Cell 11: 06-70\n",
      "Cell 12: 06-93\n",
      "Cell 13: 09-3111\n",
      "Cell 14: 08-91\n",
      "Cell 15: 07-72\n",
      "Cell 16: 07-71\n",
      "Cell 17: 04-3541\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def is_dirty_cell(cell):\n",
    "    # Define a regular expression pattern for dirty cells (digits-digits)\n",
    "    dirty_pattern = r'^\\d+-\\d+$'\n",
    "    \n",
    "    # Use the re.match function to check if the cell matches the dirty pattern\n",
    "    return bool(re.match(dirty_pattern, cell))\n",
    "\n",
    "def clean_dirty_cell(dirty_cell):\n",
    "    # Try to clean the dirty cell\n",
    "    if is_dirty_cell(dirty_cell):\n",
    "        parts = dirty_cell.split('-')\n",
    "        if len(parts) == 2:\n",
    "            if parts[0].isdigit() and parts[1].isdigit():\n",
    "                return f\"{parts[1].zfill(2)}-{parts[0].zfill(2)}\"\n",
    "    \n",
    "    # If it cannot be cleaned, it's considered clean\n",
    "    return dirty_cell\n",
    "\n",
    "# Example usage:\n",
    "dirty_cells = ['51-5', '70-6', '93-6', '3111-9', '91-8', '72-7', '71-7', '3541-4']\n",
    "clean_cells = ['May-51', 'Jun-70', 'Jun-93', '11-Sep', 'Aug-91', 'Jul-72', 'Jul-71', 'Apr-41', '1937-1958']\n",
    "\n",
    "# Clean the dirty cells and append them to the clean cells list\n",
    "for dirty_cell in dirty_cells:\n",
    "    cleaned_cell = clean_dirty_cell(dirty_cell)\n",
    "    clean_cells.append(cleaned_cell)\n",
    "\n",
    "print(\"Clean Cells:\")\n",
    "for i, cell in enumerate(clean_cells):\n",
    "    print(f'Cell {i + 1}: {cell}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1078,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>article_title</th>\n",
       "      <th>article_language</th>\n",
       "      <th>journal_title</th>\n",
       "      <th>jounral_abbreviation</th>\n",
       "      <th>journal_issn</th>\n",
       "      <th>article_jvolumn</th>\n",
       "      <th>article_jissue</th>\n",
       "      <th>article_jcreated_at</th>\n",
       "      <th>article_pagination</th>\n",
       "      <th>author_list</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>729712</td>\n",
       "      <td>Long-term effects of spontaneous breathing dur...</td>\n",
       "      <td>English; ABSTRACT LANGUAGE:English</td>\n",
       "      <td>American journal of respiratory and critical c...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>164</td>\n",
       "      <td>1</td>\n",
       "      <td>7/1/01</td>\n",
       "      <td>43-49</td>\n",
       "      <td>{\"St��_ber F\",\"Mutz N\",\"Wrigge H\",\"Putensen C\"...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>660168</td>\n",
       "      <td>Fat and neurosurgery: does obesity affect outc...</td>\n",
       "      <td>eng</td>\n",
       "      <td>Neurosurgery</td>\n",
       "      <td>Neurosurgery</td>\n",
       "      <td>1524-4040</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>2/1/09</td>\n",
       "      <td>316-26; discussion 326-7</td>\n",
       "      <td>{\"R Loch Macdonald\",\"Farbod Asgarzadie-Gadim\",...</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>318931</td>\n",
       "      <td>Community associated methicillin-resistant Sta...</td>\n",
       "      <td></td>\n",
       "      <td>AIDS Reviews</td>\n",
       "      <td></td>\n",
       "      <td>1139-6121</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>1/1/10</td>\n",
       "      <td>153-163</td>\n",
       "      <td>{\"M. Pujol\",\"P. Barrag��n\",\"J.M. Tiraboschi\",\"...</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>349150</td>\n",
       "      <td>The relationship between capsulorhexis size an...</td>\n",
       "      <td>eng</td>\n",
       "      <td>Ophthalmic surgery and lasers</td>\n",
       "      <td>Ophthalmic Surg Lasers</td>\n",
       "      <td>1082-3069</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>3/1/99</td>\n",
       "      <td>185-90</td>\n",
       "      <td>{\"O Ceki̤\",\"C Batman\"}</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>826030</td>\n",
       "      <td>A cognitive behavior intervention program in w...</td>\n",
       "      <td>eng</td>\n",
       "      <td>European journal of cardiovascular nursing : j...</td>\n",
       "      <td>Eur J Cardiovasc Nurs</td>\n",
       "      <td>1474-5151</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1/1/12</td>\n",
       "      <td>183-9</td>\n",
       "      <td>{\"Cecilia Bj̦rkelund\",\"Margaretha Jerlock\"}</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>83012</td>\n",
       "      <td>Type I and II endometrial cancers: have they d...</td>\n",
       "      <td>eng</td>\n",
       "      <td>Journal of clinical oncology : official journa...</td>\n",
       "      <td>J. Clin. Oncol.</td>\n",
       "      <td>1527-7755</td>\n",
       "      <td>31</td>\n",
       "      <td>20</td>\n",
       "      <td>7/10/13</td>\n",
       "      <td>2607-18</td>\n",
       "      <td>{\"Chu Chen\",\"Brian L Strom\",\"Susan E McCann\",\"...</td>\n",
       "      <td>931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>134738</td>\n",
       "      <td>Are stretches effective in the prevention and ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1/1/12</td>\n",
       "      <td>261-270 ST  - Are stretches effective in the p...</td>\n",
       "      <td>{\"Stuart Calver\",\"Julia Nichols\",\"Rachel C1  -...</td>\n",
       "      <td>949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>215820</td>\n",
       "      <td>Preval̻ncia de bruxismo e dist̼rbio do sono em...</td>\n",
       "      <td>pt</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>159-166</td>\n",
       "      <td>{\"Ana Paula de Lima Ferreira\",\"Marcelo de Souz...</td>\n",
       "      <td>952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>575935</td>\n",
       "      <td>The association between urinary cortisol excre...</td>\n",
       "      <td>eng</td>\n",
       "      <td>Steroids</td>\n",
       "      <td>Steroids</td>\n",
       "      <td>1878-5867</td>\n",
       "      <td>101</td>\n",
       "      <td></td>\n",
       "      <td>9/1/15</td>\n",
       "      <td>71-7</td>\n",
       "      <td>{\"Kerstin Landin-Wilhelmsen\",\"G̦ran Oler̦d\",\"O...</td>\n",
       "      <td>975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>84783</td>\n",
       "      <td>Mild endoplasmic reticulum stress augments the...</td>\n",
       "      <td>eng</td>\n",
       "      <td>Endocrinology</td>\n",
       "      <td>Endocrinology</td>\n",
       "      <td>1945-7170</td>\n",
       "      <td>153</td>\n",
       "      <td>7</td>\n",
       "      <td>7/1/12</td>\n",
       "      <td>3017-28</td>\n",
       "      <td>{\"Michela Miani\",\"Decio L Eizirik\",\"Laurence L...</td>\n",
       "      <td>979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                      article_title  \\\n",
       "7    729712  Long-term effects of spontaneous breathing dur...   \n",
       "37   660168  Fat and neurosurgery: does obesity affect outc...   \n",
       "67   318931  Community associated methicillin-resistant Sta...   \n",
       "69   349150  The relationship between capsulorhexis size an...   \n",
       "71   826030  A cognitive behavior intervention program in w...   \n",
       "..      ...                                                ...   \n",
       "931   83012  Type I and II endometrial cancers: have they d...   \n",
       "949  134738  Are stretches effective in the prevention and ...   \n",
       "952  215820  Preval̻ncia de bruxismo e dist̼rbio do sono em...   \n",
       "975  575935  The association between urinary cortisol excre...   \n",
       "979   84783  Mild endoplasmic reticulum stress augments the...   \n",
       "\n",
       "                       article_language  \\\n",
       "7    English; ABSTRACT LANGUAGE:English   \n",
       "37                                  eng   \n",
       "67                                        \n",
       "69                                  eng   \n",
       "71                                  eng   \n",
       "..                                  ...   \n",
       "931                                 eng   \n",
       "949                                       \n",
       "952                                  pt   \n",
       "975                                 eng   \n",
       "979                                 eng   \n",
       "\n",
       "                                         journal_title  \\\n",
       "7    American journal of respiratory and critical c...   \n",
       "37                                        Neurosurgery   \n",
       "67                                        AIDS Reviews   \n",
       "69                       Ophthalmic surgery and lasers   \n",
       "71   European journal of cardiovascular nursing : j...   \n",
       "..                                                 ...   \n",
       "931  Journal of clinical oncology : official journa...   \n",
       "949                                                      \n",
       "952                                                      \n",
       "975                                           Steroids   \n",
       "979                                      Endocrinology   \n",
       "\n",
       "       jounral_abbreviation journal_issn article_jvolumn article_jissue  \\\n",
       "7                                                    164              1   \n",
       "37             Neurosurgery    1524-4040              64              2   \n",
       "67                             1139-6121              12              3   \n",
       "69   Ophthalmic Surg Lasers    1082-3069              30              3   \n",
       "71    Eur J Cardiovasc Nurs    1474-5151              11              2   \n",
       "..                      ...          ...             ...            ...   \n",
       "931         J. Clin. Oncol.    1527-7755              31             20   \n",
       "949                                                   17              0   \n",
       "952                                                   26              1   \n",
       "975                Steroids    1878-5867             101                  \n",
       "979           Endocrinology    1945-7170             153              7   \n",
       "\n",
       "    article_jcreated_at                                 article_pagination  \\\n",
       "7                7/1/01                                              43-49   \n",
       "37               2/1/09                           316-26; discussion 326-7   \n",
       "67               1/1/10                                            153-163   \n",
       "69               3/1/99                                             185-90   \n",
       "71               1/1/12                                              183-9   \n",
       "..                  ...                                                ...   \n",
       "931             7/10/13                                            2607-18   \n",
       "949              1/1/12  261-270 ST  - Are stretches effective in the p...   \n",
       "952                                                                159-166   \n",
       "975              9/1/15                                               71-7   \n",
       "979              7/1/12                                            3017-28   \n",
       "\n",
       "                                           author_list  index  \n",
       "7    {\"St��_ber F\",\"Mutz N\",\"Wrigge H\",\"Putensen C\"...      7  \n",
       "37   {\"R Loch Macdonald\",\"Farbod Asgarzadie-Gadim\",...     37  \n",
       "67   {\"M. Pujol\",\"P. Barrag��n\",\"J.M. Tiraboschi\",\"...     67  \n",
       "69                              {\"O Ceki̤\",\"C Batman\"}     69  \n",
       "71         {\"Cecilia Bj̦rkelund\",\"Margaretha Jerlock\"}     71  \n",
       "..                                                 ...    ...  \n",
       "931  {\"Chu Chen\",\"Brian L Strom\",\"Susan E McCann\",\"...    931  \n",
       "949  {\"Stuart Calver\",\"Julia Nichols\",\"Rachel C1  -...    949  \n",
       "952  {\"Ana Paula de Lima Ferreira\",\"Marcelo de Souz...    952  \n",
       "975  {\"Kerstin Landin-Wilhelmsen\",\"G̦ran Oler̦d\",\"O...    975  \n",
       "979  {\"Michela Miani\",\"Decio L Eizirik\",\"Laurence L...    979  \n",
       "\n",
       "[84 rows x 12 columns]"
      ]
     },
     "execution_count": 1078,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_dirty[rayyan_dirty.iloc[:,-2]!=rayyan_clean.iloc[:,-2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1038,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 1: Dirty\n",
      "Row 2: Dirty\n",
      "Row 3: Dirty\n",
      "Row 4: Dirty\n",
      "Row 5: Dirty\n",
      "Row 6: Dirty\n",
      "Row 7: Dirty\n",
      "Row 8: Dirty\n",
      "Row 9: Dirty\n",
      "Row 10: Dirty\n",
      "Row 11: Dirty\n",
      "Row 12: Dirty\n",
      "Row 13: Dirty\n",
      "Row 14: Dirty\n",
      "Row 15: Dirty\n",
      "Row 16: Dirty\n",
      "Row 17: Dirty\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def is_dirty_cell(cell):\n",
    "    # Define a regular expression pattern for the dirty cell format (M/D/YY)\n",
    "    dirty_pattern = r'^\\d{1,2}/\\d{1,2}/\\d{2}$'\n",
    "    \n",
    "    # Use the re.match function to check if the cell matches the dirty pattern\n",
    "    if re.match(dirty_pattern, cell):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_clean_cell(cell):\n",
    "    # Define a regular expression pattern for the clean cell format (M/DD/YY)\n",
    "    clean_pattern = r'^\\d{1,2}/\\d{2}/\\d{2}$'\n",
    "    \n",
    "    # Use the re.match function to check if the cell matches the clean pattern\n",
    "    if re.match(clean_pattern, cell):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def judge_cells(cells_list):\n",
    "    result = []\n",
    "    for pair in cells_list:\n",
    "        dirty_cell, clean_cell = pair\n",
    "        if is_dirty_cell(dirty_cell):\n",
    "            result.append('Dirty')\n",
    "        elif is_clean_cell(clean_cell):\n",
    "            result.append('Clean')\n",
    "        else:\n",
    "            result.append('Invalid')\n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "cells_list = [['1/1/13', '1/13/01'], ['1/1/07', '1/7/01'], ['1/1/08', '1/8/01'], ['5/1/13', '1/13/05'], ['1/1/09', '1/9/01'], ['7/1/08', '1/8/07'], ['2/6/14', '6/14/02'], ['10/1/08', '1/8/10'], ['1/1/15', '1/15/01'], ['9/1/15', '1/15/09'], ['7/1/12', '1/12/07'], ['7/1/08', '1/8/07'], ['1/1/13', '1/13/01'], ['1/1/02', '1/2/01'], ['8/1/06', '1/6/08'], ['4/1/15', '1/15/04'], ['3/1/02', '1/2/03']]\n",
    "\n",
    "results = judge_cells(cells_list)\n",
    "for i, result in enumerate(results):\n",
    "    print(f'Row {i + 1}: {result}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1042,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1042,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_dirty_cell('1/13/01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1037,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['1/1/13', '1/13/01'],\n",
       "       ['1/1/07', '1/7/01'],\n",
       "       ['1/1/08', '1/8/01'],\n",
       "       ['5/1/13', '1/13/05'],\n",
       "       ['1/1/09', '1/9/01'],\n",
       "       ['7/1/08', '1/8/07'],\n",
       "       ['2/6/14', '6/14/02'],\n",
       "       ['10/1/08', '1/8/10'],\n",
       "       ['1/1/15', '1/15/01'],\n",
       "       ['9/1/15', '1/15/09'],\n",
       "       ['7/1/12', '1/12/07'],\n",
       "       ['7/1/08', '1/8/07'],\n",
       "       ['1/1/13', '1/13/01'],\n",
       "       ['1/1/02', '1/2/01'],\n",
       "       ['8/1/06', '1/6/08'],\n",
       "       ['4/1/15', '1/15/04'],\n",
       "       ['3/1/02', '1/2/03']], dtype='<U7')"
      ]
     },
     "execution_count": 1037,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([dirty_cells,clean_cells]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1029,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dirty Cells: []\n",
      "Clean Cells: ['1/1/13', '1/1/07', '1/1/08', '5/1/13', '1/1/09', '7/1/08', '2/6/14', '10/1/08', '1/1/15', '9/1/15', '7/1/12', '7/1/08', '1/1/13', '1/1/02', '8/1/06', '4/1/15', '3/1/02']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def is_clean_date(date):\n",
    "    # Define a regular expression pattern for the clean date format (MM/DD/YY)\n",
    "    clean_pattern = r'^\\d{1,2}\\/\\d{1,2}\\/\\d{2}$'\n",
    "    \n",
    "    # Use the re.match function to check if the date matches the pattern\n",
    "    if re.match(clean_pattern, date):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Example usage:\n",
    "dates = ['1/1/13', '1/1/07', '1/1/08', '5/1/13', '1/1/09', '7/1/08', '2/6/14', '10/1/08', '1/1/15', '9/1/15', '7/1/12', '7/1/08', '1/1/13', '1/1/02', '8/1/06', '4/1/15', '3/1/02']\n",
    "\n",
    "dirty_cells = [date for date in dates if not is_clean_date(date)]\n",
    "clean_cells = [date for date in dates if is_clean_date(date)]\n",
    "\n",
    "print(\"Dirty Cells:\", dirty_cells)\n",
    "print(\"Clean Cells:\", clean_cells)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_pattern = r\"^(\\d{1,2})/(\\d{2,4})/(\\d{1,2})$\"\n",
    "re.match(clean_pattern, '1/1/07')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1/1/13' 是 dirty cell\n",
      "'1/1/07' 是 dirty cell\n",
      "'1/1/08' 是 dirty cell\n",
      "'5/1/13' 是 dirty cell\n",
      "'1/1/09' 是 dirty cell\n",
      "'1/13/01' 是 dirty cell\n",
      "'1/7/01' 是 dirty cell\n",
      "'1/8/01' 是 dirty cell\n",
      "'1/13/05' 是 dirty cell\n",
      "'1/9/01' 是 dirty cell\n",
      "'12/19/13' 是 clean cell\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def is_dirty_or_clean(cell):\n",
    "    # 定义正则表达式模式\n",
    "    pattern = r'^\\d{1,2}/\\d{1,2}/\\d{2}$'\n",
    "\n",
    "    # 使用正则表达式匹配\n",
    "    if re.match(pattern, cell):\n",
    "        if cell[1] == '/':\n",
    "            return \"dirty\"\n",
    "        else:\n",
    "            return \"clean\"\n",
    "    else:\n",
    "        return \"invalid\"\n",
    "\n",
    "# 示例用法\n",
    "cells = ['1/1/13', '1/1/07', '1/1/08', '5/1/13', '1/1/09', '1/13/01', '1/7/01', '1/8/01', '1/13/05', '1/9/01', '12/19/13']\n",
    "\n",
    "for cell in cells:\n",
    "    result = is_dirty_or_clean(cell)\n",
    "    if result == \"dirty\":\n",
    "        print(f\"'{cell}' 是 dirty cell\")\n",
    "    elif result == \"clean\":\n",
    "        print(f\"'{cell}' 是 clean cell\")\n",
    "    else:\n",
    "        print(f\"'{cell}' 无效\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1/13/01' is dirty\n",
      "'1/7/01' is dirty\n",
      "'1/8/01' is dirty\n",
      "'1/13/05' is dirty\n",
      "'1/9/01' is dirty\n",
      "'1/8/07' is dirty\n",
      "'6/14/02' is dirty\n",
      "'1/8/10' is dirty\n",
      "'1/15/01' is dirty\n",
      "'1/15/09' is dirty\n",
      "'1/12/07' is dirty\n",
      "'1/8/07' is dirty\n",
      "'1/13/01' is dirty\n",
      "'1/2/01' is dirty\n",
      "'1/6/08' is dirty\n",
      "'1/15/04' is dirty\n",
      "'1/2/03' is dirty\n",
      "'1/1/13' is dirty\n",
      "'1/1/07' is dirty\n",
      "'1/1/08' is dirty\n",
      "'5/1/13' is dirty\n",
      "'1/1/09' is dirty\n",
      "'7/1/08' is dirty\n",
      "'2/6/14' is dirty\n",
      "'10/1/08' is dirty\n",
      "'1/1/15' is dirty\n",
      "'9/1/15' is dirty\n",
      "'7/1/12' is dirty\n",
      "'7/1/08' is dirty\n",
      "'1/1/13' is dirty\n",
      "'1/1/02' is dirty\n",
      "'8/1/06' is dirty\n",
      "'4/1/15' is dirty\n",
      "'3/1/02' is dirty\n",
      "'' is dirty\n",
      "'' is dirty\n",
      "'12/19/13' is clean\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def is_dirty_date_cell(cell):\n",
    "    # Define a regular expression pattern for flexible date cells\n",
    "    clean_pattern = r'^(0[1-9]|1[0-2])/(0[1-9]|[12][0-9]|3[01])/\\d{2,4}$'\n",
    "    \n",
    "    # Check if the cell matches the clean pattern\n",
    "    if re.match(clean_pattern, cell):\n",
    "        return False  # The cell is clean\n",
    "    else:\n",
    "        return True  # The cell is dirty\n",
    "\n",
    "# Example cells\n",
    "dirty_cells = ['1/1/13', '1/1/07', '1/1/08', '5/1/13', '1/1/09', '7/1/08', '2/6/14', '10/1/08', '1/1/15', '9/1/15', '7/1/12', '7/1/08', '1/1/13', '1/1/02', '8/1/06', '4/1/15', '3/1/02', '', '', '12/19/13']\n",
    "clean_cells = ['1/13/01', '1/7/01', '1/8/01', '1/13/05', '1/9/01', '1/8/07', '6/14/02', '1/8/10', '1/15/01', '1/15/09', '1/12/07', '1/8/07', '1/13/01', '1/2/01', '1/6/08', '1/15/04', '1/2/03']\n",
    "\n",
    "# Test the function for the provided clean cells and dirty cells\n",
    "for cell in clean_cells:\n",
    "    if is_dirty_date_cell(cell):\n",
    "        print(f\"'{cell}' is dirty\")\n",
    "    else:\n",
    "        print(f\"'{cell}' is clean\")\n",
    "\n",
    "for cell in dirty_cells:\n",
    "    if is_dirty_date_cell(cell):\n",
    "        print(f\"'{cell}' is dirty\")\n",
    "    else:\n",
    "        print(f\"'{cell}' is clean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 998,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'-1' is dirty\n",
      "'-1' is dirty\n",
      "'-1' is dirty\n",
      "'4' is clean\n",
      "'2' is clean\n",
      "'0' is clean\n",
      "'3' is clean\n",
      "'3' is clean\n",
      "'1' is clean\n",
      "'3' is clean\n",
      "'7486' is clean\n",
      "'10' is clean\n",
      "'2' is clean\n",
      "'7' is clean\n",
      "'4' is clean\n",
      "'7' is clean\n",
      "'27' is clean\n",
      "'3' is clean\n",
      "'4' is clean\n",
      "'3' is clean\n",
      "'6' is clean\n",
      "'17' is clean\n",
      "'0' is clean\n",
      "'' is dirty\n",
      "'2' is clean\n",
      "'3' is clean\n",
      "'2' is clean\n",
      "'2' is clean\n",
      "'10' is clean\n",
      "'3' is clean\n",
      "'1' is clean\n",
      "'10' is clean\n",
      "'7' is clean\n",
      "'1' is clean\n",
      "'12' is clean\n",
      "'10' is clean\n",
      "'0' is clean\n",
      "'' is dirty\n",
      "'11' is clean\n",
      "'0' is clean\n",
      "'2' is clean\n",
      "'5' is clean\n",
      "'11' is clean\n",
      "'6' is clean\n",
      "'3' is clean\n",
      "'' is dirty\n",
      "'6' is clean\n",
      "'0' is clean\n",
      "'0' is clean\n",
      "'12' is clean\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def is_dirty_cell(cell):\n",
    "    # Define a regular expression pattern for clean cells (numeric values)\n",
    "    clean_pattern = r'^\\d+$'\n",
    "    \n",
    "    # Check if the cell matches the clean pattern\n",
    "    if re.match(clean_pattern, cell):\n",
    "        return False  # The cell is clean\n",
    "    else:\n",
    "        return True  # The cell is dirty\n",
    "\n",
    "# Example cells\n",
    "dirty_cells = ['', '', '']\n",
    "clean_cells = ['-1', '-1', '-1']\n",
    "additional_clean_cells = ['4', '2', '0', '3', '3', '1', '3', '7486', '10', '2', '7', '4', '7', '27', '3', '4', '3']\n",
    "\n",
    "# Test the function for the provided clean cells and dirty cells\n",
    "for cell in clean_cells:\n",
    "    if is_dirty_cell(cell):\n",
    "        print(f\"'{cell}' is dirty\")\n",
    "    else:\n",
    "        print(f\"'{cell}' is clean\")\n",
    "\n",
    "for cell in additional_clean_cells:\n",
    "    if is_dirty_cell(cell):\n",
    "        print(f\"'{cell}' is dirty\")\n",
    "    else:\n",
    "        print(f\"'{cell}' is clean\")\n",
    "\n",
    "for cell in rayyan_dirty.iloc[30:60,7].to_list():\n",
    "    if is_dirty_cell(cell):\n",
    "        print(f\"'{cell}' is dirty\")\n",
    "    else:\n",
    "        print(f\"'{cell}' is clean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def is_dirty_issn(cell):\n",
    "    # Define a regular expression pattern to match clean cells in the 'DD-Mon' or 'DD-Mon-YY' format\n",
    "    clean_pattern = r'^\\d{1,2}-[A-Za-z]{3}(?:-\\d{2})?$'\n",
    "    \n",
    "    # Use the regular expression to check if the cell matches the clean pattern\n",
    "    if re.match(clean_pattern, cell):\n",
    "        return False  # Cell is clean\n",
    "    else:\n",
    "        return True   # Cell is dirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 980,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Jan-15' is dirty\n",
      "'0370-0747' is clean\n",
      "'9788480000000' is dirty\n",
      "'2115-7863(Electronic);2115-8789(Print)' is clean\n",
      "'1578-2735' is clean\n",
      "'1473-0480' is clean\n",
      "'0214-1221' is clean\n",
      "'1476-4687' is clean\n",
      "'1460-2385' is clean\n",
      "'' is clean\n",
      "'1699-5198' is clean\n",
      "'1878-5867' is clean\n",
      "'1945-7170' is clean\n",
      "'0041-4301' is clean\n",
      "'Feb-14' is dirty\n",
      "'1873-7544' is clean\n",
      "'0041-5782 (Print) 0041-5782' is clean\n",
      "'0214-9915' is clean\n",
      "'1365-2346' is clean\n",
      "'Mar-09' is dirty\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def is_clean_cell(cell):\n",
    "    # Define regular expression patterns for clean cells\n",
    "    clean_patterns = [\n",
    "        r'^\\s*$',  # Empty string pattern\n",
    "        r'^\\d{4}-\\d{4}(\\(Electronic\\))?(;\\d{4}-\\d{4}(\\([A-Za-z]+\\))?)*$',  # Pattern with hyphens and optional (Electronic)\n",
    "        r'^\\d{4}-\\d{4}(\\s*\\(Print\\)\\s*\\d{4}-\\d{4})*$'  # Pattern with (Print)\n",
    "    ]\n",
    "\n",
    "    # Check if the cell matches any of the clean patterns\n",
    "    for pattern in clean_patterns:\n",
    "        if re.match(pattern, cell):\n",
    "            return False  # The cell is clean\n",
    "\n",
    "    return True  # The cell is not clean\n",
    "\n",
    "# Example cell\n",
    "cell = '0370-0747'\n",
    "\n",
    "# Test the function\n",
    "for cell in ['Jan-15', '0370-0747', '9788480000000', '2115-7863(Electronic);2115-8789(Print)', '1578-2735', '1473-0480', '0214-1221', '1476-4687', '1460-2385', '', '1699-5198', '1878-5867', '1945-7170', '0041-4301', 'Feb-14', '1873-7544', '0041-5782 (Print) 0041-5782', '0214-9915', '1365-2346', 'Mar-09']:\n",
    "    if is_clean_cell(cell):\n",
    "        print(f\"'{cell}' is dirty\")\n",
    "    else:\n",
    "        print(f\"'{cell}' is clean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 983,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 983,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for index,row in rayyan_dirty.iterrows():\n",
    "    if (is_clean_cell(row[5])):\n",
    "        count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 969,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Jan-15' is a dirty cell.\n",
      "'0370-0747' is a clean cell.\n",
      "'9788480000000' is a dirty cell.\n",
      "'2115-7863(Electronic);2115-8789(Print)' is a dirty cell.\n",
      "'1578-2735' is a clean cell.\n",
      "'1473-0480' is a clean cell.\n",
      "'0214-1221' is a clean cell.\n",
      "'1476-4687' is a clean cell.\n",
      "'1460-2385' is a clean cell.\n",
      "'' is a dirty cell.\n",
      "'1699-5198' is a clean cell.\n",
      "'1878-5867' is a clean cell.\n",
      "'1945-7170' is a clean cell.\n",
      "'0041-4301' is a clean cell.\n",
      "'Feb-14' is a dirty cell.\n",
      "'1873-7544' is a clean cell.\n",
      "'0041-5782 (Print) 0041-5782' is a dirty cell.\n",
      "'0214-9915' is a clean cell.\n",
      "'1365-2346' is a clean cell.\n",
      "'Mar-09' is a dirty cell.\n",
      "'15-Jan' is a dirty cell.\n",
      "'0370-0747' is a clean cell.\n",
      "'9790000000000' is a dirty cell.\n",
      "'2115-7863(Electronic);2115-8789(Print)' is a dirty cell.\n",
      "'1578-2735' is a clean cell.\n",
      "'1473-0480' is a clean cell.\n",
      "'0214-1221' is a clean cell.\n",
      "'1476-4687' is a clean cell.\n",
      "'1460-2385' is a clean cell.\n",
      "'' is a dirty cell.\n",
      "'1699-5198' is a clean cell.\n",
      "'1878-5867' is a clean cell.\n",
      "'1945-7170' is a clean cell.\n",
      "'0041-4301' is a clean cell.\n",
      "'14-Feb' is a dirty cell.\n",
      "'1873-7544' is a clean cell.\n",
      "'0041-5782 (Print) 0041-5782' is a dirty cell.\n",
      "'0214-9915' is a clean cell.\n",
      "'1365-2346' is a clean cell.\n",
      "'9-Mar' is a dirty cell.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def is_dirty_issn(cell):\n",
    "    # Define a regular expression pattern to match clean cells\n",
    "    clean_pattern = r'^\\d{4}-\\d{4}(?:\\(Electronic\\);?\\d{4}\\(Print\\))?$'\n",
    "    \n",
    "    # Use the regular expression to check if the cell matches the clean pattern\n",
    "    if re.match(clean_pattern, cell):\n",
    "        return False  # Cell is clean\n",
    "    else:\n",
    "        return True   # Cell is dirty\n",
    "\n",
    "# Test the function with your input examples\n",
    "dirty_cells = ['Jan-15', '0370-0747', '9788480000000', '2115-7863(Electronic);2115-8789(Print)', '1578-2735', '1473-0480', '0214-1221', '1476-4687', '1460-2385', '', '1699-5198', '1878-5867', '1945-7170', '0041-4301', 'Feb-14', '1873-7544', '0041-5782 (Print) 0041-5782', '0214-9915', '1365-2346', 'Mar-09']\n",
    "clean_cells = ['15-Jan', '0370-0747', '9790000000000', '2115-7863(Electronic);2115-8789(Print)', '1578-2735', '1473-0480', '0214-1221', '1476-4687', '1460-2385', '', '1699-5198', '1878-5867', '1945-7170', '0041-4301', '14-Feb', '1873-7544', '0041-5782 (Print) 0041-5782', '0214-9915', '1365-2346', '9-Mar']\n",
    "\n",
    "for cell in dirty_cells:\n",
    "    if is_dirty_issn(cell):\n",
    "        print(f\"'{cell}' is a dirty cell.\")\n",
    "    else:\n",
    "        print(f\"'{cell}' is a clean cell.\")\n",
    "\n",
    "for cell in clean_cells:\n",
    "    if is_dirty_issn(cell):\n",
    "        print(f\"'{cell}' is a dirty cell.\")\n",
    "    else:\n",
    "        print(f\"'{cell}' is a clean cell.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_dirty_issn('14-Feb')\n",
    "# ['journal_issn'].unique()\n",
    "# rayyan_dirty[rayyan_clean['journal_issn']!=rayyan_dirty['journal_issn']]\n",
    "rayyan_dirty['journal_issn'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1537-7385', '1537-7385')"
      ]
     },
     "execution_count": 915,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_dirty.iloc[52,5],rayyan_clean.iloc[52,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1049,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,  14,   0,   9,   0,  12,  22,  53, 722,  32,  84,   0])"
      ]
     },
     "execution_count": 1049,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix_rayyan.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1050,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_jcreated_at\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "913"
      ]
     },
     "execution_count": 1050,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "col_name = rayyan_label_clean.columns[8]\n",
    "print(col_name)\n",
    "# def is_dirty_cell(cell):\n",
    "#     # Define a regular expression pattern to match clean cells\n",
    "#     clean_pattern = r'^[a-zA-Z0-9\\s\\-.,()\\'\"\\[\\]]+$'\n",
    "    \n",
    "#     # Use the regular expression to check if the cell matches the clean pattern\n",
    "#     if re.match(clean_pattern, cell):\n",
    "#         return False  # Cell is clean\n",
    "#     else:\n",
    "#         return True   # Cell is dirty\n",
    "# is_dirty_cell(dirty_list[4])\n",
    "count = 0 \n",
    "pred = []\n",
    "truth = []\n",
    "for index,row in rayyan_dirty.iterrows():\n",
    "    if(is_dirty_cell(row[col_name])):\n",
    "        count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,  14,   0,   9,   0,  12,  22,  53, 722,  32,  84])"
      ]
     },
     "execution_count": 904,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix_rayyan.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check the detection result of rayyan\n",
    "detector_rayyan = np.load('datasets/rayyan/detector/detection.npy')\n",
    "detector_rayyan = detector_rayyan.reshape((1000,10))\n",
    "for i in [0,2,4,6]:\n",
    "    detector_rayyan[:,i] = 0 ## Remove all non-labelled values in rayyan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  4,  0,  5,  0,  4,  0,  3, 17,  8, 17])"
      ]
     },
     "execution_count": 924,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix_rayyan[rayyan_label_index].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7676767676767676, 0.10526315789473684, 0.1851400730816078, 99, 722)"
      ]
     },
     "execution_count": 951,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_matrix_rayyan\n",
    "i = 8 ## col index(without id col)\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score\n",
    "precision_score(y_pred=detector_rayyan[:,i],y_true=input_matrix_rayyan[:,i]),recall_score(y_pred=detector_rayyan[:,i],y_true=input_matrix_rayyan[:,i]),f1_score(y_pred=detector_rayyan[:,i],y_true=input_matrix_rayyan[:,i]),sum(detector_rayyan[:,i]),sum(input_matrix_rayyan[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1081,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 1081,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.where(detector_rayyan[:,i]==1),np.where(input_matrix_rayyan[:,i]==1)\n",
    "detector_rayyan[:,8].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1103,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rayyan Detector Training\n",
    "\n",
    "input_matrix_select_rayyan = input_matrix_rayyan[rayyan_label_index]\n",
    "detector_list_rayyan = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "for label_tuple in rayyan_label_index:\n",
    "    for i in range(1,len(rayyan_clean.columns)-1,1):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        col_name = rayyan_clean.columns[i]\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = rayyan_clean.iloc[label_tuple]\n",
    "        dirty_context = rayyan_dirty.iloc[label_tuple]\n",
    "        clean_cell = rayyan_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = rayyan_dirty.iloc[label_tuple,i]\n",
    "        single_context_clean = 'COL %s VAL %s ' % (rayyan_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (rayyan_clean.columns[i],dirty_cell)\n",
    "        for c in range(1,11,1):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (rayyan_clean.columns[c],rayyan_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (rayyan_dirty.columns[c],rayyan_dirty.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            # detector_list_rayyan.append([all_context_dirty,single_context_dirty,1])\n",
    "            # detector_list_rayyan.append([all_context_clean,single_context_clean,0])\n",
    "            # detector_list_rayyan.append([all_context_dirty,single_context_clean,0])\n",
    "            detector_list_rayyan.append([col_name,dirty_cell,1])\n",
    "            detector_list_rayyan.append([col_name,clean_cell,0])\n",
    "            # detector_list.append([single_context_dirty,1])\n",
    "        else:\n",
    "            # detector_list_rayyan.append([all_context_dirty,single_context_dirty,0])\n",
    "            # detector_list_rayyan.append([all_context_clean,single_context_clean,0])\n",
    "            # detector_list_rayyan.append([all_context_dirty,single_context_clean,0])\n",
    "            detector_list_rayyan.append([col_name,dirty_cell,0])\n",
    "            detector_list_rayyan.append([col_name,clean_cell,0])\n",
    "        # else:\n",
    "        #     detector_list.append([all_context_dirty,1])\n",
    "        #     detector_list.append([single_context_dirty,1])\n",
    "            \n",
    "                # break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b274111091c7442593a2be32aa442ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_matrix_select_rayyan = input_matrix_rayyan[rayyan_label_index]\n",
    "detector_list_rayyan = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "# for label_tuple in tqdm(selected_rows):\n",
    "for label_tuple in tqdm(range(len(rayyan_clean))):\n",
    "    for i in range(1,len(rayyan_clean.columns)-1,1):\n",
    "        col_name = rayyan_clean.columns[i]\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = rayyan_clean.iloc[label_tuple]\n",
    "        dirty_context = rayyan_dirty.iloc[label_tuple]\n",
    "        clean_cell = rayyan_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = rayyan_dirty.iloc[label_tuple,i]\n",
    "        single_context_clean = 'COL %s VAL %s ' % (rayyan_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (rayyan_clean.columns[i],dirty_cell)\n",
    "        for c in range(1,11,1):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (rayyan_clean.columns[c],rayyan_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (rayyan_dirty.columns[c],rayyan_dirty.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            detector_list_rayyan.append([col_name,dirty_cell,1])\n",
    "        else:\n",
    "            detector_list_rayyan.append([col_name,dirty_cell,0])\n",
    "            # detector_list.append([single_context_dirty,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article_title           40\n",
       "article_language        40\n",
       "journal_title           40\n",
       "jounral_abbreviation    40\n",
       "journal_issn            40\n",
       "article_jvolumn         40\n",
       "article_jissue          40\n",
       "article_jcreated_at     40\n",
       "article_pagination      40\n",
       "author_list             40\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 1104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(detector_list_rayyan)[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1102,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_rayyan).to_csv('datasets/rayyan/detector/test_cell.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[158,\n",
       " 455,\n",
       " 85,\n",
       " 118,\n",
       " 322,\n",
       " 384,\n",
       " 392,\n",
       " 615,\n",
       " 656,\n",
       " 796,\n",
       " 862,\n",
       " 975,\n",
       " 979,\n",
       " 357,\n",
       " 130,\n",
       " 532,\n",
       " 212,\n",
       " 694,\n",
       " 924,\n",
       " 918]"
      ]
     },
     "execution_count": 1106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rayyan_label_cell = \n",
    "input_matrix_rayyan[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1124,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Only the Dirty Datas\n",
    "training_list_label = []\n",
    "for d in rayyan_label_index:\n",
    "    index = d\n",
    "    # i = d[1] + 1 ## 重要，我们在detector中忽略了index列\n",
    "    for i in range(1,11,1):\n",
    "        dirty_cell = rayyan_dirty.iloc[index,i]\n",
    "        clean_cell = rayyan_clean.iloc[index,i]\n",
    "        col_name = rayyan_clean.columns[i]\n",
    "        template_dict = {}\n",
    "        template_dict[col_name] = ''\n",
    "        temp_dict = rayyan_dirty.iloc[index,1:-1].to_dict()\n",
    "        clean_dict = {}\n",
    "        clean_dict[col_name] = clean_cell\n",
    "        coreset_reference = np.random.choice([c for c in rayyan_label_index if c!=index],3,replace=False)\n",
    "        # if(dirty_cell!=clean_cell): ## 在LLM Inference中忽略\n",
    "        text_head = 'You are an expert in cleaning Rayyan Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\nTake these rows as reference:\\n\\n' % (col_name, json.dumps(template_dict), json.dumps(temp_dict))\n",
    "        dict_0 = rayyan_clean.iloc[coreset_reference[0],1:].to_dict()\n",
    "        # dict_1 = rayyan_clean.iloc[coreset_reference[1],1:].to_dict()\n",
    "        # dict_2 = rayyan_clean.iloc[coreset_reference[2],1:].to_dict()\n",
    "        # ICL_text = '%s\\n\\n%s\\n\\n%s\\n\\n' % (json.dumps(dict_0),json.dumps(dict_1),json.dumps(dict_2))\n",
    "        ICL_text = '%s\\n\\n' % (json.dumps(dict_0))\n",
    "        training_list_label.append([text_head,ICL_text,'',json.dumps(clean_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1132,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For test in All Data Cells,\n",
    "training_list_label = []\n",
    "for d in range(1000):\n",
    "    index = d\n",
    "    # i = d[1] + 1 ## 重要，我们在detector中忽略了index列\n",
    "    for i in range(1,11,1):\n",
    "        dirty_cell = rayyan_dirty.iloc[index,i]\n",
    "        clean_cell = rayyan_clean.iloc[index,i]\n",
    "        col_name = rayyan_clean.columns[i]\n",
    "        template_dict = {}\n",
    "        template_dict[col_name] = ''\n",
    "        temp_dict = rayyan_dirty.iloc[index,1:-1].to_dict()\n",
    "        clean_dict = {}\n",
    "        clean_dict[col_name] = clean_cell\n",
    "        coreset_reference = np.random.choice([c for c in rayyan_label_index if c!=index],3,replace=False)\n",
    "        # if(dirty_cell!=clean_cell): ## 在LLM Inference中忽略\n",
    "        text_head = 'You are an expert in cleaning Rayyan Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\nTake these rows as reference:\\n\\n' % (col_name, json.dumps(template_dict), json.dumps(temp_dict))\n",
    "        dict_0 = rayyan_clean.iloc[coreset_reference[0],1:].to_dict()\n",
    "        # dict_1 = rayyan_clean.iloc[coreset_reference[1],1:].to_dict()\n",
    "        # dict_2 = rayyan_clean.iloc[coreset_reference[2],1:].to_dict()\n",
    "        # ICL_text = '%s\\n\\n%s\\n\\n%s\\n\\n' % (json.dumps(dict_0),json.dumps(dict_1),json.dumps(dict_2))\n",
    "        ICL_text = '%s\\n\\n' % (json.dumps(dict_0))\n",
    "        training_list_label.append([text_head,ICL_text,'',json.dumps(clean_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pd.DataFrame(training_list_label).iloc[49,0]),print(pd.DataFrame(training_list_label).iloc[49,-1])\n",
    "training_list_pd = pd.DataFrame(training_list_label)\n",
    "training_list_pd['instruction'] = training_list_pd[0] + training_list_pd[1]\n",
    "training_list_pd['input'] = training_list_pd[2]\n",
    "training_list_pd['output'] = training_list_pd[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td>{\"article_title\": \"Effet d'une intervention no...</td>\n",
       "      <td></td>\n",
       "      <td>{\"article_title\": \"Late repair of injuries of ...</td>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td></td>\n",
       "      <td>{\"article_title\": \"Late repair of injuries of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td>{\"article_title\": \"Amitriptyline, minocycline ...</td>\n",
       "      <td></td>\n",
       "      <td>{\"article_language\": \"eng\"}</td>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td></td>\n",
       "      <td>{\"article_language\": \"eng\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td>{\"article_title\": \"Amitriptyline, minocycline ...</td>\n",
       "      <td></td>\n",
       "      <td>{\"journal_title\": \"Proc R Soc Med\"}</td>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td></td>\n",
       "      <td>{\"journal_title\": \"Proc R Soc Med\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td>{\"article_title\": \"Child social skills trainin...</td>\n",
       "      <td></td>\n",
       "      <td>{\"jounral_abbreviation\": \"Proceedings of the R...</td>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td></td>\n",
       "      <td>{\"jounral_abbreviation\": \"Proceedings of the R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td>{\"article_title\": \"Osteoid osteoma in a 16-yea...</td>\n",
       "      <td></td>\n",
       "      <td>{\"journal_issn\": \"0035-9157 (Print) 0035-9157\"}</td>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td></td>\n",
       "      <td>{\"journal_issn\": \"0035-9157 (Print) 0035-9157\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td>{\"article_title\": \"[Osteoporosis prevention in...</td>\n",
       "      <td></td>\n",
       "      <td>{\"article_jvolumn\": \"13\"}</td>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td></td>\n",
       "      <td>{\"article_jvolumn\": \"13\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td>{\"article_title\": \"Bruxismo em crian_as Bruxis...</td>\n",
       "      <td></td>\n",
       "      <td>{\"article_jissue\": \"1\"}</td>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td></td>\n",
       "      <td>{\"article_jissue\": \"1\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td>{\"article_title\": \"Mild endoplasmic reticulum ...</td>\n",
       "      <td></td>\n",
       "      <td>{\"article_jcreated_at\": \"1/12/01\"}</td>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td></td>\n",
       "      <td>{\"article_jcreated_at\": \"1/12/01\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td>{\"article_title\": \"Bruxismo em crian_as Bruxis...</td>\n",
       "      <td></td>\n",
       "      <td>{\"article_pagination\": \"71-80\"}</td>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td></td>\n",
       "      <td>{\"article_pagination\": \"71-80\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td>{\"article_title\": \"Effet d'une intervention no...</td>\n",
       "      <td></td>\n",
       "      <td>{\"author_list\": \"{\\\"S. Iannazzo\\\",\\\"M. Miravit...</td>\n",
       "      <td>You are an expert in cleaning Rayyan Dataset. ...</td>\n",
       "      <td></td>\n",
       "      <td>{\"author_list\": \"{\\\"S. Iannazzo\\\",\\\"M. Miravit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0  \\\n",
       "0     You are an expert in cleaning Rayyan Dataset. ...   \n",
       "1     You are an expert in cleaning Rayyan Dataset. ...   \n",
       "2     You are an expert in cleaning Rayyan Dataset. ...   \n",
       "3     You are an expert in cleaning Rayyan Dataset. ...   \n",
       "4     You are an expert in cleaning Rayyan Dataset. ...   \n",
       "...                                                 ...   \n",
       "9995  You are an expert in cleaning Rayyan Dataset. ...   \n",
       "9996  You are an expert in cleaning Rayyan Dataset. ...   \n",
       "9997  You are an expert in cleaning Rayyan Dataset. ...   \n",
       "9998  You are an expert in cleaning Rayyan Dataset. ...   \n",
       "9999  You are an expert in cleaning Rayyan Dataset. ...   \n",
       "\n",
       "                                                      1 2   \\\n",
       "0     {\"article_title\": \"Effet d'une intervention no...      \n",
       "1     {\"article_title\": \"Amitriptyline, minocycline ...      \n",
       "2     {\"article_title\": \"Amitriptyline, minocycline ...      \n",
       "3     {\"article_title\": \"Child social skills trainin...      \n",
       "4     {\"article_title\": \"Osteoid osteoma in a 16-yea...      \n",
       "...                                                 ... ..   \n",
       "9995  {\"article_title\": \"[Osteoporosis prevention in...      \n",
       "9996  {\"article_title\": \"Bruxismo em crian_as Bruxis...      \n",
       "9997  {\"article_title\": \"Mild endoplasmic reticulum ...      \n",
       "9998  {\"article_title\": \"Bruxismo em crian_as Bruxis...      \n",
       "9999  {\"article_title\": \"Effet d'une intervention no...      \n",
       "\n",
       "                                                      3  \\\n",
       "0     {\"article_title\": \"Late repair of injuries of ...   \n",
       "1                           {\"article_language\": \"eng\"}   \n",
       "2                   {\"journal_title\": \"Proc R Soc Med\"}   \n",
       "3     {\"jounral_abbreviation\": \"Proceedings of the R...   \n",
       "4       {\"journal_issn\": \"0035-9157 (Print) 0035-9157\"}   \n",
       "...                                                 ...   \n",
       "9995                          {\"article_jvolumn\": \"13\"}   \n",
       "9996                            {\"article_jissue\": \"1\"}   \n",
       "9997                 {\"article_jcreated_at\": \"1/12/01\"}   \n",
       "9998                    {\"article_pagination\": \"71-80\"}   \n",
       "9999  {\"author_list\": \"{\\\"S. Iannazzo\\\",\\\"M. Miravit...   \n",
       "\n",
       "                                            instruction input  \\\n",
       "0     You are an expert in cleaning Rayyan Dataset. ...         \n",
       "1     You are an expert in cleaning Rayyan Dataset. ...         \n",
       "2     You are an expert in cleaning Rayyan Dataset. ...         \n",
       "3     You are an expert in cleaning Rayyan Dataset. ...         \n",
       "4     You are an expert in cleaning Rayyan Dataset. ...         \n",
       "...                                                 ...   ...   \n",
       "9995  You are an expert in cleaning Rayyan Dataset. ...         \n",
       "9996  You are an expert in cleaning Rayyan Dataset. ...         \n",
       "9997  You are an expert in cleaning Rayyan Dataset. ...         \n",
       "9998  You are an expert in cleaning Rayyan Dataset. ...         \n",
       "9999  You are an expert in cleaning Rayyan Dataset. ...         \n",
       "\n",
       "                                                 output  \n",
       "0     {\"article_title\": \"Late repair of injuries of ...  \n",
       "1                           {\"article_language\": \"eng\"}  \n",
       "2                   {\"journal_title\": \"Proc R Soc Med\"}  \n",
       "3     {\"jounral_abbreviation\": \"Proceedings of the R...  \n",
       "4       {\"journal_issn\": \"0035-9157 (Print) 0035-9157\"}  \n",
       "...                                                 ...  \n",
       "9995                          {\"article_jvolumn\": \"13\"}  \n",
       "9996                            {\"article_jissue\": \"1\"}  \n",
       "9997                 {\"article_jcreated_at\": \"1/12/01\"}  \n",
       "9998                    {\"article_pagination\": \"71-80\"}  \n",
       "9999  {\"author_list\": \"{\\\"S. Iannazzo\\\",\\\"M. Miravit...  \n",
       "\n",
       "[10000 rows x 7 columns]"
      ]
     },
     "execution_count": 1134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_list_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1135,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(training_list_pd.iloc[:,-3:].to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/rayyan/rayyan-test.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2118/4042012506.py:3: DtypeWarning: Columns (12,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tax_dirty = pd.read_csv('datasets/tax/dirty.csv').fillna('').astype(str)\n"
     ]
    }
   ],
   "source": [
    "### Tax Data\n",
    "tax_clean = pd.read_csv('datasets/tax/clean.csv').fillna('').astype(str)\n",
    "tax_dirty = pd.read_csv('datasets/tax/dirty.csv').fillna('').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0,1,5,6,7,8,9,12,14]\n",
    "0,1 ''\n",
    "5 6 -*\n",
    "7 zip-FD\n",
    "8,9 Marriage/Children not fixable?\n",
    "12,14 -*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_name</th>\n",
       "      <th>l_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>area_code</th>\n",
       "      <th>phone</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>has_child</th>\n",
       "      <th>salary</th>\n",
       "      <th>rate</th>\n",
       "      <th>single_exemp</th>\n",
       "      <th>married_exemp</th>\n",
       "      <th>child_exemp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17614</th>\n",
       "      <td>Peing</td>\n",
       "      <td>Bitter</td>\n",
       "      <td>M</td>\n",
       "      <td>251</td>\n",
       "      <td>848-9280</td>\n",
       "      <td>BIRMINGHAM</td>\n",
       "      <td>AL</td>\n",
       "      <td>35295</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>85000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17657</th>\n",
       "      <td>Chaomei</td>\n",
       "      <td>Bryce</td>\n",
       "      <td>F</td>\n",
       "      <td>334</td>\n",
       "      <td>518-3841</td>\n",
       "      <td>WOODLAND</td>\n",
       "      <td>AL</td>\n",
       "      <td>36280</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>25000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17727</th>\n",
       "      <td>Furio</td>\n",
       "      <td>Namjoshi</td>\n",
       "      <td>F</td>\n",
       "      <td>334</td>\n",
       "      <td>600-7468</td>\n",
       "      <td>PINCKARD</td>\n",
       "      <td>AL</td>\n",
       "      <td>36371</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17730</th>\n",
       "      <td>Atsuyuki</td>\n",
       "      <td>Gohring</td>\n",
       "      <td>F</td>\n",
       "      <td>334</td>\n",
       "      <td>739-5652</td>\n",
       "      <td>BIRMINGHAM</td>\n",
       "      <td>AL</td>\n",
       "      <td>35238</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>65000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17835</th>\n",
       "      <td>Suraj</td>\n",
       "      <td>Beilner</td>\n",
       "      <td>F</td>\n",
       "      <td>205</td>\n",
       "      <td>727-2792</td>\n",
       "      <td>ANNISTON</td>\n",
       "      <td>AL</td>\n",
       "      <td>36207</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>15000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39167</th>\n",
       "      <td>Munehiko</td>\n",
       "      <td>Bauknecht</td>\n",
       "      <td>M</td>\n",
       "      <td>205</td>\n",
       "      <td>495-7113</td>\n",
       "      <td>LAWLEY</td>\n",
       "      <td>AL</td>\n",
       "      <td>36793</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>85000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39173</th>\n",
       "      <td>Kimaya</td>\n",
       "      <td>Favero</td>\n",
       "      <td>M</td>\n",
       "      <td>256</td>\n",
       "      <td>398-3478</td>\n",
       "      <td>ELBERTA</td>\n",
       "      <td>AL</td>\n",
       "      <td>36530</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39180</th>\n",
       "      <td>Horia</td>\n",
       "      <td>Guth</td>\n",
       "      <td>M</td>\n",
       "      <td>256</td>\n",
       "      <td>383-8287</td>\n",
       "      <td>CHUNCHULA</td>\n",
       "      <td>AL</td>\n",
       "      <td>36521</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>10000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39182</th>\n",
       "      <td>Kern</td>\n",
       "      <td>Polt</td>\n",
       "      <td>F</td>\n",
       "      <td>334</td>\n",
       "      <td>908-2662</td>\n",
       "      <td>BIRMINGHAM</td>\n",
       "      <td>AL</td>\n",
       "      <td>35208</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>65000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39254</th>\n",
       "      <td>Benoit</td>\n",
       "      <td>Wadel</td>\n",
       "      <td>M</td>\n",
       "      <td>334</td>\n",
       "      <td>872-3922</td>\n",
       "      <td>WEAVER</td>\n",
       "      <td>AL</td>\n",
       "      <td>36277</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>95000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         f_name     l_name gender area_code     phone        city state  \\\n",
       "17614     Peing     Bitter      M       251  848-9280  BIRMINGHAM    AL   \n",
       "17657   Chaomei      Bryce      F       334  518-3841    WOODLAND    AL   \n",
       "17727     Furio   Namjoshi      F       334  600-7468    PINCKARD    AL   \n",
       "17730  Atsuyuki    Gohring      F       334  739-5652  BIRMINGHAM    AL   \n",
       "17835     Suraj    Beilner      F       205  727-2792    ANNISTON    AL   \n",
       "...         ...        ...    ...       ...       ...         ...   ...   \n",
       "39167  Munehiko  Bauknecht      M       205  495-7113      LAWLEY    AL   \n",
       "39173    Kimaya     Favero      M       256  398-3478     ELBERTA    AL   \n",
       "39180     Horia       Guth      M       256  383-8287   CHUNCHULA    AL   \n",
       "39182      Kern       Polt      F       334  908-2662  BIRMINGHAM    AL   \n",
       "39254    Benoit      Wadel      M       334  872-3922      WEAVER    AL   \n",
       "\n",
       "         zip marital_status has_child salary rate single_exemp married_exemp  \\\n",
       "17614  35295              M         N  85000  5.0         1500             0   \n",
       "17657  36280              M         N  25000  5.0         1500             0   \n",
       "17727  36371              M         N   5000  5.0         1500             0   \n",
       "17730  35238              M         N  65000  5.0         1500             0   \n",
       "17835  36207              M         N  15000  5.0         1500             0   \n",
       "...      ...            ...       ...    ...  ...          ...           ...   \n",
       "39167  36793              M         N  85000  5.0         1500             0   \n",
       "39173  36530              M         Y  40000  5.0         1500             0   \n",
       "39180  36521              M         N  10000  5.0         1500             0   \n",
       "39182  35208              M         N  65000  5.0         1500             0   \n",
       "39254  36277              M         N  95000  5.0         1500             0   \n",
       "\n",
       "      child_exemp  \n",
       "17614         300  \n",
       "17657         300  \n",
       "17727           0  \n",
       "17730         300  \n",
       "17835           0  \n",
       "...           ...  \n",
       "39167           0  \n",
       "39173         300  \n",
       "39180           0  \n",
       "39182           0  \n",
       "39254           0  \n",
       "\n",
       "[200 rows x 15 columns]"
      ]
     },
     "execution_count": 730,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 8\n",
    "tax_dirty[tax_clean.iloc[:,i]!=tax_dirty.iloc[:,i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AK-*    199\n",
       "AK      163\n",
       "NJ        1\n",
       "Name: state, dtype: int64"
      ]
     },
     "execution_count": 1373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_dirty[tax_dirty['city']=='ANCHORAGE']['state'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f_name            272\n",
       "l_name            694\n",
       "gender              0\n",
       "area_code           0\n",
       "phone               0\n",
       "city              200\n",
       "state             600\n",
       "zip               400\n",
       "marital_status    200\n",
       "has_child         200\n",
       "salary              0\n",
       "rate                0\n",
       "single_exemp      200\n",
       "married_exemp       0\n",
       "child_exemp       200\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tax_clean!=tax_dirty).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_name</th>\n",
       "      <th>l_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>area_code</th>\n",
       "      <th>phone</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>has_child</th>\n",
       "      <th>salary</th>\n",
       "      <th>rate</th>\n",
       "      <th>single_exemp</th>\n",
       "      <th>married_exemp</th>\n",
       "      <th>child_exemp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>Shalosh</td>\n",
       "      <td>Vrancken</td>\n",
       "      <td>F</td>\n",
       "      <td>205</td>\n",
       "      <td>471-4927</td>\n",
       "      <td>HATCHECHUBBEE</td>\n",
       "      <td>AL</td>\n",
       "      <td>36858</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>65000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2029</th>\n",
       "      <td>Jeane</td>\n",
       "      <td>Ballinger</td>\n",
       "      <td>F</td>\n",
       "      <td>251</td>\n",
       "      <td>997-4661</td>\n",
       "      <td>GROVE HILL</td>\n",
       "      <td>AL</td>\n",
       "      <td>36451</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>20000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2097</th>\n",
       "      <td>Rong</td>\n",
       "      <td>Cummings</td>\n",
       "      <td>M</td>\n",
       "      <td>251</td>\n",
       "      <td>952-3637</td>\n",
       "      <td>NOTASULGA</td>\n",
       "      <td>AL</td>\n",
       "      <td>36866</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2173</th>\n",
       "      <td>Lois</td>\n",
       "      <td>Fontana</td>\n",
       "      <td>M</td>\n",
       "      <td>334</td>\n",
       "      <td>869-6752</td>\n",
       "      <td>MILLRY</td>\n",
       "      <td>AL</td>\n",
       "      <td>36558</td>\n",
       "      <td>S</td>\n",
       "      <td>Y</td>\n",
       "      <td>40000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2273</th>\n",
       "      <td>Dannz</td>\n",
       "      <td>Bhattacharjee</td>\n",
       "      <td>M</td>\n",
       "      <td>251</td>\n",
       "      <td>195-7571</td>\n",
       "      <td>FLORENCE</td>\n",
       "      <td>AL</td>\n",
       "      <td>35633</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>75000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20793</th>\n",
       "      <td>Byoung</td>\n",
       "      <td>Lebah</td>\n",
       "      <td>F</td>\n",
       "      <td>205</td>\n",
       "      <td>773-5181</td>\n",
       "      <td>BIRMINGHAM</td>\n",
       "      <td>AL</td>\n",
       "      <td>35210</td>\n",
       "      <td>S</td>\n",
       "      <td>Y</td>\n",
       "      <td>25000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21180</th>\n",
       "      <td>Ross</td>\n",
       "      <td>Stround</td>\n",
       "      <td>F</td>\n",
       "      <td>334</td>\n",
       "      <td>516-7873</td>\n",
       "      <td>SLOCOMB</td>\n",
       "      <td>AL</td>\n",
       "      <td>36375</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>95000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21480</th>\n",
       "      <td>Mihir</td>\n",
       "      <td>Olech</td>\n",
       "      <td>M</td>\n",
       "      <td>256</td>\n",
       "      <td>737-7930</td>\n",
       "      <td>FOLEY</td>\n",
       "      <td>AL</td>\n",
       "      <td>36536</td>\n",
       "      <td>S</td>\n",
       "      <td>Y</td>\n",
       "      <td>100000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21678</th>\n",
       "      <td>Moheb</td>\n",
       "      <td>Doddapaneni</td>\n",
       "      <td>M</td>\n",
       "      <td>256</td>\n",
       "      <td>289-7064</td>\n",
       "      <td>GORDON</td>\n",
       "      <td>AL</td>\n",
       "      <td>36343</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>5000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21880</th>\n",
       "      <td>Dieter</td>\n",
       "      <td>Doroslovacki</td>\n",
       "      <td>M</td>\n",
       "      <td>251</td>\n",
       "      <td>253-5032</td>\n",
       "      <td>CUBA</td>\n",
       "      <td>AL</td>\n",
       "      <td>36907</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>60000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        f_name         l_name gender area_code     phone           city state  \\\n",
       "1991   Shalosh       Vrancken      F       205  471-4927  HATCHECHUBBEE    AL   \n",
       "2029     Jeane      Ballinger      F       251  997-4661     GROVE HILL    AL   \n",
       "2097      Rong       Cummings      M       251  952-3637      NOTASULGA    AL   \n",
       "2173      Lois        Fontana      M       334  869-6752         MILLRY    AL   \n",
       "2273     Dannz  Bhattacharjee      M       251  195-7571       FLORENCE    AL   \n",
       "...        ...            ...    ...       ...       ...            ...   ...   \n",
       "20793   Byoung          Lebah      F       205  773-5181     BIRMINGHAM    AL   \n",
       "21180     Ross        Stround      F       334  516-7873        SLOCOMB    AL   \n",
       "21480    Mihir          Olech      M       256  737-7930          FOLEY    AL   \n",
       "21678    Moheb    Doddapaneni      M       256  289-7064         GORDON    AL   \n",
       "21880   Dieter   Doroslovacki      M       251  253-5032           CUBA    AL   \n",
       "\n",
       "         zip marital_status has_child  salary rate single_exemp married_exemp  \\\n",
       "1991   36858              M         Y   65000  5.0            0          3000   \n",
       "2029   36451              M         Y   20000  5.0            0          3000   \n",
       "2097   36866              M         Y   40000  5.0            0          3000   \n",
       "2173   36558              S         Y   40000  5.0         1500             0   \n",
       "2273   35633              M         Y   75000  5.0            0          3000   \n",
       "...      ...            ...       ...     ...  ...          ...           ...   \n",
       "20793  35210              S         Y   25000  5.0         1500             0   \n",
       "21180  36375              M         Y   95000  5.0            0          3000   \n",
       "21480  36536              S         Y  100000  5.0         1500             0   \n",
       "21678  36343              M         Y    5000  5.0            0          3000   \n",
       "21880  36907              M         Y   60000  5.0            0          3000   \n",
       "\n",
       "      child_exemp  \n",
       "1991          300  \n",
       "2029          300  \n",
       "2097          300  \n",
       "2173          300  \n",
       "2273          300  \n",
       "...           ...  \n",
       "20793         300  \n",
       "21180         300  \n",
       "21480         300  \n",
       "21678         300  \n",
       "21880         300  \n",
       "\n",
       "[200 rows x 15 columns]"
      ]
     },
     "execution_count": 1181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_clean[tax_clean.iloc[:,i]!=tax_dirty.iloc[:,i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000,)"
      ]
     },
     "execution_count": 1196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix_tax.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tax_clean[tax_clean['city']=='BARING']\n",
    "tax_dirty_count = tax_dirty\n",
    "tax_dirty_count['count'] = input_matrix_tax.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 17730,  17853,   4694,  18600,  81673,  19800, 124538,   3555,\n",
       "            146948,  18960,  20793,  10347,  17614,  19250,  19694,  20191,\n",
       "             18764,  86465,  10273],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 1201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_dirty_count[tax_dirty_count['count']==2].sample(n=19).index[:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "选中的20行索引: [19039, 3555, 9924, 825, 2150, 2652, 2923, 3338, 3534, 3630, 5400, 9234, 10347, 11305, 12145, 14408, 15975, 16157, 16326, 17044]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def find_rows_to_cover(matrix):\n",
    "    # if matrix.shape != (1000, 15):\n",
    "    #     return \"输入矩阵的大小必须为1000x15。\"\n",
    "\n",
    "    num_rows, num_cols = matrix.shape\n",
    "    selected_rows = []\n",
    "\n",
    "    while len(selected_rows) < 20:\n",
    "        max_covered_count = 0\n",
    "        max_covered_row = None\n",
    "\n",
    "        for row in range(num_rows):\n",
    "            if row in selected_rows:\n",
    "                continue\n",
    "\n",
    "            covered_cols = [col for col in range(num_cols) if matrix[row, col] == 1]\n",
    "            covered_count = sum(1 for col in covered_cols if all(matrix[r, col] == 1 for r in selected_rows))\n",
    "\n",
    "            if covered_count > max_covered_count:\n",
    "                max_covered_count = covered_count\n",
    "                max_covered_row = row\n",
    "\n",
    "        if max_covered_row is not None:\n",
    "            selected_rows.append(max_covered_row)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return selected_rows\n",
    "\n",
    "# 创建一个示例的1000x15的NumPy数组（假设数组名为matrix）\n",
    "# 请替换这个示例数组为您自己的数据\n",
    "matrix = np.random.randint(2, size=(1000, 15))\n",
    "\n",
    "# 查找符合条件的20行\n",
    "selected_rows = find_rows_to_cover(input_matrix_tax)\n",
    "\n",
    "# 输出结果\n",
    "print(\"选中的20行索引:\", selected_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1316,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_label_index = [19039, 3555, 9924, 825, 2150, 2652, 2923, 3338, 3534, 3630, 5400, 9234, 10347, 11305, 12145, 14408, 15975, 16157, 16326, 17044]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20,  0,  0,  0,  0,  0,  1,  0,  1,  3,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 1317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix_tax[tax_label_index].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_name</th>\n",
       "      <th>l_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>area_code</th>\n",
       "      <th>phone</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>has_child</th>\n",
       "      <th>salary</th>\n",
       "      <th>rate</th>\n",
       "      <th>single_exemp</th>\n",
       "      <th>married_exemp</th>\n",
       "      <th>child_exemp</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3555</th>\n",
       "      <td>M''Lissa</td>\n",
       "      <td>Baumann</td>\n",
       "      <td>M</td>\n",
       "      <td>251</td>\n",
       "      <td>857-5924</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>AL</td>\n",
       "      <td>36314</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>60000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19039</th>\n",
       "      <td>Irs''hak</td>\n",
       "      <td>Guha</td>\n",
       "      <td>M</td>\n",
       "      <td>205</td>\n",
       "      <td>888-6448</td>\n",
       "      <td>BIRMINGHAM</td>\n",
       "      <td>AL</td>\n",
       "      <td>35205</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>10000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         f_name   l_name gender area_code     phone        city state    zip  \\\n",
       "3555   M''Lissa  Baumann      M       251  857-5924       BLACK    AL  36314   \n",
       "19039  Irs''hak     Guha      M       205  888-6448  BIRMINGHAM    AL  35205   \n",
       "\n",
       "      marital_status has_child salary rate single_exemp married_exemp  \\\n",
       "3555               S         N  60000  5.0         1500             0   \n",
       "19039              M         N  10000  5.0         1500             0   \n",
       "\n",
       "      child_exemp  count  \n",
       "3555          300      2  \n",
       "19039         300      3  "
      ]
     },
     "execution_count": 1226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 0\n",
    "tax_dirty_count[tax_clean.iloc[:,c]!=tax_dirty.iloc[:,c]].sort_values('count').iloc[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1227,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [0,1,5,6,7,8,9,12,14]\n",
    "index_all = []\n",
    "for c in col:\n",
    "    index_all.append(list(tax_dirty_count[tax_clean.iloc[:,c]!=tax_dirty.iloc[:,c]].sort_values('count').iloc[-3:].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1228,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_all = set(np.array(index_all).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[164643,\n",
       " 164166,\n",
       " 78015,\n",
       " 93823,\n",
       " 18960,\n",
       " 10273,\n",
       " 146948,\n",
       " 20793,\n",
       " 20663,\n",
       " 17853,\n",
       " 86465,\n",
       " 4677,\n",
       " 81673,\n",
       " 4682,\n",
       " 4694,\n",
       " 12263,\n",
       " 3555,\n",
       " 124538,\n",
       " 10347,\n",
       " 19039]"
      ]
     },
     "execution_count": 1231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_all = list(tax_dirty_count.iloc[list(index_all)].sort_values('count').iloc[-20:].index)\n",
    "index_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_matrix_tax[index_all].sum(axis=0)\n",
    "tax_label_index = index_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([272, 694,   0,   0,   0, 200, 600, 400, 200, 200,   0,   0, 200,\n",
       "          0, 200]),\n",
       " array([5, 7, 0, 0, 0, 2, 4, 3, 4, 8, 0, 0, 2, 0, 2]))"
      ]
     },
     "execution_count": 1320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix_tax.sum(axis=0),input_matrix_tax[index_all].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1243,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list_label = []\n",
    "for d in tax_label_index:\n",
    "    index = d\n",
    "    # i = d[1] + 1 ## 重要，我们在detector中忽略了index列\n",
    "    for i in range(15):\n",
    "        dirty_cell = tax_dirty.iloc[index,i]\n",
    "        clean_cell = tax_clean.iloc[index,i]\n",
    "        col_name = tax_clean.columns[i]\n",
    "        template_dict = {}\n",
    "        template_dict[col_name] = ''\n",
    "        temp_dict = tax_dirty.iloc[index,1:-1].to_dict()\n",
    "        clean_dict = {}\n",
    "        clean_dict[col_name] = clean_cell\n",
    "        coreset_reference = np.random.choice([c for c in tax_label_index if c!=index],3,replace=False)\n",
    "        # if(dirty_cell!=clean_cell): ## 在LLM Inference中忽略\n",
    "        text_head = 'You are an expert in cleaning Tax Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\nTake these rows as reference:\\n\\n' % (col_name, json.dumps(template_dict), json.dumps(temp_dict))\n",
    "        dict_0 = tax_clean.iloc[coreset_reference[0]].to_dict()\n",
    "        dict_1 = tax_clean.iloc[coreset_reference[1]].to_dict()\n",
    "        dict_2 = tax_clean.iloc[coreset_reference[2]].to_dict()\n",
    "        ICL_text = '%s\\n\\n%s\\n\\n%s\\n\\n' % (json.dumps(dict_0),json.dumps(dict_1),json.dumps(dict_2))\n",
    "        # ICL_text = '%s\\n\\n' % (json.dumps(dict_0))\n",
    "        training_list_label.append([text_head,ICL_text,'',json.dumps(clean_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"article_jcreated_at\": \"1/1/12\"}\n"
     ]
    }
   ],
   "source": [
    "rayyan_result = pd.read_csv('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/rayyan-test.csv',index_col=0)\n",
    "print(rayyan_result[rayyan_result['output']!=rayyan_result['predict']].iloc[-1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"M'hamed\", \"l_name\": \"O'Boyle\", \"ge...</td>\n",
       "      <td></td>\n",
       "      <td>{\"f_name\": \"Torbjorn\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"f_name\": \"Torbjorn\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"M'hamed\", \"l_name\": \"O'Boyle\", \"ge...</td>\n",
       "      <td></td>\n",
       "      <td>{\"l_name\": \"Kesselman\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"l_name\": \"Kesselman\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"Toni\", \"l_name\": \"McElwain\", \"gend...</td>\n",
       "      <td></td>\n",
       "      <td>{\"gender\": \"F\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"gender\": \"F\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"Irs'hak\", \"l_name\": \"Guha\", \"gende...</td>\n",
       "      <td></td>\n",
       "      <td>{\"area_code\": \"907\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"area_code\": \"907\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"Laurie\", \"l_name\": \"Fornaciari\", \"...</td>\n",
       "      <td></td>\n",
       "      <td>{\"phone\": \"861-3988\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"phone\": \"861-3988\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"Mehul\", \"l_name\": \"L'Excellent\", \"...</td>\n",
       "      <td></td>\n",
       "      <td>{\"salary\": \"10000\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"salary\": \"10000\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"Laurie\", \"l_name\": \"Fornaciari\", \"...</td>\n",
       "      <td></td>\n",
       "      <td>{\"rate\": \"5.0\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"rate\": \"5.0\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"Shibin\", \"l_name\": \"Raufmann\", \"ge...</td>\n",
       "      <td></td>\n",
       "      <td>{\"single_exemp\": \"1500\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"single_exemp\": \"1500\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"M'Lissa\", \"l_name\": \"Lugli\", \"gend...</td>\n",
       "      <td></td>\n",
       "      <td>{\"married_exemp\": \"0\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"married_exemp\": \"0\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"Emin\", \"l_name\": \"Fouks\", \"gender\"...</td>\n",
       "      <td></td>\n",
       "      <td>{\"child_exemp\": \"300\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"child_exemp\": \"300\"}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0  \\\n",
       "0    You are an expert in cleaning Tax Dataset. Giv...   \n",
       "1    You are an expert in cleaning Tax Dataset. Giv...   \n",
       "2    You are an expert in cleaning Tax Dataset. Giv...   \n",
       "3    You are an expert in cleaning Tax Dataset. Giv...   \n",
       "4    You are an expert in cleaning Tax Dataset. Giv...   \n",
       "..                                                 ...   \n",
       "295  You are an expert in cleaning Tax Dataset. Giv...   \n",
       "296  You are an expert in cleaning Tax Dataset. Giv...   \n",
       "297  You are an expert in cleaning Tax Dataset. Giv...   \n",
       "298  You are an expert in cleaning Tax Dataset. Giv...   \n",
       "299  You are an expert in cleaning Tax Dataset. Giv...   \n",
       "\n",
       "                                                     1 2   \\\n",
       "0    {\"f_name\": \"M'hamed\", \"l_name\": \"O'Boyle\", \"ge...      \n",
       "1    {\"f_name\": \"M'hamed\", \"l_name\": \"O'Boyle\", \"ge...      \n",
       "2    {\"f_name\": \"Toni\", \"l_name\": \"McElwain\", \"gend...      \n",
       "3    {\"f_name\": \"Irs'hak\", \"l_name\": \"Guha\", \"gende...      \n",
       "4    {\"f_name\": \"Laurie\", \"l_name\": \"Fornaciari\", \"...      \n",
       "..                                                 ... ..   \n",
       "295  {\"f_name\": \"Mehul\", \"l_name\": \"L'Excellent\", \"...      \n",
       "296  {\"f_name\": \"Laurie\", \"l_name\": \"Fornaciari\", \"...      \n",
       "297  {\"f_name\": \"Shibin\", \"l_name\": \"Raufmann\", \"ge...      \n",
       "298  {\"f_name\": \"M'Lissa\", \"l_name\": \"Lugli\", \"gend...      \n",
       "299  {\"f_name\": \"Emin\", \"l_name\": \"Fouks\", \"gender\"...      \n",
       "\n",
       "                            3  \\\n",
       "0      {\"f_name\": \"Torbjorn\"}   \n",
       "1     {\"l_name\": \"Kesselman\"}   \n",
       "2             {\"gender\": \"F\"}   \n",
       "3        {\"area_code\": \"907\"}   \n",
       "4       {\"phone\": \"861-3988\"}   \n",
       "..                        ...   \n",
       "295       {\"salary\": \"10000\"}   \n",
       "296           {\"rate\": \"5.0\"}   \n",
       "297  {\"single_exemp\": \"1500\"}   \n",
       "298    {\"married_exemp\": \"0\"}   \n",
       "299    {\"child_exemp\": \"300\"}   \n",
       "\n",
       "                                           instruction input  \\\n",
       "0    You are an expert in cleaning Tax Dataset. Giv...         \n",
       "1    You are an expert in cleaning Tax Dataset. Giv...         \n",
       "2    You are an expert in cleaning Tax Dataset. Giv...         \n",
       "3    You are an expert in cleaning Tax Dataset. Giv...         \n",
       "4    You are an expert in cleaning Tax Dataset. Giv...         \n",
       "..                                                 ...   ...   \n",
       "295  You are an expert in cleaning Tax Dataset. Giv...         \n",
       "296  You are an expert in cleaning Tax Dataset. Giv...         \n",
       "297  You are an expert in cleaning Tax Dataset. Giv...         \n",
       "298  You are an expert in cleaning Tax Dataset. Giv...         \n",
       "299  You are an expert in cleaning Tax Dataset. Giv...         \n",
       "\n",
       "                       output  \n",
       "0      {\"f_name\": \"Torbjorn\"}  \n",
       "1     {\"l_name\": \"Kesselman\"}  \n",
       "2             {\"gender\": \"F\"}  \n",
       "3        {\"area_code\": \"907\"}  \n",
       "4       {\"phone\": \"861-3988\"}  \n",
       "..                        ...  \n",
       "295       {\"salary\": \"10000\"}  \n",
       "296           {\"rate\": \"5.0\"}  \n",
       "297  {\"single_exemp\": \"1500\"}  \n",
       "298    {\"married_exemp\": \"0\"}  \n",
       "299    {\"child_exemp\": \"300\"}  \n",
       "\n",
       "[300 rows x 7 columns]"
      ]
     },
     "execution_count": 1244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_list_pd = pd.DataFrame(training_list_label)\n",
    "training_list_pd['instruction'] = training_list_pd[0] + training_list_pd[1]\n",
    "training_list_pd['input'] = training_list_pd[2]\n",
    "training_list_pd['output'] = training_list_pd[3]\n",
    "training_list_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1245,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(training_list_pd.iloc[:,-3:].to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/tax/tax-train.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 1257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "rayyan_result['item'] = ''\n",
    "rayyan_result['gt'] = ''\n",
    "for index,row in rayyan_result.iterrows():\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        temp_dict = ast.literal_eval(row[3])\n",
    "        rayyan_result.iloc[index,-2] = list(temp_dict.values())[0]\n",
    "    except:\n",
    "        print(index)\n",
    "    try:\n",
    "        gt_dict = ast.literal_eval(row[2])\n",
    "        rayyan_result.iloc[index,-1] = list(gt_dict.values())[0]\n",
    "    except:\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# rayyan_dirty.iloc[3,9],rayyan_clean.iloc[3,9]\n",
    "input_matrix_rayyan = input_matrix_rayyan[:,1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24178403755868544 0.10864978902953587 0.14992721979621543\n"
     ]
    }
   ],
   "source": [
    "# input_matrix_rayyan_flatten = input_matrix_rayyan.flatten()\n",
    "All_Data_Error_rayyan = input_matrix_rayyan.sum()\n",
    "Add_Fixed_Error_rayyan = 0\n",
    "Correct_Fixed_Error_rayyan = 0\n",
    "for d in np.argwhere(input_matrix_rayyan==1):\n",
    "    x = d[0]\n",
    "    y = d[1]\n",
    "    result_index = x * 10 + y\n",
    "    dirty_cell = rayyan_dirty.iloc[x,y+1]\n",
    "    correction_result = rayyan_result.iloc[result_index,-2]\n",
    "    \n",
    "    \n",
    "    # clean_cell = rayyan_clean.iloc[x,y+1] ## Ignore id\n",
    "    clean_cell = rayyan_result.iloc[result_index,-1]\n",
    "    if(dirty_cell!=correction_result):\n",
    "        assert dirty_cell!=clean_cell\n",
    "        Add_Fixed_Error_rayyan += 1\n",
    "    if(clean_cell==correction_result):\n",
    "        Correct_Fixed_Error_rayyan += 1\n",
    "\n",
    "Precision_rayyan = Correct_Fixed_Error_rayyan / Add_Fixed_Error_rayyan\n",
    "Recall_rayyan = Correct_Fixed_Error_rayyan / All_Data_Error_rayyan\n",
    "F1_rayyan = (2 * Precision_rayyan * Recall_rayyan) / (Precision_rayyan + Recall_rayyan)\n",
    "print(Precision_rayyan,Recall_rayyan,F1_rayyan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.7774261603375527 0.7774261603375527 0.7774261603375527 Rayyan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection for Tax Dataset, DO NOT CONCLUDE Violation of FD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1326,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tax Detector Training\n",
    "\n",
    "input_matrix_select_tax = input_matrix_tax[tax_label_index]\n",
    "detector_list_tax = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "for label_tuple in tax_label_index:\n",
    "    for i in range(15):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        col_name = tax_clean.columns[i]\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = tax_clean.iloc[label_tuple]\n",
    "        dirty_context = tax_dirty.iloc[label_tuple]\n",
    "        clean_cell = tax_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = tax_dirty.iloc[label_tuple,i]\n",
    "        single_context_clean = 'COL %s VAL %s ' % (tax_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (tax_clean.columns[i],dirty_cell)\n",
    "        for c in range(15):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (tax_clean.columns[c],tax_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (tax_dirty.columns[c],tax_dirty.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            detector_list_tax.append([all_context_dirty,single_context_dirty,1])\n",
    "            detector_list_tax.append([all_context_clean,single_context_clean,0])\n",
    "            detector_list_tax.append([all_context_dirty,single_context_clean,0])\n",
    "            # detector_list_tax.append([col_name,dirty_cell,1])\n",
    "            # detector_list_tax.append([col_name,clean_cell,0])\n",
    "            # detector_list.append([single_context_dirty,1])\n",
    "        else:\n",
    "            detector_list_tax.append([all_context_dirty,single_context_dirty,0])\n",
    "            detector_list_tax.append([all_context_clean,single_context_clean,0])\n",
    "            detector_list_tax.append([all_context_dirty,single_context_clean,0])\n",
    "            # detector_list_tax.append([col_name,dirty_cell,0])\n",
    "            # detector_list_tax.append([col_name,clean_cell,0])\n",
    "        # else:\n",
    "        #     detector_list.append([all_context_dirty,1])\n",
    "        #     detector_list.append([single_context_dirty,1])\n",
    "            \n",
    "                # break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(detector_list_tax)[2].value_counts()\n",
    "detector_list_tax_pd = pd.DataFrame(detector_list_tax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b595ddd1d0e044f6938f14e4ce1eb256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2929 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_matrix_select_tax = input_matrix_tax[tax_label_index]\n",
    "detector_list_tax = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "# for label_tuple in tqdm(selected_rows):\n",
    "# for label_tuple in tqdm(range(len(rayyan_clean))):\n",
    "for label_tuple in tqdm(selected_test_index): ## select 3000 rows to detect\n",
    "    for i in range(15):\n",
    "        col_name = tax_clean.columns[i]\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = tax_clean.iloc[label_tuple]\n",
    "        dirty_context = tax_dirty.iloc[label_tuple]\n",
    "        clean_cell = tax_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = tax_dirty.iloc[label_tuple,i]\n",
    "        single_context_clean = 'COL %s VAL %s ' % (tax_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (tax_dirty.columns[i],dirty_cell)\n",
    "        for c in range(15):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (tax_clean.columns[c],tax_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (tax_dirty.columns[c],tax_dirty.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            detector_list_tax.append([all_context_dirty,single_context_dirty,1])\n",
    "        else:\n",
    "            detector_list_tax.append([all_context_dirty,single_context_dirty,0])\n",
    "            # detector_list.append([single_context_dirty,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1301,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_test_index = np.where(input_matrix_tax.sum(axis=1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1311,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_tax).to_csv('datasets/tax/detector/test.csv')\n",
    "# pd.DataFrame(detector_list_tax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 1330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector_tax = np.load('datasets/tax/detector/detection_cell.npy')\n",
    "detector_tax = detector_tax.reshape((-1,15))\n",
    "detector_tax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix_tax[:,-2].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: married_exemp, dtype: int64)"
      ]
     },
     "execution_count": 1348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_dirty[(tax_dirty['marital_status']!=tax_clean['marital_status']) & (tax_dirty['marital_status']=='S')]['married_exemp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    print(tax_clean.columns[i])\n",
    "    print(i)\n",
    "    print(precision_score(y_pred = detector_tax[:,i],y_true = input_matrix_tax[selected_test_index,i]),recall_score(y_pred = detector_tax[:,i],y_true = input_matrix_tax[selected_test_index,i]),f1_score(y_pred = detector_tax[:,i],y_true = input_matrix_tax[selected_test_index,i]),input_matrix_tax[selected_test_index,i].sum()) ## selected_test_index is all dirty rows, for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input \n",
      "\n",
      "['AK-*', 'NJ', 'AK-*', 'NJ']\n",
      "\n",
      "are some dirty cells from table Tax column state, and the input \n",
      "\n",
      "['AK', 'NY', 'AK', 'LA']\n",
      "\n",
      " are corrected clean cells, and ['AK', 'AK', 'AK', 'CO', 'VA', 'AL', 'AL', 'AL', 'AL', 'AK', 'AL', 'MD', 'AL', 'AL', 'WA', 'AL'] are examples of corresponding corrected clean cells. please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28194/1479885975.py:6: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  clean_list = tax_label_clean[tax_dirty[col_name]!=tax_clean[col_name]][col_name].to_list()\n",
      "/tmp/ipykernel_28194/1479885975.py:7: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  dirty_list = tax_label_dirty[tax_dirty[col_name]!=tax_clean[col_name]][col_name].to_list()\n"
     ]
    }
   ],
   "source": [
    "## Loop Refinement\n",
    "i = 6\n",
    "tax_label_dirty = tax_dirty.iloc[tax_label_index]\n",
    "tax_label_clean = tax_clean.iloc[tax_label_index]\n",
    "col_name = tax_clean.columns[i]\n",
    "clean_list = tax_label_clean[tax_dirty[col_name]!=tax_clean[col_name]][col_name].to_list()\n",
    "dirty_list = tax_label_dirty[tax_dirty[col_name]!=tax_clean[col_name]][col_name].to_list()\n",
    "clean_list_origin = tax_label_clean[tax_label_dirty[col_name]==tax_label_clean[col_name]][col_name].to_list()\n",
    "# clean_list = rayyan_label_clean[col_name].to_list()\n",
    "# dirty_list = rayyan_label_dirty[col_name].to_list()\n",
    "# clean_list\n",
    "# detector_inference = \"The input \\n\\n%s\\n\\nare some dirty cells from table rayyan column %s, and the input \\n\\n%s\\n\\n are corresponding clean ones, please write a general function to detect whether a given cell is dirty or not\" % (dirty_list,col_name,clean_list)\n",
    "detector_inference = \"The input \\n\\n%s\\n\\nare some dirty cells from table Tax column %s, and the input \\n\\n%s\\n\\n are corrected clean cells, and %s are examples of corresponding corrected clean cells. please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (dirty_list,col_name,clean_list,clean_list_origin)\n",
    "# print(detector_inference)\n",
    "print(detector_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "694"
      ]
     },
     "execution_count": 1360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([is_dirty_cell(c) for c in tax_dirty.iloc[:,1].to_list()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Shin'ya\", \"M'Lissa\", \"M'hamed\", \"M'Lissa\", \"Irs'hak\"]"
      ]
     },
     "execution_count": 1368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    " ## f_name,l_name\n",
    "def is_dirty_cell(cell):\n",
    "    # 使用正则表达式来查找两个连续的单引号\n",
    "    dirty_pattern = re.compile(r\"''\")\n",
    "    \n",
    "    # 检查单元格是否包含脏模式\n",
    "    if re.search(dirty_pattern, cell):\n",
    "        return True\n",
    "    return False\n",
    "def clean_dirty_cell(dirty_cell):\n",
    "    # 使用字符串的替换函数将两个连续的单引号替换为一个单引号\n",
    "    clean_cell = dirty_cell.replace(\"''\", \"'\")\n",
    "    return clean_cell\n",
    "# 示例输入\n",
    "dirty_cells = [\"Shin''ya\", \"M''Lissa\", \"M''hamed\", \"M''Lissa\", \"Irs''hak\"]\n",
    "clean_cells = [\"Shin'ya\", \"M'Lissa\", \"M'hamed\", \"M'Lissa\", \"Irs'hak\"]\n",
    "other_clean_cells = ['Torbjorn', 'Emin', 'Toni', 'Jackson', 'Hendra', 'Mehul', 'Byoung', 'Iskender', 'Laurie', 'Seongtaek', 'Shibin', 'Leonor', 'Dentcho', 'Cedric', 'Roji']\n",
    "[clean_dirty_cell(c) for c in dirty_cells]\n",
    "# 使用函数检测脏单元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 1369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    " ## city\n",
    "def is_dirty_cell(cell):\n",
    "    # 使用正则表达式来查找以连字符（-*）结尾的单元格\n",
    "    dirty_pattern = re.compile(r'-\\*$')\n",
    "    \n",
    "    # 检查单元格是否匹配脏模式\n",
    "    if re.search(dirty_pattern, cell):\n",
    "        return True\n",
    "    return False\n",
    "def clean_dirty_cell(dirty_cell):\n",
    "    # 使用字符串的替换函数将连字符（-*）删除\n",
    "    clean_cell = dirty_cell.replace('-*', '')\n",
    "    return clean_cell\n",
    "# sum([is_dirty_cell(c) for c in tax_dirty.iloc[:,5].to_list()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1379,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('datasets/tax/detector/index.npy',tax_label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1380,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('datasets/rayyan/detector/index.npy',rayyan_label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1382,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('datasets/beers/detector/index.npy',beer_label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1385,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('datasets/hospital/detector/index.npy',selected_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1399,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('datasets/flights/detector/index.npy',flight_label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_label_index = np.load('datasets/beers/detector/index.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1389,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flight_unique\n",
    "flight_label_index = []\n",
    "for i in range(20):\n",
    "    flight_cluster_name = flight_unique[cluster_select_flight[i]]\n",
    "    flight_cluter_df = list(flight_dirty[flight_dirty['flight']==flight_cluster_name].index)[0]\n",
    "    flight_label_index.append(flight_cluter_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1400,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_detector = np.load('datasets/tax/detector/detection_cell.npy')\n",
    "tax_detector = tax_detector.reshape((-1,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    1],\n",
       "       [   1,    0],\n",
       "       [   2,    1],\n",
       "       ...,\n",
       "       [2926,    6],\n",
       "       [2927,    0],\n",
       "       [2928,    1]])"
      ]
     },
     "execution_count": 1402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_detector_indice = np.argwhere(tax_detector==1)\n",
    "tax_detector_indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1523,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_dirty_select = tax_dirty.iloc[selected_test_index].reset_index(drop=True)\n",
    "tax_clean_select = tax_clean.iloc[selected_test_index].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1407,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list_label = []\n",
    "for d in tax_detector_indice:\n",
    "    index = d[0]\n",
    "    i = d[1]\n",
    "    # i = d[1] + 1 ## 重要，我们在detector中忽略了index列\n",
    "    # for i in range(15):\n",
    "    dirty_cell = tax_dirty_select.iloc[index,i]\n",
    "    clean_cell = tax_clean_select.iloc[index,i]\n",
    "    col_name = tax_clean.columns[i]\n",
    "    template_dict = {}\n",
    "    template_dict[col_name] = ''\n",
    "    temp_dict = tax_dirty_select.iloc[index,:-1].to_dict()\n",
    "    clean_dict = {}\n",
    "    clean_dict[col_name] = clean_cell\n",
    "    coreset_reference = np.random.choice([c for c in tax_label_index if c!=index],3,replace=False)\n",
    "    # if(dirty_cell!=clean_cell): ## 在LLM Inference中忽略\n",
    "    text_head = 'You are an expert in cleaning Tax Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\nTake these rows as reference:\\n\\n' % (col_name, json.dumps(template_dict), json.dumps(temp_dict))\n",
    "    dict_0 = tax_clean.iloc[coreset_reference[0]].to_dict()\n",
    "    dict_1 = tax_clean.iloc[coreset_reference[1]].to_dict()\n",
    "    dict_2 = tax_clean.iloc[coreset_reference[2]].to_dict()\n",
    "    ICL_text = '%s\\n\\n%s\\n\\n%s\\n\\n' % (json.dumps(dict_0),json.dumps(dict_1),json.dumps(dict_2))\n",
    "    # ICL_text = '%s\\n\\n' % (json.dumps(dict_0))\n",
    "    training_list_label.append([text_head,ICL_text,'',json.dumps(clean_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"Hendra\", \"l_name\": \"O'Boyle\", \"gen...</td>\n",
       "      <td></td>\n",
       "      <td>{\"l_name\": \"d'Argence\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"l_name\": \"d'Argence\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"Hendra\", \"l_name\": \"O'Boyle\", \"gen...</td>\n",
       "      <td></td>\n",
       "      <td>{\"f_name\": \"Jun'ichi\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"f_name\": \"Jun'ichi\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"Toni\", \"l_name\": \"McElwain\", \"gend...</td>\n",
       "      <td></td>\n",
       "      <td>{\"l_name\": \"D'Amiano\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"l_name\": \"D'Amiano\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"Leonor\", \"l_name\": \"O'Young\", \"gen...</td>\n",
       "      <td></td>\n",
       "      <td>{\"state\": \"NJ\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"state\": \"NJ\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"Dentcho\", \"l_name\": \"Stancampiano\"...</td>\n",
       "      <td></td>\n",
       "      <td>{\"l_name\": \"Give'on\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"l_name\": \"Give'on\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"M'Lissa\", \"l_name\": \"Baumann\", \"ge...</td>\n",
       "      <td></td>\n",
       "      <td>{\"city\": \"ARCHBALD\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"city\": \"ARCHBALD\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"Torbjorn\", \"l_name\": \"Kesselman\", ...</td>\n",
       "      <td></td>\n",
       "      <td>{\"f_name\": \"Surveyors'\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"f_name\": \"Surveyors'\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"Shibin\", \"l_name\": \"Raufmann\", \"ge...</td>\n",
       "      <td></td>\n",
       "      <td>{\"state\": \"AK\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"state\": \"AK\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"Jackson\", \"l_name\": \"Froberg\", \"ge...</td>\n",
       "      <td></td>\n",
       "      <td>{\"f_name\": \"Irs'hak\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"f_name\": \"Irs'hak\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2471</th>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td>{\"f_name\": \"M'hamed\", \"l_name\": \"O'Boyle\", \"ge...</td>\n",
       "      <td></td>\n",
       "      <td>{\"l_name\": \"L'Ecuyer\"}</td>\n",
       "      <td>You are an expert in cleaning Tax Dataset. Giv...</td>\n",
       "      <td></td>\n",
       "      <td>{\"l_name\": \"L'Ecuyer\"}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2472 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0  \\\n",
       "0     You are an expert in cleaning Tax Dataset. Giv...   \n",
       "1     You are an expert in cleaning Tax Dataset. Giv...   \n",
       "2     You are an expert in cleaning Tax Dataset. Giv...   \n",
       "3     You are an expert in cleaning Tax Dataset. Giv...   \n",
       "4     You are an expert in cleaning Tax Dataset. Giv...   \n",
       "...                                                 ...   \n",
       "2467  You are an expert in cleaning Tax Dataset. Giv...   \n",
       "2468  You are an expert in cleaning Tax Dataset. Giv...   \n",
       "2469  You are an expert in cleaning Tax Dataset. Giv...   \n",
       "2470  You are an expert in cleaning Tax Dataset. Giv...   \n",
       "2471  You are an expert in cleaning Tax Dataset. Giv...   \n",
       "\n",
       "                                                      1 2   \\\n",
       "0     {\"f_name\": \"Hendra\", \"l_name\": \"O'Boyle\", \"gen...      \n",
       "1     {\"f_name\": \"Hendra\", \"l_name\": \"O'Boyle\", \"gen...      \n",
       "2     {\"f_name\": \"Toni\", \"l_name\": \"McElwain\", \"gend...      \n",
       "3     {\"f_name\": \"Leonor\", \"l_name\": \"O'Young\", \"gen...      \n",
       "4     {\"f_name\": \"Dentcho\", \"l_name\": \"Stancampiano\"...      \n",
       "...                                                 ... ..   \n",
       "2467  {\"f_name\": \"M'Lissa\", \"l_name\": \"Baumann\", \"ge...      \n",
       "2468  {\"f_name\": \"Torbjorn\", \"l_name\": \"Kesselman\", ...      \n",
       "2469  {\"f_name\": \"Shibin\", \"l_name\": \"Raufmann\", \"ge...      \n",
       "2470  {\"f_name\": \"Jackson\", \"l_name\": \"Froberg\", \"ge...      \n",
       "2471  {\"f_name\": \"M'hamed\", \"l_name\": \"O'Boyle\", \"ge...      \n",
       "\n",
       "                             3  \\\n",
       "0      {\"l_name\": \"d'Argence\"}   \n",
       "1       {\"f_name\": \"Jun'ichi\"}   \n",
       "2       {\"l_name\": \"D'Amiano\"}   \n",
       "3              {\"state\": \"NJ\"}   \n",
       "4        {\"l_name\": \"Give'on\"}   \n",
       "...                        ...   \n",
       "2467      {\"city\": \"ARCHBALD\"}   \n",
       "2468  {\"f_name\": \"Surveyors'\"}   \n",
       "2469           {\"state\": \"AK\"}   \n",
       "2470     {\"f_name\": \"Irs'hak\"}   \n",
       "2471    {\"l_name\": \"L'Ecuyer\"}   \n",
       "\n",
       "                                            instruction input  \\\n",
       "0     You are an expert in cleaning Tax Dataset. Giv...         \n",
       "1     You are an expert in cleaning Tax Dataset. Giv...         \n",
       "2     You are an expert in cleaning Tax Dataset. Giv...         \n",
       "3     You are an expert in cleaning Tax Dataset. Giv...         \n",
       "4     You are an expert in cleaning Tax Dataset. Giv...         \n",
       "...                                                 ...   ...   \n",
       "2467  You are an expert in cleaning Tax Dataset. Giv...         \n",
       "2468  You are an expert in cleaning Tax Dataset. Giv...         \n",
       "2469  You are an expert in cleaning Tax Dataset. Giv...         \n",
       "2470  You are an expert in cleaning Tax Dataset. Giv...         \n",
       "2471  You are an expert in cleaning Tax Dataset. Giv...         \n",
       "\n",
       "                        output  \n",
       "0      {\"l_name\": \"d'Argence\"}  \n",
       "1       {\"f_name\": \"Jun'ichi\"}  \n",
       "2       {\"l_name\": \"D'Amiano\"}  \n",
       "3              {\"state\": \"NJ\"}  \n",
       "4        {\"l_name\": \"Give'on\"}  \n",
       "...                        ...  \n",
       "2467      {\"city\": \"ARCHBALD\"}  \n",
       "2468  {\"f_name\": \"Surveyors'\"}  \n",
       "2469           {\"state\": \"AK\"}  \n",
       "2470     {\"f_name\": \"Irs'hak\"}  \n",
       "2471    {\"l_name\": \"L'Ecuyer\"}  \n",
       "\n",
       "[2472 rows x 7 columns]"
      ]
     },
     "execution_count": 1409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.DataFrame(training_list_label)\n",
    "training_list_pd = pd.DataFrame(training_list_label)\n",
    "training_list_pd['instruction'] = training_list_pd[0] + training_list_pd[1]\n",
    "training_list_pd['input'] = training_list_pd[2]\n",
    "training_list_pd['output'] = training_list_pd[3]\n",
    "training_list_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1410,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(training_list_pd.iloc[:,-3:].to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/tax/tax-test.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1411,
   "metadata": {},
   "outputs": [],
   "source": [
    "rayyan_clean.to_csv('datasets/rayyan/detector/clean.csv')\n",
    "rayyan_dirty.to_csv('datasets/rayyan/detector/dirty.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1412,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_clean.to_csv('datasets/flights/detector/clean.csv')\n",
    "flight_dirty.to_csv('datasets/flights/detector/dirty.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1413,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_clean.to_csv('datasets/hospital/detector/clean.csv')\n",
    "hospital_dirty.to_csv('datasets/hospital/detector/dirty.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1414,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_clean.to_csv('datasets/tax/detector/clean.csv')\n",
    "tax_dirty.to_csv('datasets/tax/detector/dirty.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1415,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_clean.to_csv('datasets/beers/detector/beer_clean.csv')\n",
    "beer_dirty.to_csv('datasets/beers/detector/beer_dirty.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>article_title</th>\n",
       "      <th>article_language</th>\n",
       "      <th>journal_title</th>\n",
       "      <th>jounral_abbreviation</th>\n",
       "      <th>journal_issn</th>\n",
       "      <th>article_jvolumn</th>\n",
       "      <th>article_jissue</th>\n",
       "      <th>article_jcreated_at</th>\n",
       "      <th>article_pagination</th>\n",
       "      <th>author_list</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>62306</td>\n",
       "      <td>Mandibular second molar with C-shaped canal mo...</td>\n",
       "      <td>eng</td>\n",
       "      <td>General dentistry</td>\n",
       "      <td>Gen Dent</td>\n",
       "      <td>0363-6771</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>5/1/04</td>\n",
       "      <td>253-4</td>\n",
       "      <td>{\"Fred W Benenati\"}</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                      article_title  \\\n",
       "149  62306  Mandibular second molar with C-shaped canal mo...   \n",
       "\n",
       "    article_language      journal_title jounral_abbreviation journal_issn  \\\n",
       "149              eng  General dentistry             Gen Dent    0363-6771   \n",
       "\n",
       "    article_jvolumn article_jissue article_jcreated_at article_pagination  \\\n",
       "149              52              3              5/1/04              253-4   \n",
       "\n",
       "             author_list  index  \n",
       "149  {\"Fred W Benenati\"}    149  "
      ]
     },
     "execution_count": 1428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_dirty[rayyan_dirty['article_jcreated_at']=='5/1/04']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>article_title</th>\n",
       "      <th>article_language</th>\n",
       "      <th>journal_title</th>\n",
       "      <th>jounral_abbreviation</th>\n",
       "      <th>journal_issn</th>\n",
       "      <th>article_jvolumn</th>\n",
       "      <th>article_jissue</th>\n",
       "      <th>article_jcreated_at</th>\n",
       "      <th>article_pagination</th>\n",
       "      <th>author_list</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>62306</td>\n",
       "      <td>Mandibular second molar with C-shaped canal mo...</td>\n",
       "      <td>eng</td>\n",
       "      <td>General dentistry</td>\n",
       "      <td>Gen Dent</td>\n",
       "      <td>0363-6771</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>5/1/04</td>\n",
       "      <td>253-4</td>\n",
       "      <td>{\"Fred W Benenati\"}</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                      article_title  \\\n",
       "149  62306  Mandibular second molar with C-shaped canal mo...   \n",
       "\n",
       "    article_language      journal_title jounral_abbreviation journal_issn  \\\n",
       "149              eng  General dentistry             Gen Dent    0363-6771   \n",
       "\n",
       "    article_jvolumn article_jissue article_jcreated_at article_pagination  \\\n",
       "149              52              3              5/1/04              253-4   \n",
       "\n",
       "             author_list  index  \n",
       "149  {\"Fred W Benenati\"}    149  "
      ]
     },
     "execution_count": 1439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tax_clean.columns\n",
    "# len(rayyan_clean['journal_title'].unique())\n",
    "rayyan_dirty[rayyan_clean['journal_title']=='General dentistry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>article_title</th>\n",
       "      <th>article_language</th>\n",
       "      <th>journal_title</th>\n",
       "      <th>jounral_abbreviation</th>\n",
       "      <th>journal_issn</th>\n",
       "      <th>article_jvolumn</th>\n",
       "      <th>article_jissue</th>\n",
       "      <th>article_jcreated_at</th>\n",
       "      <th>article_pagination</th>\n",
       "      <th>author_list</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>62306</td>\n",
       "      <td>Mandibular second molar with C-shaped canal mo...</td>\n",
       "      <td>eng</td>\n",
       "      <td>General dentistry</td>\n",
       "      <td>Gen Dent</td>\n",
       "      <td>0363-6771</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>1/4/05</td>\n",
       "      <td>253-4</td>\n",
       "      <td>{\"Fred W Benenati\"}</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                      article_title  \\\n",
       "149  62306  Mandibular second molar with C-shaped canal mo...   \n",
       "\n",
       "    article_language      journal_title jounral_abbreviation journal_issn  \\\n",
       "149              eng  General dentistry             Gen Dent    0363-6771   \n",
       "\n",
       "    article_jvolumn article_jissue article_jcreated_at article_pagination  \\\n",
       "149              52              3              1/4/05              253-4   \n",
       "\n",
       "             author_list  index  \n",
       "149  {\"Fred W Benenati\"}    149  "
      ]
     },
     "execution_count": 1440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_clean[rayyan_clean['journal_title']=='General dentistry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1584\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "tax_result = pd.read_csv('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/tax-test.csv',index_col=0)\n",
    "tax_result['item'] = ''\n",
    "tax_result['gt'] = ''\n",
    "for index,row in tax_result.iterrows():\n",
    "    try:\n",
    "        temp_dict = ast.literal_eval(row[3])\n",
    "    \n",
    "        tax_result.iloc[index,-2] = list(temp_dict.values())[0]\n",
    "    except:\n",
    "        print(index)\n",
    "    try:\n",
    "        gt_dict = ast.literal_eval(row[2])\n",
    "        tax_result.iloc[index,-1] = list(gt_dict.values())[0]\n",
    "    except:\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_result[tax_result['gt']!=tax_result['item']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_clean.columns\n",
    "pattern = tax_clean.iloc[:,-7] + '|' +  tax_clean.iloc[:,-3] +'|' + tax_clean.iloc[:,-2]\n",
    "pd.Series(pattern).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([272, 694,   0,   0,   0, 200, 600, 400, 200, 200,   0,   0, 200,\n",
       "         0, 200])"
      ]
     },
     "execution_count": 1458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix_tax[selected_test_index].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['f_name', 'l_name', 'city', 'state', 'zip', 'marital_status',\n",
       "       'has_child', 'single_exemp', 'child_exemp'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_col = [0,1,5,6,7,8,9,12,14]\n",
    "tax_clean.columns[tax_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1563,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_dirty_select = tax_dirty.iloc[selected_test_index].reset_index(drop=True)\n",
    "tax_clean_select = tax_clean.iloc[selected_test_index].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1564,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_dirty_correct = tax_dirty_select\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1565,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Special Correction\n",
    "# tax_dirty_correct.iloc[:,0] = tax_dirty_select.iloc[:,0].str.replace(\"''\",\"'\")\n",
    "# tax_dirty_correct.iloc[:,1] = tax_dirty_select.iloc[:,1].str.replace(\"''\",\"'\")\n",
    "# tax_dirty_correct.iloc[:,5] = tax_dirty_select.iloc[:,5].str.replace('*', '').replace('-', '')\n",
    "# tax_dirty_correct.iloc[:,6] = tax_dirty_select.iloc[:,6].str.replace('*', '').replace('-', '')\n",
    "# tax_dirty_correct.iloc[:,7] = tax_dirty_select.iloc[:,7].str.replace('*', '').replace('-', '')\n",
    "# tax_dirty_correct.iloc[:,8] = tax_dirty_select.iloc[:,8].str.replace('*', '').replace('-', '')\n",
    "# tax_dirty_correct.iloc[:,9] = tax_dirty_select.iloc[:,9].str.replace('*', '').replace('-', '')\n",
    "# tax_dirty_correct.iloc[:,12] = tax_dirty_select.iloc[:,12].replace('*', '').replace('-', '')\n",
    "# tax_dirty_correct.iloc[:,14] = tax_dirty_select.iloc[:,14].replace('*', '').replace('-', '')\n",
    "# tax_dirty_correct = tax_dirty_correct.str.replace(\"''\",\"'\").replace('*', '').replace('-', '')\n",
    "def FormatTax(row):\n",
    "    for x,y in row[[0,1,5,6,7,8,9,12,14]].items():\n",
    "        row[x] = y.replace(\"''\",\"'\").replace('-*', '')\n",
    "    return row\n",
    "tax_dirty_correct = tax_dirty_select.apply(FormatTax,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1566,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Graph_Dependency_State(row):\n",
    "    ## Dependencies for city-state\n",
    "    city = row['city']\n",
    "    replace = tax_dirty[tax_dirty['city']==city]['state'].mode().values[0]\n",
    "    return replace\n",
    "def Graph_Dependency_Zip(row):\n",
    "    ## Dependencies for city-state\n",
    "    if(row['zip']=='1907'):\n",
    "        city = row['city']\n",
    "        state = row['state']\n",
    "        replace = tax_dirty[(tax_dirty['city']==city) & (tax_dirty['state']==state)]['zip'].mode().values[0]\n",
    "        return replace\n",
    "    else:\n",
    "        return row['zip']\n",
    "# Graph_Dependency_Zip(tax_dirty.iloc[4549])\n",
    "tax_dirty_correct.iloc[:,7] = tax_dirty_correct.apply(Graph_Dependency_Zip,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1567,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Graph_Dependency_Marriage(row):\n",
    "    if(row['married_exemp']=='0') and (row['single_exemp']=='0'):\n",
    "        return row['marital_status']\n",
    "    else:\n",
    "        if(row['married_exemp']!='0'):\n",
    "            row['marital_status'] = 'M'\n",
    "        else:\n",
    "            row['marital_status'] = 'S'\n",
    "        return row['marital_status']\n",
    "tax_dirty_correct.iloc[:,8] = tax_dirty_correct.apply(Graph_Dependency_Marriage,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1568,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Graph_Dependency_Child(row):\n",
    "    if(row['child_exemp']!='0'):\n",
    "        row['has_child'] = 'Y'\n",
    "    return row['has_child']\n",
    "tax_dirty_correct.iloc[:,9] = tax_dirty_correct.apply(Graph_Dependency_Child,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Precision_tax = (496-57) / 496\n",
    "Recall_tax = (496-57) / input_matrix.sum()\n",
    "F1_hospital = (2 * Precision_hospital * Recall_hospital) / (Precision_hospital + Recall_hospital)\n",
    "Precision_hospital,Recall_hospital,F1_hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1585,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2966, 2438, 2550)"
      ]
     },
     "execution_count": 1585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_dirty_select = tax_dirty.iloc[selected_test_index].reset_index(drop=True)\n",
    "tax_clean_select = tax_clean.iloc[selected_test_index].reset_index(drop=True)\n",
    "All_Data_Error = 0\n",
    "Correct_Fixed_Error = 0\n",
    "All_Fixed_Error = 0\n",
    "list_wrong = []\n",
    "for x in range(2929):\n",
    "    for y in range(15):\n",
    "        dirty_cell = tax_dirty_select.iloc[x,y]\n",
    "        clean_cell = tax_clean_select.iloc[x,y]\n",
    "        corrected_cell = tax_dirty_correct.iloc[x,y]\n",
    "        if(clean_cell!=dirty_cell):\n",
    "            All_Data_Error += 1\n",
    "        if(dirty_cell!=corrected_cell):\n",
    "            All_Fixed_Error += 1\n",
    "            if(corrected_cell==clean_cell):\n",
    "                Correct_Fixed_Error += 1\n",
    "            else:\n",
    "                list_wrong.append([clean_cell,corrected_cell,x,y])\n",
    "All_Data_Error,Correct_Fixed_Error,All_Fixed_Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.956078431372549, 0.8219824679703304, 0.8839738941261784)"
      ]
     },
     "execution_count": 1587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Precision = Correct_Fixed_Error / All_Fixed_Error\n",
    "Recall = Correct_Fixed_Error / All_Data_Error\n",
    "F1_tax = (2 * Precision * Recall) / (Precision + Recall)\n",
    "Precision,Recall,F1_tax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.956078431372549, 0.8219824679703304, 0.8839738941261784) Tax_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'article_title', 'article_language', 'journal_title',\n",
       "       'jounral_abbreviation', 'journal_issn', 'article_jvolumn',\n",
       "       'article_jissue', 'article_jcreated_at', 'article_pagination',\n",
       "       'author_list', 'index'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 15\n",
    "tax_dirty_correct[tax_dirty_correct.iloc[:,i]!=tax_clean_select.iloc[:,i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1586,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95209</td>\n",
       "      <td>95207</td>\n",
       "      <td>58</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84157</td>\n",
       "      <td>84132</td>\n",
       "      <td>60</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99901</td>\n",
       "      <td>99950</td>\n",
       "      <td>61</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83606</td>\n",
       "      <td>83607</td>\n",
       "      <td>62</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68134</td>\n",
       "      <td>68122</td>\n",
       "      <td>63</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>25302</td>\n",
       "      <td>25304</td>\n",
       "      <td>1460</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>985</td>\n",
       "      <td>984</td>\n",
       "      <td>1461</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>27106</td>\n",
       "      <td>27198</td>\n",
       "      <td>1466</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>55573</td>\n",
       "      <td>55555</td>\n",
       "      <td>1467</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>19808</td>\n",
       "      <td>19892</td>\n",
       "      <td>1472</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1     2  3\n",
       "0    95209  95207    58  7\n",
       "1    84157  84132    60  7\n",
       "2    99901  99950    61  7\n",
       "3    83606  83607    62  7\n",
       "4    68134  68122    63  7\n",
       "..     ...    ...   ... ..\n",
       "107  25302  25304  1460  7\n",
       "108    985    984  1461  7\n",
       "109  27106  27198  1466  7\n",
       "110  55573  55555  1467  7\n",
       "111  19808  19892  1472  7\n",
       "\n",
       "[112 rows x 4 columns]"
      ]
     },
     "execution_count": 1586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(list_wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([158, 455,  85, 118, 322, 384, 392, 615, 656, 796, 862, 975, 979,\n",
       "       357, 130, 532, 212, 694, 924, 918])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_clean = pd.read_csv('datasets/rayyan/clean.csv').fillna('')\n",
    "rayyan_dirty = pd.read_csv('datasets/rayyan/dirty.csv').fillna('')\n",
    "rayyan_label = np.load('datasets/rayyan/detector/index.npy')\n",
    "rayyan_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hospital-Vary-LabelBudget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "509"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hospital_clean = pd.read_csv('datasets/hospital/clean.csv').astype(str)\n",
    "hospital_dirty = pd.read_csv('datasets/hospital/dirty.csv').astype(str)\n",
    "hospital_query = pd.read_csv('datasets/hospital/dirty_query.csv')\n",
    "hospital_dirty.columns = hospital_clean.columns\n",
    "# hospital_query\n",
    "input_matrix = np.array(hospital_clean!=hospital_dirty)\n",
    "input_matrix.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([532, 157, 213, 228, 238, 256, 332, 386, 411, 493, 572, 698, 956,\n",
       "        22,  24,  42,  56,  57,  93,  94])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_index_hospital = np.load('datasets/hospital/detector/index.npy')\n",
    "selected_index_hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([532, 157, 213, 228, 238])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_index_hospital_5 = selected_index_hospital[:10]\n",
    "selected_index_hospital_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 4, 0, 0, 3, 4, 3, 2, 5, 5, 7, 3, 4, 0, 4, 4, 1, 2]),\n",
       " array([ 0, 28, 24, 31,  0,  0, 33, 26, 30, 39, 34, 32, 27, 27, 32, 29, 36,\n",
       "        23, 31, 27]))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_index_hospital_5 = selected_index_hospital[:20]\n",
    "input_matrix[selected_index_hospital_5].sum(axis=0),input_matrix.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ProviderNumber</th>\n",
       "      <th>HospitalName</th>\n",
       "      <th>Address1</th>\n",
       "      <th>Address2</th>\n",
       "      <th>Address3</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>ZipCode</th>\n",
       "      <th>CountyName</th>\n",
       "      <th>PhoneNumber</th>\n",
       "      <th>HospitalType</th>\n",
       "      <th>HospitalOwner</th>\n",
       "      <th>EmergencyService</th>\n",
       "      <th>Condition</th>\n",
       "      <th>MeasureCode</th>\n",
       "      <th>MeasureName</th>\n",
       "      <th>Score</th>\n",
       "      <th>Sample</th>\n",
       "      <th>Stateavg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>10007</td>\n",
       "      <td>mizell memorial hospital</td>\n",
       "      <td>702 n main st</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>opp</td>\n",
       "      <td>al</td>\n",
       "      <td>36467</td>\n",
       "      <td>covington</td>\n",
       "      <td>3344933541</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>no</td>\n",
       "      <td>heart attack</td>\n",
       "      <td>axi-4</td>\n",
       "      <td>heart attack patients given smoking cessation ...</td>\n",
       "      <td>empty</td>\n",
       "      <td>0 patients</td>\n",
       "      <td>al_ami-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>102</td>\n",
       "      <td>10007</td>\n",
       "      <td>mizell memorial hospital</td>\n",
       "      <td>702 n main st</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>opp</td>\n",
       "      <td>al</td>\n",
       "      <td>36467</td>\n",
       "      <td>covington</td>\n",
       "      <td>3344x33541</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>no</td>\n",
       "      <td>heart failure</td>\n",
       "      <td>hfx1</td>\n",
       "      <td>heart failure patients given discharge instruc...</td>\n",
       "      <td>98%</td>\n",
       "      <td>59 patients</td>\n",
       "      <td>al_hf-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>109</td>\n",
       "      <td>10007</td>\n",
       "      <td>mizell memorial hospital</td>\n",
       "      <td>702 n main st</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>opp</td>\n",
       "      <td>al</td>\n",
       "      <td>36467</td>\n",
       "      <td>covington</td>\n",
       "      <td>3344933541</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>no</td>\n",
       "      <td>pneumonia</td>\n",
       "      <td>pnx5c</td>\n",
       "      <td>pneumonia patients given initial antibiotic(s)...</td>\n",
       "      <td>95%</td>\n",
       "      <td>87 patients</td>\n",
       "      <td>al_pn-5c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>121</td>\n",
       "      <td>10008</td>\n",
       "      <td>crenshaw community hospital</td>\n",
       "      <td>101 hospital circle</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>luverne</td>\n",
       "      <td>al</td>\n",
       "      <td>36049</td>\n",
       "      <td>crenshaw</td>\n",
       "      <td>3343353374</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>government - federal</td>\n",
       "      <td>yes</td>\n",
       "      <td>heart attack</td>\n",
       "      <td>axi-2</td>\n",
       "      <td>heart attack patients given aspirin at discharge</td>\n",
       "      <td>x00%</td>\n",
       "      <td>1 patients</td>\n",
       "      <td>al_ami-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>178</td>\n",
       "      <td>10010</td>\n",
       "      <td>marshall medical center north</td>\n",
       "      <td>8000 alabama highway 69</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>guntersville</td>\n",
       "      <td>al</td>\n",
       "      <td>35976</td>\n",
       "      <td>marshall</td>\n",
       "      <td>2565718000</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>government - hospital district or authority</td>\n",
       "      <td>yes</td>\n",
       "      <td>heart failure</td>\n",
       "      <td>hf-x</td>\n",
       "      <td>heart failure patients given an evaluation of ...</td>\n",
       "      <td>84%</td>\n",
       "      <td>80 patients</td>\n",
       "      <td>al_hf-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>191</td>\n",
       "      <td>10010</td>\n",
       "      <td>marshall medical center north</td>\n",
       "      <td>8000 alabama highway 69</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>guntersville</td>\n",
       "      <td>al</td>\n",
       "      <td>35976</td>\n",
       "      <td>marshall</td>\n",
       "      <td>2565718000</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>government - hospital district or authority</td>\n",
       "      <td>yes</td>\n",
       "      <td>surgical infection prevention</td>\n",
       "      <td>scip-inx-4</td>\n",
       "      <td>all heart surgery patients whose blood sugar (...</td>\n",
       "      <td>empty</td>\n",
       "      <td>0 patients</td>\n",
       "      <td>al_scip-inf-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>265</td>\n",
       "      <td>10015</td>\n",
       "      <td>southwest alabama medical center</td>\n",
       "      <td>33700 highway x3</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>thomasville</td>\n",
       "      <td>al</td>\n",
       "      <td>36784</td>\n",
       "      <td>clarke</td>\n",
       "      <td>3346366221</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>government - federal</td>\n",
       "      <td>yes</td>\n",
       "      <td>surgical infection prevention</td>\n",
       "      <td>scxp-xnf-3</td>\n",
       "      <td>surgery patients whose preventive antibiotics ...</td>\n",
       "      <td>86%</td>\n",
       "      <td>14 patients</td>\n",
       "      <td>al_scip-inf-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>350</td>\n",
       "      <td>10056</td>\n",
       "      <td>st vincents hospital</td>\n",
       "      <td>810 st vincents drive</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>birmingham</td>\n",
       "      <td>al</td>\n",
       "      <td>35205</td>\n",
       "      <td>jefferson</td>\n",
       "      <td>2059397000</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - other</td>\n",
       "      <td>yes</td>\n",
       "      <td>heart attack</td>\n",
       "      <td>amix2</td>\n",
       "      <td>heart attack patients given aspirin at discharge</td>\n",
       "      <td>98%</td>\n",
       "      <td>260 patients</td>\n",
       "      <td>al_ami-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>392</td>\n",
       "      <td>10087</td>\n",
       "      <td>univ of south alabama medical center</td>\n",
       "      <td>2451 fillingim street</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>mobile</td>\n",
       "      <td>al</td>\n",
       "      <td>36617</td>\n",
       "      <td>mobile</td>\n",
       "      <td>2514717110</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>government - state</td>\n",
       "      <td>yes</td>\n",
       "      <td>pneumonia</td>\n",
       "      <td>pn-xb</td>\n",
       "      <td>pneumonia patients whose initial emergency roo...</td>\n",
       "      <td>89%</td>\n",
       "      <td>35 patients</td>\n",
       "      <td>al_pn-3b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>399</td>\n",
       "      <td>10108</td>\n",
       "      <td>prattville baptist hospital</td>\n",
       "      <td>124 s memorial dr</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>prattville</td>\n",
       "      <td>al</td>\n",
       "      <td>36067</td>\n",
       "      <td>autauga</td>\n",
       "      <td>3343614267</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>yes</td>\n",
       "      <td>heart failure</td>\n",
       "      <td>hx-1</td>\n",
       "      <td>heart failure patients given discharge instruc...</td>\n",
       "      <td>96%</td>\n",
       "      <td>75 patients</td>\n",
       "      <td>al_hf-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>410</td>\n",
       "      <td>10108</td>\n",
       "      <td>prattville baptist hospital</td>\n",
       "      <td>124 s memorial dr</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>prattville</td>\n",
       "      <td>al</td>\n",
       "      <td>36067</td>\n",
       "      <td>autauga</td>\n",
       "      <td>3343614267</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>yes</td>\n",
       "      <td>surgical infection prevention</td>\n",
       "      <td>scipxinfx1</td>\n",
       "      <td>surgery patients who were given an antibiotic ...</td>\n",
       "      <td>90%</td>\n",
       "      <td>10 patients</td>\n",
       "      <td>al_scip-inf-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>465</td>\n",
       "      <td>10019</td>\n",
       "      <td>helen keller memorial hospital</td>\n",
       "      <td>1300 south montgomery avenue</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>sheffield</td>\n",
       "      <td>al</td>\n",
       "      <td>35660</td>\n",
       "      <td>jefferson</td>\n",
       "      <td>2563864556</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>government - hospital district or authority</td>\n",
       "      <td>yes</td>\n",
       "      <td>surgical infection prevention</td>\n",
       "      <td>sxip-vte-1</td>\n",
       "      <td>surgery patients whose doctors ordered treatme...</td>\n",
       "      <td>85%</td>\n",
       "      <td>144 patients</td>\n",
       "      <td>al_scip-vte-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>496</td>\n",
       "      <td>10022</td>\n",
       "      <td>cherokee medical center</td>\n",
       "      <td>400 northwood dr</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>centre</td>\n",
       "      <td>al</td>\n",
       "      <td>35960</td>\n",
       "      <td>cherokee</td>\n",
       "      <td>2569275531</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>yes</td>\n",
       "      <td>heart attack</td>\n",
       "      <td>ami-x</td>\n",
       "      <td>heart attack patients given beta blocker at di...</td>\n",
       "      <td>empty</td>\n",
       "      <td>0 patients</td>\n",
       "      <td>al_ami-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>581</td>\n",
       "      <td>10025</td>\n",
       "      <td>g h lanier memorial hospital</td>\n",
       "      <td>4800 48th st</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>valley</td>\n",
       "      <td>al</td>\n",
       "      <td>36854</td>\n",
       "      <td>chambers</td>\n",
       "      <td>3347561400</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - other</td>\n",
       "      <td>yes</td>\n",
       "      <td>pneumonia</td>\n",
       "      <td>px-5c</td>\n",
       "      <td>pneumonia patients given initial antibiotic(s)...</td>\n",
       "      <td>82%</td>\n",
       "      <td>50 pxtients</td>\n",
       "      <td>al_pn-5c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>640</td>\n",
       "      <td>1xx29</td>\n",
       "      <td>east alabama medical center and snf</td>\n",
       "      <td>2000 pepperell parkway</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>opelika</td>\n",
       "      <td>al</td>\n",
       "      <td>36801</td>\n",
       "      <td>lee</td>\n",
       "      <td>3347493411</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>government - hospital district or authority</td>\n",
       "      <td>yes</td>\n",
       "      <td>surgical infection prevention</td>\n",
       "      <td>scip-vtx-1</td>\n",
       "      <td>surgery patients whose doctors ordered treatme...</td>\n",
       "      <td>92%</td>\n",
       "      <td>473 patients</td>\n",
       "      <td>al_scip-vte-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>649</td>\n",
       "      <td>10032</td>\n",
       "      <td>wedowee hospital</td>\n",
       "      <td>209 north main street</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>wedowee</td>\n",
       "      <td>al</td>\n",
       "      <td>36278</td>\n",
       "      <td>randolph</td>\n",
       "      <td>2563572111</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>government - local</td>\n",
       "      <td>no</td>\n",
       "      <td>heart failure</td>\n",
       "      <td>hfx1</td>\n",
       "      <td>heart failure patients given discharge instruc...</td>\n",
       "      <td>0%</td>\n",
       "      <td>34 patients</td>\n",
       "      <td>al_hf-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>674</td>\n",
       "      <td>10033</td>\n",
       "      <td>university of alabama hospital</td>\n",
       "      <td>619 south 19th street</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>birmingham</td>\n",
       "      <td>al</td>\n",
       "      <td>35233</td>\n",
       "      <td>jefferson</td>\n",
       "      <td>2059344011</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>government - state</td>\n",
       "      <td>yes</td>\n",
       "      <td>heart failure</td>\n",
       "      <td>xf-1</td>\n",
       "      <td>heart failure patients given discharge instruc...</td>\n",
       "      <td>46%</td>\n",
       "      <td>618 xatients</td>\n",
       "      <td>al_hf-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>677</td>\n",
       "      <td>10033</td>\n",
       "      <td>university of alabama hospital</td>\n",
       "      <td>619 south 19th street</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>birmingham</td>\n",
       "      <td>al</td>\n",
       "      <td>35233</td>\n",
       "      <td>jefferson</td>\n",
       "      <td>2059344011</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>government - state</td>\n",
       "      <td>yes</td>\n",
       "      <td>heart failure</td>\n",
       "      <td>hfx4</td>\n",
       "      <td>heart failure patients given smoking cessation...</td>\n",
       "      <td>100%</td>\n",
       "      <td>130 patients</td>\n",
       "      <td>al_hf-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>692</td>\n",
       "      <td>10034</td>\n",
       "      <td>community hospital inc</td>\n",
       "      <td>805 friendship road</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>tallassee</td>\n",
       "      <td>al</td>\n",
       "      <td>36078</td>\n",
       "      <td>elmore</td>\n",
       "      <td>3342836541</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>no</td>\n",
       "      <td>heart attack</td>\n",
       "      <td>amix1</td>\n",
       "      <td>heart attack patients given aspirin at arrival</td>\n",
       "      <td>77%</td>\n",
       "      <td>13 patients</td>\n",
       "      <td>al_ami-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>695</td>\n",
       "      <td>10034</td>\n",
       "      <td>community hospital inc</td>\n",
       "      <td>805 friendship road</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>tallassee</td>\n",
       "      <td>al</td>\n",
       "      <td>36078</td>\n",
       "      <td>elmore</td>\n",
       "      <td>3342836541</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>no</td>\n",
       "      <td>heart attack</td>\n",
       "      <td>amx-4</td>\n",
       "      <td>heart attack patients given smoking cessation ...</td>\n",
       "      <td>empty</td>\n",
       "      <td>0 patients</td>\n",
       "      <td>al_ami-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>719</td>\n",
       "      <td>10035</td>\n",
       "      <td>cullman regional medical center</td>\n",
       "      <td>1912 alabama highway 157</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>cullman</td>\n",
       "      <td>al</td>\n",
       "      <td>35058</td>\n",
       "      <td>cullman</td>\n",
       "      <td>2567372000</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>government - hospital district or authority</td>\n",
       "      <td>yes</td>\n",
       "      <td>heart attack</td>\n",
       "      <td>amx-3</td>\n",
       "      <td>heart attack patients given ace inhibitor or a...</td>\n",
       "      <td>x7%</td>\n",
       "      <td>3 patients</td>\n",
       "      <td>al_ami-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>730</td>\n",
       "      <td>10035</td>\n",
       "      <td>cullman regional medical center</td>\n",
       "      <td>1912 alabama highway 157</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>cullman</td>\n",
       "      <td>al</td>\n",
       "      <td>35058</td>\n",
       "      <td>cullman</td>\n",
       "      <td>2567372000</td>\n",
       "      <td>acutexcarexhospitals</td>\n",
       "      <td>government - hospital district or authority</td>\n",
       "      <td>yes</td>\n",
       "      <td>pneumonia</td>\n",
       "      <td>px-4</td>\n",
       "      <td>pneumonia patients given smoking cessation adv...</td>\n",
       "      <td>100%</td>\n",
       "      <td>90 patients</td>\n",
       "      <td>al_pn-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>736</td>\n",
       "      <td>10035</td>\n",
       "      <td>cullman regional medical center</td>\n",
       "      <td>1912 alabama highway 157</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>cullman</td>\n",
       "      <td>al</td>\n",
       "      <td>35058</td>\n",
       "      <td>cullman</td>\n",
       "      <td>2567372000</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>government - hospital district or authority</td>\n",
       "      <td>yes</td>\n",
       "      <td>surgical infection prevention</td>\n",
       "      <td>scix-inf-2</td>\n",
       "      <td>surgery patients who were given the  right kin...</td>\n",
       "      <td>98%</td>\n",
       "      <td>417 patients</td>\n",
       "      <td>al_scip-inf-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>750</td>\n",
       "      <td>10036</td>\n",
       "      <td>andalusia regional hospital</td>\n",
       "      <td>849 south three notch street</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>andalusia</td>\n",
       "      <td>al</td>\n",
       "      <td>36420</td>\n",
       "      <td>covington</td>\n",
       "      <td>3342228466</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>proprietary</td>\n",
       "      <td>no</td>\n",
       "      <td>heart failure</td>\n",
       "      <td>hx-2</td>\n",
       "      <td>heart failure patients given an evaluation of ...</td>\n",
       "      <td>91%</td>\n",
       "      <td>139 patients</td>\n",
       "      <td>al_hf-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>775</td>\n",
       "      <td>10039</td>\n",
       "      <td>huntsville hospital</td>\n",
       "      <td>101 sivley rd</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>huntsville</td>\n",
       "      <td>al</td>\n",
       "      <td>35801</td>\n",
       "      <td>madison</td>\n",
       "      <td>2562651000</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>government - hospital district or authority</td>\n",
       "      <td>yes</td>\n",
       "      <td>children s asthma care</td>\n",
       "      <td>xax-1</td>\n",
       "      <td>children who received reliever medication whil...</td>\n",
       "      <td>100%</td>\n",
       "      <td>193 patients</td>\n",
       "      <td>al_cac-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>789</td>\n",
       "      <td>10039</td>\n",
       "      <td>huntsville hospital</td>\n",
       "      <td>101 sivley rd</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>huntsville</td>\n",
       "      <td>al</td>\n",
       "      <td>35801</td>\n",
       "      <td>madison</td>\n",
       "      <td>2562651000</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>government - hospital district or authority</td>\n",
       "      <td>yes</td>\n",
       "      <td>pneumonia</td>\n",
       "      <td>pn-x</td>\n",
       "      <td>pneumonia patients assessed and given pneumoco...</td>\n",
       "      <td>96%</td>\n",
       "      <td>509 patients</td>\n",
       "      <td>al_pn-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>843</td>\n",
       "      <td>10043</td>\n",
       "      <td>chilton medical center</td>\n",
       "      <td>1010 lay dam road</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>clanton</td>\n",
       "      <td>al</td>\n",
       "      <td>35045</td>\n",
       "      <td>chilton</td>\n",
       "      <td>2057552500</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>yes</td>\n",
       "      <td>pneumonia</td>\n",
       "      <td>pnx6</td>\n",
       "      <td>pneumonia patients given the most appropriate ...</td>\n",
       "      <td>95%</td>\n",
       "      <td>38 patients</td>\n",
       "      <td>al_pnx6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>866</td>\n",
       "      <td>10044</td>\n",
       "      <td>marion regional medical center</td>\n",
       "      <td>1256 military street south</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>hamilton</td>\n",
       "      <td>al</td>\n",
       "      <td>35570</td>\n",
       "      <td>marion</td>\n",
       "      <td>20x9216200</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>yes</td>\n",
       "      <td>pneumonia</td>\n",
       "      <td>px-4</td>\n",
       "      <td>pneumonia patients given smoking cessation adv...</td>\n",
       "      <td>90%</td>\n",
       "      <td>42 patients</td>\n",
       "      <td>al_pn-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>931</td>\n",
       "      <td>10047</td>\n",
       "      <td>georgiana hospital</td>\n",
       "      <td>515 miranda st</td>\n",
       "      <td>empty</td>\n",
       "      <td>empty</td>\n",
       "      <td>georgiana</td>\n",
       "      <td>al</td>\n",
       "      <td>36033</td>\n",
       "      <td>butler</td>\n",
       "      <td>3343762205</td>\n",
       "      <td>acute care hospitals</td>\n",
       "      <td>voluntary non-profit - private</td>\n",
       "      <td>no</td>\n",
       "      <td>heart attack</td>\n",
       "      <td>amix1</td>\n",
       "      <td>heart attack patients given aspirin at arrival</td>\n",
       "      <td>53%</td>\n",
       "      <td>19 patients</td>\n",
       "      <td>al_ami-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index ProviderNumber                          HospitalName  \\\n",
       "97     98          10007              mizell memorial hospital   \n",
       "101   102          10007              mizell memorial hospital   \n",
       "108   109          10007              mizell memorial hospital   \n",
       "120   121          10008           crenshaw community hospital   \n",
       "177   178          10010         marshall medical center north   \n",
       "190   191          10010         marshall medical center north   \n",
       "264   265          10015      southwest alabama medical center   \n",
       "349   350          10056                  st vincents hospital   \n",
       "391   392          10087  univ of south alabama medical center   \n",
       "398   399          10108           prattville baptist hospital   \n",
       "409   410          10108           prattville baptist hospital   \n",
       "464   465          10019        helen keller memorial hospital   \n",
       "495   496          10022               cherokee medical center   \n",
       "580   581          10025          g h lanier memorial hospital   \n",
       "639   640          1xx29   east alabama medical center and snf   \n",
       "648   649          10032                      wedowee hospital   \n",
       "673   674          10033        university of alabama hospital   \n",
       "676   677          10033        university of alabama hospital   \n",
       "691   692          10034                community hospital inc   \n",
       "694   695          10034                community hospital inc   \n",
       "718   719          10035       cullman regional medical center   \n",
       "729   730          10035       cullman regional medical center   \n",
       "735   736          10035       cullman regional medical center   \n",
       "749   750          10036           andalusia regional hospital   \n",
       "774   775          10039                   huntsville hospital   \n",
       "788   789          10039                   huntsville hospital   \n",
       "842   843          10043                chilton medical center   \n",
       "865   866          10044        marion regional medical center   \n",
       "930   931          10047                    georgiana hospital   \n",
       "\n",
       "                         Address1 Address2 Address3          City State  \\\n",
       "97                  702 n main st    empty    empty           opp    al   \n",
       "101                 702 n main st    empty    empty           opp    al   \n",
       "108                 702 n main st    empty    empty           opp    al   \n",
       "120           101 hospital circle    empty    empty       luverne    al   \n",
       "177       8000 alabama highway 69    empty    empty  guntersville    al   \n",
       "190       8000 alabama highway 69    empty    empty  guntersville    al   \n",
       "264              33700 highway x3    empty    empty   thomasville    al   \n",
       "349         810 st vincents drive    empty    empty    birmingham    al   \n",
       "391         2451 fillingim street    empty    empty        mobile    al   \n",
       "398             124 s memorial dr    empty    empty    prattville    al   \n",
       "409             124 s memorial dr    empty    empty    prattville    al   \n",
       "464  1300 south montgomery avenue    empty    empty     sheffield    al   \n",
       "495              400 northwood dr    empty    empty        centre    al   \n",
       "580                  4800 48th st    empty    empty        valley    al   \n",
       "639        2000 pepperell parkway    empty    empty       opelika    al   \n",
       "648         209 north main street    empty    empty       wedowee    al   \n",
       "673         619 south 19th street    empty    empty    birmingham    al   \n",
       "676         619 south 19th street    empty    empty    birmingham    al   \n",
       "691           805 friendship road    empty    empty     tallassee    al   \n",
       "694           805 friendship road    empty    empty     tallassee    al   \n",
       "718      1912 alabama highway 157    empty    empty       cullman    al   \n",
       "729      1912 alabama highway 157    empty    empty       cullman    al   \n",
       "735      1912 alabama highway 157    empty    empty       cullman    al   \n",
       "749  849 south three notch street    empty    empty     andalusia    al   \n",
       "774                 101 sivley rd    empty    empty    huntsville    al   \n",
       "788                 101 sivley rd    empty    empty    huntsville    al   \n",
       "842             1010 lay dam road    empty    empty       clanton    al   \n",
       "865    1256 military street south    empty    empty      hamilton    al   \n",
       "930                515 miranda st    empty    empty     georgiana    al   \n",
       "\n",
       "    ZipCode CountyName PhoneNumber          HospitalType  \\\n",
       "97    36467  covington  3344933541  acute care hospitals   \n",
       "101   36467  covington  3344x33541  acute care hospitals   \n",
       "108   36467  covington  3344933541  acute care hospitals   \n",
       "120   36049   crenshaw  3343353374  acute care hospitals   \n",
       "177   35976   marshall  2565718000  acute care hospitals   \n",
       "190   35976   marshall  2565718000  acute care hospitals   \n",
       "264   36784     clarke  3346366221  acute care hospitals   \n",
       "349   35205  jefferson  2059397000  acute care hospitals   \n",
       "391   36617     mobile  2514717110  acute care hospitals   \n",
       "398   36067    autauga  3343614267  acute care hospitals   \n",
       "409   36067    autauga  3343614267  acute care hospitals   \n",
       "464   35660  jefferson  2563864556  acute care hospitals   \n",
       "495   35960   cherokee  2569275531  acute care hospitals   \n",
       "580   36854   chambers  3347561400  acute care hospitals   \n",
       "639   36801        lee  3347493411  acute care hospitals   \n",
       "648   36278   randolph  2563572111  acute care hospitals   \n",
       "673   35233  jefferson  2059344011  acute care hospitals   \n",
       "676   35233  jefferson  2059344011  acute care hospitals   \n",
       "691   36078     elmore  3342836541  acute care hospitals   \n",
       "694   36078     elmore  3342836541  acute care hospitals   \n",
       "718   35058    cullman  2567372000  acute care hospitals   \n",
       "729   35058    cullman  2567372000  acutexcarexhospitals   \n",
       "735   35058    cullman  2567372000  acute care hospitals   \n",
       "749   36420  covington  3342228466  acute care hospitals   \n",
       "774   35801    madison  2562651000  acute care hospitals   \n",
       "788   35801    madison  2562651000  acute care hospitals   \n",
       "842   35045    chilton  2057552500  acute care hospitals   \n",
       "865   35570     marion  20x9216200  acute care hospitals   \n",
       "930   36033     butler  3343762205  acute care hospitals   \n",
       "\n",
       "                                   HospitalOwner EmergencyService  \\\n",
       "97                voluntary non-profit - private               no   \n",
       "101               voluntary non-profit - private               no   \n",
       "108               voluntary non-profit - private               no   \n",
       "120                         government - federal              yes   \n",
       "177  government - hospital district or authority              yes   \n",
       "190  government - hospital district or authority              yes   \n",
       "264                         government - federal              yes   \n",
       "349                 voluntary non-profit - other              yes   \n",
       "391                           government - state              yes   \n",
       "398               voluntary non-profit - private              yes   \n",
       "409               voluntary non-profit - private              yes   \n",
       "464  government - hospital district or authority              yes   \n",
       "495               voluntary non-profit - private              yes   \n",
       "580                 voluntary non-profit - other              yes   \n",
       "639  government - hospital district or authority              yes   \n",
       "648                           government - local               no   \n",
       "673                           government - state              yes   \n",
       "676                           government - state              yes   \n",
       "691               voluntary non-profit - private               no   \n",
       "694               voluntary non-profit - private               no   \n",
       "718  government - hospital district or authority              yes   \n",
       "729  government - hospital district or authority              yes   \n",
       "735  government - hospital district or authority              yes   \n",
       "749                                  proprietary               no   \n",
       "774  government - hospital district or authority              yes   \n",
       "788  government - hospital district or authority              yes   \n",
       "842               voluntary non-profit - private              yes   \n",
       "865               voluntary non-profit - private              yes   \n",
       "930               voluntary non-profit - private               no   \n",
       "\n",
       "                         Condition MeasureCode  \\\n",
       "97                    heart attack       axi-4   \n",
       "101                  heart failure        hfx1   \n",
       "108                      pneumonia       pnx5c   \n",
       "120                   heart attack       axi-2   \n",
       "177                  heart failure        hf-x   \n",
       "190  surgical infection prevention  scip-inx-4   \n",
       "264  surgical infection prevention  scxp-xnf-3   \n",
       "349                   heart attack       amix2   \n",
       "391                      pneumonia       pn-xb   \n",
       "398                  heart failure        hx-1   \n",
       "409  surgical infection prevention  scipxinfx1   \n",
       "464  surgical infection prevention  sxip-vte-1   \n",
       "495                   heart attack       ami-x   \n",
       "580                      pneumonia       px-5c   \n",
       "639  surgical infection prevention  scip-vtx-1   \n",
       "648                  heart failure        hfx1   \n",
       "673                  heart failure        xf-1   \n",
       "676                  heart failure        hfx4   \n",
       "691                   heart attack       amix1   \n",
       "694                   heart attack       amx-4   \n",
       "718                   heart attack       amx-3   \n",
       "729                      pneumonia        px-4   \n",
       "735  surgical infection prevention  scix-inf-2   \n",
       "749                  heart failure        hx-2   \n",
       "774         children s asthma care       xax-1   \n",
       "788                      pneumonia        pn-x   \n",
       "842                      pneumonia        pnx6   \n",
       "865                      pneumonia        px-4   \n",
       "930                   heart attack       amix1   \n",
       "\n",
       "                                           MeasureName  Score        Sample  \\\n",
       "97   heart attack patients given smoking cessation ...  empty    0 patients   \n",
       "101  heart failure patients given discharge instruc...    98%   59 patients   \n",
       "108  pneumonia patients given initial antibiotic(s)...    95%   87 patients   \n",
       "120   heart attack patients given aspirin at discharge   x00%    1 patients   \n",
       "177  heart failure patients given an evaluation of ...    84%   80 patients   \n",
       "190  all heart surgery patients whose blood sugar (...  empty    0 patients   \n",
       "264  surgery patients whose preventive antibiotics ...    86%   14 patients   \n",
       "349   heart attack patients given aspirin at discharge    98%  260 patients   \n",
       "391  pneumonia patients whose initial emergency roo...    89%   35 patients   \n",
       "398  heart failure patients given discharge instruc...    96%   75 patients   \n",
       "409  surgery patients who were given an antibiotic ...    90%   10 patients   \n",
       "464  surgery patients whose doctors ordered treatme...    85%  144 patients   \n",
       "495  heart attack patients given beta blocker at di...  empty    0 patients   \n",
       "580  pneumonia patients given initial antibiotic(s)...    82%   50 pxtients   \n",
       "639  surgery patients whose doctors ordered treatme...    92%  473 patients   \n",
       "648  heart failure patients given discharge instruc...     0%   34 patients   \n",
       "673  heart failure patients given discharge instruc...    46%  618 xatients   \n",
       "676  heart failure patients given smoking cessation...   100%  130 patients   \n",
       "691     heart attack patients given aspirin at arrival    77%   13 patients   \n",
       "694  heart attack patients given smoking cessation ...  empty    0 patients   \n",
       "718  heart attack patients given ace inhibitor or a...    x7%    3 patients   \n",
       "729  pneumonia patients given smoking cessation adv...   100%   90 patients   \n",
       "735  surgery patients who were given the  right kin...    98%  417 patients   \n",
       "749  heart failure patients given an evaluation of ...    91%  139 patients   \n",
       "774  children who received reliever medication whil...   100%  193 patients   \n",
       "788  pneumonia patients assessed and given pneumoco...    96%  509 patients   \n",
       "842  pneumonia patients given the most appropriate ...    95%   38 patients   \n",
       "865  pneumonia patients given smoking cessation adv...    90%   42 patients   \n",
       "930     heart attack patients given aspirin at arrival    53%   19 patients   \n",
       "\n",
       "          Stateavg  \n",
       "97        al_ami-4  \n",
       "101        al_hf-1  \n",
       "108       al_pn-5c  \n",
       "120       al_ami-2  \n",
       "177        al_hf-2  \n",
       "190  al_scip-inf-4  \n",
       "264  al_scip-inf-3  \n",
       "349       al_ami-2  \n",
       "391       al_pn-3b  \n",
       "398        al_hf-1  \n",
       "409  al_scip-inf-1  \n",
       "464  al_scip-vte-1  \n",
       "495       al_ami-5  \n",
       "580       al_pn-5c  \n",
       "639  al_scip-vte-1  \n",
       "648        al_hf-1  \n",
       "673        al_hf-1  \n",
       "676        al_hf-4  \n",
       "691       al_ami-1  \n",
       "694       al_ami-4  \n",
       "718       al_ami-3  \n",
       "729        al_pn-4  \n",
       "735  al_scip-inf-2  \n",
       "749        al_hf-2  \n",
       "774       al_cac-1  \n",
       "788        al_pn-2  \n",
       "842        al_pnx6  \n",
       "865        al_pn-4  \n",
       "930       al_ami-1  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hospital_dirty[hospital_dirty.iloc[:,-5]!=hospital_clean.iloc[:,-5]]\n",
    "# hospital_dirty.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hospital_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/yanmy/raha/raha-master/detector.ipynb 单元格 298\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y604sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y604sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m hospital_clean_set \u001b[39m=\u001b[39m hospital_clean\u001b[39m.\u001b[39miloc[selected_index_hospital]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y604sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m hospital_dirty_set \u001b[39m=\u001b[39m hospital_dirty\u001b[39m.\u001b[39miloc[selected_index_hospital]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y604sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# hospital_dirty[hospital_dirty.iloc[:,i]!=hospital_clean.iloc[:,i]]\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y604sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# detector_inference = \"The input \\n\\n%s\\n\\nare some dirty cells from table Hospital column %s, and the input \\n\\n%s\\n\\n are corrected clean cells, and %s are examples of corresponding corrected clean cells. please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (dirty_list,col_name,clean_list,clean_list_origin)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Y604sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# detector_inference = \"The input \\n\\n%s\\n\\nare [dirty,clean] cell pairs from table Hospital column %s, and %s are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (dirty_list,col_name,clean_list,clean_list_origin)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hospital_clean' is not defined"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "hospital_clean_set = hospital_clean.iloc[selected_index_hospital]\n",
    "hospital_dirty_set = hospital_dirty.iloc[selected_index_hospital]\n",
    "# hospital_dirty[hospital_dirty.iloc[:,i]!=hospital_clean.iloc[:,i]]\n",
    "# detector_inference = \"The input \\n\\n%s\\n\\nare some dirty cells from table Hospital column %s, and the input \\n\\n%s\\n\\n are corrected clean cells, and %s are examples of corresponding corrected clean cells. please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (dirty_list,col_name,clean_list,clean_list_origin)\n",
    "# detector_inference = \"The input \\n\\n%s\\n\\nare [dirty,clean] cell pairs from table Hospital column %s, and %s are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (dirty_list,col_name,clean_list,clean_list_origin)\n",
    "\n",
    "dirty_list = []\n",
    "col_name = hospital_clean.columns[i]\n",
    "clean_list = list(hospital_dirty.iloc[:,i].unique())\n",
    "for index,row in hospital_dirty_set[hospital_dirty_set.iloc[:,i]!=hospital_clean_set.iloc[:,i]].iterrows():\n",
    "    # print(hospital_clean.iloc[index,i],hospital_dirty.iloc[index,i])\n",
    "    dirty_list.append([hospital_clean.iloc[index,i],hospital_dirty.iloc[index,i]])\n",
    "clean_list_part = list(hospital_clean_set.iloc[:,i].unique())\n",
    "dirty_list_part = list(hospital_dirty_set[hospital_dirty_set.iloc[:,i]!=hospital_clean_set.iloc[:,i]].iloc[:,i].unique())\n",
    "    # print(index)\n",
    "detector_inference = \"The input \\n\\n%s\\n\\nare [clean,dirty] cell pairs from table Hospital column %s, and %s are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (dirty_list,col_name,clean_list[:15])\n",
    "# detector_inference = \"The input \\n\\n%s\\n\\nare clean cells, and \\n\\n%s\\n\\nare dirty cells from table Hospital column %s, and %s are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (clean_list_part,dirty_list_part,col_name,clean_list[:15])\n",
    "detector_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input \n",
      "\n",
      "[]\n",
      "\n",
      "are [clean,dirty] cell pairs from table Tax column brewery-name, and ['10 Barrel Brewing Company', '18th Street Brewery', '2 Towns Ciderhouse', '21st Amendment Brewery', '3 Daughters Brewing', '4 Hands Brewing Company', '450 North Brewing Company', '7 Seas Brewing Company', '7venth Sun', 'AC Golden Brewing Company', 'Abita Brewing Company', 'Against The Grain Brewery', 'Against the Grain Brewery', 'Airways Brewing Company', 'Alameda Brewing'] are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not. Input and output are all string format.\n"
     ]
    }
   ],
   "source": [
    "i = 8\n",
    "beer_clean_set = beer_clean.iloc[beer_label_index]\n",
    "beer_dirty_set = beer_dirty.iloc[beer_label_index]\n",
    "# hospital_dirty[hospital_dirty.iloc[:,i]!=hospital_clean.iloc[:,i]]\n",
    "# detector_inference = \"The input \\n\\n%s\\n\\nare some dirty cells from table Hospital column %s, and the input \\n\\n%s\\n\\n are corrected clean cells, and %s are examples of corresponding corrected clean cells. please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (dirty_list,col_name,clean_list,clean_list_origin)\n",
    "# detector_inference = \"The input \\n\\n%s\\n\\nare [dirty,clean] cell pairs from table Hospital column %s, and %s are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (dirty_list,col_name,clean_list,clean_list_origin)\n",
    "\n",
    "dirty_list = []\n",
    "col_name = beer_clean.columns[i]\n",
    "clean_list = list(beer_dirty.iloc[:,i].unique())\n",
    "for index,row in beer_dirty_set[beer_dirty_set.iloc[:,i]!=beer_clean_set.iloc[:,i]].iterrows():\n",
    "    # print(hospital_clean.iloc[index,i],hospital_dirty.iloc[index,i])\n",
    "    dirty_list.append([beer_clean.iloc[index,i],beer_dirty.iloc[index,i]])\n",
    "# clean_list_part = list(beer_clean_set.iloc[:,i].unique())\n",
    "# dirty_list_part = list(beer_dirty[beer_dirty_set.iloc[:,i]!=beer_clean_set.iloc[:,i]].iloc[:,i].unique())\n",
    "#     # print(index)\n",
    "detector_inference = \"The input \\n\\n%s\\n\\nare [clean,dirty] cell pairs from table Beers column %s, and %s are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not. Input and output are all string format.\" % (dirty_list,col_name,clean_list[:15])\n",
    "# detector_inference = \"The input \\n\\n%s\\n\\nare clean cells, and \\n\\n%s\\n\\nare dirty cells from table Hospital column %s, and %s are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (clean_list_part,dirty_list_part,col_name,clean_list[:15])\n",
    "print(detector_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_label_index = np.load('datasets/tax/detector/index.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S,1370,0'"
      ]
     },
     "execution_count": 919,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def concat_row(row):\n",
    "    text = ''\n",
    "    for x,y in row.items():\n",
    "        text += '%s,' % y\n",
    "    return text[:-1]\n",
    "concat_row(tax_dirty.iloc[8,[8,12,13]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'M,0,0',\n",
       " 'M,0,12000',\n",
       " 'M,0,1400',\n",
       " 'M,0,174',\n",
       " 'M,0,1800',\n",
       " 'M,0,2000',\n",
       " 'M,0,206',\n",
       " 'M,0,2080',\n",
       " 'M,0,220',\n",
       " 'M,0,24500',\n",
       " 'M,0,2600',\n",
       " 'M,0,2740',\n",
       " 'M,0,3000',\n",
       " 'M,0,318',\n",
       " 'M,0,3800',\n",
       " 'M,0,40',\n",
       " 'M,0,4000',\n",
       " 'M,0,4200',\n",
       " 'M,0,4500',\n",
       " 'M,0,4800',\n",
       " 'M,0,4950',\n",
       " 'M,0,5400',\n",
       " 'M,0,5700',\n",
       " 'M,0,6200',\n",
       " 'M,0,6600',\n",
       " 'M,0,7150',\n",
       " 'M,0,80',\n",
       " 'M,0,9000',\n",
       " 'M,0-*,0',\n",
       " 'M,1500,0',\n",
       " 'S,0,0',\n",
       " 'S,1000,0',\n",
       " 'S,103,0',\n",
       " 'S,1040,0',\n",
       " 'S,110,0',\n",
       " 'S,12750,0',\n",
       " 'S,1300,0',\n",
       " 'S,1370,0',\n",
       " 'S,1500,0',\n",
       " 'S,159,0',\n",
       " 'S,1900,0',\n",
       " 'S,20,0',\n",
       " 'S,2000,0',\n",
       " 'S,2100,0',\n",
       " 'S,2250,0',\n",
       " 'S,2400,0',\n",
       " 'S,2475,0',\n",
       " 'S,2700,0',\n",
       " 'S,2850,0',\n",
       " 'S,3100,0',\n",
       " 'S,3300,0',\n",
       " 'S,3575,0',\n",
       " 'S,40,0',\n",
       " 'S,4500,0',\n",
       " 'S,6000,0',\n",
       " 'S,700,0',\n",
       " 'S,87,0',\n",
       " 'S,900,0'}"
      ]
     },
     "execution_count": 923,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list = tax_dirty.iloc[:,[8,12,13]].apply(concat_row,axis=1)\n",
    "set(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_name</th>\n",
       "      <th>l_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>area_code</th>\n",
       "      <th>phone</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>has_child</th>\n",
       "      <th>salary</th>\n",
       "      <th>rate</th>\n",
       "      <th>single_exemp</th>\n",
       "      <th>married_exemp</th>\n",
       "      <th>child_exemp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20793</th>\n",
       "      <td>Byoung</td>\n",
       "      <td>Lebah</td>\n",
       "      <td>F</td>\n",
       "      <td>205</td>\n",
       "      <td>773-5181</td>\n",
       "      <td>BIRMINGHAM</td>\n",
       "      <td>AL</td>\n",
       "      <td>35210</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>25000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20663</th>\n",
       "      <td>Iskender</td>\n",
       "      <td>Hoest</td>\n",
       "      <td>F</td>\n",
       "      <td>251</td>\n",
       "      <td>786-4658</td>\n",
       "      <td>VALHERMOSO SPRINGS</td>\n",
       "      <td>AL</td>\n",
       "      <td>35775</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>55000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17853</th>\n",
       "      <td>Laurie</td>\n",
       "      <td>Fornaciari</td>\n",
       "      <td>F</td>\n",
       "      <td>205</td>\n",
       "      <td>556-3443</td>\n",
       "      <td>FAIRHOPE</td>\n",
       "      <td>AL</td>\n",
       "      <td>36532</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>20000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19039</th>\n",
       "      <td>Irs''hak</td>\n",
       "      <td>Guha</td>\n",
       "      <td>M</td>\n",
       "      <td>205</td>\n",
       "      <td>888-6448</td>\n",
       "      <td>BIRMINGHAM</td>\n",
       "      <td>AL</td>\n",
       "      <td>35205</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>10000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         f_name      l_name gender area_code     phone                city  \\\n",
       "20793    Byoung       Lebah      F       205  773-5181          BIRMINGHAM   \n",
       "20663  Iskender       Hoest      F       251  786-4658  VALHERMOSO SPRINGS   \n",
       "17853    Laurie  Fornaciari      F       205  556-3443            FAIRHOPE   \n",
       "19039  Irs''hak        Guha      M       205  888-6448          BIRMINGHAM   \n",
       "\n",
       "      state    zip marital_status has_child salary rate single_exemp  \\\n",
       "20793    AL  35210              M         N  25000  5.0         1500   \n",
       "20663    AL  35775              M         N  55000  5.0         1500   \n",
       "17853    AL  36532              M         N  20000  5.0         1500   \n",
       "19039    AL  35205              M         N  10000  5.0         1500   \n",
       "\n",
       "      married_exemp child_exemp  \n",
       "20793             0         300  \n",
       "20663             0         300  \n",
       "17853             0         300  \n",
       "19039             0         300  "
      ]
     },
     "execution_count": 926,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_dirty_set[tax_dirty_set.iloc[:,i]!=tax_clean_set.iloc[:,i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M    200\n",
       "Name: marital_status, dtype: int64"
      ]
     },
     "execution_count": 929,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_dirty[tax_dirty.iloc[:,i]!=tax_clean.iloc[:,i]]['marital_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/yanmy/raha/raha-master/detector.ipynb 单元格 301\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1302sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m clean_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(tax_dirty\u001b[39m.\u001b[39miloc[:,i]\u001b[39m.\u001b[39munique())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1302sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m index,row \u001b[39min\u001b[39;00m tax_dirty_set[tax_dirty_set\u001b[39m.\u001b[39miloc[:,i]\u001b[39m!=\u001b[39mtax_clean_set\u001b[39m.\u001b[39miloc[:,i]]\u001b[39m.\u001b[39miterrows():\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1302sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# print(hospital_clean.iloc[index,i],hospital_dirty.iloc[index,i])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1302sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# dirty_list.append([tax_clean.iloc[index,i],tax_dirty.iloc[index,i]])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1302sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m# dirty_list.append([[tax_clean.iloc[index,i-1],tax_clean.iloc[index,i]],[tax_clean.iloc[index,i-1],tax_dirty.iloc[index,i]]])\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1302sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     dirty_list\u001b[39m.\u001b[39mappend([concat_row(tax_clean_set\u001b[39m.\u001b[39;49miloc[index,[\u001b[39m8\u001b[39;49m,\u001b[39m12\u001b[39;49m,\u001b[39m13\u001b[39;49m]]),concat_row(tax_dirty_set\u001b[39m.\u001b[39miloc[index,[\u001b[39m8\u001b[39m,\u001b[39m12\u001b[39m,\u001b[39m13\u001b[39m]])])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1302sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# clean_list_part = list(beer_clean_set.iloc[:,i].unique())\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1302sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# dirty_list_part = list(beer_dirty[beer_dirty_set.iloc[:,i]!=beer_clean_set.iloc[:,i]].iloc[:,i].unique())\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1302sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m#     # print(index)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1302sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m col_name \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(tax_clean\u001b[39m.\u001b[39mcolumns[[\u001b[39m8\u001b[39m,\u001b[39m12\u001b[39m,\u001b[39m13\u001b[39m]])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1067\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1065\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m   1066\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_value(\u001b[39m*\u001b[39mkey, takeable\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_takeable)\n\u001b[0;32m-> 1067\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_tuple(key)\n\u001b[1;32m   1068\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1069\u001b[0m     \u001b[39m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m     axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1563\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1561\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_getitem_tuple\u001b[39m(\u001b[39mself\u001b[39m, tup: \u001b[39mtuple\u001b[39m):\n\u001b[0;32m-> 1563\u001b[0m     tup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_tuple_indexer(tup)\n\u001b[1;32m   1564\u001b[0m     \u001b[39mwith\u001b[39;00m suppress(IndexingError):\n\u001b[1;32m   1565\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_lowerdim(tup)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:873\u001b[0m, in \u001b[0;36m_LocationIndexer._validate_tuple_indexer\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[39mfor\u001b[39;00m i, k \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(key):\n\u001b[1;32m    872\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 873\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_key(k, i)\n\u001b[1;32m    874\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    875\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    876\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mLocation based indexing can only have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    877\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_valid_types\u001b[39m}\u001b[39;00m\u001b[39m] types\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    878\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1466\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_key\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1464\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1465\u001b[0m \u001b[39melif\u001b[39;00m is_integer(key):\n\u001b[0;32m-> 1466\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_integer(key, axis)\n\u001b[1;32m   1467\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m   1468\u001b[0m     \u001b[39m# a tuple should already have been caught by this point\u001b[39;00m\n\u001b[1;32m   1469\u001b[0m     \u001b[39m# so don't treat a tuple as a valid indexer\u001b[39;00m\n\u001b[1;32m   1470\u001b[0m     \u001b[39mraise\u001b[39;00m IndexingError(\u001b[39m\"\u001b[39m\u001b[39mToo many indexers\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1557\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1555\u001b[0m len_axis \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis(axis))\n\u001b[1;32m   1556\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m len_axis \u001b[39mor\u001b[39;00m key \u001b[39m<\u001b[39m \u001b[39m-\u001b[39mlen_axis:\n\u001b[0;32m-> 1557\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39msingle positional indexer is out-of-bounds\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "i = 8\n",
    "tax_clean_set = tax_clean.iloc[tax_label_index]\n",
    "tax_dirty_set = tax_dirty.iloc[tax_label_index]\n",
    "# hospital_dirty[hospital_dirty.iloc[:,i]!=hospital_clean.iloc[:,i]]\n",
    "# detector_inference = \"The input \\n\\n%s\\n\\nare some dirty cells from table Hospital column %s, and the input \\n\\n%s\\n\\n are corrected clean cells, and %s are examples of corresponding corrected clean cells. please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (dirty_list,col_name,clean_list,clean_list_origin)\n",
    "# detector_inference = \"The input \\n\\n%s\\n\\nare [dirty,clean] cell pairs from table Hospital column %s, and %s are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (dirty_list,col_name,clean_list,clean_list_origin)\n",
    "\n",
    "dirty_list = []\n",
    "col_name = tax_clean.columns[i]\n",
    "clean_list = list(tax_dirty.iloc[:,i].unique())\n",
    "for index,row in tax_dirty_set[tax_dirty_set.iloc[:,i]!=tax_clean_set.iloc[:,i]].iterrows():\n",
    "    # print(hospital_clean.iloc[index,i],hospital_dirty.iloc[index,i])\n",
    "    # dirty_list.append([tax_clean.iloc[index,i],tax_dirty.iloc[index,i]])\n",
    "    # dirty_list.append([[tax_clean.iloc[index,i-1],tax_clean.iloc[index,i]],[tax_clean.iloc[index,i-1],tax_dirty.iloc[index,i]]])\n",
    "    dirty_list.append([concat_row(tax_clean_set.iloc[index,[8,12,13]]),concat_row(tax_dirty_set.iloc[index,[8,12,13]])])\n",
    "# clean_list_part = list(beer_clean_set.iloc[:,i].unique())\n",
    "# dirty_list_part = list(beer_dirty[beer_dirty_set.iloc[:,i]!=beer_clean_set.iloc[:,i]].iloc[:,i].unique())\n",
    "#     # print(index)\n",
    "\n",
    "col_name = list(tax_clean.columns[[8,12,13]])\n",
    "\n",
    "detector_inference = \"The input \\n\\n%s\\n\\nare [clean,dirty] cell pairs from table Tax column %s, and %s are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not. Input and output are all string format.\" % (dirty_list,col_name,set(text_list))\n",
    "# detector_inference = \"The input \\n\\n%s\\n\\nare clean cells, and \\n\\n%s\\n\\nare dirty cells from table Hospital column %s, and %s are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (clean_list_part,dirty_list_part,col_name,clean_list[:15])\n",
    "print(detector_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([158, 455,  85, 118, 322, 384, 392, 615, 656, 796, 862, 975, 979,\n",
       "       357, 130, 532, 212, 694, 924, 918])"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_label_dirty = np.load('datasets/rayyan/detector/index.npy')\n",
    "rayyan_label_dirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          87\n",
       "1/1/14    66\n",
       "1/1/13    55\n",
       "1/1/12    44\n",
       "1/1/11    43\n",
       "          ..\n",
       "5/1/00     1\n",
       "6/1/08     1\n",
       "8/1/06     1\n",
       "7/1/00     1\n",
       "1/1/70     1\n",
       "Name: article_jcreated_at, Length: 263, dtype: int64"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_dirty[rayyan_dirty.iloc[:,8]!=rayyan_clean.iloc[:,8]].iloc[:,8].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_clean(cell):\n",
    "    # Regular expression to detect if the cell is in the format M/D/YY\n",
    "    # where M is 1-12 and D is 1-31\n",
    "    pattern = re.compile(r'^([1-9]|1[0-2])/([1-9]|[12][0-9]|3[01])/\\d{2}$')\n",
    "    return bool(pattern.match(cell))\n",
    "for c in rayyan_dirty[rayyan_dirty.iloc[:,8]!=rayyan_clean.iloc[:,8]].iloc[:,8].to_list():\n",
    "    if not is_clean(c):\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The input \\n\\n[]\\n\\nare [clean,dirty] cell pairs from table rayyan column article_jvolumn, and ['64', '', '54', '84', '33', '10', '29', '164', '25', '79', '14', '8', '92', '35', '140', '7', '0', '139', '39', '6', '40', '37', '27', '47', '69'] are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not. Input and output are all string format.\""
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 6\n",
    "rayyan_clean_set = rayyan_clean.iloc[rayyan_label_dirty]\n",
    "rayyan_dirty_set = rayyan_dirty.iloc[rayyan_label_dirty]\n",
    "\n",
    "\n",
    "dirty_list = []\n",
    "col_name = rayyan_clean.columns[i]\n",
    "clean_list = list(rayyan_dirty.iloc[:,i].unique())\n",
    "for index,row in rayyan_dirty_set[rayyan_dirty_set.iloc[:,i]!=rayyan_clean_set.iloc[:,i]].iterrows():\n",
    "    # print(hospital_clean.iloc[index,i],hospital_dirty.iloc[index,i])\n",
    "    dirty_list.append([rayyan_clean.iloc[index,i],rayyan_dirty.iloc[index,i]])\n",
    "# clean_list_part = list(beer_clean_set.iloc[:,i].unique())\n",
    "# dirty_list_part = list(beer_dirty[beer_dirty_set.iloc[:,i]!=beer_clean_set.iloc[:,i]].iloc[:,i].unique())\n",
    "#     # print(index)\n",
    "detector_inference = \"The input \\n\\n%s\\n\\nare [clean,dirty] cell pairs from table rayyan column %s, and %s are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not. Input and output are all string format.\" % (dirty_list,col_name,clean_list[:25])\n",
    "# detector_inference = \"The input \\n\\n%s\\n\\nare clean cells, and \\n\\n%s\\n\\nare dirty cells from table Hospital column %s, and %s are examples of all cells from this columns. Please conclude a general pattern for dirty and clean cells, and write a general function with regular expression to detect whether a given cell is dirty or not\" % (clean_list_part,dirty_list_part,col_name,clean_list[:15])\n",
    "detector_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>article_title</th>\n",
       "      <th>article_language</th>\n",
       "      <th>journal_title</th>\n",
       "      <th>jounral_abbreviation</th>\n",
       "      <th>journal_issn</th>\n",
       "      <th>article_jvolumn</th>\n",
       "      <th>article_jissue</th>\n",
       "      <th>article_jcreated_at</th>\n",
       "      <th>article_pagination</th>\n",
       "      <th>author_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>498345</td>\n",
       "      <td>Ebola Virus GP Gene Polyadenylation Versus RNA...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>The Journal of infectious diseases</td>\n",
       "      <td>J. Infect. Dis.</td>\n",
       "      <td>1537-6613</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4/2/15</td>\n",
       "      <td></td>\n",
       "      <td>{\"Valentina A Volchkova\",\"Jaroslav Vorac\",\"Phi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>255432</td>\n",
       "      <td>Pool fencing for preventing drowning in children.</td>\n",
       "      <td>eng</td>\n",
       "      <td>The Cochrane Database Of Systematic Reviews</td>\n",
       "      <td>Cochrane Database Syst Rev</td>\n",
       "      <td>1469-493X</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>1/1/00</td>\n",
       "      <td>CD001047</td>\n",
       "      <td>{\"D C Thompson\",\"F P Rivara\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>657210</td>\n",
       "      <td>From task characteristics to learning: A syste...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>Scandinavian journal of psychology</td>\n",
       "      <td>Scand J Psychol</td>\n",
       "      <td>1467-9450</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2/17/10</td>\n",
       "      <td></td>\n",
       "      <td>{\"Michiel A J Kompier\",\"Etty G A Wielenga-Meij...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>143029</td>\n",
       "      <td>Steroids and antihistamines synergize to inhib...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>European archives of oto-rhino-laryngology : o...</td>\n",
       "      <td>Eur Arch Otorhinolaryngol</td>\n",
       "      <td>1434-4726</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>8/13/14</td>\n",
       "      <td></td>\n",
       "      <td>{\"Shao-Cheng Liu\",\"Chi-Chung Wu\",\"Hsing-Won Wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>979059</td>\n",
       "      <td>Difference in fascicle behaviors between super...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>Muscle &amp; nerve</td>\n",
       "      <td>Muscle Nerve</td>\n",
       "      <td>1097-4598</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>9/10/15</td>\n",
       "      <td></td>\n",
       "      <td>{\"Hiroshi Akima\",\"Kazunori Nosaka\",\"Aya Tomita...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>735083</td>\n",
       "      <td>Muscle variables of importance for physiologic...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>European journal of applied physiology</td>\n",
       "      <td>Eur. J. Appl. Physiol.</td>\n",
       "      <td>1439-6327</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>10/8/15</td>\n",
       "      <td></td>\n",
       "      <td>{\"Sebastien Racinais\",\"Olivier Girard\",\"Lars N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>226419</td>\n",
       "      <td>Anterior approach unilateral right sacrospinou...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>International journal of colorectal disease</td>\n",
       "      <td>Int J Colorectal Dis</td>\n",
       "      <td>1432-1262</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1/7/15</td>\n",
       "      <td></td>\n",
       "      <td>{\"Enie Akhtar Bt Nawawi\",\"Ahlam M Al-Kharabshe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>734870</td>\n",
       "      <td>Stress across the life course and depression i...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>International journal of geriatric psychiatry</td>\n",
       "      <td>Int J Geriatr Psychiatry</td>\n",
       "      <td>1099-1166</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>10/9/15</td>\n",
       "      <td></td>\n",
       "      <td>{\"Stephen E Gilman\",\"Chaoqiang Jiang\",\"Kar Keu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>232896</td>\n",
       "      <td>Detecting and monitoring the symptoms of Parki...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>Parkinsonism &amp; related disorders</td>\n",
       "      <td>Parkinsonism Relat. Disord.</td>\n",
       "      <td>1873-5126</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>3/7/15</td>\n",
       "      <td></td>\n",
       "      <td>{\"S Arora\",\"A Zhan\",\"S Donohue\",\"E R Dorsey\",\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>81959</td>\n",
       "      <td>Disruption of the gastroesophageal junction by...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>Diseases of the esophagus : official journal o...</td>\n",
       "      <td>Dis. Esophagus</td>\n",
       "      <td>1442-2050</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2/28/14</td>\n",
       "      <td></td>\n",
       "      <td>{\"Y Y Lee\",\"K E L McColl\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>98562</td>\n",
       "      <td>Prevalence of Masked Hypertension in African A...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>Journal of clinical hypertension (Greenwich, C...</td>\n",
       "      <td>J Clin Hypertens (Greenwich)</td>\n",
       "      <td>1751-7176</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>10/20/14</td>\n",
       "      <td></td>\n",
       "      <td>{\"Frances Williams\",\"Alehegn Gelaye\",\"Susan St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>600827</td>\n",
       "      <td>Nutritional supplementation for hip fracture a...</td>\n",
       "      <td>eng</td>\n",
       "      <td>Cochrane Database Syst. Rev.</td>\n",
       "      <td></td>\n",
       "      <td>1469-493X</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>1/1/00</td>\n",
       "      <td>CD001880</td>\n",
       "      <td>{\"A Avenell\",\"H H Handoll\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>30272</td>\n",
       "      <td>Glial cells in amyotrophic lateral sclerosis.</td>\n",
       "      <td>ENG</td>\n",
       "      <td>Experimental neurology</td>\n",
       "      <td>Exp. Neurol.</td>\n",
       "      <td>1090-2430</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>5/22/14</td>\n",
       "      <td></td>\n",
       "      <td>{\"T Philips\",\"J D Rothstein\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>26475</td>\n",
       "      <td>Situating universal design architecture: desig...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>Disability and rehabilitation</td>\n",
       "      <td>Disabil Rehabil</td>\n",
       "      <td>1464-5165</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>7/28/14</td>\n",
       "      <td>6-Jan</td>\n",
       "      <td>{\"Paul Jones\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>476191</td>\n",
       "      <td>Endovascular Treatment of Ruptured Large or Wi...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>Journal of stroke and cerebrovascular diseases...</td>\n",
       "      <td>J Stroke Cerebrovasc Dis</td>\n",
       "      <td>1532-8511</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>7/24/15</td>\n",
       "      <td></td>\n",
       "      <td>{\"Yibin Fang\",\"Pengfei Yang\",\"Jianmin Liu\",\"Yi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>847600</td>\n",
       "      <td>Experiences of African-American Women with Sma...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>Public health nursing (Boston, Mass.)</td>\n",
       "      <td>Public Health Nurs</td>\n",
       "      <td>1525-1446</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>11/4/15</td>\n",
       "      <td></td>\n",
       "      <td>{\"Ashley McDonald\",\"Shannon N Zenk\",\"Colleen C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>18298</td>\n",
       "      <td>Ulcer Healing After Peripheral Intervention.</td>\n",
       "      <td>ENG</td>\n",
       "      <td>Circulation journal : official journal of the ...</td>\n",
       "      <td>Circ. J.</td>\n",
       "      <td>1347-4820</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>7/4/14</td>\n",
       "      <td></td>\n",
       "      <td>{\"Daiki Uchida\",\"Yukihiro Saito\",\"Hisashi Uchi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>100986</td>\n",
       "      <td>Association of bleeding, mortality and sex in ...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>Journal of cardiovascular medicine (Hagerstown...</td>\n",
       "      <td>J Cardiovasc Med (Hagerstown)</td>\n",
       "      <td>1558-2035</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>9/23/14</td>\n",
       "      <td></td>\n",
       "      <td>{\"Rossana De Palma\",\"Paolo Ortolani\",\"Caterina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>71351</td>\n",
       "      <td>Differential dendritic cell-mediated activatio...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>Oral diseases</td>\n",
       "      <td>Oral Dis</td>\n",
       "      <td>1601-0825</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>3/22/14</td>\n",
       "      <td></td>\n",
       "      <td>{\"Nk Shukla\",\"Sn Das\",\"P Gaur\",\"Ak Singh\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>603381</td>\n",
       "      <td>WITHDRAWN: Zinc for the common cold.</td>\n",
       "      <td>eng</td>\n",
       "      <td>Cochrane Database Syst. Rev.</td>\n",
       "      <td></td>\n",
       "      <td>1469-493X</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>1/1/06</td>\n",
       "      <td>CD001364</td>\n",
       "      <td>{\"I Marshall\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>98479</td>\n",
       "      <td>The ROX coupler: Creation of a fixed iliofemor...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>Catheterization and cardiovascular interventio...</td>\n",
       "      <td>Catheter Cardiovasc Interv</td>\n",
       "      <td>1522-726X</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>10/24/14</td>\n",
       "      <td></td>\n",
       "      <td>{\"Adam Witkowski\",\"Ajay K Jain\",\"Ivan Casserly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>226417</td>\n",
       "      <td>Correct Performance of Pelvic Muscle Exercises...</td>\n",
       "      <td>ENG</td>\n",
       "      <td>Female pelvic medicine &amp; reconstructive surgery</td>\n",
       "      <td>Female Pelvic Med Reconstr Surg</td>\n",
       "      <td>2154-4212</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>10/27/14</td>\n",
       "      <td></td>\n",
       "      <td>{\"Katharine O'Dell\",\"Padma Kandadai\",\"Jyot Sai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                      article_title  \\\n",
       "1    498345  Ebola Virus GP Gene Polyadenylation Versus RNA...   \n",
       "109  255432  Pool fencing for preventing drowning in children.   \n",
       "181  657210  From task characteristics to learning: A syste...   \n",
       "196  143029  Steroids and antihistamines synergize to inhib...   \n",
       "209  979059  Difference in fascicle behaviors between super...   \n",
       "312  735083  Muscle variables of importance for physiologic...   \n",
       "323  226419  Anterior approach unilateral right sacrospinou...   \n",
       "340  734870  Stress across the life course and depression i...   \n",
       "356  232896  Detecting and monitoring the symptoms of Parki...   \n",
       "360   81959  Disruption of the gastroesophageal junction by...   \n",
       "445   98562  Prevalence of Masked Hypertension in African A...   \n",
       "522  600827  Nutritional supplementation for hip fracture a...   \n",
       "533   30272      Glial cells in amyotrophic lateral sclerosis.   \n",
       "549   26475  Situating universal design architecture: desig...   \n",
       "550  476191  Endovascular Treatment of Ruptured Large or Wi...   \n",
       "642  847600  Experiences of African-American Women with Sma...   \n",
       "768   18298       Ulcer Healing After Peripheral Intervention.   \n",
       "791  100986  Association of bleeding, mortality and sex in ...   \n",
       "861   71351  Differential dendritic cell-mediated activatio...   \n",
       "866  603381               WITHDRAWN: Zinc for the common cold.   \n",
       "887   98479  The ROX coupler: Creation of a fixed iliofemor...   \n",
       "997  226417  Correct Performance of Pelvic Muscle Exercises...   \n",
       "\n",
       "    article_language                                      journal_title  \\\n",
       "1                ENG                 The Journal of infectious diseases   \n",
       "109              eng        The Cochrane Database Of Systematic Reviews   \n",
       "181              ENG                 Scandinavian journal of psychology   \n",
       "196              ENG  European archives of oto-rhino-laryngology : o...   \n",
       "209              ENG                                     Muscle & nerve   \n",
       "312              ENG             European journal of applied physiology   \n",
       "323              ENG        International journal of colorectal disease   \n",
       "340              ENG      International journal of geriatric psychiatry   \n",
       "356              ENG                   Parkinsonism & related disorders   \n",
       "360              ENG  Diseases of the esophagus : official journal o...   \n",
       "445              ENG  Journal of clinical hypertension (Greenwich, C...   \n",
       "522              eng                       Cochrane Database Syst. Rev.   \n",
       "533              ENG                             Experimental neurology   \n",
       "549              ENG                      Disability and rehabilitation   \n",
       "550              ENG  Journal of stroke and cerebrovascular diseases...   \n",
       "642              ENG              Public health nursing (Boston, Mass.)   \n",
       "768              ENG  Circulation journal : official journal of the ...   \n",
       "791              ENG  Journal of cardiovascular medicine (Hagerstown...   \n",
       "861              ENG                                      Oral diseases   \n",
       "866              eng                       Cochrane Database Syst. Rev.   \n",
       "887              ENG  Catheterization and cardiovascular interventio...   \n",
       "997              ENG    Female pelvic medicine & reconstructive surgery   \n",
       "\n",
       "                jounral_abbreviation journal_issn article_jvolumn  \\\n",
       "1                    J. Infect. Dis.    1537-6613                   \n",
       "109       Cochrane Database Syst Rev    1469-493X                   \n",
       "181                  Scand J Psychol    1467-9450                   \n",
       "196        Eur Arch Otorhinolaryngol    1434-4726                   \n",
       "209                     Muscle Nerve    1097-4598                   \n",
       "312           Eur. J. Appl. Physiol.    1439-6327                   \n",
       "323             Int J Colorectal Dis    1432-1262                   \n",
       "340         Int J Geriatr Psychiatry    1099-1166                   \n",
       "356      Parkinsonism Relat. Disord.    1873-5126                   \n",
       "360                   Dis. Esophagus    1442-2050                   \n",
       "445     J Clin Hypertens (Greenwich)    1751-7176                   \n",
       "522                                     1469-493X                   \n",
       "533                     Exp. Neurol.    1090-2430                   \n",
       "549                  Disabil Rehabil    1464-5165                   \n",
       "550         J Stroke Cerebrovasc Dis    1532-8511                   \n",
       "642               Public Health Nurs    1525-1446                   \n",
       "768                         Circ. J.    1347-4820                   \n",
       "791    J Cardiovasc Med (Hagerstown)    1558-2035                   \n",
       "861                         Oral Dis    1601-0825                   \n",
       "866                                     1469-493X                   \n",
       "887       Catheter Cardiovasc Interv    1522-726X                   \n",
       "997  Female Pelvic Med Reconstr Surg    2154-4212                   \n",
       "\n",
       "    article_jissue article_jcreated_at article_pagination  \\\n",
       "1                               4/2/15                      \n",
       "109              2              1/1/00           CD001047   \n",
       "181                            2/17/10                      \n",
       "196                            8/13/14                      \n",
       "209                            9/10/15                      \n",
       "312                            10/8/15                      \n",
       "323                             1/7/15                      \n",
       "340                            10/9/15                      \n",
       "356                             3/7/15                      \n",
       "360                            2/28/14                      \n",
       "445                           10/20/14                      \n",
       "522              4              1/1/00           CD001880   \n",
       "533                            5/22/14                      \n",
       "549                            7/28/14              6-Jan   \n",
       "550                            7/24/15                      \n",
       "642                            11/4/15                      \n",
       "768                             7/4/14                      \n",
       "791                            9/23/14                      \n",
       "861                            3/22/14                      \n",
       "866              3              1/1/06           CD001364   \n",
       "887                           10/24/14                      \n",
       "997                           10/27/14                      \n",
       "\n",
       "                                           author_list  \n",
       "1    {\"Valentina A Volchkova\",\"Jaroslav Vorac\",\"Phi...  \n",
       "109                      {\"D C Thompson\",\"F P Rivara\"}  \n",
       "181  {\"Michiel A J Kompier\",\"Etty G A Wielenga-Meij...  \n",
       "196  {\"Shao-Cheng Liu\",\"Chi-Chung Wu\",\"Hsing-Won Wa...  \n",
       "209  {\"Hiroshi Akima\",\"Kazunori Nosaka\",\"Aya Tomita...  \n",
       "312  {\"Sebastien Racinais\",\"Olivier Girard\",\"Lars N...  \n",
       "323  {\"Enie Akhtar Bt Nawawi\",\"Ahlam M Al-Kharabshe...  \n",
       "340  {\"Stephen E Gilman\",\"Chaoqiang Jiang\",\"Kar Keu...  \n",
       "356  {\"S Arora\",\"A Zhan\",\"S Donohue\",\"E R Dorsey\",\"...  \n",
       "360                         {\"Y Y Lee\",\"K E L McColl\"}  \n",
       "445  {\"Frances Williams\",\"Alehegn Gelaye\",\"Susan St...  \n",
       "522                        {\"A Avenell\",\"H H Handoll\"}  \n",
       "533                      {\"T Philips\",\"J D Rothstein\"}  \n",
       "549                                     {\"Paul Jones\"}  \n",
       "550  {\"Yibin Fang\",\"Pengfei Yang\",\"Jianmin Liu\",\"Yi...  \n",
       "642  {\"Ashley McDonald\",\"Shannon N Zenk\",\"Colleen C...  \n",
       "768  {\"Daiki Uchida\",\"Yukihiro Saito\",\"Hisashi Uchi...  \n",
       "791  {\"Rossana De Palma\",\"Paolo Ortolani\",\"Caterina...  \n",
       "861         {\"Nk Shukla\",\"Sn Das\",\"P Gaur\",\"Ak Singh\"}  \n",
       "866                                     {\"I Marshall\"}  \n",
       "887  {\"Adam Witkowski\",\"Ajay K Jain\",\"Ivan Casserly...  \n",
       "997  {\"Katharine O'Dell\",\"Padma Kandadai\",\"Jyot Sai...  "
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_dirty[rayyan_dirty.iloc[:,6]!=rayyan_clean.iloc[:,6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_MODE=disabled accelerate launch src/train_bash.py     --stage sft     --model_name_or_path /data/yanmengyi/huggingface/vicuna-13b-1.3     --do_train     --finetuning_type lora     --dataset beer-train-15     --output_dir lora_weight/beer/beer-train-15 --overwrite_output_dir     --lr_scheduler_type cosine     --num_train_epochs 15.0     --gradient_accumulation_steps 8     --per_device_eval_batch_size 8     --fp16     --template vicuna     --lora_r 16     --quantization_bit 8 --logging_steps 5 --plot_loss  --lora_target q_proj,v_proj --save_steps 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean: 1/2/03\n",
    "dirty: 3/1/02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>article_title</th>\n",
       "      <th>article_language</th>\n",
       "      <th>journal_title</th>\n",
       "      <th>jounral_abbreviation</th>\n",
       "      <th>journal_issn</th>\n",
       "      <th>article_jvolumn</th>\n",
       "      <th>article_jissue</th>\n",
       "      <th>article_jcreated_at</th>\n",
       "      <th>article_pagination</th>\n",
       "      <th>author_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>589465</td>\n",
       "      <td>Effect of a cooling hood on physiological resp...</td>\n",
       "      <td>eng</td>\n",
       "      <td>J Appl Physiol</td>\n",
       "      <td>Journal of applied physiology</td>\n",
       "      <td>0021-8987 (Print)     0021-8987</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>1/1/70</td>\n",
       "      <td>36-9</td>\n",
       "      <td>{\"E. Shvartz\"}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                      article_title  \\\n",
       "705  589465  Effect of a cooling hood on physiological resp...   \n",
       "\n",
       "    article_language   journal_title           jounral_abbreviation  \\\n",
       "705              eng  J Appl Physiol  Journal of applied physiology   \n",
       "\n",
       "                        journal_issn article_jvolumn article_jissue  \\\n",
       "705  0021-8987 (Print)     0021-8987              29              1   \n",
       "\n",
       "    article_jcreated_at article_pagination     author_list  \n",
       "705              1/1/70               36-9  {\"E. Shvartz\"}  "
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_dirty[rayyan_dirty['article_jcreated_at']=='1/1/70']\n",
    "# rayyan_dirty[rayyan_dirty['journal_title']=='Anaesthesia']\n",
    "# rayyan_clean.iloc[:,8].value_counts()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell: 9788480000000\n",
      "Status: Clean\n",
      "\n",
      "Cell: Mar-22\n",
      "Status: Dirty\n",
      "\n",
      "Cell: Feb-14\n",
      "Status: Dirty\n",
      "\n",
      "Cell: Jan-15\n",
      "Status: Dirty\n",
      "\n",
      "Cell: Mar-17\n",
      "Status: Dirty\n",
      "\n",
      "Cell: Feb-14\n",
      "Status: Dirty\n",
      "\n",
      "Cell: Jan-14\n",
      "Status: Dirty\n",
      "\n",
      "Cell: Sep-22\n",
      "Status: Dirty\n",
      "\n",
      "Cell: Sep-22\n",
      "Status: Dirty\n",
      "\n",
      "Cell: Sep-22\n",
      "Status: Dirty\n",
      "\n",
      "Cell: Mar-22\n",
      "Status: Dirty\n",
      "\n",
      "Cell: Mar-09\n",
      "Status: Dirty\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def is_dirty(cell):\n",
    "    # Regular expression to detect if the cell is in the format Mon-DD\n",
    "    date_pattern = re.compile(r'^[A-Za-z]{3}-\\d{1,2}$')\n",
    "    \n",
    "    # Regular expression to detect if the ISSN starts with a number other than 9\n",
    "    issn_pattern = re.compile(r'^[^9]\\d{12}$')\n",
    "    \n",
    "    return bool(date_pattern.match(cell) or issn_pattern.match(cell))\n",
    "def generate_dirty_cell(cell):\n",
    "    # If the cell matches the format DD-Mon, reverse it to Mon-DD\n",
    "    date_match = re.match(r'(\\d{1,2})-([A-Za-z]{3})', cell)\n",
    "    if date_match:\n",
    "        return f\"{date_match.group(2)}-{date_match.group(1)}\"\n",
    "    \n",
    "    # If the cell matches the ISSN format starting with '9', change the leading '9' to another number\n",
    "    issn_match = re.match(r'^9\\d{12}$', cell)\n",
    "    if issn_match:\n",
    "        return f\"{random.choice(['0', '1', '2', '3', '4', '5', '6', '7', '8'])}{cell[1:]}\"\n",
    "    \n",
    "    return cell\n",
    "# Test\n",
    "cells_to_check = ['0035-9157 (Print) 0035-9157', '1537-6613', '0301-4738', '', '1558-0520', '0007-1420', '1385-4046', '1361-9209', '1940-5901', '0001-6349 (Print) 0001-6349', '1547-5271', '0007-0963', '1178-1998', '1469-493X', 'Jan-68', '0195-9131', '1940-6215', '1432-086X', '1527-7755', '1916-9736', '1555-2101', '1932-6203', '15265161 (ISSN)', '1598-9992', '0195-668X']\n",
    "\n",
    "for cells in rayyan_dirty[rayyan_dirty.iloc[:,5]!=rayyan_clean.iloc[:,5]].iloc[:,5].to_list():\n",
    "    cell = cells\n",
    "    print(f\"Cell: {cell}\")\n",
    "    print(f\"Status: {'Dirty' if is_dirty(cell) else 'Clean'}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def is_dirty(cell):\n",
    "    # Regular expression to detect if the cell contains special characters or combining diacritical marks\n",
    "    pattern = re.compile(r'[�\\u0300-\\u036F]')\n",
    "    return bool(pattern.search(cell))\n",
    "\n",
    "# Test\n",
    "# cells_to_check = ['Late repair of injuries of the anal sphincter', 'Ebola Virus GP Gene Polyadenylation Versus RNA Editing.', 'Duane retraction syndrome associated with oculocutaneous albinism: an ocular miswiring.', '[Noninvasive prenatal diagnosis of trisomy 21, 18 and 13 using cell-free fetal DNA]', 'Diagnosis and Management of Cutaneous B-cell Lymphoma.']\n",
    "\n",
    "# for cells in rayyan_dirty[rayyan_dirty.iloc[:,1]!=rayyan_clean.iloc[:,1]].iloc[:,1].to_list():\n",
    "#     cell = cells\n",
    "#     print(f\"Cell: {cell}\")\n",
    "#     print(f\"Status: {'Dirty' if is_dirty(cell) else 'Clean'}\\n\")\n",
    "import re\n",
    "\n",
    "def clean_cell(cell):\n",
    "    # Remove special characters like �\n",
    "    cleaned = re.sub(r'�', '', cell)\n",
    "    \n",
    "    # Remove combining diacritical marks (from Unicode range U+0300 to U+036F)\n",
    "    cleaned = re.sub(r'[\\u0300-\\u036F]', '', cleaned)\n",
    "    \n",
    "    return cleaned\n",
    "def generate_dirty_cell(cell):\n",
    "    # List of special characters and combining diacritical marks\n",
    "    special_chars = ['�', '\\u0301', '\\u0300', '\\u0302', '\\u0303', '\\u0304']\n",
    "    \n",
    "    # Randomly choose a special character\n",
    "    char = random.choice(special_chars)\n",
    "    \n",
    "    # Randomly choose a position to insert the special character\n",
    "    position = random.randint(0, len(cell))\n",
    "    \n",
    "    # Insert the special character at the chosen position\n",
    "    dirty_cell = cell[:position] + char + cell[position:]\n",
    "    \n",
    "    return dirty_cell\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/beer-test-20.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[result['predict']!=result['output']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def is_dirty(cell):\n",
    "    # Regular expression to detect the character 'x' in places where it's likely an error\n",
    "    pattern = re.compile(r'\\bx|\\bx\\b|[^a-z]x[^a-z]', re.IGNORECASE)\n",
    "    \n",
    "    if pattern.search(cell):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Test\n",
    "cells =clean_list\n",
    "\n",
    "for cell in cells:\n",
    "    print(f\"{cell}: {'Dirty' if is_dirty(cell) else 'Clean'}\")\n",
    "    print(cell.__contains__('x'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['10018', '10019', '10001', '10005', '10006', '10007', '10008', '10009', '10010', '10011', '10012', '10015', '10016', '10038', '10055', '10056', '10085', '10086', '10087', '10108', '10158', '10164', '20017', '20018', '10021', '10022', '10023', '10024', '10025', '10027', '10029', '10032', '10033', '10034', '10035', '10036', '10039', '10040', '10043', '10044', '10045', '10046', '10047', '10049', '10050'] are clean cells with pattern re.match(r'^\\\\d{5}$', and ['1xx19', 'x0005', '1000x', 'x00xx', 'x00x5', '1xx15', '1xx16', '100x8', '100x6', 'x0x08', '1xx24', 'x0027', 'x0029', '1xx29', '1xx32', '100x4', '100x5', '1xx35', '1xx36', '1003x', '1xx39', '100x9', '1xx44', '1xx45', 'x0045', '1xx47', '1004x'] are dirty cells. Please conclude a general pattern for dirty and clean cells, and write a general Python function to transfer a given clean cell to dirty cell.\""
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_dirty(cell):\n",
    "    \"\"\"Return True if the cell is dirty, otherwise False.\"\"\"\n",
    "    # If the cell matches the pattern of 5 digits, it's clean.\n",
    "    # If not, it's dirty.\n",
    "    return not bool(re.match(r'^\\d{5}$', cell))\n",
    "dirty = [c for c in clean_list if is_dirty(c)]\n",
    "clean = [c for c in clean_list if not is_dirty(c)]\n",
    "query = '%s are clean cells with pattern %s, and %s are dirty cells. Please conclude a general pattern for dirty and clean cells, and write a general Python function to transfer a given clean cell to dirty cell.' % (clean,\"re.match(r'^\\d{5}$'\",dirty)\n",
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From Here, We Try to write a function to construct Multi-view Framework for the Plain Version and Augmented Version of Critic LM Model\n",
    "- Single-View: COL ProviderID VAL 1001x SEP COL ProviderID VAL 1001x\n",
    "- Multi-View: COL A VAL ValueA COL B VAL ValueB SEP COL A VAL ValueA\n",
    "- Column-View: Extract from Same Cluster\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([532, 157, 213, 228, 238])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hospital_dirty.shape\n",
    "selected_index_hospital_5 = selected_index_hospital[:5]\n",
    "# selected_index_hospital_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Construct the Init Multi-view Training Set\n",
    "## 找到选中的20个tuple\n",
    "selected_index_hospital_5 = selected_index_hospital[:20]\n",
    "input_matrix_select = input_matrix[selected_index_hospital_5]\n",
    "row_list = []\n",
    "single_list = []\n",
    "column_list = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "for label_tuple in selected_index_hospital_5:\n",
    "    cluster_index = hospital_clean.iloc[label_tuple,1]\n",
    "    cluster_set_index = hospital_clean[hospital_clean['ProviderNumber']==cluster_index].index\n",
    "    cluster_set = hospital_dirty.iloc[cluster_set_index]\n",
    "    for i in range(len(hospital_clean.columns)):\n",
    "        columns_unique = list(cluster_set.iloc[:,i].unique())\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = hospital_clean.iloc[label_tuple]\n",
    "        dirty_context = hospital_dirty.iloc[label_tuple]\n",
    "        clean_cell = hospital_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = hospital_dirty.iloc[label_tuple,i]\n",
    "        single_context_clean = 'COL %s VAL %s ' % (hospital_dirty.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (hospital_dirty.columns[i],dirty_cell)\n",
    "        column_context = 'COL %s VAL %s ' % (hospital_dirty.columns[i],columns_unique)\n",
    "        for c in range(20):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (hospital_clean.columns[c],hospital_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (hospital_dirty.columns[c],hospital_dirty.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            row_list.append([all_context_dirty,single_context_dirty,1])\n",
    "            row_list.append([all_context_clean,single_context_clean,0])\n",
    "            single_list.append([single_context_dirty,single_context_dirty,1])\n",
    "            single_list.append([single_context_clean,single_context_clean,0])\n",
    "            column_list.append([column_context,single_context_dirty,1])\n",
    "            column_list.append([column_context,single_context_clean,0])\n",
    "            # detector_list.append([all_context_dirty,single_context_clean,0])\n",
    "            # detector_list.append([single_context_dirty,1])\n",
    "        else:\n",
    "            row_list.append([all_context_dirty,single_context_dirty,0])\n",
    "            row_list.append([all_context_clean,single_context_clean,0])\n",
    "            single_list.append([single_context_clean,single_context_clean,0])\n",
    "            column_list.append([column_context,single_context_clean,0])\n",
    "            # detector_list.append([all_context_dirty,single_context_clean,0])\n",
    "        # else:\n",
    "        #     detector_list.append([all_context_dirty,1])\n",
    "        #     detector_list.append([single_context_dirty,1])\n",
    "            \n",
    "                # break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.concatenate([single_list,row_list,column_list])).to_csv('/home/yanmy/raha/raha-master/datasets/hospital/detector/multi-view/train_20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def insert_randomly(lst, element):\n",
    "    # 选择一个随机的索引\n",
    "    index = random.randint(0, len(lst))\n",
    "    \n",
    "    # 在随机位置插入新元素\n",
    "    lst.insert(index, element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27048/2524762027.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dirty_row[i] = dirty_cell\n"
     ]
    }
   ],
   "source": [
    "# input_matrix_select = input_matrix[selected_index_hospital_5]\n",
    "## Coreset Generation, used to augment Critic Model \n",
    "hospital_coreset = hospital_dirty[hospital_dirty['count']==0].index\n",
    "noise_col = np.where(input_matrix[selected_index_hospital_5].sum(axis=0)!=0)[0] ## 不使用不可靠的detector\n",
    "row_list = []\n",
    "single_list = []\n",
    "column_list = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "for label_tuple in hospital_coreset:\n",
    "    cluster_index = hospital_clean.iloc[label_tuple,1]\n",
    "    cluster_set_index = hospital_clean[hospital_clean['ProviderNumber']==cluster_index].index\n",
    "    cluster_set = hospital_dirty.iloc[cluster_set_index]\n",
    "    noise_col_subset = np.random.choice(noise_col,5,replace=False) ## 取5个col作为注入噪声的对象\n",
    "    for i in noise_col_subset:\n",
    "        original_row = hospital_dirty.iloc[label_tuple]\n",
    "        dirty_row = original_row\n",
    "        clean_cell = hospital_dirty.iloc[label_tuple,i]\n",
    "        dirty_cell = replace_random_char_with_x(clean_cell) ## Inject Noise\n",
    "        columns_unique = list(cluster_set.iloc[:,i].unique())\n",
    "        insert_randomly(columns_unique,dirty_cell)\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        dirty_row[i] = dirty_cell\n",
    "        single_context_clean = 'COL %s VAL %s ' % (hospital_dirty.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (hospital_dirty.columns[i],dirty_cell)\n",
    "        column_context = 'COL %s VAL %s ' % (hospital_dirty.columns[i],columns_unique)\n",
    "        for c in range(20):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (hospital_dirty.columns[c],original_row[c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (hospital_dirty.columns[c],dirty_row[c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            row_list.append([all_context_dirty,single_context_dirty,1])\n",
    "            row_list.append([all_context_clean,single_context_clean,0])\n",
    "            single_list.append([single_context_dirty,single_context_dirty,1])\n",
    "            single_list.append([single_context_clean,single_context_clean,0])\n",
    "            column_list.append([column_context,single_context_dirty,1])\n",
    "            column_list.append([column_context,single_context_clean,0])\n",
    "            # detector_list.append([all_context_dirty,single_context_clean,0])\n",
    "            # detector_list.append([single_context_dirty,1])\n",
    "        else:\n",
    "            row_list.append([all_context_dirty,single_context_dirty,0])\n",
    "            row_list.append([all_context_clean,single_context_clean,0])\n",
    "            single_list.append([single_context_clean,single_context_clean,0])\n",
    "            column_list.append([column_context,single_context_clean,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.concatenate([single_list,row_list,column_list])).to_csv('/home/yanmy/raha/raha-master/datasets/hospital/detector/multi-view/train_augment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Construct the Init Multi-view Training Set\n",
    "## 找到选中的20个tuple\n",
    "# input_matrix_select = input_matrix[selected_index_hospital]\n",
    "row_list = []\n",
    "single_list = []\n",
    "column_list = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "for label_tuple in range(len(hospital_dirty)):\n",
    "    cluster_index = hospital_clean.iloc[label_tuple,1]\n",
    "    cluster_set_index = hospital_clean[hospital_clean['ProviderNumber']==cluster_index].index\n",
    "    cluster_set = hospital_dirty.iloc[cluster_set_index]\n",
    "    for i in range(len(hospital_clean.columns)):\n",
    "        columns_unique = list(cluster_set.iloc[:,i].unique())\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = hospital_clean.iloc[label_tuple]\n",
    "        dirty_context = hospital_dirty.iloc[label_tuple]\n",
    "        clean_cell = hospital_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = hospital_dirty.iloc[label_tuple,i]\n",
    "        single_context_clean = 'COL %s VAL %s ' % (hospital_dirty.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (hospital_dirty.columns[i],dirty_cell)\n",
    "        column_context = 'COL %s VAL %s ' % (hospital_dirty.columns[i],columns_unique)\n",
    "        for c in range(20):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (hospital_clean.columns[c],hospital_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (hospital_dirty.columns[c],hospital_dirty.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            row_list.append([all_context_dirty,single_context_dirty,1])\n",
    "            # row_list.append([all_context_clean,single_context_clean,0])\n",
    "            single_list.append([single_context_dirty,single_context_dirty,1])\n",
    "            # single_list.append([single_context_clean,single_context_clean,0])\n",
    "            column_list.append([column_context,single_context_dirty,1])\n",
    "            # column_list.append([column_context,single_context_clean,0])\n",
    "            # detector_list.append([all_context_dirty,single_context_clean,0])\n",
    "            # detector_list.append([single_context_dirty,1])\n",
    "        else:\n",
    "            row_list.append([all_context_dirty,single_context_dirty,0])\n",
    "            # row_list.append([all_context_clean,single_context_clean,0])\n",
    "            single_list.append([single_context_clean,single_context_clean,0])\n",
    "            column_list.append([column_context,single_context_clean,0])\n",
    "            # detector_list.append([all_context_dirty,single_context_clean,0])\n",
    "        # else:\n",
    "        #     detector_list.append([all_context_dirty,1])\n",
    "        #     detector_list.append([single_context_dirty,1])\n",
    "            \n",
    "                # break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.concatenate([single_list,row_list,column_list])).to_csv('/home/yanmy/raha/raha-master/datasets/hospital/detector/multi-view/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hospital_dirty\n",
    "def is_dirty(cell):\n",
    "    # Regular expression to detect the character 'x'\n",
    "    pattern = re.compile(r'x', re.IGNORECASE)\n",
    "    \n",
    "    if pattern.search(cell):\n",
    "        return True\n",
    "    return False\n",
    "def generate_dirty_cell(cells):\n",
    "    \"\"\"Generate a random dirty cell based on the input cells.\"\"\"\n",
    "    # Randomly select a cell from the input cells.\n",
    "    cell = random.choice([c for c in cells if not is_dirty(c)])\n",
    "    \n",
    "    # Randomly select a position in the cell.\n",
    "    position = random.randint(0, len(cell) - 1)\n",
    "    \n",
    "    # Replace the character at that position with 'x'.\n",
    "    dirty_cell = cell[:position] + 'x' + cell[position+1:]\n",
    "    \n",
    "    return cell,dirty_cell\n",
    "def function_set_value_count(row):\n",
    "    count = 0\n",
    "    for x,y in row.items():\n",
    "        if(is_dirty(y)):\n",
    "            count += 1\n",
    "    return count\n",
    "hospital_dirty['count'] = hospital_dirty.apply(function_set_value_count,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([532, 157, 213, 228, 238, 256, 332, 386, 411, 493, 572, 698, 956,\n",
       "        22,  24,  42,  56,  57,  93,  94])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_index_hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 20)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector = np.load('datasets/hospital/detector/detector_5.npy').reshape((1000,-1))\n",
    "detector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coreset_detect = detector\n",
    "coreset_detect = np.where(detector.sum(axis=1)==0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,  16],\n",
       "       [  3,   6],\n",
       "       [  7,   6],\n",
       "       ...,\n",
       "       [986,   3],\n",
       "       [987,   2],\n",
       "       [997,  16]])"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(detector==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:00<00:00, 66.69it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "noise_col = np.where(detector.sum(axis=0)>0)[0]\n",
    "header = list(hospital_dirty.columns)\n",
    "safe_value = ['empty'] ## 不注入噪声的类型\n",
    "training_list = []\n",
    "for h in tqdm(hospital_cluster): ## 对比学习的采样策略在同cluster内进行\n",
    "    coreset_subset = [n for n in hospital_cluster[h] if n in coreset_detect]\n",
    "    for coreset_tuple in coreset_subset: ## 取遍coreset所有的tuple，剩下的n-1 tuple是参考对象,coreset_tuple是index\n",
    "        noise_col_subset = np.random.choice(noise_col,5,replace=False) ## 取5个col作为注入噪声的对象\n",
    "        for noise_col_single in noise_col_subset:\n",
    "            col_name = header[noise_col_single] ## 从index转成列名\n",
    "            if(hospital_dirty_dict[coreset_tuple][col_name] not in safe_value and len(coreset_subset)>2):\n",
    "                temp_dict = hospital_dirty.iloc[coreset_tuple,1:].to_dict() ## coreset tuple字典\n",
    "                clean_cell = temp_dict[col_name]\n",
    "                dirty_cell = replace_random_char_with_x(clean_cell) ## 注入噪声\n",
    "                temp_dict[col_name] = dirty_cell\n",
    "                coreset_reference = select_two_different_elements(coreset_subset,coreset_tuple)\n",
    "                template_dict = {}\n",
    "                clean_dict = {}\n",
    "                template_dict[col_name] = ''\n",
    "                clean_dict[col_name] = clean_cell\n",
    "                text_head = 'You are an expert in Cleaning Hospital Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\nTake these rows as reference:\\n\\n' % (col_name, json.dumps(template_dict), json.dumps(temp_dict))\n",
    "                \n",
    "                ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]), json.dumps(hospital_dirty_dict[coreset_reference[1]]))\n",
    "                \n",
    "                # ICL_text = '%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]))\n",
    "                \n",
    "                training_list.append([text_head,ICL_text,'',json.dumps(clean_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(training_list).to_csv('datasets/hospital/detector/correction_aug.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Labeling Data\n",
    "training_list_label = []\n",
    "# for label_tuple in tqdm(selected_index_hospital):\n",
    "#     for noise_col_single in range(len(header)):\n",
    "for label_tuple,noise_col_single in np.argwhere(input_matrix==1):\n",
    "    if(label_tuple in selected_index_hospital):\n",
    "        col_name = header[noise_col_single]\n",
    "        template_dict = {}\n",
    "        template_dict[col_name] = ''\n",
    "        clean_dict = {}\n",
    "        temp_dict = hospital_dirty.iloc[label_tuple,1:].to_dict()\n",
    "        clean_cell = hospital_clean.iloc[label_tuple,noise_col_single]\n",
    "        dirty_cell = hospital_dirty.iloc[label_tuple,noise_col_single]\n",
    "        clean_dict[col_name] = clean_cell\n",
    "        # if(clean_cell!=dirty_cell):\n",
    "        text_head = 'You are an expert in Cleaning Hospital Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\nTake these rows as reference:\\n\\n' % (col_name, json.dumps(template_dict), json.dumps(temp_dict))\n",
    "        cluster = [hospital_cluster[h] for h in hospital_cluster if hospital_cluster[h].__contains__(label_tuple)][0] ## 找到label tuple所在的簇\n",
    "        cluster_coreset = [c for c in cluster if c in coreset_detect] ## 找到簇内coreset元素\n",
    "        try:\n",
    "                coreset_reference = np.random.choice(cluster_coreset,2) ## 取两个\n",
    "                ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]), json.dumps(hospital_dirty_dict[coreset_reference[1]]))\n",
    "                # ICL_text = '%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]))\n",
    "        except:\n",
    "                coreset_reference = np.random.choice(cluster_coreset,1) ## 取两个\n",
    "                # ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]), json.dumps(hospital_dirty_dict[coreset_reference[1]]))\n",
    "                ICL_text = '%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]))\n",
    "        training_list_label.append([text_head,ICL_text,'',json.dumps(clean_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test\n",
    "training_list_label = []\n",
    "# for label_tuple in tqdm(selected_index_hospital):\n",
    "#     for noise_col_single in range(len(header)):\n",
    "for label_tuple,noise_col_single in np.argwhere(detector==1):\n",
    "        col_name = header[noise_col_single]\n",
    "        template_dict = {}\n",
    "        template_dict[col_name] = ''\n",
    "        clean_dict = {}\n",
    "        temp_dict = hospital_dirty.iloc[label_tuple,1:].to_dict()\n",
    "        clean_cell = hospital_clean.iloc[label_tuple,noise_col_single]\n",
    "        dirty_cell = hospital_dirty.iloc[label_tuple,noise_col_single]\n",
    "        clean_dict[col_name] = clean_cell\n",
    "        # if(clean_cell!=dirty_cell):\n",
    "        text_head = 'You are an expert in Cleaning Hospital Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\nTake these rows as reference:\\n\\n' % (col_name, json.dumps(template_dict), json.dumps(temp_dict))\n",
    "        cluster = [hospital_cluster[h] for h in hospital_cluster if hospital_cluster[h].__contains__(label_tuple)][0] ## 找到label tuple所在的簇\n",
    "        cluster_coreset = [c for c in cluster if c in coreset_detect] ## 找到簇内coreset元素\n",
    "        try:\n",
    "                coreset_reference = np.random.choice(cluster_coreset,2) ## 取两个\n",
    "                ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]), json.dumps(hospital_dirty_dict[coreset_reference[1]]))\n",
    "                # ICL_text = '%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]))\n",
    "        except:\n",
    "                coreset_reference = np.random.choice(cluster_coreset,1) ## 取两个\n",
    "                # ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]), json.dumps(hospital_dirty_dict[coreset_reference[1]]))\n",
    "                ICL_text = '%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]))\n",
    "        training_list_label.append([text_head,ICL_text,'',json.dumps(clean_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...</td>\n",
       "      <td></td>\n",
       "      <td>{\"MeasureCode\": \"scip-inf-1\"}</td>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td></td>\n",
       "      <td>{\"MeasureCode\": \"scip-inf-1\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...</td>\n",
       "      <td></td>\n",
       "      <td>{\"EmergencyService\": \"yes\"}</td>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td></td>\n",
       "      <td>{\"EmergencyService\": \"yes\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...</td>\n",
       "      <td></td>\n",
       "      <td>{\"PhoneNumber\": \"2053258100\"}</td>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td></td>\n",
       "      <td>{\"PhoneNumber\": \"2053258100\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ZipCode\": \"35233\"}</td>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ZipCode\": \"35233\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...</td>\n",
       "      <td></td>\n",
       "      <td>{\"State\": \"al\"}</td>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td></td>\n",
       "      <td>{\"State\": \"al\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10034\", \"HospitalName\": \"c...</td>\n",
       "      <td></td>\n",
       "      <td>{\"Condition\": \"heart failure\"}</td>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td></td>\n",
       "      <td>{\"Condition\": \"heart failure\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10034\", \"HospitalName\": \"c...</td>\n",
       "      <td></td>\n",
       "      <td>{\"Score\": \"88%\"}</td>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td></td>\n",
       "      <td>{\"Score\": \"88%\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10049\", \"HospitalName\": \"m...</td>\n",
       "      <td></td>\n",
       "      <td>{\"HospitalName\": \"medical center enterprise\"}</td>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td></td>\n",
       "      <td>{\"HospitalName\": \"medical center enterprise\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10049\", \"HospitalName\": \"m...</td>\n",
       "      <td></td>\n",
       "      <td>{\"Address1\": \"400 n edwards street\"}</td>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td></td>\n",
       "      <td>{\"Address1\": \"400 n edwards street\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td>{\"ProviderNumber\": \"10049\", \"HospitalName\": \"m...</td>\n",
       "      <td></td>\n",
       "      <td>{\"HospitalType\": \"acute care hospitals\"}</td>\n",
       "      <td>You are an expert in Cleaning Hospital Dataset...</td>\n",
       "      <td></td>\n",
       "      <td>{\"HospitalType\": \"acute care hospitals\"}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3055 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0  \\\n",
       "0   You are an expert in Cleaning Hospital Dataset...   \n",
       "1   You are an expert in Cleaning Hospital Dataset...   \n",
       "2   You are an expert in Cleaning Hospital Dataset...   \n",
       "3   You are an expert in Cleaning Hospital Dataset...   \n",
       "4   You are an expert in Cleaning Hospital Dataset...   \n",
       "..                                                ...   \n",
       "49  You are an expert in Cleaning Hospital Dataset...   \n",
       "50  You are an expert in Cleaning Hospital Dataset...   \n",
       "51  You are an expert in Cleaning Hospital Dataset...   \n",
       "52  You are an expert in Cleaning Hospital Dataset...   \n",
       "53  You are an expert in Cleaning Hospital Dataset...   \n",
       "\n",
       "                                                    1 2   \\\n",
       "0   {\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...      \n",
       "1   {\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...      \n",
       "2   {\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...      \n",
       "3   {\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...      \n",
       "4   {\"ProviderNumber\": \"10018\", \"HospitalName\": \"c...      \n",
       "..                                                ... ..   \n",
       "49  {\"ProviderNumber\": \"10034\", \"HospitalName\": \"c...      \n",
       "50  {\"ProviderNumber\": \"10034\", \"HospitalName\": \"c...      \n",
       "51  {\"ProviderNumber\": \"10049\", \"HospitalName\": \"m...      \n",
       "52  {\"ProviderNumber\": \"10049\", \"HospitalName\": \"m...      \n",
       "53  {\"ProviderNumber\": \"10049\", \"HospitalName\": \"m...      \n",
       "\n",
       "                                                3  \\\n",
       "0                   {\"MeasureCode\": \"scip-inf-1\"}   \n",
       "1                     {\"EmergencyService\": \"yes\"}   \n",
       "2                   {\"PhoneNumber\": \"2053258100\"}   \n",
       "3                            {\"ZipCode\": \"35233\"}   \n",
       "4                                 {\"State\": \"al\"}   \n",
       "..                                            ...   \n",
       "49                 {\"Condition\": \"heart failure\"}   \n",
       "50                               {\"Score\": \"88%\"}   \n",
       "51  {\"HospitalName\": \"medical center enterprise\"}   \n",
       "52           {\"Address1\": \"400 n edwards street\"}   \n",
       "53       {\"HospitalType\": \"acute care hospitals\"}   \n",
       "\n",
       "                                          instruction input  \\\n",
       "0   You are an expert in Cleaning Hospital Dataset...         \n",
       "1   You are an expert in Cleaning Hospital Dataset...         \n",
       "2   You are an expert in Cleaning Hospital Dataset...         \n",
       "3   You are an expert in Cleaning Hospital Dataset...         \n",
       "4   You are an expert in Cleaning Hospital Dataset...         \n",
       "..                                                ...   ...   \n",
       "49  You are an expert in Cleaning Hospital Dataset...         \n",
       "50  You are an expert in Cleaning Hospital Dataset...         \n",
       "51  You are an expert in Cleaning Hospital Dataset...         \n",
       "52  You are an expert in Cleaning Hospital Dataset...         \n",
       "53  You are an expert in Cleaning Hospital Dataset...         \n",
       "\n",
       "                                           output  \n",
       "0                   {\"MeasureCode\": \"scip-inf-1\"}  \n",
       "1                     {\"EmergencyService\": \"yes\"}  \n",
       "2                   {\"PhoneNumber\": \"2053258100\"}  \n",
       "3                            {\"ZipCode\": \"35233\"}  \n",
       "4                                 {\"State\": \"al\"}  \n",
       "..                                            ...  \n",
       "49                 {\"Condition\": \"heart failure\"}  \n",
       "50                               {\"Score\": \"88%\"}  \n",
       "51  {\"HospitalName\": \"medical center enterprise\"}  \n",
       "52           {\"Address1\": \"400 n edwards street\"}  \n",
       "53       {\"HospitalType\": \"acute care hospitals\"}  \n",
       "\n",
       "[3055 rows x 7 columns]"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_list = pd.concat([pd.DataFrame(training_list),pd.DataFrame(training_list_label)]).drop_duplicates()\n",
    "training_list['instruction'] = training_list[0] + training_list[1]\n",
    "training_list['input'] = training_list[2]\n",
    "training_list['output'] = training_list[3]\n",
    "training_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list_pd = pd.DataFrame(training_list_label)\n",
    "training_list_pd['instruction'] = training_list_pd[0] + training_list_pd[1]\n",
    "training_list_pd['input'] = training_list_pd[2]\n",
    "training_list_pd['output'] = training_list_pd[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json.dump(training_list_pd.iloc[:,-3:].to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/hospital/hospital-train-5.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(training_list.iloc[:,-3:].to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/hospital/hospital-train-20.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,    0,    0,    0, 2410,  693,    0,    0,    0,  127,  127]),\n",
       " array([0, 0, 0, 0, 5, 5, 0, 0, 0, 5, 5]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Beer Detection Function By Rows\n",
    "input_matrix_beer = np.array(beer_clean!=beer_dirty).astype(int)\n",
    "input_matrix_beer.sum(axis=0),input_matrix_beer[beer_label_index[:5]].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ounces', 'abv', 'city', 'state'], dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer_clean.columns[[4,5,-2,-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_dirty(cell):\n",
    "    # Regular expression to detect if the cell contains anything other than whole numbers represented as strings\n",
    "    pattern = re.compile(r'^\\d+$')\n",
    "    return not bool(pattern.match(cell))\n",
    "is_dirty('19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2410, 2410)"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for c in beer_dirty[beer_dirty.iloc[:,i]!=beer_clean.iloc[:,i]].iloc[:,i].to_list():\n",
    "    if(Beer_Detection_Ounces(c)):\n",
    "        count += 1\n",
    "count,len(beer_dirty[beer_dirty.iloc[:,i]!=beer_clean.iloc[:,i]].iloc[:,i].to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'16 OZ.'"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Beer Detection Function By Rows\n",
    "import random\n",
    "def Beer_Detection_Ounces(cell):\n",
    "    pattern = re.compile(r'^\\d+$')\n",
    "    return not bool(pattern.match(cell))\n",
    "def correct_dirty_cell(cell):\n",
    "    # Regular expression to extract the numeric value from the cell\n",
    "    match = re.search(r'(\\d+(\\.\\d+)?)', cell)\n",
    "    if match:\n",
    "        value = match.group(1)\n",
    "        # Convert to integer string if the value ends with \".0\"\n",
    "        if value.endswith('.0'):\n",
    "            return str(int(float(value)))\n",
    "        return value\n",
    "    return cell\n",
    "def Beer_Generation_Ounces(cell):\n",
    "    descriptors = [\" oz.\", \" ounce\", \" OZ.\", \" oz. Alumi-Tek\", \" oz. Silo Can\"]\n",
    "    value = correct_dirty_cell(cell)\n",
    "    descriptor = random.choice(descriptors)\n",
    "    \n",
    "    # Append the descriptor to the cell to make it dirty\n",
    "    dirty_cell = value + descriptor\n",
    "    \n",
    "    return dirty_cell\n",
    "\n",
    "def Beer_Detection_abv(cell):\n",
    "    # Regular expression to detect if the cell contains \"%\" or has more than three decimal places\n",
    "    pattern = re.compile(r'%|^\\d+\\.\\d{4,}$')\n",
    "    return bool(pattern.search(cell))\n",
    "def Beer_Generation_abv(cell):\n",
    "    choices = [\"append_percent\"]\n",
    "    action = random.choice(choices)\n",
    "    \n",
    "    if action == \"append_percent\":\n",
    "        return cell + \"%\"\n",
    "    elif action == \"alter_float\" and \".\" in cell:\n",
    "        # Introduce a small random change to the floating point\n",
    "        parts = cell.split(\".\")\n",
    "        if len(parts[1]) == 3:\n",
    "            last_digit = str(int(parts[1][2]) + random.choice([-1, 1]) % 10)  # Increment or decrement the last digit\n",
    "            return parts[0] + \".\" + parts[1][:2] + last_digit + \"%\"\n",
    "    \n",
    "def Beer_Detection_city(cell):\n",
    "    pattern = re.compile(r'\\b[A-Z]{2}$')\n",
    "    return bool(pattern.search(cell))\n",
    "def Beer_Generation_city(cell):\n",
    "    state_abbreviations = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']\n",
    "    \n",
    "    # Randomly choose a state abbreviation\n",
    "    state = random.choice(state_abbreviations)\n",
    "    \n",
    "    # Append the state abbreviation to the cell to make it dirty\n",
    "    dirty_cell = cell + \" \" + state\n",
    "    \n",
    "    return dirty_cell\n",
    "def Beer_Detection_state(cell):\n",
    "    return cell == \"\"\n",
    "def Beer_Generation_state(cell):\n",
    "    return \"\"\n",
    "def Beer_Row_Detection(x,y):\n",
    "    # for x,y in row.items():\n",
    "        if(x=='ounces'):\n",
    "            return Beer_Detection_Ounces(y)\n",
    "        elif(x=='abv'):\n",
    "            return Beer_Detection_abv(y)\n",
    "        elif(x=='city'):\n",
    "            return Beer_Detection_city(y)\n",
    "        elif(x=='state'):\n",
    "            return Beer_Detection_state(y)\n",
    "        else:\n",
    "            return False\n",
    "def Beer_Row_Generation(x,y):\n",
    "    # for x,y in row.items():\n",
    "        if(x=='ounces'): ## try to Correct Ounces\n",
    "            return Beer_Generation_Ounces(y)\n",
    "            # return correct_dirty_cell(y)\n",
    "        elif(x=='abv'):\n",
    "            return Beer_Generation_abv(y)\n",
    "        elif(x=='city'):\n",
    "            return Beer_Generation_city(y)\n",
    "        elif(x=='state'):\n",
    "            return Beer_Generation_state(y)\n",
    "        else:\n",
    "            return False\n",
    "# def Beer_Row_Generation(x,y):\n",
    "    \n",
    "# Beer_Row_Detection('abv','0.599') ## True Means Error\n",
    "# Beer_Generation_city('Washington')\n",
    "Beer_Row_Generation('ounces','16.0 ounces')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_clean = pd.read_csv('datasets/beers/clean.csv').fillna('')\n",
    "beer_dirty = pd.read_csv('datasets/beers/dirty.csv').fillna('')\n",
    "beer_dirty.columns = beer_clean.columns\n",
    "def try_convert_to_int(row):\n",
    "    for x,y in row.items():\n",
    "        if(x in ['ounces','ibu']):\n",
    "            try:\n",
    "                row[x] = int(y)\n",
    "            except:\n",
    "                row[x] = y\n",
    "    return row\n",
    "beer_clean = beer_clean.apply(try_convert_to_int,axis=1).astype(str)\n",
    "beer_dirty = beer_dirty.apply(try_convert_to_int,axis=1).astype(str)\n",
    "beer_dirty_augment = beer_dirty.copy()\n",
    "beer_dirty_augment['ounces'] = beer_dirty_augment['ounces'].apply(correct_dirty_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beers Dataset Correction \n",
    "# beer_dirty_augment = beer_dirty\n",
    "# beer_dirty_augment['ounces'] = beer_dirty_augment['ounces'].apply(correct_dirty_cell)\n",
    "## Coreset Generation, used to augment Critic Model \n",
    "# hospital_coreset = hospital_dirty[hospital_dirty['count']==0].index\n",
    "# noise_col = np.where(input_matrix[selected_index_hospital_5].sum(axis=0)!=0)[0] ## 不使用不可靠的detector\n",
    "row_list = []\n",
    "single_list = []\n",
    "column_list = []\n",
    "# ## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "#     # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "#     #     print(hospital_clean.columns[i])\n",
    "# for label_tuple in hospital_coreset:\n",
    "#     cluster_index = hospital_clean.iloc[label_tuple,1]\n",
    "#     cluster_set_index = hospital_clean[hospital_clean['ProviderNumber']==cluster_index].index\n",
    "#     cluster_set = hospital_dirty.iloc[cluster_set_index]\n",
    "#     noise_col_subset = np.random.choice(noise_col,5,replace=False) ## 取5个col作为注入噪声的对象\n",
    "#     for i in noise_col_subset:\n",
    "#         original_row = hospital_dirty.iloc[label_tuple]\n",
    "#         dirty_row = original_row\n",
    "#         clean_cell = hospital_dirty.iloc[label_tuple,i]\n",
    "#         dirty_cell = replace_random_char_with_x(clean_cell) ## Inject Noise\n",
    "#         columns_unique = list(cluster_set.iloc[:,i].unique())\n",
    "#         insert_randomly(columns_unique,dirty_cell)\n",
    "#         # if(input_matrix[label_tuple,i]==1):\n",
    "#         all_context_clean = ''\n",
    "#         all_context_dirty = ''\n",
    "#         dirty_row[i] = dirty_cell\n",
    "#         single_context_clean = 'COL %s VAL %s ' % (hospital_dirty.columns[i],clean_cell)\n",
    "#         single_context_dirty = 'COL %s VAL %s ' % (hospital_dirty.columns[i],dirty_cell)\n",
    "#         column_context = 'COL %s VAL %s ' % (hospital_dirty.columns[i],columns_unique)\n",
    "#         for c in range(20):\n",
    "#             all_context_clean += 'COL %s VAL %s ' % (hospital_dirty.columns[c],original_row[c])\n",
    "#             all_context_dirty += 'COL %s VAL %s ' % (hospital_dirty.columns[c],dirty_row[c])\n",
    "#         # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "#         # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "#         # detector_list.append([single_context_clean,0])        \n",
    "#         # detector_list.append([all_context_clean,0])\n",
    "#         if(dirty_cell!=clean_cell):\n",
    "#             row_list.append([all_context_dirty,single_context_dirty,1])\n",
    "#             row_list.append([all_context_clean,single_context_clean,0])\n",
    "#             single_list.append([single_context_dirty,single_context_dirty,1])\n",
    "#             single_list.append([single_context_clean,single_context_clean,0])\n",
    "#             column_list.append([column_context,single_context_dirty,1])\n",
    "#             column_list.append([column_context,single_context_clean,0])\n",
    "#             # detector_list.append([all_context_dirty,single_context_clean,0])\n",
    "#             # detector_list.append([single_context_dirty,1])\n",
    "#         else:\n",
    "#             row_list.append([all_context_dirty,single_context_dirty,0])\n",
    "#             row_list.append([all_context_clean,single_context_clean,0])\n",
    "#             single_list.append([single_context_clean,single_context_clean,0])\n",
    "#             column_list.append([column_context,single_context_clean,0])\n",
    "\n",
    "for index,row in beer_dirty_augment.iterrows():\n",
    "    for x,y in row.items():\n",
    "        if(x in ['ounces', 'abv', 'city', 'state']):\n",
    "            detection = Beer_Row_Detection(x,y) \n",
    "            if not detection: ## No Outlier in Given Rows, inject noise\n",
    "                all_context_clean = ''\n",
    "                all_context_dirty = ''\n",
    "                dirty_row = row.copy()\n",
    "                original_row = row.copy()\n",
    "                if(x in ['ounces']): ## Correct Ounces Error, since it occurs in all values\n",
    "                    clean_cell = correct_dirty_cell(y)\n",
    "                    original_row[x] = clean_cell\n",
    "                else:\n",
    "                    clean_cell = y ## identical value\n",
    "                dirty_cell = Beer_Row_Generation(x,clean_cell)\n",
    "                # if(dirty_cell!='') and (dirty_cell!=clean_cell):\n",
    "                dirty_row[x] = dirty_cell\n",
    "                # columns_unique = np.array(beer_dirty[x].unique())\n",
    "                # columns_unique = list(np.random.choice(columns_unique,min(15,len(columns_unique)))) ## 截断\n",
    "                # insert_randomly(columns_unique,dirty_cell) \n",
    "                single_context_clean = 'COL %s VAL %s ' % (x,clean_cell)\n",
    "                single_context_dirty = 'COL %s VAL %s ' % (x,dirty_cell)\n",
    "                # column_context = 'COL %s VAL %s ' % (x,columns_unique)\n",
    "                for i in range(2,11,1):\n",
    "                    attr = beer_dirty_augment.columns[i]\n",
    "                    all_context_clean += 'COL %s VAL %s ' % (attr,original_row[attr]) ## Clean Result\n",
    "                    all_context_dirty += 'COL %s VAL %s ' % (attr,dirty_row[attr])\n",
    "                # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "                # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "                row_list.append([all_context_dirty,single_context_dirty,1])\n",
    "                row_list.append([all_context_clean,single_context_clean,0])\n",
    "                single_list.append([single_context_dirty,single_context_dirty,1])\n",
    "                single_list.append([single_context_clean,single_context_clean,0])\n",
    "                    # column_list.append([column_context,single_context_dirty,1])\n",
    "                    # column_list.append([column_context,single_context_clean,0])\n",
    "        else:\n",
    "            all_context_clean = ''    \n",
    "            original_row = row.copy()\n",
    "            clean_cell = y\n",
    "            single_context_clean = 'COL %s VAL %s ' % (x,clean_cell)\n",
    "            for i in range(2,11,1):\n",
    "                attr = beer_dirty.columns[i]\n",
    "                all_context_clean += 'COL %s VAL %s ' % (attr,original_row[attr]) ## Clean Result \n",
    "            all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "            row_list.append([all_context_clean,single_context_clean,0])\n",
    "            single_list.append([single_context_clean,single_context_clean,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_list = []\n",
    "single_list = []\n",
    "column_list = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "for label_tuple in hospital_coreset:\n",
    "    cluster_index = hospital_clean.iloc[label_tuple,1]\n",
    "    cluster_set_index = hospital_clean[hospital_clean['ProviderNumber']==cluster_index].index\n",
    "    cluster_set = hospital_dirty.iloc[cluster_set_index]\n",
    "    noise_col_subset = np.random.choice(noise_col,5,replace=False) ## 取5个col作为注入噪声的对象\n",
    "    for i in noise_col_subset:\n",
    "        original_row = hospital_dirty.iloc[label_tuple]\n",
    "        dirty_row = original_row\n",
    "        clean_cell = hospital_dirty.iloc[label_tuple,i]\n",
    "        dirty_cell = replace_random_char_with_x(clean_cell) ## Inject Noise\n",
    "        columns_unique = list(cluster_set.iloc[:,i].unique())\n",
    "        insert_randomly(columns_unique,dirty_cell)\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        dirty_row[i] = dirty_cell\n",
    "        single_context_clean = 'COL %s VAL %s ' % (hospital_dirty.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (hospital_dirty.columns[i],dirty_cell)\n",
    "        column_context = 'COL %s VAL %s ' % (hospital_dirty.columns[i],columns_unique)\n",
    "        for c in range(20):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (hospital_dirty.columns[c],original_row[c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (hospital_dirty.columns[c],dirty_row[c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            row_list.append([all_context_dirty,single_context_dirty,1])\n",
    "            row_list.append([all_context_clean,single_context_clean,0])\n",
    "            single_list.append([single_context_dirty,single_context_dirty,1])\n",
    "            single_list.append([single_context_clean,single_context_clean,0])\n",
    "            column_list.append([column_context,single_context_dirty,1])\n",
    "            column_list.append([column_context,single_context_clean,0])\n",
    "            # detector_list.append([all_context_dirty,single_context_clean,0])\n",
    "            # detector_list.append([single_context_dirty,1])\n",
    "        else:\n",
    "            row_list.append([all_context_dirty,single_context_dirty,0])\n",
    "            row_list.append([all_context_clean,single_context_clean,0])\n",
    "            single_list.append([single_context_clean,single_context_clean,0])\n",
    "            column_list.append([column_context,single_context_clean,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Beer_Row_Detection('ounces','16.0 ounces'),Beer_Row_Detection('city','Washington')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beer_dirty.iloc[0\n",
    "pd.DataFrame(row_list).to_csv('/home/yanmy/raha/raha-master/datasets/beers/detector/multi-view/train_aug.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(row_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 28/2410 [00:00<00:08, 270.32it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2410/2410 [00:09<00:00, 266.37it/s]\n"
     ]
    }
   ],
   "source": [
    "## Detector Inference\n",
    "# input_matrix_select_beer = input_matrix_beer[selected_rows_beer]\n",
    "detector_list_beer = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "# for label_tuple in tqdm(selected_rows):\n",
    "for label_tuple in tqdm(range(len(beer_clean))):\n",
    "    for i in range(2,len(beer_clean.columns),1):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = beer_clean.iloc[label_tuple]\n",
    "        dirty_context = beer_dirty.iloc[label_tuple]\n",
    "        clean_cell = beer_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = beer_dirty.iloc[label_tuple,i]\n",
    "        single_context_clean = 'COL %s VAL %s ' % (beer_clean.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (beer_clean.columns[i],dirty_cell)\n",
    "        for c in range(2,11,1):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (beer_clean.columns[c],beer_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (beer_dirty.columns[c],beer_dirty.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            detector_list_beer.append([all_context_dirty,single_context_dirty,1])\n",
    "        else:\n",
    "            detector_list_beer.append([all_context_dirty,single_context_dirty,0])\n",
    "            # detector_list.append([single_context_dirty,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_beer).to_csv('/home/yanmy/raha/raha-master/datasets/beers/detector/multi-view/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Beer Training List\n",
    "import json\n",
    "noise_col = np.where(detector.sum(axis=0)>0)[0]\n",
    "header = list(hospital_dirty.columns)\n",
    "safe_value = ['empty'] ## 不注入噪声的类型\n",
    "training_list = []\n",
    "for h in tqdm(hospital_cluster): ## 对比学习的采样策略在同cluster内进行\n",
    "    coreset_subset = [n for n in hospital_cluster[h] if n in coreset_detect]\n",
    "    for coreset_tuple in coreset_subset: ## 取遍coreset所有的tuple，剩下的n-1 tuple是参考对象,coreset_tuple是index\n",
    "        noise_col_subset = np.random.choice(noise_col,5,replace=False) ## 取5个col作为注入噪声的对象\n",
    "        for noise_col_single in noise_col_subset:\n",
    "            col_name = header[noise_col_single] ## 从index转成列名\n",
    "            if(hospital_dirty_dict[coreset_tuple][col_name] not in safe_value and len(coreset_subset)>2):\n",
    "                temp_dict = hospital_dirty.iloc[coreset_tuple,1:].to_dict() ## coreset tuple字典\n",
    "                clean_cell = temp_dict[col_name]\n",
    "                dirty_cell = replace_random_char_with_x(clean_cell) ## 注入噪声\n",
    "                temp_dict[col_name] = dirty_cell\n",
    "                coreset_reference = select_two_different_elements(coreset_subset,coreset_tuple)\n",
    "                template_dict = {}\n",
    "                clean_dict = {}\n",
    "                template_dict[col_name] = ''\n",
    "                clean_dict[col_name] = clean_cell\n",
    "                text_head = 'You are an expert in Cleaning Hospital Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\nTake these rows as reference:\\n\\n' % (col_name, json.dumps(template_dict), json.dumps(temp_dict))\n",
    "                \n",
    "                ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]), json.dumps(hospital_dirty_dict[coreset_reference[1]]))\n",
    "                \n",
    "                # ICL_text = '%s\\n\\n' % (json.dumps(hospital_dirty_dict[coreset_reference[0]]))\n",
    "                \n",
    "                training_list.append([text_head,ICL_text,'',json.dumps(clean_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'article_title', 'article_language', 'journal_title',\n",
       "       'jounral_abbreviation', 'journal_issn', 'article_jvolumn',\n",
       "       'article_jissue', 'article_jcreated_at', 'article_pagination',\n",
       "       'author_list'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 659,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Augmented Training Data in LLM Generation, size controlled into 3000-4000 pairs\n",
    "import json\n",
    "\n",
    "beer_label_index_select = beer_label_index[:15]\n",
    "dirty_example_dict = {}\n",
    "dirty_example_dict_index = {'ounces':4, 'abv':5, 'city':9, 'state':10}\n",
    "for x in ['ounces', 'abv', 'city', 'state']:\n",
    "    dirty_example = []\n",
    "    j = dirty_example_dict_index[x]\n",
    "    for i in beer_label_index_select:\n",
    "        dirty_example.append([beer_dirty.iloc[i,j],beer_clean.iloc[i,j]])\n",
    "    dirty_example_dict[x] = dirty_example\n",
    "    # print(\"The input \\n\\n%s\\n\\nare [dirty,clean] cell pairs\" % str(dirty_example))\n",
    "training_list = []\n",
    "for index,row in beer_dirty_augment.iterrows():\n",
    "    for x,y in row.items():\n",
    "        if(x in ['ounces', 'abv', 'city', 'state']):\n",
    "            detection = Beer_Row_Detection(x,y) \n",
    "            if not detection: ## No Outlier in Given Rows, inject noise\n",
    "                dirty_example = dirty_example_dict[x]\n",
    "                all_context_clean = ''\n",
    "                all_context_dirty = ''\n",
    "                dirty_row = row.copy()\n",
    "                original_row = row.copy()\n",
    "                if(x in ['ounces']): ## Correct Ounces Error, since it occurs in all values\n",
    "                    clean_cell = correct_dirty_cell(y)\n",
    "                    original_row[x] = clean_cell\n",
    "                else:\n",
    "                    clean_cell = y ## identical value\n",
    "                dirty_cell = Beer_Row_Generation(x,clean_cell)\n",
    "                # if(dirty_cell!='') and (dirty_cell!=clean_cell):\n",
    "                dirty_row[x] = dirty_cell\n",
    "                # columns_unique = np.array(beer_dirty[x].unique())\n",
    "                # columns_unique = list(np.random.choice(columns_unique,min(15,len(columns_unique)))) ## 截断\n",
    "                # insert_randomly(columns_unique,dirty_cell) \n",
    "                template_dict = {}\n",
    "                temp_dict = dirty_row[2:].to_dict()\n",
    "                template_dict[x] = ''\n",
    "                coreset_reference = np.random.choice(beer_label_index_select,2,replace=False)\n",
    "                # clean_dict = original_row[2:].to_dict()\n",
    "                clean_dict = {}\n",
    "                clean_dict[x] = clean_cell\n",
    "                Correction_format = \"The input \\n\\n%s\\n\\nare [dirty,clean] cell pairs for value %s\" % (str(dirty_example),x)\n",
    "                text_head = 'You are an expert in Cleaning Beers Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\n%s\\n\\nTake these clean rows as reference:\\n\\n' % (x, json.dumps(template_dict), json.dumps(temp_dict),Correction_format)\n",
    "                ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(beer_clean.iloc[coreset_reference[0],2:].to_dict()), json.dumps(beer_clean.iloc[coreset_reference[1],2:].to_dict()))\n",
    "                \n",
    "                training_list.append([text_head,ICL_text,'',json.dumps(clean_dict)])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Labeling Result\n",
    "import json\n",
    "\n",
    "beer_label_index_select = beer_label_index[:15]\n",
    "dirty_example_dict = {}\n",
    "dirty_example_dict_index = {'ounces':4, 'abv':5, 'city':9, 'state':10}\n",
    "for x in ['ounces', 'abv', 'city', 'state']:\n",
    "    dirty_example = []\n",
    "    j = dirty_example_dict_index[x]\n",
    "    for i in beer_label_index_select:\n",
    "        dirty_example.append([beer_dirty.iloc[i,j],beer_clean.iloc[i,j]])\n",
    "    dirty_example_dict[x] = dirty_example\n",
    "    # print(\"The input \\n\\n%s\\n\\nare [dirty,clean] cell pairs\" % str(dirty_example))\n",
    "training_list_label = []\n",
    "for index,row in beer_dirty.iterrows():\n",
    "    if(index in beer_label_index_select):\n",
    "        for i in range(11):\n",
    "            if(beer_clean.iloc[index,i]!=beer_dirty.iloc[index,i]):\n",
    "                clean_cell = beer_clean.iloc[index,i]\n",
    "                x = beer_clean.columns[i]\n",
    "                dirty_example = dirty_example_dict[x]\n",
    "                # if(dirty_cell!='') and (dirty_cell!=clean_cell):\n",
    "\n",
    "                # columns_unique = np.array(beer_dirty[x].unique())\n",
    "                # columns_unique = list(np.random.choice(columns_unique,min(15,len(columns_unique)))) ## 截断\n",
    "                # insert_randomly(columns_unique,dirty_cell) \n",
    "                template_dict = {}\n",
    "                temp_dict = beer_dirty.iloc[index,2:].to_dict()\n",
    "                template_dict[x] = ''\n",
    "                coreset_reference = np.random.choice(beer_label_index_select,2,replace=False)\n",
    "                # clean_dict = original_row[2:].to_dict()\n",
    "                clean_dict = {}\n",
    "                clean_dict[x] = clean_cell\n",
    "                Correction_format = \"The input \\n\\n%s\\n\\nare [dirty,clean] cell pairs for value %s\" % (str(dirty_example),x)\n",
    "                text_head = 'You are an expert in Cleaning Beers Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\n%s\\n\\nTake these clean rows as reference:\\n\\n' % (x, json.dumps(template_dict), json.dumps(temp_dict),Correction_format)\n",
    "                ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(beer_clean.iloc[coreset_reference[0],2:].to_dict()), json.dumps(beer_clean.iloc[coreset_reference[1],2:].to_dict()))   \n",
    "                training_list_label.append([text_head,ICL_text,'',json.dumps(clean_dict)])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input \n",
      "\n",
      "[['16.0 oz.', '16'], ['12.0 oz', '12'], ['12.0 oz.', '12'], ['16.0 oz.', '16'], ['12.0 oz', '12']]\n",
      "\n",
      "are [dirty,clean] cell pairs\n",
      "The input \n",
      "\n",
      "[['0.047%', '0.047'], ['0.068%', '0.068'], ['0.06%', '0.06'], ['0.057999999999999996%', '0.058'], ['0.06%', '0.06']]\n",
      "\n",
      "are [dirty,clean] cell pairs\n",
      "The input \n",
      "\n",
      "[['Hayward WI', 'Hayward'], ['Nellysford VA', 'Nellysford'], ['Duluth MN', 'Duluth'], ['Plainfield IN', 'Plainfield'], ['Stevens Point WI', 'Stevens Point']]\n",
      "\n",
      "are [dirty,clean] cell pairs\n",
      "The input \n",
      "\n",
      "[['', 'WI'], ['', 'VA'], ['', 'MN'], ['', 'IN'], ['', 'WI']]\n",
      "\n",
      "are [dirty,clean] cell pairs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Testing Result\n",
    "import json\n",
    "\n",
    "beer_label_index_select = beer_label_index[:5]\n",
    "dirty_example_dict = {}\n",
    "dirty_example_dict_index = {'ounces':4, 'abv':5, 'city':9, 'state':10}\n",
    "for x in ['ounces', 'abv', 'city', 'state']:\n",
    "    dirty_example = []\n",
    "    j = dirty_example_dict_index[x]\n",
    "    for i in beer_label_index_select:\n",
    "        dirty_example.append([beer_dirty.iloc[i,j],beer_clean.iloc[i,j]])\n",
    "    dirty_example_dict[x] = dirty_example\n",
    "    print(\"The input \\n\\n%s\\n\\nare [dirty,clean] cell pairs\" % str(dirty_example))\n",
    "training_list_label = []\n",
    "for index,i in np.argwhere(detector_beer==1):\n",
    "    # if(index in beer_label_index_select):\n",
    "        # for i in range(11):\n",
    "                i = i + 2 ## ignore index and id\n",
    "            # if(beer_clean.iloc[index,i]!=beer_dirty.iloc[index,i]):\n",
    "                clean_cell = beer_clean.iloc[index,i]\n",
    "                x = beer_clean.columns[i]\n",
    "                dirty_example = dirty_example_dict[x]\n",
    "                # if(dirty_cell!='') and (dirty_cell!=clean_cell):\n",
    "\n",
    "                # columns_unique = np.array(beer_dirty[x].unique())\n",
    "                # columns_unique = list(np.random.choice(columns_unique,min(15,len(columns_unique)))) ## 截断\n",
    "                # insert_randomly(columns_unique,dirty_cell) \n",
    "                template_dict = {}\n",
    "                temp_dict = beer_dirty.iloc[index,2:].to_dict()\n",
    "                template_dict[x] = ''\n",
    "                coreset_reference = np.random.choice(beer_label_index_select,2,replace=False)\n",
    "                # clean_dict = original_row[2:].to_dict()\n",
    "                clean_dict = {}\n",
    "                clean_dict[x] = clean_cell\n",
    "                Correction_format = \"The input \\n\\n%s\\n\\nare [dirty,clean] cell pairs for value %s\" % (str(dirty_example),x)\n",
    "                text_head = 'You are an expert in Cleaning Beers Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\n%s\\n\\nTake these clean rows as reference:\\n\\n' % (x, json.dumps(template_dict), json.dumps(temp_dict),Correction_format)\n",
    "                ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(beer_clean.iloc[coreset_reference[0],2:].to_dict()), json.dumps(beer_clean.iloc[coreset_reference[1],2:].to_dict()))   \n",
    "                training_list_label.append([text_head,ICL_text,'',json.dumps(clean_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"beer-name\": \"Blonde Hunny\", \"style\": \"Belgia...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"beer-name\": \"Blonde Hunny\", \"style\": \"Belgia...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"beer-name\": \"Insert Hop Reference\", \"style\":...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"beer-name\": \"Bent Paddle Black Ale\", \"style\"...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"beer-name\": \"Bent Paddle Black Ale\", \"style\"...</td>\n",
       "      <td></td>\n",
       "      <td>{\"abv\": \"0.09\"}</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"abv\": \"0.09\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3359</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"beer-name\": \"Bent Paddle Black Ale\", \"style\"...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3360</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"beer-name\": \"Insert Hop Reference\", \"style\":...</td>\n",
       "      <td></td>\n",
       "      <td>{\"abv\": \"0.055\"}</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"abv\": \"0.055\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3361</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"beer-name\": \"JP's Ould Sod Irish Red IPA\", \"...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3362</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"beer-name\": \"Insert Hop Reference\", \"style\":...</td>\n",
       "      <td></td>\n",
       "      <td>{\"abv\": \"0.055\"}</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"abv\": \"0.055\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3363</th>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td>{\"beer-name\": \"Blonde Hunny\", \"style\": \"Belgia...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "      <td>You are an expert in Cleaning Beers Dataset. G...</td>\n",
       "      <td></td>\n",
       "      <td>{\"ounces\": \"12\"}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3364 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0  \\\n",
       "0     You are an expert in Cleaning Beers Dataset. G...   \n",
       "1     You are an expert in Cleaning Beers Dataset. G...   \n",
       "2     You are an expert in Cleaning Beers Dataset. G...   \n",
       "3     You are an expert in Cleaning Beers Dataset. G...   \n",
       "4     You are an expert in Cleaning Beers Dataset. G...   \n",
       "...                                                 ...   \n",
       "3359  You are an expert in Cleaning Beers Dataset. G...   \n",
       "3360  You are an expert in Cleaning Beers Dataset. G...   \n",
       "3361  You are an expert in Cleaning Beers Dataset. G...   \n",
       "3362  You are an expert in Cleaning Beers Dataset. G...   \n",
       "3363  You are an expert in Cleaning Beers Dataset. G...   \n",
       "\n",
       "                                                      1 2                  3  \\\n",
       "0     {\"beer-name\": \"Blonde Hunny\", \"style\": \"Belgia...     {\"ounces\": \"12\"}   \n",
       "1     {\"beer-name\": \"Blonde Hunny\", \"style\": \"Belgia...     {\"ounces\": \"12\"}   \n",
       "2     {\"beer-name\": \"Insert Hop Reference\", \"style\":...     {\"ounces\": \"12\"}   \n",
       "3     {\"beer-name\": \"Bent Paddle Black Ale\", \"style\"...     {\"ounces\": \"12\"}   \n",
       "4     {\"beer-name\": \"Bent Paddle Black Ale\", \"style\"...      {\"abv\": \"0.09\"}   \n",
       "...                                                 ... ..               ...   \n",
       "3359  {\"beer-name\": \"Bent Paddle Black Ale\", \"style\"...     {\"ounces\": \"12\"}   \n",
       "3360  {\"beer-name\": \"Insert Hop Reference\", \"style\":...     {\"abv\": \"0.055\"}   \n",
       "3361  {\"beer-name\": \"JP's Ould Sod Irish Red IPA\", \"...     {\"ounces\": \"12\"}   \n",
       "3362  {\"beer-name\": \"Insert Hop Reference\", \"style\":...     {\"abv\": \"0.055\"}   \n",
       "3363  {\"beer-name\": \"Blonde Hunny\", \"style\": \"Belgia...     {\"ounces\": \"12\"}   \n",
       "\n",
       "                                            instruction input  \\\n",
       "0     You are an expert in Cleaning Beers Dataset. G...         \n",
       "1     You are an expert in Cleaning Beers Dataset. G...         \n",
       "2     You are an expert in Cleaning Beers Dataset. G...         \n",
       "3     You are an expert in Cleaning Beers Dataset. G...         \n",
       "4     You are an expert in Cleaning Beers Dataset. G...         \n",
       "...                                                 ...   ...   \n",
       "3359  You are an expert in Cleaning Beers Dataset. G...         \n",
       "3360  You are an expert in Cleaning Beers Dataset. G...         \n",
       "3361  You are an expert in Cleaning Beers Dataset. G...         \n",
       "3362  You are an expert in Cleaning Beers Dataset. G...         \n",
       "3363  You are an expert in Cleaning Beers Dataset. G...         \n",
       "\n",
       "                output  \n",
       "0     {\"ounces\": \"12\"}  \n",
       "1     {\"ounces\": \"12\"}  \n",
       "2     {\"ounces\": \"12\"}  \n",
       "3     {\"ounces\": \"12\"}  \n",
       "4      {\"abv\": \"0.09\"}  \n",
       "...                ...  \n",
       "3359  {\"ounces\": \"12\"}  \n",
       "3360  {\"abv\": \"0.055\"}  \n",
       "3361  {\"ounces\": \"12\"}  \n",
       "3362  {\"abv\": \"0.055\"}  \n",
       "3363  {\"ounces\": \"12\"}  \n",
       "\n",
       "[3364 rows x 7 columns]"
      ]
     },
     "execution_count": 799,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_list_label_pd = pd.DataFrame(training_list_label)\n",
    "training_list_label_pd['instruction'] = training_list_label_pd[0] + training_list_label_pd[1]\n",
    "training_list_label_pd['input'] = training_list_label_pd[2]\n",
    "training_list_label_pd['output'] = training_list_label_pd[3]\n",
    "training_list_label_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list_pd = pd.DataFrame(training_list).sample(n=(150 * len(beer_label_index_select)))\n",
    "training_list_pd['instruction'] = training_list_pd[0] + training_list_pd[1]\n",
    "training_list_pd['input'] = training_list_pd[2]\n",
    "training_list_pd['output'] = training_list_pd[3]\n",
    "training_list_label_pd = pd.DataFrame(training_list_label)\n",
    "training_list_label_pd['instruction'] = training_list_label_pd[0] + training_list_label_pd[1]\n",
    "training_list_label_pd['input'] = training_list_label_pd[2]\n",
    "training_list_label_pd['output'] = training_list_label_pd[3]\n",
    "training_list_output = pd.concat([training_list_pd,training_list_label_pd])\n",
    "training_list_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(np.concatenate([training_list,training_list_label]))\n",
    "# json.dump(training_list_output.iloc[:,-3:].to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/beer/beer-train-15.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n",
    "json.dump(training_list_label_pd.iloc[:,-3:].to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/beer/beer-test-5.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1559,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "detector_beer = np.load('datasets/beers/detector/multi-view/detection_cell_5.npy').reshape((-1,9))\n",
    "## Only the labelled attributes is dirty\n",
    "# detector_beer.sum(axis=0)\n",
    "detector_beer[:,0] = 0\n",
    "detector_beer[:,1] = 0\n",
    "detector_beer[:,4] = 0\n",
    "detector_beer[:,5] = 0\n",
    "detector_beer[:,6] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3364"
      ]
     },
     "execution_count": 793,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(detector_beer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4259489363.py, line 184)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[812], line 184\u001b[0;36m\u001b[0m\n\u001b[0;31m    if(cell!='')\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### Try Rayyan Dataset\n",
    "def Rayyan_Detect_atitle(cell):\n",
    "    # Regular expression to detect if the cell contains special characters or combining diacritical marks\n",
    "    pattern = re.compile(r'[�\\u0300-\\u036F]')\n",
    "    return bool(pattern.search(cell))\n",
    "def Rayyan_Generate_atitle(cell):\n",
    "    # List of special characters and combining diacritical marks\n",
    "    special_chars = ['�', '\\u0301', '\\u0300', '\\u0302', '\\u0303', '\\u0304']\n",
    "    \n",
    "    # Randomly choose a special character\n",
    "    char = random.choice(special_chars)\n",
    "    \n",
    "    # Randomly choose a position to insert the special character\n",
    "    position = random.randint(0, len(cell))\n",
    "    \n",
    "    # Insert the special character at the chosen position\n",
    "    dirty_cell = cell[:position] + char + cell[position:]\n",
    "    \n",
    "    return dirty_cell\n",
    "def Rayyan_Clean_atitle(cell):\n",
    "    # Remove special characters like �\n",
    "    cleaned = re.sub(r'�', '', cell)\n",
    "    \n",
    "    # Remove combining diacritical marks (from Unicode range U+0300 to U+036F)\n",
    "    cleaned = re.sub(r'[\\u0300-\\u036F]', '', cleaned)\n",
    "    \n",
    "    return cleaned\n",
    "def Rayyan_Detect_jtitle(cell):\n",
    "    # Regular expression to detect if the cell contains special characters or combining diacritical marks\n",
    "    pattern = re.compile(r'[�\\u0300-\\u036F]')\n",
    "    return bool(pattern.search(cell))\n",
    "def Rayyan_Generate_jtitle(cell):\n",
    "    # List of special characters and combining diacritical marks\n",
    "    special_chars = ['�', '\\u0301', '\\u0300', '\\u0302', '\\u0303', '\\u0304']\n",
    "    \n",
    "    # Randomly choose a special character\n",
    "    char = random.choice(special_chars)\n",
    "    \n",
    "    # Randomly choose a position to insert the special character\n",
    "    position = random.randint(0, len(cell))\n",
    "    \n",
    "    # Insert the special character at the chosen position\n",
    "    dirty_cell = cell[:position] + char + cell[position:]\n",
    "    \n",
    "    return dirty_cell\n",
    "def Rayyan_Clean_jtitle(cell):\n",
    "    # Remove special characters like �\n",
    "    cleaned = re.sub(r'�', '', cell)\n",
    "    \n",
    "    # Remove combining diacritical marks (from Unicode range U+0300 to U+036F)\n",
    "    cleaned = re.sub(r'[\\u0300-\\u036F]', '', cleaned)\n",
    "    \n",
    "    return cleaned\n",
    "def Rayyan_Detect_author(cell):\n",
    "    # Regular expression to detect if the cell contains special characters or combining diacritical marks\n",
    "    pattern = re.compile(r'[�\\u0300-\\u036F]')\n",
    "    return bool(pattern.search(cell))\n",
    "def Rayyan_Generate_author(cell):\n",
    "    # List of special characters and combining diacritical marks\n",
    "    special_chars = ['�', '\\u0301', '\\u0300', '\\u0302', '\\u0303', '\\u0304']\n",
    "    \n",
    "    # Randomly choose a special character\n",
    "    char = random.choice(special_chars)\n",
    "    \n",
    "    # Randomly choose a position to insert the special character\n",
    "    position = random.randint(0, len(cell))\n",
    "    \n",
    "    # Insert the special character at the chosen position\n",
    "    dirty_cell = cell[:position] + char + cell[position:]\n",
    "    \n",
    "    return dirty_cell\n",
    "def Rayyan_Detect_issn(cell):\n",
    "    # Regular expression to detect if the cell is in the format Mon-DD\n",
    "    date_pattern = re.compile(r'^[A-Za-z]{3}-\\d{1,2}$')\n",
    "    \n",
    "    # Regular expression to detect if the ISSN starts with a number other than 9\n",
    "    issn_pattern = re.compile(r'^[^9]\\d{12}$')\n",
    "    \n",
    "    return bool(date_pattern.match(cell) or issn_pattern.match(cell))\n",
    "def Rayyan_Correct_issn(cell):\n",
    "    date_match = re.match(r'([A-Za-z]{3})-(\\d{1,2})', cell)\n",
    "    if date_match:\n",
    "        return f\"{date_match.group(2)}-{date_match.group(1)}\"\n",
    "    \n",
    "    # If the cell matches the ISSN format not starting with '9', replace the leading digit with '9'\n",
    "    issn_match = re.match(r'^[^9]\\d{12}$', cell)\n",
    "    if issn_match:\n",
    "        return f\"9{cell[1:]}\"\n",
    "    \n",
    "    return cell  # If no patterns match, return the original cel\n",
    "def Rayyan_Generate_issn(cell):\n",
    "    # If the cell matches the format DD-Mon, reverse it to Mon-DD\n",
    "    date_match = re.match(r'(\\d{1,2})-([A-Za-z]{3})', cell)\n",
    "    if date_match:\n",
    "        return f\"{date_match.group(2)}-{date_match.group(1)}\"\n",
    "    \n",
    "    # If the cell matches the ISSN format starting with '9', change the leading '9' to another number\n",
    "    issn_match = re.match(r'^9\\d{12}$', cell)\n",
    "    if issn_match:\n",
    "        return f\"{random.choice(['0', '1', '2', '3', '4', '5', '6', '7', '8'])}{cell[1:]}\"\n",
    "    \n",
    "    return cell\n",
    "def Rayyan_Detect_jissue(cell):\n",
    "    pattern = re.compile(r'^\\s*$')\n",
    "    return bool(pattern.match(cell))\n",
    "def Rayyan_Generate_jissue(cell):\n",
    "    return \"\"\n",
    "def Rayyan_Correct_jissue(cell):\n",
    "    return \"-1\"\n",
    "def Rayyan_Detect_pagination(cell):\n",
    "    clean_pattern = re.compile(r'^(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)-\\d{2}$')\n",
    "    \n",
    "    # Dirty pattern\n",
    "    dirty_pattern1 = re.compile(r'^\\d{2}-(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)$')\n",
    "    dirty_pattern2 = re.compile(r'^\\d{2}-\\d$')\n",
    "    \n",
    "    if clean_pattern.match(cell):\n",
    "        return False\n",
    "    elif dirty_pattern1.match(cell) or dirty_pattern2.match(cell):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def Rayyan_Correct_pagination(cell):\n",
    "    clean_pattern = re.compile(r'^(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)-\\d{2}$')\n",
    "    \n",
    "    # Dirty pattern\n",
    "    dirty_pattern1 = re.compile(r'^(\\d{2})-(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)$')\n",
    "    dirty_pattern2 = re.compile(r'^(\\d{2})-(\\d)$')\n",
    "    \n",
    "    # If the cell matches the clean pattern, return it as is\n",
    "    if clean_pattern.match(cell):\n",
    "        return cell\n",
    "    \n",
    "    # If the cell matches the first dirty pattern, reverse month and year\n",
    "    match = dirty_pattern1.match(cell)\n",
    "    if match:\n",
    "        return f\"{match.group(2)}-{match.group(1)}\"\n",
    "    \n",
    "    # If the cell matches the second dirty pattern, reverse month and year\n",
    "    match = dirty_pattern2.match(cell)\n",
    "    if match:\n",
    "        month_map = {\n",
    "            '1': 'Jan', '2': 'Feb', '3': 'Mar', '4': 'Apr', '5': 'May', '6': 'Jun',\n",
    "            '7': 'Jul', '8': 'Aug', '9': 'Sep', '10': 'Oct', '11': 'Nov', '12': 'Dec'\n",
    "        }\n",
    "        return f\"{month_map[match.group(2)]}-{match.group(1)}\"\n",
    "    \n",
    "    # If the cell doesn't match any pattern, return it as is\n",
    "    return cell\n",
    "def Rayyan_Generate_pagination(cell):\n",
    "    # Extract month and year from the clean format\n",
    "    match = re.match(r'([A-Za-z]{3})-(\\d{2})', cell)\n",
    "    if not match:\n",
    "        return cell  # Return the original cell if it doesn't match the clean format\n",
    "    \n",
    "    month, year = match.groups()\n",
    "    \n",
    "    # Convert month to its corresponding number\n",
    "    month_to_num = {\n",
    "        'Jan': '1', 'Feb': '2', 'Mar': '3', 'Apr': '4', 'May': '5', 'Jun': '6',\n",
    "        'Jul': '7', 'Aug': '8', 'Sep': '9', 'Oct': '10', 'Nov': '11', 'Dec': '12'\n",
    "    }\n",
    "    month_num = month_to_num.get(month, '')\n",
    "    \n",
    "    # Randomly decide to add extra numbers or not\n",
    "    if random.choice([True, False]):\n",
    "        year = str(random.randint(1000, 9999)) + year\n",
    "    \n",
    "    # Return the dirty format\n",
    "    return f\"{year}-{month_num}\"\n",
    "def Rayyan_Detect_jcreate(cell):\n",
    "    try:\n",
    "        YY,MM,DD = cell.split('/')\n",
    "        YY = int(YY)\n",
    "        MM = int(MM)\n",
    "        DD = int(DD)\n",
    "        if (1 <= MM <= 12) and (1 <= DD <= 31):\n",
    "            return True ## Satisfied Dirty Pattern, maybe all value qualified dirty pattern\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return False \n",
    "def Rayyan_Correct_jcreate(cell): ## Dirty -> Clean\n",
    "    if(cell!='')\n",
    "        YY,MM,DD = cell.split('/')\n",
    "        YY = int(YY)\n",
    "        MM = int(MM)\n",
    "        DD = int(DD)\n",
    "        YY = \"{:02}\".format(YY)\n",
    "        MM = str(MM)\n",
    "        DD = str(DD)\n",
    "        return '%s/%s/%s' % (MM,DD,YY)\n",
    "    else:\n",
    "        return ''\n",
    "def Rayyan_Generate_jcreate(cell): ## Clean -> Dirty\n",
    "    MM,DD,YY = cell.split('/')\n",
    "    YY = int(YY)\n",
    "    MM = int(MM)\n",
    "    DD = int(DD)\n",
    "    DD = \"{:02}\".format(DD)\n",
    "    MM = str(MM)\n",
    "    YY = str(YY)\n",
    "    return '%s/%s/%s' % (YY,MM,DD)\n",
    "def Rayyan_Row_Detect(x,cell):\n",
    "    if(x=='article_title'):\n",
    "        return Rayyan_Detect_atitle(cell)\n",
    "    elif(x=='journal_title'):\n",
    "        return Rayyan_Detect_jtitle(cell)\n",
    "    elif(x=='journal_issn'):\n",
    "        return Rayyan_Detect_issn(cell)\n",
    "    elif(x=='article_jvolumn') or (x=='article_jissue'): ## Modify when label budget shrink\n",
    "        return Rayyan_Detect_jissue(cell)\n",
    "    elif(x=='article_jcreated_at'):\n",
    "        return Rayyan_Detect_jcreate(cell)\n",
    "    elif(x=='article_pagination'):\n",
    "        return Rayyan_Detect_pagination(cell)\n",
    "    elif(x=='author_list'):\n",
    "        return Rayyan_Detect_author(cell)\n",
    "    else:\n",
    "        return False\n",
    "# Rayyan_Row_Detect('article_jissue','')\n",
    "def Rayyan_Row_Generate(x,cell): ## Input should be detected to clean, except jcreate_at\n",
    "    if(x=='article_title'):\n",
    "        return Rayyan_Generate_atitle(cell)\n",
    "    elif(x=='journal_title'):\n",
    "        return Rayyan_Generate_jtitle(cell)\n",
    "    elif(x=='journal_issn'):\n",
    "        return Rayyan_Generate_issn(cell)\n",
    "    elif(x=='article_jvolumn') or (x=='article_jissue'): ## Modify when label budget shrink\n",
    "        return Rayyan_Generate_jissue(cell)\n",
    "    elif(x=='article_jcreated_at'):\n",
    "        return Rayyan_Generate_jcreate(cell) ## Input Should Be Clean\n",
    "    elif(x=='article_pagination'):\n",
    "        return Rayyan_Generate_pagination(cell)\n",
    "    elif(x=='author_list'):\n",
    "        return Rayyan_Generate_author(cell)\n",
    "    else:\n",
    "        return False\n",
    "def Rayyan_Row_Correction(x,cell): ## Input should be detected to clean, except jcreate_at\n",
    "    if(x=='journal_issn'):\n",
    "        return Rayyan_Correct_issn(cell)\n",
    "    elif(x=='article_jvolumn') or (x=='article_jissue'): ## Modify when label budget shrink\n",
    "        return Rayyan_Correct_jissue(cell)\n",
    "    elif(x=='article_jcreated_at'):\n",
    "        return Rayyan_Correct_jcreate(cell) ## Input Should Be Clean\n",
    "    elif(x=='article_pagination'):\n",
    "        return Rayyan_Correct_pagination(cell)\n",
    "    elif(x in ['article_title','journal_title','author_list']):\n",
    "        return Rayyan_Clean_atitle(cell)\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 14)"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = -10\n",
    "count = 0\n",
    "for c in rayyan_dirty[rayyan_dirty.iloc[:,i]!=rayyan_clean.iloc[:,i]].iloc[:,i].to_list():\n",
    "    # print(c,Rayyan_Row_Detect(rayyan_dirty.columns[i],c))\n",
    "    if(Rayyan_Row_Detect(rayyan_dirty.columns[i],c)):\n",
    "        count += 1\n",
    "    # else:\n",
    "        # print(c)\n",
    "count,len(rayyan_dirty[rayyan_dirty.iloc[:,i]!=rayyan_clean.iloc[:,i]].iloc[:,i].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,  14,   0,   9,   0,  12,  22,  53, 722,  32,  84])"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix_rayyan = np.array(rayyan_clean!=rayyan_dirty).astype(int)\n",
    "input_matrix_rayyan.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['id', 'article_title', 'article_language', 'journal_title',\n",
    "       'jounral_abbreviation', 'journal_issn', 'article_jvolumn',\n",
    "       'article_jissue', 'article_jcreated_at', 'article_pagination',\n",
    "       'author_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['article_title', 'journal_title', 'journal_issn', 'article_jvolumn',\n",
       "       'article_jissue', 'article_jcreated_at', 'article_pagination',\n",
       "       'author_list'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_clean.columns[[1,3,5,6,7,8,9,10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rayyan_pagination: Pagination detect then correct as wrong-clean pairs\n",
    "Rayyan_issn: ISSN detect then correct as wrong-clean pairs\n",
    "Rayyan_jcreate: jcreate detect then correct as wrong-clean pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "rayyan_clean = pd.read_csv('datasets/rayyan/clean.csv').fillna('')\n",
    "rayyan_dirty = pd.read_csv('datasets/rayyan/dirty.csv').fillna('')\n",
    "def Str2Int(row):\n",
    "    for index in range(11):\n",
    "        temp = row[index]\n",
    "        try:\n",
    "            row[index] = str(int(temp))\n",
    "        except:\n",
    "            continue\n",
    "    return row\n",
    "rayyan_clean = rayyan_clean.apply(Str2Int,axis=1)\n",
    "rayyan_dirty = rayyan_dirty.apply(Str2Int,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "rayyan_dirty_augment = rayyan_dirty\n",
    "# rayyan_dirty_augment['article_jcreated_at'] = rayyan_dirty_augment['article_jcreated_at'].apply(Rayyan_Correct_jcreate,axis=1)\n",
    "for r in range(len(rayyan_dirty_augment)):\n",
    "    if(rayyan_dirty.iloc[r,8]!=''):\n",
    "        if(Rayyan_Detect_jcreate(rayyan_dirty.iloc[r,8])):\n",
    "            rayyan_dirty_augment.iloc[r,8] = Rayyan_Correct_jcreate(rayyan_dirty.iloc[r,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rayyan_Augmented_Data\n",
    "rayyan_clean = pd.read_csv('datasets/rayyan/clean.csv').fillna('')\n",
    "rayyan_dirty = pd.read_csv('datasets/rayyan/dirty.csv').fillna('')\n",
    "def Str2Int(row):\n",
    "    for index in range(11):\n",
    "        temp = row[index]\n",
    "        try:\n",
    "            row[index] = str(int(temp))\n",
    "        except:\n",
    "            continue\n",
    "    return row\n",
    "rayyan_clean = rayyan_clean.apply(Str2Int,axis=1)\n",
    "rayyan_dirty = rayyan_dirty.apply(Str2Int,axis=1)\n",
    "row_list = []\n",
    "single_list = []\n",
    "column_list = []\n",
    "rayyan_dirty_augment = rayyan_dirty.copy()\n",
    "\n",
    "for index,row in rayyan_dirty_augment.iterrows():\n",
    "    for x,y in row[1:].items():\n",
    "        if(x in ['article_title', 'journal_title', 'article_jvolumn',\n",
    "       'article_jissue',\n",
    "       'author_list']):\n",
    "            detection = Rayyan_Row_Detect(x,y) \n",
    "            \n",
    "            all_context_clean = ''\n",
    "            all_context_dirty = ''\n",
    "            dirty_row = row.copy()\n",
    "            original_row = row.copy()\n",
    "            # if(x in ['article_jcreated_at']): ## Correct Ounces Error, since it occurs in all values\n",
    "            #     clean_cell = correct_dirty_cell(y)\n",
    "            #     original_row[x] = clean_cell\n",
    "            # else:\n",
    "            clean_cell = y ## identical value\n",
    "            dirty_cell = Rayyan_Row_Generate(x,clean_cell)\n",
    "            # if(dirty_cell!='') and (dirty_cell!=clean_cell):\n",
    "            dirty_row[x] = dirty_cell\n",
    "            # columns_unique = np.array(beer_dirty[x].unique())\n",
    "            # columns_unique = list(np.random.choice(columns_unique,min(15,len(columns_unique)))) ## 截断\n",
    "            # insert_randomly(columns_unique,dirty_cell) \n",
    "            single_context_clean = 'COL %s VAL %s ' % (x,clean_cell)\n",
    "            single_context_dirty = 'COL %s VAL %s ' % (x,dirty_cell)\n",
    "            # column_context = 'COL %s VAL %s ' % (x,columns_unique)\n",
    "            for i in range(1,11,1):\n",
    "                attr = rayyan_dirty_augment.columns[i]\n",
    "                all_context_clean += 'COL %s VAL %s ' % (attr,original_row[attr]) ## Clean Result\n",
    "                all_context_dirty += 'COL %s VAL %s ' % (attr,dirty_row[attr])\n",
    "            # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "            # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "            if (not detection) and (single_context_dirty!=single_context_clean) : ## No Outlier in Given Rows, inject noise\n",
    "                row_list.append([all_context_dirty,single_context_dirty,1])\n",
    "                row_list.append([all_context_clean,single_context_clean,0])\n",
    "                single_list.append([single_context_dirty,single_context_dirty,1])\n",
    "                single_list.append([single_context_clean,single_context_clean,0])\n",
    "            else:\n",
    "                row_list.append([all_context_clean,single_context_clean,1])\n",
    "                single_list.append([single_context_clean,single_context_clean,1])\n",
    "                    # column_list.append([column_context,single_context_dirty,1])\n",
    "                    # column_list.append([column_context,single_context_clean,0])\n",
    "        elif(x in ['article_jcreated_at'] and y!=''):\n",
    "            detection = Rayyan_Row_Detect(x,y) \n",
    "            \n",
    "            all_context_clean = ''\n",
    "            all_context_dirty = ''\n",
    "            dirty_row = row.copy()\n",
    "            original_row = row.copy()\n",
    "            # if(x in ['article_jcreated_at']): ## Correct Ounces Error, since it occurs in all values\n",
    "            #     clean_cell = correct_dirty_cell(y)\n",
    "            #     original_row[x] = clean_cell\n",
    "            # else:\n",
    "            clean_cell = Rayyan_Correct_jcreate(y) ## identical value\n",
    "            dirty_cell = y\n",
    "            # if(dirty_cell!='') and (dirty_cell!=clean_cell):\n",
    "            dirty_row[x] = dirty_cell\n",
    "            original_row[x] = clean_cell\n",
    "            # columns_unique = np.array(beer_dirty[x].unique())\n",
    "            # columns_unique = list(np.random.choice(columns_unique,min(15,len(columns_unique)))) ## 截断\n",
    "            # insert_randomly(columns_unique,dirty_cell) \n",
    "            single_context_clean = 'COL %s VAL %s ' % (x,clean_cell)\n",
    "            single_context_dirty = 'COL %s VAL %s ' % (x,dirty_cell)\n",
    "            # column_context = 'COL %s VAL %s ' % (x,columns_unique)\n",
    "            for i in range(1,11,1):\n",
    "                attr = rayyan_dirty_augment.columns[i]\n",
    "                all_context_clean += 'COL %s VAL %s ' % (attr,original_row[attr]) ## Clean Result\n",
    "                all_context_dirty += 'COL %s VAL %s ' % (attr,dirty_row[attr])\n",
    "            # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "            # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "            if detection:\n",
    "                row_list.append([all_context_dirty,single_context_dirty,1])\n",
    "                row_list.append([all_context_clean,single_context_clean,0])\n",
    "                single_list.append([single_context_dirty,single_context_dirty,1])\n",
    "                single_list.append([single_context_clean,single_context_clean,0])                \n",
    "            else:\n",
    "                row_list.append([all_context_clean,single_context_clean,0])\n",
    "                single_list.append([single_context_clean,single_context_clean,0])\n",
    "        elif(x in ['article_pagination']):\n",
    "            detection = Rayyan_Row_Detect(x,y) \n",
    "            \n",
    "            all_context_clean = ''\n",
    "            all_context_dirty = ''\n",
    "            dirty_row = row.copy()\n",
    "            original_row = row.copy()\n",
    "            # if(x in ['article_jcreated_at']): ## Correct Ounces Error, since it occurs in all values\n",
    "            #     clean_cell = correct_dirty_cell(y)\n",
    "            #     original_row[x] = clean_cell\n",
    "            # else:\n",
    "            clean_cell = Rayyan_Correct_pagination(y) ## identical value\n",
    "            dirty_cell = y\n",
    "            # if(dirty_cell!='') and (dirty_cell!=clean_cell):\n",
    "            dirty_row[x] = dirty_cell\n",
    "            original_row[x] = clean_cell\n",
    "            # columns_unique = np.array(beer_dirty[x].unique())\n",
    "            # columns_unique = list(np.random.choice(columns_unique,min(15,len(columns_unique)))) ## 截断\n",
    "            # insert_randomly(columns_unique,dirty_cell) \n",
    "            single_context_clean = 'COL %s VAL %s ' % (x,clean_cell)\n",
    "            single_context_dirty = 'COL %s VAL %s ' % (x,dirty_cell)\n",
    "            # column_context = 'COL %s VAL %s ' % (x,columns_unique)\n",
    "            for i in range(1,11,1):\n",
    "                attr = rayyan_dirty_augment.columns[i]\n",
    "                all_context_clean += 'COL %s VAL %s ' % (attr,original_row[attr]) ## Clean Result\n",
    "                all_context_dirty += 'COL %s VAL %s ' % (attr,dirty_row[attr])\n",
    "            # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "            # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "            if detection and (single_context_dirty!=single_context_clean):\n",
    "                print(single_context_dirty)\n",
    "                row_list.append([all_context_dirty,single_context_dirty,1])\n",
    "                row_list.append([all_context_clean,single_context_clean,0])\n",
    "                single_list.append([single_context_dirty,single_context_dirty,1])\n",
    "                single_list.append([single_context_clean,single_context_clean,0])                \n",
    "            else:\n",
    "                row_list.append([all_context_clean,single_context_clean,0])\n",
    "                single_list.append([single_context_clean,single_context_clean,0])\n",
    "        elif(x in ['journal_issn']):\n",
    "            detection = Rayyan_Row_Detect(x,y) \n",
    "            \n",
    "            all_context_clean = ''\n",
    "            all_context_dirty = ''\n",
    "            dirty_row = row.copy()\n",
    "            original_row = row.copy()\n",
    "            # if(x in ['article_jcreated_at']): ## Correct Ounces Error, since it occurs in all values\n",
    "            #     clean_cell = correct_dirty_cell(y)\n",
    "            #     original_row[x] = clean_cell\n",
    "            # else:\n",
    "            clean_cell = Rayyan_Correct_issn(y) ## identical value\n",
    "            dirty_cell = y\n",
    "            # if(dirty_cell!='') and (dirty_cell!=clean_cell):\n",
    "            dirty_row[x] = dirty_cell\n",
    "            original_row[x] = clean_cell\n",
    "            # columns_unique = np.array(beer_dirty[x].unique())\n",
    "            # columns_unique = list(np.random.choice(columns_unique,min(15,len(columns_unique)))) ## 截断\n",
    "            # insert_randomly(columns_unique,dirty_cell) \n",
    "            single_context_clean = 'COL %s VAL %s ' % (x,clean_cell)\n",
    "            single_context_dirty = 'COL %s VAL %s ' % (x,dirty_cell)\n",
    "            # column_context = 'COL %s VAL %s ' % (x,columns_unique)\n",
    "            for i in range(1,11,1):\n",
    "                attr = rayyan_dirty_augment.columns[i]\n",
    "                all_context_clean += 'COL %s VAL %s ' % (attr,original_row[attr]) ## Clean Result\n",
    "                all_context_dirty += 'COL %s VAL %s ' % (attr,dirty_row[attr])\n",
    "            # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "            # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "            if detection:\n",
    "                print(single_context_dirty)\n",
    "                row_list.append([all_context_dirty,single_context_dirty,1])\n",
    "                row_list.append([all_context_clean,single_context_clean,0])\n",
    "                single_list.append([single_context_dirty,single_context_dirty,1])\n",
    "                single_list.append([single_context_clean,single_context_clean,0])                \n",
    "            else:\n",
    "                row_list.append([all_context_clean,single_context_clean,0])\n",
    "                single_list.append([single_context_clean,single_context_clean,0])\n",
    "        else:\n",
    "            all_context_clean = ''    \n",
    "            original_row = row.copy()\n",
    "            clean_cell = y\n",
    "            single_context_clean = 'COL %s VAL %s ' % (x,clean_cell)\n",
    "            for i in range(1,11,1):\n",
    "                attr = rayyan_dirty.columns[i]\n",
    "                all_context_clean += 'COL %s VAL %s ' % (attr,original_row[attr]) ## Clean Result \n",
    "            # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "            row_list.append([all_context_clean,single_context_clean,0])\n",
    "            single_list.append([single_context_clean,single_context_clean,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'article_title', 'article_language', 'journal_title',\n",
       "       'jounral_abbreviation', 'journal_issn', 'article_jvolumn',\n",
       "       'article_jissue', 'article_jcreated_at', 'article_pagination',\n",
       "       'author_list'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'article_title', 'article_language', 'journal_title',\n",
       "       'jounral_abbreviation', 'journal_issn', 'article_jvolumn',\n",
       "       'article_jissue', 'article_jcreated_at', 'article_pagination',\n",
       "       'author_list'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rayyan_Row_Detect('journal_issn','1551-1553')\n",
    "rayyan_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rayyan_Detect_pagination('835-40')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(row_list).to_csv('datasets/rayyan/detector/multi-view/train_aug.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(single_list).to_csv('datasets/rayyan/detector/multi-view/train_aug_single.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([158, 455,  85, 118, 322, 384, 392, 615, 656, 796, 862, 975, 979,\n",
       "       357, 130, 532, 212, 694, 924, 918])"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_label_index = np.load('datasets/rayyan/detector/index.npy')\n",
    "rayyan_label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71ccf007f1c414c9c43cc5ebae34cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Rayyan_Augmented_Data\n",
    "rayyan_clean = pd.read_csv('datasets/rayyan/clean.csv').fillna('')\n",
    "rayyan_dirty = pd.read_csv('datasets/rayyan/dirty.csv').fillna('')\n",
    "def Str2Int(row):\n",
    "    for index in range(11):\n",
    "        temp = row[index]\n",
    "        try:\n",
    "            row[index] = str(int(temp))\n",
    "        except:\n",
    "            continue\n",
    "    return row\n",
    "rayyan_clean = rayyan_clean.apply(Str2Int,axis=1)\n",
    "rayyan_dirty = rayyan_dirty.apply(Str2Int,axis=1)\n",
    "row_list = []\n",
    "single_list = []\n",
    "column_list = []\n",
    "rayyan_dirty_augment = rayyan_dirty.copy()\n",
    "detector_list_rayyan = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "# for label_tuple in tqdm(rayyan_label_index[:20]):\n",
    "for label_tuple in tqdm(range(len(rayyan_dirty))):\n",
    "    for i in range(1,len(rayyan_dirty.columns),1):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = rayyan_clean.iloc[label_tuple]\n",
    "        dirty_context = rayyan_dirty.iloc[label_tuple]\n",
    "        clean_cell = rayyan_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = rayyan_dirty.iloc[label_tuple,i]\n",
    "        single_context_clean = 'COL %s VAL %s ' % (rayyan_dirty.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (rayyan_dirty.columns[i],dirty_cell)\n",
    "        for c in range(1,11,1):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (rayyan_clean.columns[c],rayyan_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (rayyan_dirty.columns[c],rayyan_dirty.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        if(dirty_cell!=clean_cell):\n",
    "            detector_list_rayyan.append([all_context_dirty,single_context_dirty,1])\n",
    "            # detector_list_rayyan.append([all_context_clean,single_context_clean,0])\n",
    "        else:\n",
    "            # detector_list_rayyan.append([all_context_clean,single_context_clean,0])\n",
    "            detector_list_rayyan.append([all_context_dirty,single_context_dirty,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_rayyan).to_csv('datasets/rayyan/detector/multi-view/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_rayyan).to_csv('datasets/rayyan/detector/multi-view/train_20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'article_title', 'article_language', 'journal_title',\n",
       "       'jounral_abbreviation', 'journal_issn', 'article_jvolumn',\n",
       "       'article_jissue', 'article_jcreated_at', 'article_pagination',\n",
       "       'author_list'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_dirty.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rayyan_Row_Augment(row,x): ## if cannot make augmentation, return [False,'',''], if we can make augmentation, return [True,dirty_row,dirty_cell,clean_cell], 'article_jvolumn','article_jissue' do not envolve in augmentation\n",
    "    original_row = row.copy()\n",
    "    dirty_row = row.copy()\n",
    "    y = original_row[x]\n",
    "    if(x in ['article_title', 'journal_title', \n",
    "'author_list']):\n",
    "        detection = Rayyan_Row_Detect(x,y)\n",
    "        clean_cell = y\n",
    "        dirty_cell = Rayyan_Row_Generate(x,clean_cell)\n",
    "        dirty_row[x] = dirty_cell\n",
    "        if (not detection) and(clean_cell!=dirty_cell) : ## Valid Augmentation\n",
    "            return [True,dirty_row,dirty_cell,clean_cell]\n",
    "        else:\n",
    "            return [False,'','','']\n",
    "    elif(x in ['article_jcreated_at','article_pagination','journal_issn'] and y!=''):\n",
    "        detection = Rayyan_Row_Detect(x,y)\n",
    "        clean_cell = Rayyan_Row_Correction(x,y)\n",
    "        dirty_cell = y\n",
    "        dirty_row[x] = dirty_cell\n",
    "        original_row[x] = clean_cell\n",
    "        if(detection):\n",
    "            return [True,dirty_row,dirty_cell,clean_cell]\n",
    "        else:\n",
    "            return [False,'','','']\n",
    "    elif(x in ['article_jvolumn','article_jissue']):\n",
    "        detection = Rayyan_Row_Detect(x,y)\n",
    "        clean_cell = Rayyan_Row_Correction(x,y)\n",
    "        dirty_cell = y\n",
    "        dirty_row[x] = dirty_cell\n",
    "        original_row[x] = clean_cell\n",
    "        if(detection):\n",
    "            return [True,dirty_row,dirty_cell,clean_cell]\n",
    "        else:\n",
    "            return [False,'','','']\n",
    "    else:\n",
    "        return [False,'','','']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Augmented Training Data in LLM Generation, size controlled into 3000-4000 pairs\n",
    "import json\n",
    "rayyan_clean = pd.read_csv('datasets/rayyan/clean.csv').fillna('')\n",
    "rayyan_dirty = pd.read_csv('datasets/rayyan/dirty.csv').fillna('')\n",
    "def Str2Int(row):\n",
    "    for index in range(11):\n",
    "        temp = row[index]\n",
    "        try:\n",
    "            row[index] = str(int(temp))\n",
    "        except:\n",
    "            continue\n",
    "    return row\n",
    "rayyan_clean = rayyan_clean.apply(Str2Int,axis=1)\n",
    "rayyan_dirty = rayyan_dirty.apply(Str2Int,axis=1)\n",
    "row_list = []\n",
    "single_list = []\n",
    "column_list = []\n",
    "rayyan_dirty_augment = rayyan_dirty.copy()\n",
    "detector_list_rayyan = []\n",
    "rayyan_label_index_select = rayyan_label_index[:5]\n",
    "dirty_example_dict = {}\n",
    "dirty_example_dict_index = {'article_title':1, 'journal_title':3, 'journal_issn':5, 'article_jvolumn':6,'article_jissue':7,'article_jcreated_at':8,'article_pagination':9,'author_list':10}\n",
    "for x in dirty_example_dict_index.keys():\n",
    "    dirty_example = []\n",
    "    j = dirty_example_dict_index[x]\n",
    "    for i in rayyan_label_index_select:\n",
    "        if rayyan_dirty.iloc[i,j]!=rayyan_clean.iloc[i,j]:\n",
    "            dirty_example.append([rayyan_dirty.iloc[i,j],rayyan_clean.iloc[i,j]]) \n",
    "    dirty_example_dict[x] = dirty_example\n",
    "    # print(\"The input \\n\\n%s\\n\\nare [dirty,clean] cell pairs\" % str(dirty_example))\n",
    "training_list = []\n",
    "for index,row in rayyan_dirty_augment.iterrows():\n",
    "    row_input = row.copy()\n",
    "\n",
    "    for x,y in row.items():\n",
    "        Indicator,row_output,dirty_cell,clean_cell = Rayyan_Row_Augment(row_input,x)\n",
    "        if(Indicator):\n",
    "                dirty_example = dirty_example_dict[x]\n",
    "                if(x in ['article_title','journal_title','author_list']):\n",
    "                    dirty_example = random.sample(dirty_example,min(2,len(dirty_example)))\n",
    "                else:\n",
    "                    dirty_example = random.sample(dirty_example,min(5,len(dirty_example)))\n",
    "                dirty_row = row_output.copy()\n",
    "                original_row = row_output.copy()\n",
    "                dirty_row[x] = dirty_cell\n",
    "                # columns_unique = np.array(beer_dirty[x].unique())\n",
    "                # columns_unique = list(np.random.choice(columns_unique,min(15,len(columns_unique)))) ## 截断\n",
    "                # insert_randomly(columns_unique,dirty_cell) \n",
    "                template_dict = {}\n",
    "                temp_dict = dirty_row[1:].to_dict()\n",
    "                template_dict[x] = ''\n",
    "                coreset_reference = np.random.choice(rayyan_label_index_select,2,replace=False)\n",
    "                # clean_dict = original_row[2:].to_dict()\n",
    "                clean_dict = {}\n",
    "                clean_dict[x] = clean_cell\n",
    "                Correction_format = \"The input \\n\\n%s\\n\\nare [dirty,clean] cell pairs for value %s\" % (str(dirty_example),x)\n",
    "                text_head = 'You are an expert in Cleaning Rayyan Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\n%s\\n\\nTake these clean rows as reference:\\n\\n' % (x, json.dumps(template_dict), json.dumps(temp_dict),Correction_format)\n",
    "                ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(rayyan_clean.iloc[coreset_reference[0],1:].to_dict()), json.dumps(rayyan_clean.iloc[coreset_reference[1],1:].to_dict()))\n",
    "                \n",
    "                training_list.append([text_head,ICL_text,'',json.dumps(clean_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Augmented Training Data in LLM Generation, size controlled into 3000-4000 pairs\n",
    "import json\n",
    "rayyan_clean = pd.read_csv('datasets/rayyan/clean.csv').fillna('')\n",
    "rayyan_dirty = pd.read_csv('datasets/rayyan/dirty.csv').fillna('')\n",
    "def Str2Int(row):\n",
    "    for index in range(11):\n",
    "        temp = row[index]\n",
    "        try:\n",
    "            row[index] = str(int(temp))\n",
    "        except:\n",
    "            continue\n",
    "    return row\n",
    "rayyan_clean = rayyan_clean.apply(Str2Int,axis=1)\n",
    "rayyan_dirty = rayyan_dirty.apply(Str2Int,axis=1)\n",
    "row_list = []\n",
    "single_list = []\n",
    "column_list = []\n",
    "rayyan_dirty_augment = rayyan_dirty.copy()\n",
    "detector_list_rayyan = []\n",
    "rayyan_label_index_select = rayyan_label_index[:5]\n",
    "dirty_example_dict = {}\n",
    "dirty_example_dict_index = {'article_title':1, 'journal_title':3, 'journal_issn':5, 'article_jvolumn':6,'article_jissue':7,'article_jcreated_at':8,'article_pagination':9,'author_list':10}\n",
    "for x in dirty_example_dict_index.keys():\n",
    "    dirty_example = []\n",
    "    j = dirty_example_dict_index[x]\n",
    "    for i in rayyan_label_index_select:\n",
    "        if rayyan_dirty.iloc[i,j]!=rayyan_clean.iloc[i,j]:\n",
    "            dirty_example.append([rayyan_dirty.iloc[i,j],rayyan_clean.iloc[i,j]]) \n",
    "    dirty_example_dict[x] = dirty_example\n",
    "    # print(\"The input \\n\\n%s\\n\\nare [dirty,clean] cell pairs\" % str(dirty_example))\n",
    "training_list_label = []\n",
    "for index,row in rayyan_dirty.iterrows():\n",
    "    if(index in rayyan_label_index_select):\n",
    "        for i in range(11):\n",
    "            if(rayyan_clean.iloc[index,i]!=rayyan_dirty.iloc[index,i]):\n",
    "                clean_cell = rayyan_clean.iloc[index,i]\n",
    "                x = rayyan_clean.columns[i]\n",
    "                dirty_example = dirty_example_dict[x]\n",
    "                # if(dirty_cell!='') and (dirty_cell!=clean_cell):\n",
    "                if(x in ['article_title','journal_title','author_list']):\n",
    "                    dirty_example = random.sample(dirty_example,min(2,len(dirty_example)))\n",
    "                else:\n",
    "                    dirty_example = random.sample(dirty_example,min(5,len(dirty_example)))\n",
    "                # columns_unique = np.array(beer_dirty[x].unique())\n",
    "                # columns_unique = list(np.random.choice(columns_unique,min(15,len(columns_unique)))) ## 截断\n",
    "                # insert_randomly(columns_unique,dirty_cell) \n",
    "                template_dict = {}\n",
    "                temp_dict = rayyan_dirty.iloc[index,1:].to_dict()\n",
    "                template_dict[x] = ''\n",
    "                coreset_reference = np.random.choice(rayyan_label_index_select,2,replace=False)\n",
    "                # clean_dict = original_row[2:].to_dict()\n",
    "                clean_dict = {}\n",
    "                clean_dict[x] = clean_cell\n",
    "                Correction_format = \"The input \\n\\n%s\\n\\nare [dirty,clean] cell pairs for value %s\" % (str(dirty_example),x)\n",
    "                text_head = 'You are an expert in Cleaning Rayyan Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\n%s\\n\\nTake these clean rows as reference:\\n\\n' % (x, json.dumps(template_dict), json.dumps(temp_dict),Correction_format)\n",
    "                ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(rayyan_clean.iloc[coreset_reference[0],1:].to_dict()), json.dumps(rayyan_clean.iloc[coreset_reference[1],1:].to_dict()))   \n",
    "                training_list_label.append([text_head,ICL_text,'',json.dumps(clean_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Augmented Training Data in LLM Generation, size controlled into 3000-4000 pairs\n",
    "import json\n",
    "rayyan_clean = pd.read_csv('datasets/rayyan/clean.csv').fillna('')\n",
    "rayyan_dirty = pd.read_csv('datasets/rayyan/dirty.csv').fillna('')\n",
    "def Str2Int(row):\n",
    "    for index in range(11):\n",
    "        temp = row[index]\n",
    "        try:\n",
    "            row[index] = str(int(temp))\n",
    "        except:\n",
    "            continue\n",
    "    return row\n",
    "rayyan_clean = rayyan_clean.apply(Str2Int,axis=1)\n",
    "rayyan_dirty = rayyan_dirty.apply(Str2Int,axis=1)\n",
    "row_list = []\n",
    "single_list = []\n",
    "column_list = []\n",
    "rayyan_dirty_augment = rayyan_dirty.copy()\n",
    "detector_list_rayyan = []\n",
    "rayyan_label_index_select = rayyan_label_index[:20]\n",
    "dirty_example_dict = {}\n",
    "dirty_example_dict_index = {'article_title':1, 'journal_title':3, 'journal_issn':5, 'article_jvolumn':6,'article_jissue':7,'article_jcreated_at':8,'article_pagination':9,'author_list':10}\n",
    "for x in dirty_example_dict_index.keys():\n",
    "    dirty_example = []\n",
    "    j = dirty_example_dict_index[x]\n",
    "    for i in rayyan_label_index_select:\n",
    "        if rayyan_dirty.iloc[i,j]!=rayyan_clean.iloc[i,j]:\n",
    "            dirty_example.append([rayyan_dirty.iloc[i,j],rayyan_clean.iloc[i,j]]) \n",
    "    dirty_example_dict[x] = dirty_example\n",
    "    # print(\"The input \\n\\n%s\\n\\nare [dirty,clean] cell pairs\" % str(dirty_example))\n",
    "training_list_label = []\n",
    "for index,i in np.argwhere(rayyan_detector==1):\n",
    "    # if(index in rayyan_label_index_select):\n",
    "        # for i in range(11):\n",
    "            i = i + 1 ## Exclude id\n",
    "            # if(rayyan_clean.iloc[index,i]!=rayyan_dirty.iloc[index,i]):\n",
    "            clean_cell = rayyan_clean.iloc[index,i]\n",
    "            x = rayyan_clean.columns[i]\n",
    "            dirty_example = dirty_example_dict[x]\n",
    "            if(x in ['article_title','journal_title','author_list']):\n",
    "                dirty_example = random.sample(dirty_example,min(2,len(dirty_example)))\n",
    "            else:\n",
    "                dirty_example = random.sample(dirty_example,min(5,len(dirty_example)))\n",
    "            # columns_unique = np.array(beer_dirty[x].unique())\n",
    "            # columns_unique = list(np.random.choice(columns_unique,min(15,len(columns_unique)))) ## 截断\n",
    "            # insert_randomly(columns_unique,dirty_cell) \n",
    "            template_dict = {}\n",
    "            temp_dict = rayyan_dirty.iloc[index,1:].to_dict()\n",
    "            template_dict[x] = ''\n",
    "            coreset_reference = np.random.choice(rayyan_label_index_select,2,replace=False)\n",
    "            # clean_dict = original_row[2:].to_dict()\n",
    "            clean_dict = {}\n",
    "            clean_dict[x] = clean_cell\n",
    "            Correction_format = \"The input \\n\\n%s\\n\\nare [dirty,clean] cell pairs for value %s\" % (str(dirty_example),x)\n",
    "            text_head = 'You are an expert in Cleaning Rayyan Dataset. Given the dirty row Entity 1, you are required to correct the values of %s in Entity 1.\\n\\nReturn in json format.\\n\\nOutput Format Example:\\n\\n%s\\n\\nEntity 1:\\n\\n%s\\n\\n%s\\n\\nTake these clean rows as reference:\\n\\n' % (x, json.dumps(template_dict), json.dumps(temp_dict),Correction_format)\n",
    "            ICL_text = '%s\\n\\n%s\\n\\n' % (json.dumps(rayyan_clean.iloc[coreset_reference[0],1:].to_dict()), json.dumps(rayyan_clean.iloc[coreset_reference[1],1:].to_dict()))   \n",
    "            training_list_label.append([text_head,ICL_text,'',json.dumps(clean_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list_label_pd = pd.DataFrame(training_list_label)\n",
    "training_list_label_pd['instruction'] = training_list_label_pd[0] + training_list_label_pd[1]\n",
    "training_list_label_pd['input'] = training_list_label_pd[2]\n",
    "training_list_label_pd['output'] = training_list_label_pd[3]\n",
    "# training_list_label_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(training_list_label_pd.iloc[:,-3:].to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/rayyan/rayyan-test-20.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_list_pd = pd.DataFrame(training_list).sample(n=(150 * len(beer_label_index_select)))\n",
    "training_list_pd = pd.DataFrame(training_list)\n",
    "training_list_pd['instruction'] = training_list_pd[0] + training_list_pd[1]\n",
    "training_list_pd['input'] = training_list_pd[2]\n",
    "training_list_pd['output'] = training_list_pd[3]\n",
    "training_list_label_pd = pd.DataFrame(training_list_label)\n",
    "training_list_label_pd['instruction'] = training_list_label_pd[0] + training_list_label_pd[1]\n",
    "training_list_label_pd['input'] = training_list_label_pd[2]\n",
    "training_list_label_pd['output'] = training_list_label_pd[3]\n",
    "training_list_output = pd.concat([training_list_pd,training_list_label_pd])\n",
    "training_list_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(training_list_output.iloc[:,-3:].to_dict(orient='records'), open('/home/yanmy/LLaMA-Efficient-Tuning-main/data/rayyan/rayyan-train-5.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10)"
      ]
     },
     "execution_count": 707,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rayyan_detector = np.load('datasets/rayyan/detector/multi-view/detector_20.npy').reshape((1000,10))\n",
    "rayyan_detector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_label_index = np.load('datasets/tax/detector/index.npy')\n",
    "example = []\n",
    "for t in tax_label_index[:10]:\n",
    "    example.append([tax_clean.iloc[t].to_dict(),tax_dirty.iloc[t].to_dict()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0, 0, 0, 2, 2, 0, 3, 3, 0, 0, 2, 0, 1])"
      ]
     },
     "execution_count": 737,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_set = tax_clean.iloc[tax_label_index[:10]]\n",
    "dirty_set = tax_dirty.iloc[tax_label_index[:10]]\n",
    "np.array(clean_set!=dirty_set).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try to test recall on Rayyan Dataset\n",
    "import json\n",
    "rayyan_clean = pd.read_csv('datasets/rayyan/clean.csv').fillna('')\n",
    "rayyan_dirty = pd.read_csv('datasets/rayyan/dirty.csv').fillna('')\n",
    "def Str2Int(row):\n",
    "    for index in range(11):\n",
    "        temp = row[index]\n",
    "        try:\n",
    "            row[index] = str(int(temp))\n",
    "        except:\n",
    "            continue\n",
    "    return row\n",
    "rayyan_clean = rayyan_clean.apply(Str2Int,axis=1)\n",
    "rayyan_dirty = rayyan_dirty.apply(Str2Int,axis=1)\n",
    "count = 0\n",
    "valid_count = 0\n",
    "rayyan_correction = rayyan_dirty.copy()\n",
    "import ast\n",
    "for d in np.argwhere(rayyan_detector==1):\n",
    "    i = d[0]\n",
    "    j = d[1] + 1 ## Ignore Index\n",
    "    try:\n",
    "        predict = list(ast.literal_eval(result.iloc[count,-1]).values())[0]\n",
    "        rayyan_correction.iloc[i,j] = predict\n",
    "        valid_count += 1\n",
    "    except:\n",
    "        predict = result.iloc[count,-1]\n",
    "        rayyan_correction.iloc[i,j] = predict\n",
    "    count += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state - zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_name</th>\n",
       "      <th>l_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>area_code</th>\n",
       "      <th>phone</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>has_child</th>\n",
       "      <th>salary</th>\n",
       "      <th>rate</th>\n",
       "      <th>single_exemp</th>\n",
       "      <th>married_exemp</th>\n",
       "      <th>child_exemp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pengyuan</td>\n",
       "      <td>Zendler</td>\n",
       "      <td>F</td>\n",
       "      <td>508</td>\n",
       "      <td>744-9007</td>\n",
       "      <td>SWAMPSCOTT</td>\n",
       "      <td>MA</td>\n",
       "      <td>1907</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>90000</td>\n",
       "      <td>5.3</td>\n",
       "      <td>0</td>\n",
       "      <td>7150</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4411</th>\n",
       "      <td>Sonja</td>\n",
       "      <td>Fullerton</td>\n",
       "      <td>F</td>\n",
       "      <td>339</td>\n",
       "      <td>672-1352</td>\n",
       "      <td>SWAMPSCOTT</td>\n",
       "      <td>MA</td>\n",
       "      <td>1907</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>100000</td>\n",
       "      <td>5.3</td>\n",
       "      <td>0</td>\n",
       "      <td>7150</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27163</th>\n",
       "      <td>Piera</td>\n",
       "      <td>Brodie</td>\n",
       "      <td>F</td>\n",
       "      <td>617</td>\n",
       "      <td>533-3461</td>\n",
       "      <td>SWAMPSCOTT</td>\n",
       "      <td>MA</td>\n",
       "      <td>1907</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>10000</td>\n",
       "      <td>5.3</td>\n",
       "      <td>0</td>\n",
       "      <td>7150</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77457</th>\n",
       "      <td>Premachandran</td>\n",
       "      <td>Nepomnjashchaja</td>\n",
       "      <td>M</td>\n",
       "      <td>978</td>\n",
       "      <td>571-1299</td>\n",
       "      <td>SWAMPSCOTT</td>\n",
       "      <td>MA</td>\n",
       "      <td>1907</td>\n",
       "      <td>S</td>\n",
       "      <td>Y</td>\n",
       "      <td>75000</td>\n",
       "      <td>5.3</td>\n",
       "      <td>3575</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101971</th>\n",
       "      <td>Arden</td>\n",
       "      <td>Leach</td>\n",
       "      <td>M</td>\n",
       "      <td>617</td>\n",
       "      <td>953-8814</td>\n",
       "      <td>SWAMPSCOTT</td>\n",
       "      <td>MA</td>\n",
       "      <td>1907</td>\n",
       "      <td>S</td>\n",
       "      <td>Y</td>\n",
       "      <td>45000</td>\n",
       "      <td>5.3</td>\n",
       "      <td>3575</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125068</th>\n",
       "      <td>Mohua</td>\n",
       "      <td>Ducret</td>\n",
       "      <td>M</td>\n",
       "      <td>351</td>\n",
       "      <td>516-5755</td>\n",
       "      <td>SWAMPSCOTT</td>\n",
       "      <td>MA</td>\n",
       "      <td>1907</td>\n",
       "      <td>S</td>\n",
       "      <td>Y</td>\n",
       "      <td>30000</td>\n",
       "      <td>5.3</td>\n",
       "      <td>3575</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128619</th>\n",
       "      <td>Yasuaki</td>\n",
       "      <td>Glodjo</td>\n",
       "      <td>M</td>\n",
       "      <td>413</td>\n",
       "      <td>313-1978</td>\n",
       "      <td>SWAMPSCOTT</td>\n",
       "      <td>MA</td>\n",
       "      <td>1907</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>30000</td>\n",
       "      <td>5.3</td>\n",
       "      <td>3575</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163559</th>\n",
       "      <td>Rushikesh</td>\n",
       "      <td>Kaka</td>\n",
       "      <td>M</td>\n",
       "      <td>857</td>\n",
       "      <td>384-7061</td>\n",
       "      <td>SWAMPSCOTT</td>\n",
       "      <td>MA</td>\n",
       "      <td>1907</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>35000</td>\n",
       "      <td>5.3</td>\n",
       "      <td>3575</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191027</th>\n",
       "      <td>Klichiro</td>\n",
       "      <td>Davy</td>\n",
       "      <td>M</td>\n",
       "      <td>413</td>\n",
       "      <td>110-1863</td>\n",
       "      <td>SWAMPSCOTT</td>\n",
       "      <td>MA</td>\n",
       "      <td>1907</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>65000</td>\n",
       "      <td>5.3</td>\n",
       "      <td>0</td>\n",
       "      <td>7150</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               f_name           l_name gender area_code     phone        city  \\\n",
       "0            Pengyuan          Zendler      F       508  744-9007  SWAMPSCOTT   \n",
       "4411            Sonja        Fullerton      F       339  672-1352  SWAMPSCOTT   \n",
       "27163           Piera           Brodie      F       617  533-3461  SWAMPSCOTT   \n",
       "77457   Premachandran  Nepomnjashchaja      M       978  571-1299  SWAMPSCOTT   \n",
       "101971          Arden            Leach      M       617  953-8814  SWAMPSCOTT   \n",
       "125068          Mohua           Ducret      M       351  516-5755  SWAMPSCOTT   \n",
       "128619        Yasuaki           Glodjo      M       413  313-1978  SWAMPSCOTT   \n",
       "163559      Rushikesh             Kaka      M       857  384-7061  SWAMPSCOTT   \n",
       "191027       Klichiro             Davy      M       413  110-1863  SWAMPSCOTT   \n",
       "\n",
       "       state   zip marital_status has_child  salary rate single_exemp  \\\n",
       "0         MA  1907              M         N   90000  5.3            0   \n",
       "4411      MA  1907              M         Y  100000  5.3            0   \n",
       "27163     MA  1907              M         N   10000  5.3            0   \n",
       "77457     MA  1907              S         Y   75000  5.3         3575   \n",
       "101971    MA  1907              S         Y   45000  5.3         3575   \n",
       "125068    MA  1907              S         Y   30000  5.3         3575   \n",
       "128619    MA  1907              S         N   30000  5.3         3575   \n",
       "163559    MA  1907              S         N   35000  5.3         3575   \n",
       "191027    MA  1907              M         N   65000  5.3            0   \n",
       "\n",
       "       married_exemp child_exemp  \n",
       "0               7150           0  \n",
       "4411            7150        1000  \n",
       "27163           7150           0  \n",
       "77457              0        1000  \n",
       "101971             0        1000  \n",
       "125068             0        1000  \n",
       "128619             0           0  \n",
       "163559             0           0  \n",
       "191027          7150           0  "
      ]
     },
     "execution_count": 839,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_clean[tax_dirty['city']=='SWAMPSCOTT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_name</th>\n",
       "      <th>l_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>area_code</th>\n",
       "      <th>phone</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>has_child</th>\n",
       "      <th>salary</th>\n",
       "      <th>rate</th>\n",
       "      <th>single_exemp</th>\n",
       "      <th>married_exemp</th>\n",
       "      <th>child_exemp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4547</th>\n",
       "      <td>Shoaib</td>\n",
       "      <td>Khedr</td>\n",
       "      <td>F</td>\n",
       "      <td>573</td>\n",
       "      <td>108-9947</td>\n",
       "      <td>BARING</td>\n",
       "      <td>MO</td>\n",
       "      <td>1907</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>70000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5344</th>\n",
       "      <td>Cyndi</td>\n",
       "      <td>Gutta</td>\n",
       "      <td>M</td>\n",
       "      <td>509</td>\n",
       "      <td>682-4764</td>\n",
       "      <td>BARING</td>\n",
       "      <td>WA</td>\n",
       "      <td>98224</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28624</th>\n",
       "      <td>Hayong</td>\n",
       "      <td>Lippok</td>\n",
       "      <td>M</td>\n",
       "      <td>660</td>\n",
       "      <td>311-6091</td>\n",
       "      <td>BARING</td>\n",
       "      <td>MO</td>\n",
       "      <td>63531</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>40000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36369</th>\n",
       "      <td>Naotaka</td>\n",
       "      <td>Thier</td>\n",
       "      <td>M</td>\n",
       "      <td>573</td>\n",
       "      <td>697-1836</td>\n",
       "      <td>BARING</td>\n",
       "      <td>MO</td>\n",
       "      <td>63531</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>100000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4200</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76559</th>\n",
       "      <td>Kersten</td>\n",
       "      <td>Stiemerling</td>\n",
       "      <td>F</td>\n",
       "      <td>314</td>\n",
       "      <td>443-9586</td>\n",
       "      <td>BARING</td>\n",
       "      <td>MO</td>\n",
       "      <td>63531</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>90000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84076</th>\n",
       "      <td>Sophia</td>\n",
       "      <td>Iliopoulos</td>\n",
       "      <td>F</td>\n",
       "      <td>360</td>\n",
       "      <td>869-6691</td>\n",
       "      <td>BARING</td>\n",
       "      <td>WA</td>\n",
       "      <td>98224</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85841</th>\n",
       "      <td>Keven</td>\n",
       "      <td>Schroff</td>\n",
       "      <td>F</td>\n",
       "      <td>206</td>\n",
       "      <td>369-5318</td>\n",
       "      <td>BARING</td>\n",
       "      <td>WA</td>\n",
       "      <td>98224</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>45000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130329</th>\n",
       "      <td>Libby</td>\n",
       "      <td>Mansouri</td>\n",
       "      <td>M</td>\n",
       "      <td>816</td>\n",
       "      <td>278-2029</td>\n",
       "      <td>BARING</td>\n",
       "      <td>MO</td>\n",
       "      <td>63531</td>\n",
       "      <td>S</td>\n",
       "      <td>Y</td>\n",
       "      <td>75000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2100</td>\n",
       "      <td>0</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139615</th>\n",
       "      <td>Behcet</td>\n",
       "      <td>Gohara</td>\n",
       "      <td>M</td>\n",
       "      <td>253</td>\n",
       "      <td>548-6329</td>\n",
       "      <td>BARING</td>\n",
       "      <td>WA</td>\n",
       "      <td>98224</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158320</th>\n",
       "      <td>Raquel</td>\n",
       "      <td>Roxin</td>\n",
       "      <td>F</td>\n",
       "      <td>253</td>\n",
       "      <td>359-9294</td>\n",
       "      <td>BARING</td>\n",
       "      <td>WA</td>\n",
       "      <td>98224</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165479</th>\n",
       "      <td>Yoshiji</td>\n",
       "      <td>Bruchert</td>\n",
       "      <td>M</td>\n",
       "      <td>816</td>\n",
       "      <td>693-8324</td>\n",
       "      <td>BARING</td>\n",
       "      <td>MO</td>\n",
       "      <td>63531</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>70000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174161</th>\n",
       "      <td>Tjalling</td>\n",
       "      <td>Claybrook</td>\n",
       "      <td>M</td>\n",
       "      <td>314</td>\n",
       "      <td>715-7259</td>\n",
       "      <td>BARING</td>\n",
       "      <td>MO</td>\n",
       "      <td>63531</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0</td>\n",
       "      <td>4200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          f_name       l_name gender area_code     phone    city state    zip  \\\n",
       "4547      Shoaib        Khedr      F       573  108-9947  BARING    MO   1907   \n",
       "5344       Cyndi        Gutta      M       509  682-4764  BARING    WA  98224   \n",
       "28624     Hayong       Lippok      M       660  311-6091  BARING    MO  63531   \n",
       "36369    Naotaka        Thier      M       573  697-1836  BARING    MO  63531   \n",
       "76559    Kersten  Stiemerling      F       314  443-9586  BARING    MO  63531   \n",
       "84076     Sophia   Iliopoulos      F       360  869-6691  BARING    WA  98224   \n",
       "85841      Keven      Schroff      F       206  369-5318  BARING    WA  98224   \n",
       "130329     Libby     Mansouri      M       816  278-2029  BARING    MO  63531   \n",
       "139615    Behcet       Gohara      M       253  548-6329  BARING    WA  98224   \n",
       "158320    Raquel        Roxin      F       253  359-9294  BARING    WA  98224   \n",
       "165479   Yoshiji     Bruchert      M       816  693-8324  BARING    MO  63531   \n",
       "174161  Tjalling    Claybrook      M       314  715-7259  BARING    MO  63531   \n",
       "\n",
       "       marital_status has_child  salary  rate single_exemp married_exemp  \\\n",
       "4547                S         N   70000   6.0         2100             0   \n",
       "5344                M         Y    5000   0.0            0             0   \n",
       "28624               S         N   40000   6.0         2100             0   \n",
       "36369               M         Y  100000   6.0            0          4200   \n",
       "76559               S         N   90000   6.0         2100             0   \n",
       "84076               M         Y    5000   0.0            0             0   \n",
       "85841               M         Y   45000   0.0            0             0   \n",
       "130329              S         Y   75000   6.0         2100             0   \n",
       "139615              M         N   30000   0.0            0             0   \n",
       "158320              M         Y   10000   0.0            0             0   \n",
       "165479              S         N   70000   6.0         2100             0   \n",
       "174161              M         N    5000  3.75            0          4200   \n",
       "\n",
       "       child_exemp  \n",
       "4547             0  \n",
       "5344             0  \n",
       "28624            0  \n",
       "36369         1200  \n",
       "76559            0  \n",
       "84076            0  \n",
       "85841            0  \n",
       "130329        1200  \n",
       "139615           0  \n",
       "158320           0  \n",
       "165479           0  \n",
       "174161           0  "
      ]
     },
     "execution_count": 845,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_dirty[tax_dirty['city']=='BARING']\n",
    "# tax_dirty[tax_dirty['zip']=='89140']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "rayyan_clean = pd.read_csv('datasets/rayyan/clean.csv').fillna('')\n",
    "rayyan_dirty = pd.read_csv('datasets/rayyan/dirty.csv').fillna('')\n",
    "def Str2Int(row):\n",
    "    for index in range(11):\n",
    "        temp = row[index]\n",
    "        try:\n",
    "            row[index] = str(int(temp))\n",
    "        except:\n",
    "            continue\n",
    "    return row\n",
    "rayyan_clean = rayyan_clean.apply(Str2Int,axis=1)\n",
    "rayyan_dirty = rayyan_dirty.apply(Str2Int,axis=1)\n",
    "rayyan_correction_function = rayyan_dirty.copy()\n",
    "# if(x in ['journal_issn','article_jvolumn','article_jissue','article_jcreated_at','article_pagination']):\n",
    "for i,j in np.argwhere(rayyan_detector==1):\n",
    "    j = j + 1 ## Skip Index\n",
    "    dirty_cell = rayyan_dirty.iloc[i,j]\n",
    "    x = rayyan_dirty.columns[j]\n",
    "    correction = Rayyan_Row_Correction(x,dirty_cell)\n",
    "    if correction!=False:\n",
    "        rayyan_correction_function.iloc[i,j] = correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [],
   "source": [
    "rayyan_correction.to_csv('datasets/rayyan/detector/multi-view/correction_20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [],
   "source": [
    "rayyan_correction_function.to_csv('datasets/rayyan/detector/multi-view/correction_function_20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correction_list = []\n",
    "correction_pd = rayyan_correction_function.copy()\n",
    "for i,j in np.argwhere(rayyan_detector==1):\n",
    "    all_context_clean = ''\n",
    "    j = j + 1 ## Skip Index\n",
    "    x = rayyan_dirty.columns[j]\n",
    "    clean_cell = correction_pd.iloc[i,j]\n",
    "    single_context_clean = 'COL %s VAL %s ' % (x,clean_cell)\n",
    "    for c in range(1,11,1):\n",
    "        all_context_clean += 'COL %s VAL %s ' % (rayyan_clean.columns[c],correction_pd.iloc[i,c])\n",
    "    if(clean_cell!=rayyan_clean.iloc[i,j]):\n",
    "        correction_list.append([all_context_clean,single_context_clean,1])\n",
    "    else:\n",
    "        correction_list.append([all_context_clean,single_context_clean,0])\n",
    "pd.DataFrame(correction_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(correction_list).to_csv('datasets/rayyan/detector/multi-view/correction_LLM_20.csv')\n",
    "pd.DataFrame(correction_list).to_csv('datasets/rayyan/detector/multi-view/correction_function_20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [],
   "source": [
    "rayyan_correction_final = rayyan_dirty.copy()\n",
    "correction_function = np.load('/home/yanmy/raha/raha-master/datasets/rayyan/detector/multi-view/correction_function_20.npy')\n",
    "correction_LLM = np.load('/home/yanmy/raha/raha-master/datasets/rayyan/detector/multi-view/correction_LLM_20.npy')\n",
    "count = 0\n",
    "for i,j in np.argwhere(rayyan_detector==1):\n",
    "    j = j + 1 ## Skip Index\n",
    "    LLM = correction_LLM[count]\n",
    "    function = correction_function[count]\n",
    "    LLM_output = rayyan_correction.iloc[i,j]\n",
    "    function_output = rayyan_correction_function.iloc[i,j]\n",
    "    if(LLM<function): ## the small predict score, indicate that the value is more normal, and should be accelpted as correction value\n",
    "        rayyan_correction_final.iloc[i,j] = LLM_output\n",
    "    else:\n",
    "        rayyan_correction_final.iloc[i,j] = function_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.796028880866426, 0.930379746835443, 0.8579766536964981)"
      ]
     },
     "execution_count": 832,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "rayyan_clean = pd.read_csv('datasets/rayyan/clean.csv').fillna('')\n",
    "rayyan_dirty = pd.read_csv('datasets/rayyan/dirty.csv').fillna('')\n",
    "def Str2Int(row):\n",
    "    for index in range(11):\n",
    "        temp = row[index]\n",
    "        try:\n",
    "            row[index] = str(int(temp))\n",
    "        except:\n",
    "            continue\n",
    "    return row\n",
    "rayyan_clean = rayyan_clean.apply(Str2Int,axis=1)\n",
    "rayyan_dirty = rayyan_dirty.apply(Str2Int,axis=1)\n",
    "All_Data_Error = 0\n",
    "All_Fixed_Error = 0\n",
    "Correct_Fixed_Error = 0\n",
    "for i in range(1000):\n",
    "    for j in range(11):\n",
    "        dirty_cell = rayyan_dirty.iloc[i,j]\n",
    "        clean_cell = rayyan_clean.iloc[i,j]\n",
    "        correct_cell = rayyan_correction_final.iloc[i,j]\n",
    "        if(correct_cell!=dirty_cell):\n",
    "            All_Fixed_Error += 1\n",
    "        if(clean_cell!=dirty_cell):\n",
    "            All_Data_Error += 1\n",
    "            if(correct_cell==clean_cell):\n",
    "                Correct_Fixed_Error += 1\n",
    "Precision_hospital = Correct_Fixed_Error / All_Fixed_Error\n",
    "Recall_hospital = Correct_Fixed_Error / All_Data_Error\n",
    "F1_hospital = (2 * Precision_hospital * Recall_hospital) / (Precision_hospital + Recall_hospital)\n",
    "Precision_hospital,Recall_hospital,F1_hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_result = pd.read_csv('/home/yanmy/LLaMA-Efficient-Tuning-main/inference/beer-test-20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 778,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer_detector = np.load('/home/yanmy/raha/raha-master/datasets/beers/detector/detection.npy')\n",
    "beer_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3357"
      ]
     },
     "execution_count": 792,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(beer_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0, 2410,  700,    0,    0,    0,  127,  127])"
      ]
     },
     "execution_count": 790,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(detector_beer==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_clean = pd.read_csv('datasets/beers/clean.csv').fillna('')\n",
    "beer_dirty = pd.read_csv('datasets/beers/dirty.csv').fillna('')\n",
    "beer_dirty.columns = beer_clean.columns\n",
    "def try_convert_to_int(row):\n",
    "    for x,y in row.items():\n",
    "        if(x in ['ounces','ibu']):\n",
    "            try:\n",
    "                row[x] = int(y)\n",
    "            except:\n",
    "                row[x] = y\n",
    "    return row\n",
    "beer_clean = beer_clean.apply(try_convert_to_int,axis=1).astype(str)\n",
    "beer_dirty = beer_dirty.apply(try_convert_to_int,axis=1).astype(str)\n",
    "count = 0\n",
    "valid_count = 0\n",
    "beer_correction = beer_dirty.copy()\n",
    "import ast\n",
    "for d in np.argwhere(rayyan_detector==1):\n",
    "    i = d[0]\n",
    "    j = d[1] + 1 ## Ignore Index\n",
    "    try:\n",
    "        predict = list(ast.literal_eval(result.iloc[count,-1]).values())[0]\n",
    "        rayyan_correction.iloc[i,j] = predict\n",
    "        valid_count += 1\n",
    "    except:\n",
    "        predict = result.iloc[count,-1]\n",
    "        rayyan_correction.iloc[i,j] = predict\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From Here We Tackle with Tax Dataset Function Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tax_Detect_fname(cell):\n",
    "    return bool(re.search(r\"''\", cell))\n",
    "def Tax_Generate_fname(cell):\n",
    "    # Find all positions of single quotes in the cell\n",
    "    positions = [i for i, char in enumerate(cell) if char == \"'\"]\n",
    "    \n",
    "    # If there's no single quote, return the original cell\n",
    "    if not positions:\n",
    "        return cell\n",
    "    \n",
    "    # Randomly choose a position from the positions of single quotes\n",
    "    chosen_position = random.choice(positions)\n",
    "    \n",
    "    # Insert an additional single quote at the chosen position\n",
    "    dirty_cell = cell[:chosen_position] + \"'\" + cell[chosen_position:]\n",
    "    \n",
    "    return dirty_cell\n",
    "def Tax_Correct_fname(dirty_cell):\n",
    "    return dirty_cell.replace(\"''\", \"'\")\n",
    "def Tax_Detect_city(cell):\n",
    "    return bool(re.search(r'-\\*$', cell))\n",
    "def Tax_Generate_city(cell,num_samples=5):\n",
    "    dirty_cell = cell + '-*'\n",
    "    return dirty_cell\n",
    "def Tax_Correct_city(cell):\n",
    "    # Use regex to remove the '-*' pattern from the end of the string\n",
    "    corrected_value = re.sub('-\\*$', '', cell)\n",
    "    return corrected_value\n",
    "def Tax_Detect_zip(cell):\n",
    "    return cell == '1907'\n",
    "def Tax_Generate_zip(cell):\n",
    "    return '1907'\n",
    "def Tax_Row_Detection(x,cell):\n",
    "    if(x in ['f_name','l_name']):\n",
    "        return Tax_Detect_fname(cell)\n",
    "    elif(x in ['city','state','single_exemp','child_exemp']):\n",
    "        return Tax_Detect_city(cell)\n",
    "    elif(x in ['zip']):\n",
    "        return Tax_Detect_zip(cell)\n",
    "    else:\n",
    "        return False\n",
    "def Tax_Row_Generate(x,cell):\n",
    "    if(x in ['f_name','l_name']):\n",
    "        return Tax_Generate_fname(cell)\n",
    "    elif(x in ['city','state','single_exemp','child_exemp']):\n",
    "        return Tax_Generate_city(cell)\n",
    "    elif(x in ['zip']):\n",
    "        return Tax_Generate_zip(cell)\n",
    "    else:\n",
    "        return False\n",
    "def Tax_Row_Correction(x,cell):\n",
    "    if(x in ['f_name','l_name']):\n",
    "        return Tax_Correct_fname(cell)\n",
    "    elif(x in ['city','state','single_exemp','child_exemp']):\n",
    "        return Tax_Correct_city(cell)\n",
    "    else:\n",
    "        return False   \n",
    "def Tax_Row_Correction_pd(row):\n",
    "    \n",
    "    if(x in ['f_name','l_name']):\n",
    "        return Tax_Correct_fname(cell)\n",
    "    elif(x in ['city','state','single_exemp','child_exemp']):\n",
    "        return Tax_Correct_city(cell)\n",
    "    else:\n",
    "        return False   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_error = np.where(np.array(tax_dirty!=tax_clean).sum(axis=1)!=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2318f083db848879fb6a879f236df71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2929 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "detector_list_tax = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "# for label_tuple in tqdm(tax_label_index[:5]):\n",
    "# for label_tuple in tqdm(tax_label_index[:5]):\n",
    "for label_tuple in tqdm(tax_error):\n",
    "    for i in range(len(tax_dirty.columns)):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = tax_clean.iloc[label_tuple]\n",
    "        dirty_context = tax_dirty.iloc[label_tuple]\n",
    "        clean_cell = tax_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = tax_dirty.iloc[label_tuple,i]\n",
    "        single_context_clean = 'COL %s VAL %s ' % (tax_dirty.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (tax_dirty.columns[i],dirty_cell)\n",
    "        for c in range(len(tax_dirty.columns)):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (tax_clean.columns[c],tax_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (tax_dirty.columns[c],tax_dirty.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        \n",
    "        if(dirty_cell!=clean_cell):\n",
    "            detector_list_tax.append([all_context_dirty,single_context_dirty,1])\n",
    "        else:\n",
    "            detector_list_tax.append([all_context_dirty,single_context_dirty,0])\n",
    "        \n",
    "        # if(dirty_cell!=clean_cell):\n",
    "        #     detector_list_tax.append([all_context_dirty,single_context_dirty,1])\n",
    "        #     detector_list_tax.append([all_context_clean,single_context_clean,0])\n",
    "        # else:\n",
    "        #     detector_list_tax.append([all_context_dirty,single_context_dirty,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07c343c4540d48a2b09b78d4277b16f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2929 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Test Data Generation for Tax\n",
    "detector_list_tax = []\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "# for label_tuple in tqdm(tax_label_index[:5]):\n",
    "# for label_tuple in tqdm(tax_label_index[:5]):\n",
    "for label_tuple in tqdm(tax_error):\n",
    "    for i in range(len(tax_dirty.columns)):\n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = tax_dirty.iloc[label_tuple].copy()\n",
    "        dirty_context = tax_dirty.iloc[label_tuple].copy()\n",
    "        clean_cell = tax_clean.iloc[label_tuple,i]\n",
    "        dirty_cell = tax_dirty.iloc[label_tuple,i]\n",
    "        single_context_clean = 'COL %s VAL %s ' % (tax_dirty.columns[i],clean_cell)\n",
    "        single_context_dirty = 'COL %s VAL %s ' % (tax_dirty.columns[i],dirty_cell)\n",
    "        for c in range(len(tax_dirty.columns)):\n",
    "            all_context_clean += 'COL %s VAL %s ' % (tax_clean.columns[c],tax_clean.iloc[label_tuple,c])\n",
    "            all_context_dirty += 'COL %s VAL %s ' % (tax_dirty.columns[c],tax_dirty.iloc[label_tuple,c])\n",
    "        # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "        # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "        # detector_list.append([single_context_clean,0])        \n",
    "        # detector_list.append([all_context_clean,0])\n",
    "        \n",
    "        if(dirty_cell!=clean_cell):\n",
    "            detector_list_tax.append([all_context_dirty,single_context_dirty,1])\n",
    "        else:\n",
    "            detector_list_tax.append([all_context_dirty,single_context_dirty,0])\n",
    "        \n",
    "        # if(dirty_cell!=clean_cell):\n",
    "        #     detector_list_tax.append([all_context_dirty,single_context_dirty,1])\n",
    "        #     detector_list_tax.append([all_context_clean,single_context_clean,0])\n",
    "        # else:\n",
    "        #     detector_list_tax.append([all_context_dirty,single_context_dirty,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COL f_name VAL Elisa COL l_name VAL d''Argence...</td>\n",
       "      <td>COL f_name VAL Elisa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COL f_name VAL Elisa COL l_name VAL d''Argence...</td>\n",
       "      <td>COL l_name VAL d''Argence</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COL f_name VAL Elisa COL l_name VAL d''Argence...</td>\n",
       "      <td>COL gender VAL M</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COL f_name VAL Elisa COL l_name VAL d''Argence...</td>\n",
       "      <td>COL area_code VAL 208</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COL f_name VAL Elisa COL l_name VAL d''Argence...</td>\n",
       "      <td>COL phone VAL 476-1187</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43930</th>\n",
       "      <td>COL f_name VAL Waheed COL l_name VAL L''Ecuyer...</td>\n",
       "      <td>COL salary VAL 15000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43931</th>\n",
       "      <td>COL f_name VAL Waheed COL l_name VAL L''Ecuyer...</td>\n",
       "      <td>COL rate VAL 5.35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43932</th>\n",
       "      <td>COL f_name VAL Waheed COL l_name VAL L''Ecuyer...</td>\n",
       "      <td>COL single_exemp VAL 3300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43933</th>\n",
       "      <td>COL f_name VAL Waheed COL l_name VAL L''Ecuyer...</td>\n",
       "      <td>COL married_exemp VAL 0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43934</th>\n",
       "      <td>COL f_name VAL Waheed COL l_name VAL L''Ecuyer...</td>\n",
       "      <td>COL child_exemp VAL 0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43935 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0  \\\n",
       "0      COL f_name VAL Elisa COL l_name VAL d''Argence...   \n",
       "1      COL f_name VAL Elisa COL l_name VAL d''Argence...   \n",
       "2      COL f_name VAL Elisa COL l_name VAL d''Argence...   \n",
       "3      COL f_name VAL Elisa COL l_name VAL d''Argence...   \n",
       "4      COL f_name VAL Elisa COL l_name VAL d''Argence...   \n",
       "...                                                  ...   \n",
       "43930  COL f_name VAL Waheed COL l_name VAL L''Ecuyer...   \n",
       "43931  COL f_name VAL Waheed COL l_name VAL L''Ecuyer...   \n",
       "43932  COL f_name VAL Waheed COL l_name VAL L''Ecuyer...   \n",
       "43933  COL f_name VAL Waheed COL l_name VAL L''Ecuyer...   \n",
       "43934  COL f_name VAL Waheed COL l_name VAL L''Ecuyer...   \n",
       "\n",
       "                                1  2  \n",
       "0           COL f_name VAL Elisa   0  \n",
       "1      COL l_name VAL d''Argence   1  \n",
       "2               COL gender VAL M   0  \n",
       "3          COL area_code VAL 208   0  \n",
       "4         COL phone VAL 476-1187   0  \n",
       "...                           ... ..  \n",
       "43930       COL salary VAL 15000   0  \n",
       "43931          COL rate VAL 5.35   0  \n",
       "43932  COL single_exemp VAL 3300   0  \n",
       "43933    COL married_exemp VAL 0   0  \n",
       "43934      COL child_exemp VAL 0   0  \n",
       "\n",
       "[43935 rows x 3 columns]"
      ]
     },
     "execution_count": 949,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(detector_list_tax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 980,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87bcefd232cd4e43971cf1e70431a6f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3895 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Data Augmentation for Tax\n",
    "detector_list_tax = []\n",
    "def Rand_Insert(cell):\n",
    "    length = len(cell)\n",
    "    a = np.random.choice(length)\n",
    "    return cell[:a] + \"'\" + cell[a:]\n",
    "## 对每一列，如果sum不等于0，找到一列\n",
    "\n",
    "    # if(input_matrix_select.sum(axis=0)[i]!=0):\n",
    "    #     print(hospital_clean.columns[i])\n",
    "# for label_tuple in tqdm(tax_label_index[:5]):\n",
    "# for label_tuple in tqdm(tax_label_index[:5]):\n",
    "for label_tuple in tqdm(tax_error):\n",
    "    for i in range(len(tax_dirty.columns)):\n",
    "        x = tax_dirty.columns[i] \n",
    "        # if(input_matrix[label_tuple,i]==1):\n",
    "        all_context_clean = ''\n",
    "        all_context_dirty = ''\n",
    "        clean_context = tax_dirty.iloc[label_tuple].copy()\n",
    "        dirty_context = tax_dirty.iloc[label_tuple].copy()\n",
    "        clean_cell = tax_dirty.iloc[label_tuple,i]\n",
    "        if(not Tax_Row_Detection(x,clean_cell)) and (x in ['city','state','single_exemp','child_exemp','f_name','l_name']):\n",
    "            if(x in ['f_name','l_name']):\n",
    "                clean_cell = Rand_Insert(clean_cell)\n",
    "            dirty_cell = Tax_Row_Generate(x,clean_cell)\n",
    "            # if(clean_cell!=dirty_cell):\n",
    "            # dirty_cell = tax_dirty.iloc[label_tuple,i]\n",
    "            single_context_clean = 'COL %s VAL %s ' % (x,clean_cell)\n",
    "            single_context_dirty = 'COL %s VAL %s ' % (x,dirty_cell)\n",
    "            clean_context[x] = clean_cell\n",
    "            dirty_context[x] = dirty_cell\n",
    "            for c in range(len(tax_dirty.columns)):\n",
    "                all_context_clean += 'COL %s VAL %s ' % (tax_dirty.columns[c],clean_context[c])\n",
    "                all_context_dirty += 'COL %s VAL %s ' % (tax_dirty.columns[c],dirty_context[c])\n",
    "            # all_context_clean = all_context_clean + 'SEP ' + single_context_clean\n",
    "            # all_context_dirty = all_context_dirty + 'SEP ' + single_context_dirty\n",
    "            # detector_list.append([single_context_clean,0])        \n",
    "            # detector_list.append([all_context_clean,0])\n",
    "            \n",
    "            if(dirty_cell!=clean_cell) and (clean_cell!=False):\n",
    "                detector_list_tax.append([all_context_dirty,single_context_dirty,1,x])\n",
    "                detector_list_tax.append([all_context_clean,single_context_clean,0,x])\n",
    "        \n",
    "        # if(dirty_cell!=clean_cell):\n",
    "        #     detector_list_tax.append([all_context_dirty,single_context_dirty,1])\n",
    "        #     detector_list_tax.append([all_context_clean,single_context_clean,0])\n",
    "        # else:\n",
    "        #     detector_list_tax.append([all_context_dirty,single_context_dirty,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 969,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 969,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tax_Row_Detection('f_name',\"Eli''sa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 978,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "single_exemp    7390\n",
       "city            7388\n",
       "child_exemp     7388\n",
       "state           7386\n",
       "f_name          6700\n",
       "l_name          5012\n",
       "Name: 3, dtype: int64"
      ]
     },
     "execution_count": 978,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(detector_list_tax)[3].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 981,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2716</th>\n",
       "      <td>COL f_name VAL Elis''a COL l_name VAL d''Argen...</td>\n",
       "      <td>COL f_name VAL Elis''a</td>\n",
       "      <td>1</td>\n",
       "      <td>f_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2726</th>\n",
       "      <td>COL f_name VAL Ja''ehyung COL l_name VAL D''Am...</td>\n",
       "      <td>COL f_name VAL Ja''ehyung</td>\n",
       "      <td>1</td>\n",
       "      <td>f_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2736</th>\n",
       "      <td>COL f_name VAL ''Oystein COL l_name VAL Give''...</td>\n",
       "      <td>COL f_name VAL ''Oystein</td>\n",
       "      <td>1</td>\n",
       "      <td>f_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>COL f_name VAL ''Fukumi COL l_name VAL O''Boyl...</td>\n",
       "      <td>COL f_name VAL ''Fukumi</td>\n",
       "      <td>1</td>\n",
       "      <td>f_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756</th>\n",
       "      <td>COL f_name VAL Yen''nun COL l_name VAL O''Dono...</td>\n",
       "      <td>COL f_name VAL Yen''nun</td>\n",
       "      <td>1</td>\n",
       "      <td>f_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41194</th>\n",
       "      <td>COL f_name VAL Just''in COL l_name VAL d''Acie...</td>\n",
       "      <td>COL f_name VAL Just''in</td>\n",
       "      <td>1</td>\n",
       "      <td>f_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41204</th>\n",
       "      <td>COL f_name VAL ''Tanka COL l_name VAL Narin''a...</td>\n",
       "      <td>COL f_name VAL ''Tanka</td>\n",
       "      <td>1</td>\n",
       "      <td>f_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41214</th>\n",
       "      <td>COL f_name VAL Ce''zar COL l_name VAL Giese CO...</td>\n",
       "      <td>COL f_name VAL Ce''zar</td>\n",
       "      <td>1</td>\n",
       "      <td>f_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41234</th>\n",
       "      <td>COL f_name VAL Th''om COL l_name VAL Weinert C...</td>\n",
       "      <td>COL f_name VAL Th''om</td>\n",
       "      <td>1</td>\n",
       "      <td>f_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41254</th>\n",
       "      <td>COL f_name VAL Wahee''d COL l_name VAL L''Ecuy...</td>\n",
       "      <td>COL f_name VAL Wahee''d</td>\n",
       "      <td>1</td>\n",
       "      <td>f_name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3350 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0  \\\n",
       "2716   COL f_name VAL Elis''a COL l_name VAL d''Argen...   \n",
       "2726   COL f_name VAL Ja''ehyung COL l_name VAL D''Am...   \n",
       "2736   COL f_name VAL ''Oystein COL l_name VAL Give''...   \n",
       "2746   COL f_name VAL ''Fukumi COL l_name VAL O''Boyl...   \n",
       "2756   COL f_name VAL Yen''nun COL l_name VAL O''Dono...   \n",
       "...                                                  ...   \n",
       "41194  COL f_name VAL Just''in COL l_name VAL d''Acie...   \n",
       "41204  COL f_name VAL ''Tanka COL l_name VAL Narin''a...   \n",
       "41214  COL f_name VAL Ce''zar COL l_name VAL Giese CO...   \n",
       "41234  COL f_name VAL Th''om COL l_name VAL Weinert C...   \n",
       "41254  COL f_name VAL Wahee''d COL l_name VAL L''Ecuy...   \n",
       "\n",
       "                                1  2       3  \n",
       "2716      COL f_name VAL Elis''a   1  f_name  \n",
       "2726   COL f_name VAL Ja''ehyung   1  f_name  \n",
       "2736    COL f_name VAL ''Oystein   1  f_name  \n",
       "2746     COL f_name VAL ''Fukumi   1  f_name  \n",
       "2756     COL f_name VAL Yen''nun   1  f_name  \n",
       "...                           ... ..     ...  \n",
       "41194    COL f_name VAL Just''in   1  f_name  \n",
       "41204     COL f_name VAL ''Tanka   1  f_name  \n",
       "41214     COL f_name VAL Ce''zar   1  f_name  \n",
       "41234      COL f_name VAL Th''om   1  f_name  \n",
       "41254    COL f_name VAL Wahee''d   1  f_name  \n",
       "\n",
       "[3350 rows x 4 columns]"
      ]
     },
     "execution_count": 981,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pd.DataFrame(detector_list_tax)\n",
    "output[(output[1].str.contains('f_name')) & output[2]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 983,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_tax).sample(n=1000).to_csv('datasets/tax/detector/multi-view/train_aug_few.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['marital_status', 'single_exemp', 'married_exemp'], dtype='object')"
      ]
     },
     "execution_count": 915,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_dirty.columns[[9,14]]\n",
    "tax_dirty.columns[[8,12,13]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_tax).to_csv('datasets/tax/detector/multi-view/train_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(detector_list_tax).to_csv('datasets/tax/detector/multi-view/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From Here We Tackle With Flight Dataset Function Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 990,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['f_name', 'l_name', 'gender', 'area_code', 'phone', 'city', 'state',\n",
       "       'zip', 'marital_status', 'has_child', 'salary', 'rate', 'single_exemp',\n",
       "       'married_exemp', 'child_exemp'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 990,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_clean.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zip,city \n",
    "zip,state \n",
    "[city,state],zip \n",
    "[single_exemp,married_exemp],zip\n",
    "child_exemp,has_child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 993,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_matrix_tax = np.array(tax_clean!=tax_dirty).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 997,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  5,  6, 12, 14])"
      ]
     },
     "execution_count": 997,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(input_matrix_tax[tax_label_index[:5]].sum(axis=0)!=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 996,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['zip', 'marital_status', 'has_child'], dtype='object')"
      ]
     },
     "execution_count": 996,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_clean.columns[[7,8,9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 7, 0, 0, 0, 2, 4, 3, 4, 8, 0, 0, 2, 0, 2])"
      ]
     },
     "execution_count": 994,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix_tax[tax_label_index].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1015,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d3f3de452c4fefbc4c03357eb734e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dirty_index_col = np.where(input_matrix_tax[tax_label_index].sum(axis=0)!=0)[0]\n",
    "tax_correct_15 = tax_dirty.copy()\n",
    "correct_index_col = np.where(input_matrix_tax[tax_label_index[:15]].sum(axis=0)!=0)[0]\n",
    "for c in tqdm(correct_index_col):\n",
    "    x = tax_dirty.columns[c]\n",
    "    if x in (['f_name','l_name','city','state','single_exemp','child_exemp']):\n",
    "        for i in range(200000):\n",
    "            value = tax_dirty.iloc[i,c]\n",
    "            tax_correct_15.iloc[i,c] = Tax_Row_Correction(x,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1017,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0, 400, 400, 200, 200,   0,   0,   0,\n",
       "         0,   0])"
      ]
     },
     "execution_count": 1017,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tax_correct_15!=tax_clean).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1002,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_name</th>\n",
       "      <th>l_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>area_code</th>\n",
       "      <th>phone</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>has_child</th>\n",
       "      <th>salary</th>\n",
       "      <th>rate</th>\n",
       "      <th>single_exemp</th>\n",
       "      <th>married_exemp</th>\n",
       "      <th>child_exemp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [f_name, l_name, gender, area_code, phone, city, state, zip, marital_status, has_child, salary, rate, single_exemp, married_exemp, child_exemp]\n",
       "Index: []"
      ]
     },
     "execution_count": 1002,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_correct_5[tax_correct_5['f_name'].str.contains(\"''\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1991,  2029,  2097, ..., 50757, 50758, 50759])"
      ]
     },
     "execution_count": 1011,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.array(tax_correct_10!=tax_clean).sum(axis=1)!=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1531,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_correct_15_3.iloc[:,:-1].to_csv('datasets/tax/FD/correction_20_FD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1530,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2951/2951 [00:02<00:00, 1402.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9538357094365241, 0.947403910991234, 0.9506089309878214)"
      ]
     },
     "execution_count": 1530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_Data_Error = 0\n",
    "All_Fixed_Error = 0\n",
    "Correct_Fixed_Error = 0\n",
    "tax_correct = tax_correct_15_3.iloc[:,:-1]\n",
    "ineq_0 = np.where(np.array(tax_correct!=tax_clean).sum(axis=1)!=0)[0]\n",
    "ineq_1 = np.where(np.array(tax_dirty!=tax_clean).sum(axis=1)!=0)[0]\n",
    "ineq = list(set(np.concatenate([ineq_0,ineq_1])))\n",
    "for i in tqdm(ineq):\n",
    "    for j in range(15):\n",
    "        dirty_cell = tax_dirty.iloc[i,j]\n",
    "        clean_cell = tax_clean.iloc[i,j]\n",
    "        correct_cell = tax_correct.iloc[i,j]\n",
    "        if(correct_cell!=dirty_cell):\n",
    "            All_Fixed_Error += 1\n",
    "        if(clean_cell!=dirty_cell):\n",
    "            All_Data_Error += 1\n",
    "            if(correct_cell==clean_cell):\n",
    "                Correct_Fixed_Error += 1\n",
    "Precision_hospital = Correct_Fixed_Error / All_Fixed_Error\n",
    "Recall_hospital = Correct_Fixed_Error / All_Data_Error\n",
    "F1_hospital = (2 * Precision_hospital * Recall_hospital) / (Precision_hospital + Recall_hospital)\n",
    "Precision_hospital,Recall_hospital,F1_hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_Data_Error = 0\n",
    "All_Fixed_Error = 0\n",
    "Correct_Fixed_Error = 0\n",
    "tax_correct = tax_correct_15_3.iloc[:,:-1]\n",
    "ineq_0 = np.where(np.array(tax_correct!=tax_clean).sum(axis=1)!=0)[0]\n",
    "ineq_1 = np.where(np.array(tax_dirty!=tax_clean).sum(axis=1)!=0)[0]\n",
    "ineq = list(set(np.concatenate([ineq_0,ineq_1])))\n",
    "for i in tqdm(ineq):\n",
    "    for j in range(15):\n",
    "        dirty_cell = tax_dirty.iloc[i,j]\n",
    "        clean_cell = tax_clean.iloc[i,j]\n",
    "        correct_cell = tax_correct.iloc[i,j]\n",
    "        if(correct_cell!=dirty_cell):\n",
    "            All_Fixed_Error += 1\n",
    "        if(clean_cell!=dirty_cell):\n",
    "            All_Data_Error += 1\n",
    "            if(correct_cell==clean_cell):\n",
    "                Correct_Fixed_Error += 1\n",
    "Precision_hospital = Correct_Fixed_Error / All_Fixed_Error\n",
    "Recall_hospital = Correct_Fixed_Error / All_Data_Error\n",
    "F1_hospital = (2 * Precision_hospital * Recall_hospital) / (Precision_hospital + Recall_hospital)\n",
    "Precision_hospital,Recall_hospital,F1_hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1533,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:26<00:00,  1.78s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0009887282973972044, 1.0, 0.001975503358722037)"
      ]
     },
     "execution_count": 1533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_error_detection_metrics(clean_table, dirty_table, predict_table):\n",
    "    # 初始化计数器\n",
    "    TP = FP = FN = 0\n",
    "    \n",
    "    # 遍历表格的每个单元格\n",
    "    for column in tqdm(clean_table.columns):\n",
    "        for row in clean_table.index:\n",
    "            clean_value = clean_table.at[row, column]\n",
    "            dirty_value = dirty_table.at[row, column]\n",
    "            predicted_error = predict_table.at[row, column]\n",
    "            \n",
    "            # 检查真正的错误（dirty和clean不同）\n",
    "            actual_error = clean_value != dirty_value\n",
    "            \n",
    "            # 更新计数器\n",
    "            if predicted_error and actual_error:\n",
    "                TP += 1\n",
    "            elif predicted_error and not actual_error:\n",
    "                FP += 1\n",
    "            elif not predicted_error and actual_error:\n",
    "                FN += 1\n",
    "    \n",
    "    # 计算指标\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1\n",
    "calculate_error_detection_metrics(tax_clean,tax_dirty,tax_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9925322471147319"
      ]
     },
     "execution_count": 1537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = np.array(tax_dirty.iloc[ineq]!=tax_correct.iloc[ineq]).flatten()\n",
    "gt = np.array(tax_dirty.iloc[ineq]!=tax_clean.iloc[ineq]).flatten()\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "precision_score(y_true=gt,y_pred=predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1568,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_correct_20.to_csv('datasets/beers/detector/beer_correction_20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1549,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8411552346570397 0.9831223628691983 0.9066147859922179\n"
     ]
    }
   ],
   "source": [
    "predict = np.array(rayyan_dirty!=rayyan_correction_final).flatten().astype(int)\n",
    "gt = np.array(rayyan_dirty!=rayyan_clean).flatten().astype(int)\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "print(precision_score(y_true=gt,y_pred=predict),recall_score(y_true=gt,y_pred=predict),f1_score(y_true=gt,y_pred=predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1567,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9979191438763377 1.0 0.9989584883201905\n"
     ]
    }
   ],
   "source": [
    "predict = np.array(beer_dirty!=beer_correct_20).flatten().astype(int)\n",
    "gt = np.array(beer_dirty!=beer_clean).flatten().astype(int)\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "print(precision_score(y_true=gt,y_pred=predict),recall_score(y_true=gt,y_pred=predict),f1_score(y_true=gt,y_pred=predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_correction = pd.read_csv('datasets/beers/detector/multi-view/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1550,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'beer_correction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/yanmy/raha/raha-master/detector.ipynb 单元格 448\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1650sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m predict \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(beer_dirty\u001b[39m!=\u001b[39mbeer_correction)\u001b[39m.\u001b[39mflatten()\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1650sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m gt \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(beer_dirty\u001b[39m!=\u001b[39mbeer_clean)\u001b[39m.\u001b[39mflatten()\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1650sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m precision_score,recall_score,f1_score\n",
      "\u001b[0;31mNameError\u001b[0m: name 'beer_correction' is not defined"
     ]
    }
   ],
   "source": [
    "predict = np.array(beer_dirty!=beer_correction).flatten().astype(int)\n",
    "gt = np.array(beer_dirty!=beer_clean).flatten().astype(int)\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "print(precision_score(y_true=gt,y_pred=predict),recall_score(y_true=gt,y_pred=predict),f1_score(y_true=gt,y_pred=predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9925322471147319 0.98583951449764 0.9891745602165087\n"
     ]
    }
   ],
   "source": [
    "print(precision_score(y_true=gt,y_pred=predict),recall_score(y_true=gt,y_pred=predict),f1_score(y_true=gt,y_pred=predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1.0, 0.3614295347269049, 0.5309559187716691) 5 label no FD\n",
    "(0.9396092362344582, 0.7134187457855697, 0.8110387121502491) 5 label with FD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_correct_15.to_csv('datasets/tax/FD/correct_15.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head</th>\n",
       "      <th>relation</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [head, relation, target]\n",
       "Index: []"
      ]
     },
     "execution_count": 1021,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_subgraph_marriaga = pd.DataFrame(columns=['head','relation','target'])\n",
    "tax_subgraph_marriaga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['f_name', 'l_name', 'gender', 'area_code', 'phone', 'city', 'state',\n",
       "       'zip', 'marital_status', 'has_child', 'salary', 'rate', 'single_exemp',\n",
       "       'married_exemp', 'child_exemp'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1022,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1186,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_correct_15['index'] = tax_correct_15.index\n",
    "tax_subgraph_marriaga = pd.DataFrame(columns=['head','relation','target'])\n",
    "for x in ['marital_status','single_exemp','married_exemp','child_exemp','has_child']:\n",
    "    add = pd.DataFrame(columns=['head','relation','target'])\n",
    "    add['head'] = tax_correct_15['index']\n",
    "    add['relation'] = x\n",
    "    add['target'] = tax_correct_15[x]\n",
    "    tax_subgraph_marriaga = pd.concat([tax_subgraph_marriaga,add])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tax_correct_15['index'] = tax_correct_15.index\n",
    "tax_subgraph_marriaga = pd.DataFrame(columns=['head','relation','target'])\n",
    "for x in ['city','state','zip']:\n",
    "    add = pd.DataFrame(columns=['head','relation','target'])\n",
    "    add['head'] = tax_correct_15['index']\n",
    "    add['relation'] = x\n",
    "    add['target'] = tax_correct_15[x]\n",
    "    tax_subgraph_marriaga = pd.concat([tax_subgraph_marriaga,add])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1025,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_subgraph_marriaga.to_csv('datasets/tax/FD/tax_marriage.txt',sep='\\t',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1029,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_subgraph_marriaga.to_csv('datasets/tax/FD/tax_zip.txt',sep='\\t',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1189,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_subgraph_marriaga = tax_subgraph_marriaga.sample(frac=1,random_state=42)\n",
    "tax_subgraph_marriaga.to_csv('/home/yanmy/PyTorch-BigGraph-main/data/tax-marriage/tax-marriage-train.txt',sep='\\t',header=None)\n",
    "tax_subgraph_marriaga.iloc[700000:800000].to_csv('/home/yanmy/PyTorch-BigGraph-main/data/tax-marriage/tax-marriage-valid.txt',sep='\\t',header=None)\n",
    "tax_subgraph_marriaga.iloc[800000:].to_csv('/home/yanmy/PyTorch-BigGraph-main/data/tax-marriage/tax-marriage-test.txt',sep='\\t',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1178,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_subgraph_marriaga['target'] = tax_subgraph_marriaga['target'].str.replace(' ','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1185,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_subgraph_marriaga = tax_subgraph_marriaga.sample(frac=1,random_state=42)\n",
    "tax_subgraph_marriaga.to_csv('/home/yanmy/PyTorch-BigGraph-main/data/tax-zip/tax-zip-train.txt',sep='\\t',header=None,index=False)\n",
    "tax_subgraph_marriaga.iloc[420000:480000].to_csv('/home/yanmy/PyTorch-BigGraph-main/data/tax-zip/tax-zip-valid.txt',sep='\\t',header=None,index=False)\n",
    "tax_subgraph_marriaga.iloc[480000:].to_csv('/home/yanmy/PyTorch-BigGraph-main/data/tax-zip/tax-zip-test.txt',sep='\\t',header=None,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1077,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flight_dirty.columns[-4:]\n",
    "flight_correct = flight_dirty.copy()\n",
    "for i in range(len(flight_correct)):\n",
    "    for j in [-4,-3,-2,-1]:\n",
    "        value = flight_dirty.iloc[i,j]\n",
    "        if not is_clean_time_format(value):\n",
    "            flight_correct.iloc[i,j] = 'empty'\n",
    "        else:\n",
    "            flight_correct.iloc[i,j] = value.replace(' ','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2, ..., 2373, 2374, 2375])"
      ]
     },
     "execution_count": 1446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1447,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tax_correct_15['index'] = tax_correct_15.index\n",
    "flight_correct_sub = flight_correct.iloc[clean_index]\n",
    "flight_subgraph = pd.DataFrame(columns=['head','relation','target'])\n",
    "for c in range(1,7,1):\n",
    "    x = flight_dirty.columns[c]\n",
    "    add = pd.DataFrame(columns=['head','relation','target'])\n",
    "    add['head'] = flight_correct_sub['tuple_id']\n",
    "    add['relation'] = x\n",
    "    add['target'] = flight_correct_sub[x]\n",
    "    flight_subgraph = pd.concat([flight_subgraph,add])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1448,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in range(3,7,1):\n",
    "    x = flight_dirty.columns[c]\n",
    "    add = pd.DataFrame(columns=['head','relation','target'])\n",
    "    add['head'] = flight_correct_sub['flight']\n",
    "    add['relation'] = x\n",
    "    add['target'] = flight_correct_sub[x]\n",
    "    flight_subgraph = pd.concat([flight_subgraph,add])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head</th>\n",
       "      <th>relation</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>src</td>\n",
       "      <td>aa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>src</td>\n",
       "      <td>aa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>src</td>\n",
       "      <td>aa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>src</td>\n",
       "      <td>aa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>src</td>\n",
       "      <td>aa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2371</th>\n",
       "      <td>UA-3099-PHX-PHL</td>\n",
       "      <td>act_arr_time</td>\n",
       "      <td>5:38_p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2372</th>\n",
       "      <td>AA-4198-ORD-CLE</td>\n",
       "      <td>act_arr_time</td>\n",
       "      <td>12:50_p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2373</th>\n",
       "      <td>CO-45-EWR-MIA</td>\n",
       "      <td>act_arr_time</td>\n",
       "      <td>6:36_p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2374</th>\n",
       "      <td>AA-3809-PHX-LAX</td>\n",
       "      <td>act_arr_time</td>\n",
       "      <td>6:19_a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2375</th>\n",
       "      <td>AA-59-JFK-SFO</td>\n",
       "      <td>act_arr_time</td>\n",
       "      <td>11:12_a.m.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10690 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 head      relation      target\n",
       "0                   1           src          aa\n",
       "1                   2           src          aa\n",
       "2                   3           src          aa\n",
       "3                   4           src          aa\n",
       "4                   5           src          aa\n",
       "...               ...           ...         ...\n",
       "2371  UA-3099-PHX-PHL  act_arr_time   5:38_p.m.\n",
       "2372  AA-4198-ORD-CLE  act_arr_time  12:50_p.m.\n",
       "2373    CO-45-EWR-MIA  act_arr_time   6:36_p.m.\n",
       "2374  AA-3809-PHX-LAX  act_arr_time   6:19_a.m.\n",
       "2375    AA-59-JFK-SFO  act_arr_time  11:12_a.m.\n",
       "\n",
       "[10690 rows x 3 columns]"
      ]
     },
     "execution_count": 1449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1450,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flight_subgraph\n",
    "flight_subgraph = flight_subgraph.sample(frac=1,random_state=42)\n",
    "flight_subgraph.iloc[:10000].to_csv('/home/yanmy/PyTorch-BigGraph-main/data/flight/flight-train.txt',sep='\\t',header=None,index=False)\n",
    "flight_subgraph.iloc[10000:10300].to_csv('/home/yanmy/PyTorch-BigGraph-main/data/flight/flight-valid.txt',sep='\\t',header=None,index=False)\n",
    "flight_subgraph.iloc[10300:].to_csv('/home/yanmy/PyTorch-BigGraph-main/data/flight/flight-test.txt',sep='\\t',header=None,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "act_arr_time      2376\n",
       "act_dep_time      2376\n",
       "src               2376\n",
       "sched_dep_time    2376\n",
       "sched_arr_time    2376\n",
       "flight            2376\n",
       "Name: relation, dtype: int64"
      ]
     },
     "execution_count": 1052,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_subgraph['relation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "torchbiggraph_import_from_tsv \\\n",
    "  --lhs-col=0 --rel-col=1 --rhs-col=2 \\\n",
    "  torchbiggraph/examples/configs/tax_marriage_config_cpu.py \\\n",
    "  data/tax-marriage/tax-marriage-train.txt \\\n",
    "  data/tax-marriage/tax-marriage-valid.txt \\\n",
    "  data/tax-marriage/tax-marriage-test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "torchbiggraph_train \\\n",
    "  torchbiggraph/examples/configs/tax_marriage_config_gpu.py \\\n",
    "  -p edge_paths=data/tax-marriage/tax-marriage-train_partitioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "torchbiggraph_export_to_tsv \\\n",
    "  torchbiggraph/examples/configs/flight_config_cpu.py \\\n",
    "  --entities-output entity_embeddings.tsv \\\n",
    "  --relation-types-output relation_types_parameters.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1082,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "def load_embeddings(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as tsvfile:\n",
    "        reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "        \n",
    "        # Initialize an empty list to store embeddings and a dict for entity to index mapping\n",
    "        embeddings = []\n",
    "        entity_to_index = {}\n",
    "        \n",
    "        for i, line in enumerate(reader):\n",
    "            entity = line[0]\n",
    "            embedding = np.array(line[1:], dtype=np.float32)\n",
    "            \n",
    "            # Add the entity and its embedding to the dict and list\n",
    "            entity_to_index[entity] = i\n",
    "            embeddings.append(embedding)\n",
    "            \n",
    "        # Convert the list of embeddings to a NumPy array\n",
    "        embeddings_array = np.vstack(embeddings)\n",
    "        \n",
    "    return embeddings_array, entity_to_index\n",
    "\n",
    "# Usage\n",
    "file_path = '/home/yanmy/PyTorch-BigGraph-main/data/flight/entity_embeddings.tsv'  # Replace with your actual file path\n",
    "embeddings, entity_to_index = load_embeddings(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1086,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3063, 200)"
      ]
     },
     "execution_count": 1086,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1083,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1505"
      ]
     },
     "execution_count": 1083,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embeddings.shape\n",
    "# entity_to_index\n",
    "entity_to_index['5:49_a.m.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1191,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tuple_id</th>\n",
       "      <th>src</th>\n",
       "      <th>flight</th>\n",
       "      <th>sched_dep_time</th>\n",
       "      <th>act_dep_time</th>\n",
       "      <th>sched_arr_time</th>\n",
       "      <th>act_arr_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-3859-IAH-ORD</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:16 a.m.</td>\n",
       "      <td>9:40 a.m.</td>\n",
       "      <td>9:32 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-1733-ORD-PHX</td>\n",
       "      <td>7:45 p.m.</td>\n",
       "      <td>7:58 p.m.</td>\n",
       "      <td>10:30 p.m.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-1640-MIA-MCO</td>\n",
       "      <td>6:30 p.m.</td>\n",
       "      <td></td>\n",
       "      <td>7:25 p.m.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-518-MIA-JFK</td>\n",
       "      <td>6:40 a.m.</td>\n",
       "      <td>6:54 a.m.</td>\n",
       "      <td>9:25 a.m.</td>\n",
       "      <td>9:28 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-3756-ORD-SLC</td>\n",
       "      <td>12:15 p.m.</td>\n",
       "      <td>12:41 p.m.</td>\n",
       "      <td>2:45 p.m.</td>\n",
       "      <td>2:50 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2371</th>\n",
       "      <td>2372</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>UA-3099-PHX-PHL</td>\n",
       "      <td>11:55 a.m.</td>\n",
       "      <td>11:43 a.m.</td>\n",
       "      <td>6:17 p.m.</td>\n",
       "      <td>5:38 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2372</th>\n",
       "      <td>2373</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-4198-ORD-CLE</td>\n",
       "      <td>10:40 a.m.</td>\n",
       "      <td>10:54 a.m.</td>\n",
       "      <td>12:55 p.m.</td>\n",
       "      <td>12:50 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2373</th>\n",
       "      <td>2374</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>CO-45-EWR-MIA</td>\n",
       "      <td>4:00 p.m.</td>\n",
       "      <td>3:58 p.m.</td>\n",
       "      <td>7:05 p.m.</td>\n",
       "      <td>6:36 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2374</th>\n",
       "      <td>2375</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-3809-PHX-LAX</td>\n",
       "      <td>6:00 a.m.</td>\n",
       "      <td>6:10 a.m.</td>\n",
       "      <td>6:40 a.m.</td>\n",
       "      <td>6:19 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2375</th>\n",
       "      <td>2376</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-59-JFK-SFO</td>\n",
       "      <td>7:10 a.m.</td>\n",
       "      <td>7:39 a.m.</td>\n",
       "      <td>10:45 a.m.</td>\n",
       "      <td>11:12 a.m.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2376 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     tuple_id                   src           flight sched_dep_time  \\\n",
       "0           1                    aa  AA-3859-IAH-ORD      7:10 a.m.   \n",
       "1           2                    aa  AA-1733-ORD-PHX      7:45 p.m.   \n",
       "2           3                    aa  AA-1640-MIA-MCO      6:30 p.m.   \n",
       "3           4                    aa   AA-518-MIA-JFK      6:40 a.m.   \n",
       "4           5                    aa  AA-3756-ORD-SLC     12:15 p.m.   \n",
       "...       ...                   ...              ...            ...   \n",
       "2371     2372  world-flight-tracker  UA-3099-PHX-PHL     11:55 a.m.   \n",
       "2372     2373  world-flight-tracker  AA-4198-ORD-CLE     10:40 a.m.   \n",
       "2373     2374  world-flight-tracker    CO-45-EWR-MIA      4:00 p.m.   \n",
       "2374     2375  world-flight-tracker  AA-3809-PHX-LAX      6:00 a.m.   \n",
       "2375     2376  world-flight-tracker    AA-59-JFK-SFO      7:10 a.m.   \n",
       "\n",
       "     act_dep_time sched_arr_time act_arr_time  \n",
       "0       7:16 a.m.      9:40 a.m.    9:32 a.m.  \n",
       "1       7:58 p.m.     10:30 p.m.               \n",
       "2                      7:25 p.m.               \n",
       "3       6:54 a.m.      9:25 a.m.    9:28 a.m.  \n",
       "4      12:41 p.m.      2:45 p.m.    2:50 p.m.  \n",
       "...           ...            ...          ...  \n",
       "2371   11:43 a.m.      6:17 p.m.    5:38 p.m.  \n",
       "2372   10:54 a.m.     12:55 p.m.   12:50 p.m.  \n",
       "2373    3:58 p.m.      7:05 p.m.    6:36 p.m.  \n",
       "2374    6:10 a.m.      6:40 a.m.    6:19 a.m.  \n",
       "2375    7:39 a.m.     10:45 a.m.   11:12 a.m.  \n",
       "\n",
       "[2376 rows x 7 columns]"
      ]
     },
     "execution_count": 1191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_dirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1141,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_set = [c.replace(' ','_') for c in  flight_correct['sched_dep_time'].unique() if c!='empty']\n",
    "candidate_set.extend([str(c) for c in range(1,2377,1) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import h5py\n",
    "import torch\n",
    "from torchbiggraph.operators import ComplexDiagonalDynamicOperator,TranslationDynamicOperator\n",
    "from torchbiggraph.model import DotComparator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1453,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'10:31_p.m.' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/yanmy/raha/raha-master/detector.ipynb 单元格 468\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m relation \u001b[39m=\u001b[39m flight_correct\u001b[39m.\u001b[39mcolumns[i]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m all_possible_value \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(flight_correct\u001b[39m.\u001b[39miloc[:,i]\u001b[39m.\u001b[39munique())\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m output \u001b[39m=\u001b[39m Correction_FD_Graph_Trans(src_set\u001b[39m=\u001b[39;49mcluster_indicator_all,dest_set\u001b[39m=\u001b[39;49mall_possible_value,relation\u001b[39m=\u001b[39;49mrelation,model_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/home/yanmy/PyTorch-BigGraph-main/model/flight\u001b[39;49m\u001b[39m'\u001b[39;49m,data_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/home/yanmy/PyTorch-BigGraph-main/data/flight\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m flight_correct_20 \u001b[39m=\u001b[39m flight_correct\u001b[39m.\u001b[39mcopy()\n",
      "\u001b[1;32m/home/yanmy/raha/raha-master/detector.ipynb 单元格 468\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m comparator \u001b[39m=\u001b[39m DotComparator()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m src_entity_offset \u001b[39m=\u001b[39m [entity_names\u001b[39m.\u001b[39mindex(\u001b[39mstr\u001b[39m(c)) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m src_set]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m dest_entity_offset \u001b[39m=\u001b[39m [entity_names\u001b[39m.\u001b[39mindex(d) \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m dest_set]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m         \u001b[39m# with h5py.File(\"%s/embeddings_all_0.v50.h5\" % model_path, \"r\") as hf:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m         \u001b[39m# src_embedding = torch.from_numpy(hf[\"embeddings\"][src_entity_offset, :])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m         \u001b[39m# dest_embedding = torch.from_numpy(hf[\"embeddings\"][dest_entity_offset, :])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m src_embedding \u001b[39m=\u001b[39m src_embedding_all[src_entity_offset]\n",
      "\u001b[1;32m/home/yanmy/raha/raha-master/detector.ipynb 单元格 468\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m comparator \u001b[39m=\u001b[39m DotComparator()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m src_entity_offset \u001b[39m=\u001b[39m [entity_names\u001b[39m.\u001b[39mindex(\u001b[39mstr\u001b[39m(c)) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m src_set]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m dest_entity_offset \u001b[39m=\u001b[39m [entity_names\u001b[39m.\u001b[39;49mindex(d) \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m dest_set]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m         \u001b[39m# with h5py.File(\"%s/embeddings_all_0.v50.h5\" % model_path, \"r\") as hf:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m         \u001b[39m# src_embedding = torch.from_numpy(hf[\"embeddings\"][src_entity_offset, :])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m         \u001b[39m# dest_embedding = torch.from_numpy(hf[\"embeddings\"][dest_entity_offset, :])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.16.229/home/yanmy/raha/raha-master/detector.ipynb#Z1462sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m src_embedding \u001b[39m=\u001b[39m src_embedding_all[src_entity_offset]\n",
      "\u001b[0;31mValueError\u001b[0m: '10:31_p.m.' is not in list"
     ]
    }
   ],
   "source": [
    "def get_max_score_char(data):\n",
    "    # 假设第一列是可转换为浮点数的分数，第二列是字符\n",
    "    data = np.array(data)\n",
    "    scores = data[:, 0].astype(float)  # 转换分数列为浮点数\n",
    "    max_score_index = np.argmax(scores)  # 找到最大分数的索引\n",
    "    return data[max_score_index]  # 返回对应的字符\n",
    "def Correction_FD_Graph_Trans(src_set,dest_set,relation,model_path,data_path,vector_dimension=200):\n",
    "    final_output = []\n",
    "    with open(\"%s/entity_count_all_0.txt\" % data_path, \"rt\") as tf:\n",
    "        entity_count = int(tf.read().strip())\n",
    "    with open(\"%s/entity_names_all_0.json\" % data_path, \"rt\") as tf:\n",
    "        entity_names = json.load(tf)\n",
    "    with open(\"%s/dynamic_rel_names.json\" % data_path, \"rt\") as tf:\n",
    "        rel_type_names = json.load(tf)\n",
    "    # Load count of dynamic relations\n",
    "    with open(\"%s/dynamic_rel_count.txt\" % data_path, \"rt\") as tf:\n",
    "        dynamic_rel_count = int(tf.read().strip())\n",
    "    with h5py.File(\"%s/embeddings_all_0.v200.h5\" % model_path, \"r\") as hf:\n",
    "        src_embedding_all = torch.from_numpy(hf[\"embeddings\"][...])\n",
    "        dest_embedding_all = torch.from_numpy(hf[\"embeddings\"][...])\n",
    "    rel_type_index = rel_type_names.index(relation)\n",
    "    # Load the operator's state dict\n",
    "    with h5py.File(\"%s/model.v200.h5\" % model_path, \"r\") as hf:\n",
    "        operator_state_dict = {\n",
    "            # \"real\": torch.from_numpy(hf[\"model/relations/0/operator/rhs/translations\"][...]),\n",
    "            # \"imag\": torch.from_numpy(hf[\"model/relations/0/operator/rhs/translations\"][...]),\n",
    "            \"translations\": torch.from_numpy(hf[\"model/relations/0/operator/rhs/translations\"][...]),\n",
    "        }\n",
    "    operator = TranslationDynamicOperator(vector_dimension, dynamic_rel_count)\n",
    "    operator.load_state_dict(operator_state_dict)\n",
    "    comparator = DotComparator()\n",
    "    src_entity_offset = [entity_names.index(str(c)) for c in src_set]\n",
    "    dest_entity_offset = [entity_names.index(d) for d in dest_set]\n",
    "            # with h5py.File(\"%s/embeddings_all_0.v50.h5\" % model_path, \"r\") as hf:\n",
    "            # src_embedding = torch.from_numpy(hf[\"embeddings\"][src_entity_offset, :])\n",
    "            # dest_embedding = torch.from_numpy(hf[\"embeddings\"][dest_entity_offset, :])\n",
    "    src_embedding = src_embedding_all[src_entity_offset]\n",
    "    dest_embedding = dest_embedding_all[dest_entity_offset]\n",
    "    m = src_embedding.size(0)  # src_embedding 的实体数量\n",
    "    n = dest_embedding.size(0)  # dest_embedding 的实体数量\n",
    "    src_prepared = comparator.prepare(src_embedding.view(m, 1, vector_dimension))\n",
    "    dest_prepared = comparator.prepare(\n",
    "        operator(\n",
    "            dest_embedding.repeat(m, 1),  # 重复 dest_embedding m 次\n",
    "            torch.tensor([rel_type_index] * m).repeat_interleave(n),\n",
    "        ).view(m, n, vector_dimension)\n",
    "    )\n",
    "\n",
    "    # 执行比较\n",
    "    scores, _, _ = comparator(\n",
    "        src_prepared.expand(m, n, vector_dimension),  # 扩展 src_prepared 以匹配 dest_prepared 的形状\n",
    "        dest_prepared,\n",
    "        torch.empty(m, 0, vector_dimension),  # Left-hand side negatives, not needed\n",
    "        torch.empty(m, 0, vector_dimension),  # Right-hand side negatives, not needed\n",
    "    )\n",
    "    return scores.detach().numpy()\n",
    "i = 6\n",
    "relation = flight_correct.columns[i]\n",
    "all_possible_value = list(flight_correct.iloc[:,i].unique())\n",
    "output = Correction_FD_Graph_Trans(src_set=cluster_indicator_all,dest_set=all_possible_value,relation=relation,model_path='/home/yanmy/PyTorch-BigGraph-main/model/flight',data_path='/home/yanmy/PyTorch-BigGraph-main/data/flight')\n",
    "flight_correct_20 = flight_correct.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1431,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_importance_list = []\n",
    "flight_correct_20 = flight_correct.copy()\n",
    "flight_label_index_select = flight_label_index\n",
    "theta = 0.1\n",
    "for index,row in flight_correct_20.iterrows():\n",
    "    cluster_indicator = row['flight']\n",
    "    cluster_num = len(flight_correct_20[flight_correct_20['flight']==cluster_indicator])\n",
    "    count = 0\n",
    "    for x,y in row.items():\n",
    "        if(y=='empty'): ## Dirty Value\n",
    "            count += 1\n",
    "    if(index in flight_label_index):\n",
    "        flight_importance_list.append(1) ## Most Important\n",
    "    elif(count == 0): ## Secondary Important\n",
    "        flight_importance_list.append(1/cluster_num)\n",
    "    else:\n",
    "        # flight_importance_list.append(1/(theta * cluster_num))\n",
    "        flight_importance_list.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1445,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_index = np.where(np.array(flight_importance_list)!=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1434,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A Voting Mechanism, Process By Cluster\n",
    "# cluster_indicator_all = flight_correct['flight'].unique()\n",
    "# for cluster_indicator in cluster_indicator_all:\n",
    "## Labelling Mechanism\n",
    "def mark_presence(long_list, short_list):\n",
    "    short_set = set(short_list)  # 转换为集合以提高查找效率\n",
    "    return np.array([1 if (item in short_set) and (item!='empty') else 0 for item in long_list])\n",
    "\n",
    "# for f in flight_label_index_select:\n",
    "#     try:\n",
    "#         ground_truth = flight_clean.iloc[f,i].replace(' ','_')\n",
    "#         ground_truth_index = all_possible_value.index(ground_truth)\n",
    "#         update_score = np.zeros((1,len(all_possible_value)))\n",
    "#         update_score[0][ground_truth_index] = 16 ## Ground Truth Value\n",
    "#         output[f] = update_score\n",
    "#     except:\n",
    "#         continue\n",
    "# A Voting Mechanism, Process By Cluster\n",
    "cluster_indicator_all = flight_correct['flight'].unique()\n",
    "for cluster_indicator in cluster_indicator_all:\n",
    "    cluster_index = flight_correct[flight_correct['flight']==cluster_indicator].index\n",
    "    label_overlap = set(list(cluster_index)).intersection(set(flight_label_index_select))\n",
    "    if(len(label_overlap)==0): ## No Ground Truth\n",
    "        apperance_list = flight_correct.iloc[cluster_index,i].unique()\n",
    "        mask = mark_presence(all_possible_value, apperance_list)\n",
    "        # value_list = output[cluster_index] * np.array(flight_importance_list)[cluster_index].reshape((-1,1)) * mask ## message passing mechanism\n",
    "        value_list = output[cluster_index] *  mask ## message passing mechanism\n",
    "        value_list_output_index = np.argmax(value_list.sum(axis=0))\n",
    "        value_list_output = all_possible_value[value_list_output_index]\n",
    "        flight_correct_20.iloc[cluster_index,i] = value_list_output.replace('_',' ')\n",
    "    else:\n",
    "        ground_truth = flight_clean.iloc[list(label_overlap)[0],i]\n",
    "        flight_correct_20.iloc[cluster_index,i] = ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13.308859, 110)"
      ]
     },
     "execution_count": 1405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predict By Indicator\n",
    "def mark_presence(long_list, short_list):\n",
    "    short_set = set(short_list)  # 转换为集合以提高查找效率\n",
    "    return np.array([1 if (item in short_set) and (item!='empty') else 0 for item in long_list])\n",
    "\n",
    "# for f in flight_label_index_select:\n",
    "#     try:\n",
    "#         ground_truth = flight_clean.iloc[f,i].replace(' ','_')\n",
    "#         ground_truth_index = all_possible_value.index(ground_truth)\n",
    "#         update_score = np.zeros((1,len(all_possible_value)))\n",
    "#         update_score[0][ground_truth_index] = 16 ## Ground Truth Value\n",
    "#         output[f] = update_score\n",
    "#     except:\n",
    "#         continue\n",
    "# A Voting Mechanism, Process By Cluster\n",
    "cluster_indicator_all = flight_correct['flight'].unique()\n",
    "for cluster_indicator in cluster_indicator_all:\n",
    "    cluster_index = flight_correct[flight_correct['flight']==cluster_indicator].index\n",
    "    label_overlap = set(list(cluster_index)).intersection(set(flight_label_index_select))\n",
    "    if(len(label_overlap)==0): ## No Ground Truth\n",
    "        apperance_list = flight_correct.iloc[cluster_index,i].unique()\n",
    "        mask = mark_presence(all_possible_value, apperance_list)\n",
    "        # value_list = output[cluster_index] * np.array(flight_importance_list)[cluster_index].reshape((-1,1)) * mask ## message passing mechanism\n",
    "        value_list = output[cluster_index] *  mask ## message passing mechanism\n",
    "        value_list_output_index = np.argmax(value_list.sum(axis=0))\n",
    "        value_list_output = all_possible_value[value_list_output_index]\n",
    "        flight_correct_20.iloc[cluster_index,i] = value_list_output.replace('_',' ')\n",
    "    else:\n",
    "        ground_truth = flight_clean.iloc[list(label_overlap)[0],i]\n",
    "        flight_correct_20.iloc[cluster_index,i] = ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12:15_p.m.    21\n",
       "2:45_p.m.     19\n",
       "2:50_p.m.     15\n",
       "12:42_p.m.    14\n",
       "empty         14\n",
       "12:41_p.m.     8\n",
       "2:52_p.m.      7\n",
       "12:10_p.m.     2\n",
       "2:40_p.m.      2\n",
       "2:35_p.m.      2\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 1438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_subgraph[flight_subgraph['head']=='AA-3756-ORD-SLC']['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tuple_id</th>\n",
       "      <th>src</th>\n",
       "      <th>flight</th>\n",
       "      <th>sched_dep_time</th>\n",
       "      <th>act_dep_time</th>\n",
       "      <th>sched_arr_time</th>\n",
       "      <th>act_arr_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-3756-ORD-SLC</td>\n",
       "      <td>12:15_p.m.</td>\n",
       "      <td>12:41_p.m.</td>\n",
       "      <td>2:45_p.m.</td>\n",
       "      <td>2:40 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-3468-CVG-MIA</td>\n",
       "      <td>7:00_a.m.</td>\n",
       "      <td>7:25_a.m.</td>\n",
       "      <td>9:55_a.m.</td>\n",
       "      <td>9:42 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-466-IAH-MIA</td>\n",
       "      <td>6:00_a.m.</td>\n",
       "      <td>6:08_a.m.</td>\n",
       "      <td>9:20_a.m.</td>\n",
       "      <td>8:50 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-1886-BOS-MIA</td>\n",
       "      <td>10:45_a.m.</td>\n",
       "      <td>10:55_a.m.</td>\n",
       "      <td>2:20_p.m.</td>\n",
       "      <td>1:34 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>aa</td>\n",
       "      <td>AA-2957-DFW-CVG</td>\n",
       "      <td>7:55_a.m.</td>\n",
       "      <td>8:04_a.m.</td>\n",
       "      <td>11:05_a.m.</td>\n",
       "      <td>10:58 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2367</th>\n",
       "      <td>2368</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-789-ORD-DEN</td>\n",
       "      <td>1:05_p.m.</td>\n",
       "      <td>1:19_p.m.</td>\n",
       "      <td>2:35_p.m.</td>\n",
       "      <td>2:49 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2369</th>\n",
       "      <td>2370</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-3786-IAH-ORD</td>\n",
       "      <td>4:00_p.m.</td>\n",
       "      <td>4:12_p.m.</td>\n",
       "      <td>6:40_p.m.</td>\n",
       "      <td>6:11 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2373</th>\n",
       "      <td>2374</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>CO-45-EWR-MIA</td>\n",
       "      <td>4:00_p.m.</td>\n",
       "      <td>3:58_p.m.</td>\n",
       "      <td>7:05_p.m.</td>\n",
       "      <td>6:33 p.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2374</th>\n",
       "      <td>2375</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-3809-PHX-LAX</td>\n",
       "      <td>6:00_a.m.</td>\n",
       "      <td>6:10_a.m.</td>\n",
       "      <td>6:40_a.m.</td>\n",
       "      <td>6:07 a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2375</th>\n",
       "      <td>2376</td>\n",
       "      <td>world-flight-tracker</td>\n",
       "      <td>AA-59-JFK-SFO</td>\n",
       "      <td>7:10_a.m.</td>\n",
       "      <td>7:39_a.m.</td>\n",
       "      <td>10:45_a.m.</td>\n",
       "      <td>10:49 a.m.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1102 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     tuple_id                   src           flight sched_dep_time  \\\n",
       "4           5                    aa  AA-3756-ORD-SLC     12:15_p.m.   \n",
       "6           7                    aa  AA-3468-CVG-MIA      7:00_a.m.   \n",
       "9          10                    aa   AA-466-IAH-MIA      6:00_a.m.   \n",
       "10         11                    aa  AA-1886-BOS-MIA     10:45_a.m.   \n",
       "11         12                    aa  AA-2957-DFW-CVG      7:55_a.m.   \n",
       "...       ...                   ...              ...            ...   \n",
       "2367     2368  world-flight-tracker   AA-789-ORD-DEN      1:05_p.m.   \n",
       "2369     2370  world-flight-tracker  AA-3786-IAH-ORD      4:00_p.m.   \n",
       "2373     2374  world-flight-tracker    CO-45-EWR-MIA      4:00_p.m.   \n",
       "2374     2375  world-flight-tracker  AA-3809-PHX-LAX      6:00_a.m.   \n",
       "2375     2376  world-flight-tracker    AA-59-JFK-SFO      7:10_a.m.   \n",
       "\n",
       "     act_dep_time sched_arr_time act_arr_time  \n",
       "4      12:41_p.m.      2:45_p.m.    2:40 p.m.  \n",
       "6       7:25_a.m.      9:55_a.m.    9:42 a.m.  \n",
       "9       6:08_a.m.      9:20_a.m.    8:50 a.m.  \n",
       "10     10:55_a.m.      2:20_p.m.    1:34 p.m.  \n",
       "11      8:04_a.m.     11:05_a.m.   10:58 a.m.  \n",
       "...           ...            ...          ...  \n",
       "2367    1:19_p.m.      2:35_p.m.    2:49 p.m.  \n",
       "2369    4:12_p.m.      6:40_p.m.    6:11 p.m.  \n",
       "2373    3:58_p.m.      7:05_p.m.    6:33 p.m.  \n",
       "2374    6:10_a.m.      6:40_a.m.    6:07 a.m.  \n",
       "2375    7:39_a.m.     10:45_a.m.   10:49 a.m.  \n",
       "\n",
       "[1102 rows x 7 columns]"
      ]
     },
     "execution_count": 1452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "flight_correct_20[flight_correct_20[relation]!=flight_clean[relation]]\n",
    "# flight_correct_20[flight_correct_20['flight']=='AA-2050-ORD-MIA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_clean[flight_dirty['flight']=='AA-466-IAH-MIA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_dirty[flight_correct_20['flight']=='AA-466-IAH-MIA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_clean[flight_dirty['flight']=='AA-518-MIA-JFK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_clean[flight_correct_20['sched_dep_time']!=flight_clean['sched_dep_time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0, 400, 400, 200, 200,   0,   0,   0,\n",
       "         0,   0])"
      ]
     },
     "execution_count": 1464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tax_correct_15.iloc[:,:-1]!=tax_clean).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ad2af692584064a6b308f2780dbf32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# def State_Correct(df):\n",
    "df = tax_correct_5.iloc[:20000].copy()\n",
    "col_ind = list(df.columns).index('state')\n",
    "correction = df[df['zip']!='1907'].copy()\n",
    "## attribute Pairs exclude anomaly\n",
    "zip_unique = correction['zip'].unique()\n",
    "for z in tqdm(zip_unique):\n",
    "    most_frequent_all = correction[correction['zip']==z]['state'].value_counts()\n",
    "    if(len(most_frequent_all)>1):\n",
    "        most_frequent = most_frequent_all.idxmax()\n",
    "        indexer = correction[correction['zip']==z].index\n",
    "        correction.loc[indexer,col_ind] = most_frequent\n",
    "# tax_correct_5_0 = State_Correct(tax_correct_5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1523,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Most_Frequent_Zip_State(row):\n",
    "    zip = row['zip']\n",
    "    state = row['state']\n",
    "    if(zip!='1907'):\n",
    "        most_frequent = tax_correct_15[tax_correct_15['zip']==zip]['state'].value_counts().idxmax()\n",
    "        return most_frequent\n",
    "    else:\n",
    "        return state\n",
    "tax_correct_15_0 = tax_correct_15.copy()\n",
    "tax_correct_15_0['state'] = tax_correct_15_0.parallel_apply(Most_Frequent_Zip_State,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1524,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Most_Frequent_City_State_Zip(row):\n",
    "    city = row['city']\n",
    "    state = row['state']\n",
    "    zip = row['zip']\n",
    "    if(zip=='1907'):\n",
    "        select = tax_dirty[(tax_dirty['city']==city) & (tax_dirty['state']==state)]['zip'].value_counts().idxmax()\n",
    "        return select\n",
    "    else:\n",
    "        return zip\n",
    "tax_correct_15_1 = tax_correct_15_0.copy()\n",
    "tax_correct_15_1['zip'] = tax_correct_15_1.parallel_apply(Most_Frequent_City_State_Zip,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['f_name', 'l_name', 'gender', 'area_code', 'phone', 'city', 'state',\n",
       "       'zip', 'marital_status', 'has_child', 'salary', 'rate', 'single_exemp',\n",
       "       'married_exemp', 'child_exemp'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.array(tax_correct_5_1!=tax_clean).sum(axis=0)\n",
    "tax_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1525,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Most_Frequent_marriage(row):\n",
    "    marital_status = row['marital_status']\n",
    "    single_exemp = row['single_exemp']\n",
    "    married_exemp = row['married_exemp']\n",
    "    if not (single_exemp=='0' and married_exemp=='0'):\n",
    "        select = tax_dirty[(tax_dirty['married_exemp']==married_exemp) & (tax_dirty['single_exemp']==single_exemp)]['marital_status'].value_counts().idxmax()\n",
    "        return select\n",
    "    else:\n",
    "        return marital_status\n",
    "tax_correct_15_2 = tax_correct_15_1.copy()\n",
    "tax_correct_15_2['marital_status'] = tax_correct_15_2.parallel_apply(Most_Frequent_marriage,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1526,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Most_Frequent_child(row):\n",
    "    child_exemp = row['child_exemp']\n",
    "    has_child = row['has_child']\n",
    "    if not (child_exemp=='0'):\n",
    "        select = tax_dirty[(tax_dirty['child_exemp']==child_exemp)]['has_child'].value_counts().idxmax()\n",
    "        return select\n",
    "    else:\n",
    "        return has_child\n",
    "tax_correct_15_3 = tax_correct_15_2.copy()\n",
    "tax_correct_15_3['has_child'] = tax_correct_15_3.parallel_apply(Most_Frequent_child,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1529,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,  50, 128,   0,   0,   0,   0,   0,\n",
       "         0,   0])"
      ]
     },
     "execution_count": 1529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tax_correct_15_3.iloc[:,:-1]!=tax_clean).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1520,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_correct_5_3.to_csv('datasets/tax/FD/correct_5_FD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_name</th>\n",
       "      <th>l_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>area_code</th>\n",
       "      <th>phone</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>has_child</th>\n",
       "      <th>salary</th>\n",
       "      <th>rate</th>\n",
       "      <th>single_exemp</th>\n",
       "      <th>married_exemp</th>\n",
       "      <th>child_exemp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nik</td>\n",
       "      <td>Tacic</td>\n",
       "      <td>M</td>\n",
       "      <td>702</td>\n",
       "      <td>517-7658</td>\n",
       "      <td>LAS VEGAS</td>\n",
       "      <td>NV</td>\n",
       "      <td>89140</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>90000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Sanpei</td>\n",
       "      <td>Sobieski</td>\n",
       "      <td>F</td>\n",
       "      <td>360</td>\n",
       "      <td>921-9097</td>\n",
       "      <td>MILL CREEK</td>\n",
       "      <td>WA</td>\n",
       "      <td>98082</td>\n",
       "      <td>S</td>\n",
       "      <td>Y</td>\n",
       "      <td>20000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Ilagit</td>\n",
       "      <td>Auinger</td>\n",
       "      <td>F</td>\n",
       "      <td>907</td>\n",
       "      <td>185-6056</td>\n",
       "      <td>ANCHORAGE</td>\n",
       "      <td>AK</td>\n",
       "      <td>99530</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>70000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Avelino</td>\n",
       "      <td>English</td>\n",
       "      <td>M</td>\n",
       "      <td>401</td>\n",
       "      <td>795-3153</td>\n",
       "      <td>WESTERLY</td>\n",
       "      <td>RI</td>\n",
       "      <td>2891</td>\n",
       "      <td>S</td>\n",
       "      <td>Y</td>\n",
       "      <td>70000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Yuyan</td>\n",
       "      <td>Simao</td>\n",
       "      <td>M</td>\n",
       "      <td>603</td>\n",
       "      <td>350-5756</td>\n",
       "      <td>EAST DERRY</td>\n",
       "      <td>NH</td>\n",
       "      <td>3041</td>\n",
       "      <td>S</td>\n",
       "      <td>Y</td>\n",
       "      <td>50000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199967</th>\n",
       "      <td>Tariq</td>\n",
       "      <td>Mercankosk</td>\n",
       "      <td>F</td>\n",
       "      <td>401</td>\n",
       "      <td>735-5238</td>\n",
       "      <td>HOPKINTON</td>\n",
       "      <td>RI</td>\n",
       "      <td>2833</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199971</th>\n",
       "      <td>Jaakko</td>\n",
       "      <td>Gewali</td>\n",
       "      <td>M</td>\n",
       "      <td>605</td>\n",
       "      <td>689-5704</td>\n",
       "      <td>VOLGA</td>\n",
       "      <td>SD</td>\n",
       "      <td>57071</td>\n",
       "      <td>S</td>\n",
       "      <td>Y</td>\n",
       "      <td>100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199974</th>\n",
       "      <td>Rani</td>\n",
       "      <td>Gahleitner</td>\n",
       "      <td>F</td>\n",
       "      <td>720</td>\n",
       "      <td>796-7758</td>\n",
       "      <td>KITTREDGE</td>\n",
       "      <td>CO</td>\n",
       "      <td>80457</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>55000</td>\n",
       "      <td>4.63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199994</th>\n",
       "      <td>Jacob</td>\n",
       "      <td>Kogure</td>\n",
       "      <td>F</td>\n",
       "      <td>754</td>\n",
       "      <td>549-6684</td>\n",
       "      <td>TITUSVILLE</td>\n",
       "      <td>FL</td>\n",
       "      <td>32783</td>\n",
       "      <td>S</td>\n",
       "      <td>Y</td>\n",
       "      <td>80000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>Neri</td>\n",
       "      <td>Walpole</td>\n",
       "      <td>F</td>\n",
       "      <td>865</td>\n",
       "      <td>592-8528</td>\n",
       "      <td>NASHVILLE</td>\n",
       "      <td>TN</td>\n",
       "      <td>37248</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>15000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26769 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         f_name      l_name gender area_code     phone        city state  \\\n",
       "1           Nik       Tacic      M       702  517-7658   LAS VEGAS    NV   \n",
       "21       Sanpei    Sobieski      F       360  921-9097  MILL CREEK    WA   \n",
       "40       Ilagit     Auinger      F       907  185-6056   ANCHORAGE    AK   \n",
       "42      Avelino     English      M       401  795-3153    WESTERLY    RI   \n",
       "45        Yuyan       Simao      M       603  350-5756  EAST DERRY    NH   \n",
       "...         ...         ...    ...       ...       ...         ...   ...   \n",
       "199967    Tariq  Mercankosk      F       401  735-5238   HOPKINTON    RI   \n",
       "199971   Jaakko      Gewali      M       605  689-5704       VOLGA    SD   \n",
       "199974     Rani  Gahleitner      F       720  796-7758   KITTREDGE    CO   \n",
       "199994    Jacob      Kogure      F       754  549-6684  TITUSVILLE    FL   \n",
       "199998     Neri     Walpole      F       865  592-8528   NASHVILLE    TN   \n",
       "\n",
       "          zip marital_status has_child  salary  rate single_exemp  \\\n",
       "1       89140              S         N   90000   0.0            0   \n",
       "21      98082              S         Y   20000   0.0            0   \n",
       "40      99530              S         N   70000   0.0            0   \n",
       "42       2891              S         Y   70000   0.0            0   \n",
       "45       3041              S         Y   50000   0.0            0   \n",
       "...       ...            ...       ...     ...   ...          ...   \n",
       "199967   2833              S         N    5000   0.0            0   \n",
       "199971  57071              S         Y  100000   0.0            0   \n",
       "199974  80457              S         N   55000  4.63            0   \n",
       "199994  32783              S         Y   80000   0.0            0   \n",
       "199998  37248              S         N   15000   0.0            0   \n",
       "\n",
       "       married_exemp child_exemp  \n",
       "1                  0           0  \n",
       "21                 0           0  \n",
       "40                 0           0  \n",
       "42                 0           0  \n",
       "45                 0           0  \n",
       "...              ...         ...  \n",
       "199967             0           0  \n",
       "199971             0           0  \n",
       "199974             0           0  \n",
       "199994             0           0  \n",
       "199998             0           0  \n",
       "\n",
       "[26769 rows x 15 columns]"
      ]
     },
     "execution_count": 1513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_correct_5_2[tax_correct_5_2['marital_status']!=tax_clean['marital_status']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M    13692\n",
       "Name: marital_status, dtype: int64"
      ]
     },
     "execution_count": 1512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_correct_5_1[(tax_correct_5_1['married_exemp']=='6600') & (tax_correct_5_1['single_exemp']=='0')]['marital_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        127215\n",
       "6600      13692\n",
       "2000       5682\n",
       "4000       3831\n",
       "4200       3819\n",
       "40         3818\n",
       "3800       1984\n",
       "2600       1961\n",
       "24500      1957\n",
       "12000      1945\n",
       "1400       1944\n",
       "5700       1940\n",
       "174        1938\n",
       "7150       1931\n",
       "3000       1927\n",
       "4500       1919\n",
       "5400       1911\n",
       "2740       1909\n",
       "4950       1900\n",
       "4800       1891\n",
       "80         1884\n",
       "2080       1884\n",
       "206        1867\n",
       "318        1864\n",
       "9000       1863\n",
       "220        1858\n",
       "6200       1833\n",
       "1800       1833\n",
       "Name: married_exemp, dtype: int64"
      ]
     },
     "execution_count": 1509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_correct_5_1['married_exemp'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
